-- MySQL dump 10.13  Distrib 5.1.73, for redhat-linux-gnu (x86_64)
--
-- Host: 127.0.0.1    Database: projectdb
-- ------------------------------------------------------
-- Server version	5.1.73

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `projectdb`
--

DROP TABLE IF EXISTS `projectdb`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `projectdb` (
  `name` varchar(64) NOT NULL,
  `group` varchar(64) DEFAULT NULL,
  `status` varchar(16) DEFAULT NULL,
  `script` text,
  `comments` varchar(1024) DEFAULT NULL,
  `rate` float(11,4) DEFAULT NULL,
  `burst` float(11,4) DEFAULT NULL,
  `updatetime` double(16,4) DEFAULT NULL,
  PRIMARY KEY (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `projectdb`
--

LOCK TABLES `projectdb` WRITE;
/*!40000 ALTER TABLE `projectdb` DISABLE KEYS */;
INSERT INTO `projectdb` VALUES ('233jzs_inc','jzs','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 16:41:08\n# Project: 233kuaiji_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport re\n\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n       \'http://www.233.com/jzs2/sggl/fudao/\':[\'建筑工程\',\'辅导资料\'],\n       \'http://www.233.com/jzs2/sggl/moniti/\':[\'建筑工程\',\'模拟试题\'],\n       \'http://www.233.com/jzs2/sggl/zhenti/\':[\'建筑工程\',\'历年真题\'],\n       \'http://www.233.com/jzs2/law/fudao/\':[\'工程法规\',\'辅导资料\'],\n       \'http://www.233.com/jzs2/law/moniti/\':[\'工程法规\',\'模拟试题\'],\n       \'http://www.233.com/jzs2/law/zhenti/\':[\'工程法规\',\'历年真题\'],\n       \'http://www.233.com/jzs2/jzgc/\':[\'管理实务\',\'建筑工程\'],\n       \'http://www.233.com/jzs2/szfgc/\':[\'管理实务\',\'市政工程\'],\n       \'http://www.233.com/jzs2/jdgc/\':[\'管理实务\',\'机电工程\'],\n       \'http://www.233.com/jzs2/glgc/\':[\'管理实务\',\'公路工程\'],\n       \'http://www.233.com/jzs2/kygc/\':[\'管理实务\',\'矿业工程\'],\n       \'http://www.233.com/jzs2/slgc/\':[\'管理实务\',\'水电工程\']\n        \n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'ul.fl li\').items():\n            url = each.find(\'a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'dl > dd\').items():\n            url = each.find(\'h4 > a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h4 > a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'.column-info span.fl\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        #for each in response.doc(\'div.pagebox > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\')\n            #self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        content_list = []\n        for each in response.doc(\'div.news-body > p\').items():\n            info = each.html()\n            if info and (u\'编辑推荐\' in info or u\'手机用户\' in info or u\'小编\' in info):\n                break\n            if info and (u\'二维码\' in info):\n                break\n            if info and (u\'233网校\' in info):\n                continue         \n            if info:\n                content_list.append(removeLink(info.replace(\'233网校\',\'\')))\n        if not content_list:\n            return\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'233.com\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = \'二级建造师\'\n        return res_dict\n',NULL,1.0000,3.0000,1472615630.9237),('233kuaiji_inc','kuaiji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 16:41:08\n# Project: 233kuaiji_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport re\n\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n        \'http://www.233.com/cy/zhidao/jichu/fudao/\':[\'会计基础\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/jichu/\':[\'会计基础\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/jichu/\':[\'会计基础\',\'历年真题\'],\n        \'http://www.233.com/cy/zhidao/daode/fudao/\':[\'财经法规\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/law/\':[\'财经法规\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/law/\':[\'财经法规\',\'历年真题\'],\n        \'http://www.233.com/cy/zhidao/diansuanhua/fudao/\':[\'会计电算化\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/diansuanhua/\':[\'会计电算化\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/diansuanhua/\':[\'会计电算化\',\'历年真题\'],\n\n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'ul.column-list-bd li\').items():\n            url = each.find(\'a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'dl > dd\').items():\n            url = each.find(\'h4 > a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h4 > a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'.column-info span.fl\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'div.pagebox > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        content_list = []\n        for each in response.doc(\'div.news-body > p\').items():\n            info = each.html()\n            if u\'编辑推荐\' in info or u\'手机用户\' in info or u\'小编\' in info:\n                break\n            if info:\n                content_list.append(removeLink(info.replace(\'233网校\',\'\')))\n        if not content_list:\n            return\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'233.com\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = \'会计\'\n        return res_dict\n',NULL,1.0000,3.0000,1473302218.5374),('5118',NULL,'TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-14 11:25:08\n# Project: 5118\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.5118.com/seo/words/%E5%88%9D%E4%B8%AD?isPager=true&pageIndex=1&_=1473823913311\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'dl\').items():\n            url = each.find(\'.col2-7.word > span\').text()\n            print each.find(\'dd.center\').text()\n            self.crawl(url, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1473824921.8469),('51job_shixisheng','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-14 14:43:24\n# Project: qiuzhi_51job\n\nfrom pyspider.libs.base_handler import *\nimport traceback\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for i in range(1, 28):\n            self.crawl(\'http://search.51job.com/list/\' + \'%.2d\'%i + \'0000,000000,0000,00,4,99,%2B,2,1.html?lang=c&stype=1&postchannel=0100&workyear=99&cotype=99&degreefrom=99&jobterm=03&companysize=99&lonlat=0%2C0&radius=-1&ord_field=0&confirmdate=9&fromType=&dibiaoid=0&address=&line=&specialarea=00&from=\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.dw_table > .el\').items():\n            title = \'【%s】招聘%s实习生\'%(each.find(\'.t2\').text(), each.find(\'.tg a\').text())\n            #print title\n            date = \'2016-\' + each.find(\'.t5\').text()\n            self.crawl(each.find(\'.tg a\').attr.href,save={\'title\': title, \'date\': date}, callback=self.detail_page)\n        #for each in response.doc(\'.dw_page a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        try:\n            job_msg = response.doc(\'.job_msg\').remove(\'.i_share\').remove(\'.i_note\').html()\n            tmsg = response.doc(\'.tmsg\').html()\n            content = \'%s<p>%s</p>\'%(job_msg, tmsg)\n            content = content.replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n        except Exception, e:\n            traceback.print_exc()\n            return None\n        #print job_msg\n        #print tmsg\n        res[\'content\'] = content\n        res[\"data_weight\"] = 0\n        res[\"subject\"] = u\'求职\'\n        res[\"bread\"] = [u\'实习生\',]\n        res[\"class\"] = 46\n        res[\"source\"] = u\'51job\'\n        return res',NULL,1.0000,3.0000,1471333013.9522),('51sxue_xiaoxue_youshengxiao','xiaoxue','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-15 10:58:27\n# Project: 51sxue_xinwen\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://news.51sxue.com/newsList/classId_65.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.newslistcon li a\').items():\n            \n            self.crawl(each.attr.href, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.page_link\').items():\n            \n            self.crawl(each.attr.href, callback=self.index_page)\n        \'\'\'\n    @config(priority=2)\n    @config(age=1 * 1)\n    def detail_page(self, response):\n        content_list = [v.text() for v in response.doc(\'.news_main > p\').items()]\n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.news > h3\').text(),\n            \"date\": response.doc(\'.news_list > li\').eq(1).text()[:10],\n            \"bread\": [u\'小升初\',],\n            \"content\": \'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n            \"subject\": u\'小学\',\n            \"class\": 33,\n            \"source\": \"51sxue\",\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,1.0000,1472440787.3509),('act_bailiedu_inc','act','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 18:14:44\n# Project: act_bailiedu_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=3 * 60)\n    def on_start(self):\n        self.crawl(\'http://bailiedu.com/ACT/\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.fr > a\').items():\n            if \'zt\' in each.attr.href or \'list\' in each.attr.href:\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.article_list > .list\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h3 > a\').text().strip()\n            _dict[\'date\'] = each.find(\'dt\').text().strip() or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'h3 > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.a1\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.show_content > p\').remove(\'span\').items():\n            if u\'相关推荐\' in each.text() or u\'文章导航：\' in each.text() or u\'阅读排行\' in each.text() or u\'返回专题\' in each.text() or u\'更多推荐\' in each.text():\n                break\n            if u\'百利天下\' in each.text() or u\'小马在线专家\' in each.text():\n                continue\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"bailiedu\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":\"ACT\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474539338.3042),('act_xiaoma_inc','act','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 18:13:26\n# Project: act_xiaoma_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=3 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.xiaoma.com/act/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.fr > a\').items():\n            if \'zt\' in each.attr.href or \'list\' in each.attr.href:\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.list_ul > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.paging a\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.containerCont > p\').remove(\'span\').items():\n            if u\'相关推荐\' in each.text() or u\'文章导航：\' in each.text() or u\'阅读排行\' in each.text() or u\'返回专题\' in each.text() or u\'更多推荐\' in each.text():\n                break\n            if u\'过河官网整理的\' in each.text() or u\'小马在线专家\' in each.text():\n                continue\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"xiaoma\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":\"ACT\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474539262.5430),('aoshu_juren','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:10:01\n# Project: aoshu_juren\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://aoshu.juren.com/tiku/xiaoxueaoshu/\',callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'span > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n            \n                \n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pNext\').items():\n            self.crawl(each.attr.href,save = response.save,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        #print response.doc(\'select[name=\"page\"] > option\').html()\n        for each in response.doc(\'.listing1 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.left > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.right\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.left > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'select[name=\"page\"] > option\').items():\n            self.crawl(each.attr[\'value\'],callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.mainContent > *\').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'编辑推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"juren\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"奥数\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1475138123.5511),('aoshu_juren_inc','aoshu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:10:01\n# Project: aoshu_juren\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://aoshu.juren.com/tiku/xiaoxueaoshu/\',callback=self.index_page)\n        \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'span > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n            \n                \n\n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pNext\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list_page)\n            \n    @config(age=1)\n    def list1_page(self, response):\n        #print response.doc(\'select[name=\"page\"] > option\').html()\n        for each in response.doc(\'.listing1 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.left > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.right\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.left > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'select[name=\"page\"] > option\').items():\n         #   self.crawl(each.attr[\'value\'],callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.mainContent > *\').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'编辑推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"juren\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"奥数\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1475138128.6617),('baidu_index_tiku',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-22 19:59:25\n# Project: baidu_index_tiku\n\nfrom pyspider.libs.base_handler import *\nimport MySQLdb\nimport datetime\n\nconn = MySQLdb.connect(host=\"127.0.0.1\",user=\"root\",passwd=\"123\",port=3306)\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n\n    crawl_config = {\n        \'headers\': {\n            \'Cookie\': \'PSTM=1464750941; BIDUPSID=E86D6D1C5EB675F2CB9CF17431C6D0F8; BDUSS=1h6VmViVUNZcVdVMUk1fnVCV1IzR1ktVWFCQ0ZFUi1LcENJZWRVeXlHWm52MzFYQVFBQUFBJCQAAAAAAAAAAAEAAACa-JFPsNm80rulwaq4-sut0acAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGcyVldnMlZXa; BDSFRCVID=Er4sJeCCxG3CHUcRDzRi_30n8YUjwFHt67Bc3J; H_BDCLCKID_SF=JbIDVCIytDvWe6rxMtTJ-tCE-fTMetJyaR3Wh45bWJ5TMCoLKbOhy5twQh6NqPviWmnNXnAbafP-ShPC-tPV0PDj3n6r2nbCtaRtKUjy3l02VbLRhhQ2Wf3DMMRfatRMW23r0h7mWU5GVKFCe5-Kjj5QepJf-K6hKCoM0n-8Kb7V-P5mhqn5hnLX-U__5to05TTdQbn7KtoHDR-I5JOVBPu8bMrt26ra-D6X0U7tKRTffjQG5-cr-P4H5MoX-Tra2C6yLnIQ34_MqpcNLUbWQTtdybo2Qf3ELD5doKJ8fUTlj43Dyb8-jUDSDPCE5bj2qRFHoC0M3J; lsv=globalTjs_e63380f-wwwTcss_941ce39-routejs_6ede3cf-activityControllerjs_b6f8c66-wwwBcss_cd6b841-framejs_3109ba6-globalBjs_1d5cdae-sugjs_93b1335-wwwjs_609a8a4; H_WISE_SIDS=104381_103996_107047_100615_100040_106465_102431_107196_100098_107290_107285_106666_104340_107065_107185_103759_103999_106926_106890_104671_107325_107116_107042_104613_104638_107044_107060_107092_106795_100458; MSA_WH=320_568; MSA_PBT=92; MSA_ZOOM=1000; BAIDUID=12455DCFF0A2EEB24A1668FB3AC77E9E:FG=1; BD_HOME=1; BDRCVFR[feWj1Vr5u3D]=mk3SLVN4HKm; BD_CK_SAM=1; H_PS_PSSID=20145_18286_1453_20318_18280_20368_20388_19690_20417_19861_15142_11478; BD_UPN=123253; sug=3; sugstore=1; ORIGIN=2; bdime=0; H_PS_645EC=e22ao6ItMqNOF%2BuE7PqRhAQIo36xqOHl9e4ph0bKrJAbiYbAH8SYeru51SMIZV6tCJ%2Bk\',\n        \'Host\':\'www.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\',\n        }\n    }\n\n    @every(minutes=1 * 3)\n    def on_start(self):\n        sql = \"select url_id from zhanqundb.baidu_recruit_info where spider=0 limit 10000\"\n        try:\n            cursor.execute(sql)\n            for (url_id, ) in cursor.fetchall():\n                url = \'http://www.genshuixue.com/tiku/%s.html\'%(url_id)\n                baidu_url = \'https://www.baidu.com/s?wd=\' + url + \'&rsv_spt=1&rsv_iqid=0xdd34d799000450a7&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=7&rsv_sug1=4&rsv_sug7=100&rsv_sug2=0&inputT=1411&rsv_sug4=1412\'\n                self.crawl(baidu_url, save={\'url\': url, \'id\': url_id}, callback=self.detail_page)\n        except Exception, e:\n            print e\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        num = 0\n        for each in response.doc(\'#content_left\').items():\n            num += 1\n        res = response.save\n        \n        if num > 0:\n            sql = \"update zhanqundb.baidu_recruit_info set spider=1, recruit=1, recruit_date=\'%s\' where url_id=\'%s\'\"%(	datetime.datetime.now().strftime(\"%Y-%m-%d\"), res[\'id\'])\n        else:\n            sql = \"update zhanqundb.baidu_recruit_info set spider=1 where url_id=\'%s\'\"%res[\'id\']\n        print sql\n        \n        cursor.execute(sql)\n        conn.commit()\n        res[\'num\'] = num\n        return res\n',NULL,10.0000,10.0000,1468039519.2148),('buxiban_58',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 15:04:24\n# Project: buxiban_58\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\n\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\'\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1\', callback=self.index_page)\n        \n    @config(age=24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#ObjectType > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            for temp in nav:\n                if each.text() in temp:\n                    _dict[\'bread\'] = [temp[0], each.text()]\n                    \n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=24 * 60 * 60)\n    def list_page(self, response):\n        if response.doc(\'#xiaolei > a\'):\n            for each in response.doc(\'#xiaolei > a\').items():\n                _dict = {}\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n                else:\n                    _dict[\'title\'] = u\'附近哪里学\' + each.text() + u\'比较好\'\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'bread\'] = response.save.get(\'bread\')\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n             self.crawl(response.url, save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=10*24*60*60)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10*24*60*60)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\n        #翻页\n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10*24*60*60)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        for each in response.doc(\'.tdiv\').items():\n            _dict[\'content\'] += each.find(\'a\').eq(0).text() + \'<br/>\' + each.find(\'div\').text()+ \'<br/>\' + each.find(\'p\').text() + \'<br/>\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472120342.9155),('buxiban_58_inc',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 15:25:42\n# Project: buxiban_58_inc\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\n\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\'\n    }\n    \n    @every(minutes=3 * 60)\n    def on_start(self):\n        self.crawl(\'http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1\', callback=self.index_page)\n        \n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'#ObjectType > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            for temp in nav:\n                if each.text() in temp:\n                    _dict[\'bread\'] = [temp[0], each.text()]\n                    \n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=1*1)\n    def list_page(self, response):\n        if response.doc(\'#xiaolei > a\'):\n            for each in response.doc(\'#xiaolei > a\').items():\n                _dict = {}\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n                else:\n                    _dict[\'title\'] = u\'附近哪里学\' + each.text() + u\'比较好\'\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'bread\'] = response.save.get(\'bread\')\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n             self.crawl(response.url, save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=1*1)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=1*1)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\n        #翻页\n        #for each in response.doc(\'.next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=1*1)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        for each in response.doc(\'tr > .t\').items():\n            _dict[\'content\'] += \'<p>\'+each.find(\'a\').eq(0).text() + \'</p><p>\' + each.find(\'div\').text()+ \'</p><p>\' + each.find(\'p\').text() + \'</p>\'\n            _dict[\'content\'] += \'<br/>\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472120339.5478),('buxiban_58_m','bxb','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-25 16:44:08\n# Project: buxiban_58_m\n\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\nfrom pyquery import PyQuery as pq\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\ncity_list = [\n\'bj\',\n\'sh\',\n\'gz\',\n\'sz\',\n\'cd\',\n\'hz\',\n\'nj\',\n\'tj\',\n\'wh\',\n\'cq\',\n\'qd\',\n\'jn\',\n\'yt\',\n\'wf\',\n\'linyi\',\n\'zb\',\n\'jining\',\n\'ta\',\n\'lc\',\n\'weihai\',\n\'zaozhuang\',\n\'dz\',\n\'rizhao\',\n\'dy\',\n\'heze\',\n\'bz\',\n\'lw\',\n\'zhangqiu\',\n\'kl\',\n\'zc\',\n\'shouguang\',\n\'su\',\n\'nj\',\n\'wx\',\n\'cz\',\n\'xz\',\n\'nt\',\n\'yz\',\n\'yancheng\',\n\'ha\',\n\'lyg\',\n\'taizhou\',\n\'suqian\',\n\'zj\',\n\'shuyang\',\n\'dafeng\',\n\'rugao\',\n\'qidong\',\n\'liyang\',\n\'haimen\',\n\'donghai\',\n\'yangzhong\',\n\'xinghuashi\',\n\'xinyishi\',\n\'taixing\',\n\'rudong\',\n\'pizhou\',\n\'xzpeixian\',\n\'jingjiang\',\n\'jianhu\',\n\'haian\',\n\'dongtai\',\n\'danyang\',\n\'hz\',\n\'nb\',\n\'wz\',\n\'jh\',\n\'jx\',\n\'tz\',\n\'sx\',\n\'huzhou\',\n\'lishui\',\n\'quzhou\',\n\'zhoushan\',\n\'yueqingcity\',\n\'ruiancity\',\n\'yiwu\',\n\'yuyao\',\n\'zhuji\',\n\'xiangshanxian\',\n\'wenling\',\n\'tongxiang\',\n\'cixi\',\n\'changxing\',\n\'jiashanx\',\n\'haining\',\n\'deqing\',\n\'hf\',\n\'wuhu\',\n\'bengbu\',\n\'fy\',\n\'hn\',\n\'anqing\',\n\'suzhou\',\n\'la\',\n\'huaibei\',\n\'chuzhou\',\n\'mas\',\n\'tongling\',\n\'xuancheng\',\n\'bozhou\',\n\'huangshan\',\n\'chizhou\',\n\'ch\',\n\'hexian\',\n\'hq\',\n\'tongcheng\',\n\'ningguo\',\n\'tianchang\',\n\'sz\',\n\'gz\',\n\'dg\',\n\'fs\',\n\'zs\',\n\'zh\',\n\'huizhou\',\n\'jm\',\n\'st\',\n\'zhanjiang\',\n\'zq\',\n\'mm\',\n\'jy\',\n\'mz\',\n\'qingyuan\',\n\'yj\',\n\'sg\',\n\'heyuan\',\n\'yf\',\n\'sw\',\n\'chaozhou\',\n\'taishan\',\n\'yangchun\',\n\'sd\',\n\'huidong\',\n\'boluo\',\n\'fz\',\n\'xm\',\n\'qz\',\n\'pt\',\n\'zhangzhou\',\n\'nd\',\n\'sm\',\n\'np\',\n\'ly\',\n\'wuyishan\',\n\'shishi\',\n\'jinjiangshi\',\n\'nananshi\',\n\'nn\',\n\'liuzhou\',\n\'gl\',\n\'yulin\',\n\'wuzhou\',\n\'bh\',\n\'gg\',\n\'qinzhou\',\n\'baise\',\n\'hc\',\n\'lb\',\n\'hezhou\',\n\'fcg\',\n\'chongzuo\',\n\'haikou\',\n\'sanya\',\n\'wzs\',\n\'sansha\',\n\'qh\',\n\'zz\',\n\'luoyang\',\n\'xx\',\n\'ny\',\n\'xc\',\n\'pds\',\n\'ay\',\n\'jiaozuo\',\n\'sq\',\n\'kaifeng\',\n\'puyang\',\n\'zk\',\n\'xy\',\n\'zmd\',\n\'luohe\',\n\'smx\',\n\'hb\',\n\'jiyuan\',\n\'mg\',\n\'yanling\',\n\'yuzhou\',\n\'changge\',\n\'wh\',\n\'yc\',\n\'xf\',\n\'jingzhou\',\n\'shiyan\',\n\'hshi\',\n\'xiaogan\',\n\'hg\',\n\'es\',\n\'jingmen\',\n\'xianning\',\n\'ez\',\n\'suizhou\',\n\'qianjiang\',\n\'tm\',\n\'xiantao\',\n\'snj\',\n\'yidou\',\n\'cs\',\n\'zhuzhou\',\n\'yiyang\',\n\'changde\',\n\'hy\',\n\'xiangtan\',\n\'yy\',\n\'chenzhou\',\n\'shaoyang\',\n\'hh\',\n\'yongzhou\',\n\'ld\',\n\'xiangxi\',\n\'zjj\',\n\'nc\',\n\'ganzhou\',\n\'jj\',\n\'yichun\',\n\'ja\',\n\'sr\',\n\'px\',\n\'fuzhou\',\n\'jdz\',\n\'xinyu\',\n\'yingtan\',\n\'yxx\',\n\'sy\',\n\'dl\',\n\'as\',\n\'jinzhou\',\n\'fushun\',\n\'yk\',\n\'pj\',\n\'cy\',\n\'dandong\',\n\'liaoyang\',\n\'benxi\',\n\'hld\',\n\'tl\',\n\'fx\',\n\'pld\',\n\'wfd\',\n\'hrb\',\n\'dq\',\n\'qqhr\',\n\'mdj\',\n\'suihua\',\n\'jms\',\n\'jixi\',\n\'sys\',\n\'hegang\',\n\'heihe\',\n\'yich\',\n\'qth\',\n\'dxal\',\n\'cc\',\n\'jl\',\n\'sp\',\n\'yanbian\',\n\'songyuan\',\n\'bc\',\n\'th\',\n\'baishan\',\n\'liaoyuan\',\n\'cd\',\n\'mianyang\',\n\'deyang\',\n\'nanchong\',\n\'yb\',\n\'zg\',\n\'ls\',\n\'luzhou\',\n\'dazhou\',\n\'scnj\',\n\'suining\',\n\'panzhihua\',\n\'ms\',\n\'ga\',\n\'zy\',\n\'liangshan\',\n\'guangyuan\',\n\'ya\',\n\'bazhong\',\n\'ab\',\n\'ganzi\',\n\'km\',\n\'qj\',\n\'dali\',\n\'honghe\',\n\'yx\',\n\'lj\',\n\'ws\',\n\'cx\',\n\'bn\',\n\'zt\',\n\'dh\',\n\'pe\',\n\'bs\',\n\'lincang\',\n\'diqing\',\n\'nujiang\',\n\'gy\',\n\'zunyi\',\n\'qdn\',\n\'qn\',\n\'lps\',\n\'bijie\',\n\'tr\',\n\'anshun\',\n\'qxn\',\n\'lasa\',\n\'rkz\',\n\'sn\',\n\'linzhi\',\n\'changdu\',\n\'nq\',\n\'al\',\n\'sjz\',\n\'bd\',\n\'ts\',\n\'lf\',\n\'hd\',\n\'qhd\',\n\'cangzhou\',\n\'xt\',\n\'hs\',\n\'zjk\',\n\'chengde\',\n\'dingzhou\',\n\'gt\',\n\'zhangbei\',\n\'zx\',\n\'zd\',\n\'ty\',\n\'linfen\',\n\'dt\',\n\'yuncheng\',\n\'jz\',\n\'changzhi\',\n\'jincheng\',\n\'yq\',\n\'lvliang\',\n\'xinzhou\',\n\'shuozhou\',\n\'linyixian\',\n\'qingxu\',\n\'hu\',\n\'bt\',\n\'chifeng\',\n\'erds\',\n\'tongliao\',\n\'hlbe\',\n\'bycem\',\n\'wlcb\',\n\'xl\',\n\'xam\',\n\'wuhai\',\n\'alsm\',\n\'hlr\',\n\'xa\',\n\'xianyang\',\n\'baoji\',\n\'wn\',\n\'hanzhong\',\n\'yl\',\n\'yanan\',\n\'ankang\',\n\'sl\',\n\'tc\',\n\'xj\',\n\'changji\',\n\'bygl\',\n\'yili\',\n\'aks\',\n\'ks\',\n\'hami\',\n\'klmy\',\n\'betl\',\n\'tlf\',\n\'ht\',\n\'shz\',\n\'kzls\',\n\'ale\',\n\'wjq\',\n\'tmsk\',\n\'lz\',\n\'tianshui\',\n\'by\',\n\'qingyang\',\n\'pl\',\n\'jq\',\n\'zhangye\',\n\'wuwei\',\n\'dx\',\n\'jinchang\',\n\'ln\',\n\'linxia\',\n\'jyg\',\n\'gn\',\n\'yinchuan\',\n\'wuzhong\',\n\'szs\',\n\'zw\',\n\'guyuan\',\n\'xn\',\n\'hx\',\n\'haibei\',\n\'guoluo\',\n\'haidong\',\n\'huangnan\',\n\'ys\',\n\'hainan\',\n\'hk\',\n\'am\',\n\'tw\',\n\'diaoyudao\',\n\'cn\',\n]\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\'\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for city in city_list:\n            self.crawl(\'http://\'+city+\'.58.com/techang/\',save={\'city\':city}, callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#ObjectType > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            _dict[\'city\'] = response.save[\'city\']\n            for temp in nav:\n                if each.text() in temp:\n                    _dict[\'bread\'] = [temp[0], each.text()]\n                    \n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        if response.doc(\'#xiaolei > a\'):\n            for each in response.doc(\'#xiaolei > a\').items():\n                _dict = {}\n                _dict[\'city\'] = response.save[\'city\']\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n                else:\n                    _dict[\'title\'] = u\'附近哪里学\' + each.text() + u\'比较好\'\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'city\'] = response.save[\'city\']\n             _dict[\'bread\'] = response.save.get(\'bread\')\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n             self.crawl(response.url, save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=10 * 24 * 60 * 60)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            _dict[\'city\'] = response.save[\'city\']\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\n        #翻页\n        #for each in response.doc(\'.next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        _dict[\'content\'] = \'\'\n        i = 0\n        for each in response.doc(\'tr > .t\').items():\n            if \'adinfo\' in each.parent().attr[\'infotag\']:\n                continue\n            #print each.find(\'a\').outerHtml()\n            if \'http://jump\' in each.find(\'a\').attr.href:\n                url = \'http://m.58.com/\'+_dict[\'city\']+\'/techang/\'+each.find(\'a\').attr[\'name\']+\'x.shtml\'\n            else:\n                url = \'http://m.58.com/\'+_dict[\'city\']+\'/techang/\'+each.find(\'a\').attr[\'href\'].split(\'/\')[-1]\n            #print url\n            _dict[\'content\'] += \'<p>\'+pq(url).find(\'.tit_area h1\').eq(0).text()+\'</p>\'+\'<p>\'+pq(url).find(\'.article li\').eq(-2).text()+\'</p>\' +\'<p>\'+pq(url).find(\'.article li\').eq(-1).text()+\'</p>\'+\'<p>\'+pq(url).find(\'.firm_area h2\').eq(0).text()+\'</p>\'+\'<p>\'+pq(pq(url).find(\'.firm_area a\').eq(0).attr[\'href\']).find(\'.infoitembox li\').eq(-1).text()+\'</p>\'\n            _dict[\'content\'] += \'<br/>\'\n            i +=1\n            if i>= 10:\n                break\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'互联网\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1473741194.2858),('buxiban_title',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-25 16:44:08\n# Project: buxiban_58_m\n\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\nfrom pyquery import PyQuery as pq\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\ncity_list = [\n\'bj\',\n\'sh\',\n\'gz\',\n\'sz\',\n\'cd\',\n\'hz\',\n\'nj\',\n\'tj\',\n\'wh\',\n\'cq\',\n\'qd\',\n\'jn\',\n\'yt\',\n\'wf\',\n\'linyi\',\n\'zb\',\n\'jining\',\n\'ta\',\n\'lc\',\n\'weihai\',\n\'zaozhuang\',\n\'dz\',\n\'rizhao\',\n\'dy\',\n\'heze\',\n\'bz\',\n\'lw\',\n\'zhangqiu\',\n\'kl\',\n\'zc\',\n\'shouguang\',\n\'su\',\n\'nj\',\n\'wx\',\n\'cz\',\n\'xz\',\n\'nt\',\n\'yz\',\n\'yancheng\',\n\'ha\',\n\'lyg\',\n\'taizhou\',\n\'suqian\',\n\'zj\',\n\'shuyang\',\n\'dafeng\',\n\'rugao\',\n\'qidong\',\n\'liyang\',\n\'haimen\',\n\'donghai\',\n\'yangzhong\',\n\'xinghuashi\',\n\'xinyishi\',\n\'taixing\',\n\'rudong\',\n\'pizhou\',\n\'xzpeixian\',\n\'jingjiang\',\n\'jianhu\',\n\'haian\',\n\'dongtai\',\n\'danyang\',\n\'hz\',\n\'nb\',\n\'wz\',\n\'jh\',\n\'jx\',\n\'tz\',\n\'sx\',\n\'huzhou\',\n\'lishui\',\n\'quzhou\',\n\'zhoushan\',\n\'yueqingcity\',\n\'ruiancity\',\n\'yiwu\',\n\'yuyao\',\n\'zhuji\',\n\'xiangshanxian\',\n\'wenling\',\n\'tongxiang\',\n\'cixi\',\n\'changxing\',\n\'jiashanx\',\n\'haining\',\n\'deqing\',\n\'hf\',\n\'wuhu\',\n\'bengbu\',\n\'fy\',\n\'hn\',\n\'anqing\',\n\'suzhou\',\n\'la\',\n\'huaibei\',\n\'chuzhou\',\n\'mas\',\n\'tongling\',\n\'xuancheng\',\n\'bozhou\',\n\'huangshan\',\n\'chizhou\',\n\'ch\',\n\'hexian\',\n\'hq\',\n\'tongcheng\',\n\'ningguo\',\n\'tianchang\',\n\'sz\',\n\'gz\',\n\'dg\',\n\'fs\',\n\'zs\',\n\'zh\',\n\'huizhou\',\n\'jm\',\n\'st\',\n\'zhanjiang\',\n\'zq\',\n\'mm\',\n\'jy\',\n\'mz\',\n\'qingyuan\',\n\'yj\',\n\'sg\',\n\'heyuan\',\n\'yf\',\n\'sw\',\n\'chaozhou\',\n\'taishan\',\n\'yangchun\',\n\'sd\',\n\'huidong\',\n\'boluo\',\n\'fz\',\n\'xm\',\n\'qz\',\n\'pt\',\n\'zhangzhou\',\n\'nd\',\n\'sm\',\n\'np\',\n\'ly\',\n\'wuyishan\',\n\'shishi\',\n\'jinjiangshi\',\n\'nananshi\',\n\'nn\',\n\'liuzhou\',\n\'gl\',\n\'yulin\',\n\'wuzhou\',\n\'bh\',\n\'gg\',\n\'qinzhou\',\n\'baise\',\n\'hc\',\n\'lb\',\n\'hezhou\',\n\'fcg\',\n\'chongzuo\',\n\'haikou\',\n\'sanya\',\n\'wzs\',\n\'sansha\',\n\'qh\',\n\'zz\',\n\'luoyang\',\n\'xx\',\n\'ny\',\n\'xc\',\n\'pds\',\n\'ay\',\n\'jiaozuo\',\n\'sq\',\n\'kaifeng\',\n\'puyang\',\n\'zk\',\n\'xy\',\n\'zmd\',\n\'luohe\',\n\'smx\',\n\'hb\',\n\'jiyuan\',\n\'mg\',\n\'yanling\',\n\'yuzhou\',\n\'changge\',\n\'wh\',\n\'yc\',\n\'xf\',\n\'jingzhou\',\n\'shiyan\',\n\'hshi\',\n\'xiaogan\',\n\'hg\',\n\'es\',\n\'jingmen\',\n\'xianning\',\n\'ez\',\n\'suizhou\',\n\'qianjiang\',\n\'tm\',\n\'xiantao\',\n\'snj\',\n\'yidou\',\n\'cs\',\n\'zhuzhou\',\n\'yiyang\',\n\'changde\',\n\'hy\',\n\'xiangtan\',\n\'yy\',\n\'chenzhou\',\n\'shaoyang\',\n\'hh\',\n\'yongzhou\',\n\'ld\',\n\'xiangxi\',\n\'zjj\',\n\'nc\',\n\'ganzhou\',\n\'jj\',\n\'yichun\',\n\'ja\',\n\'sr\',\n\'px\',\n\'fuzhou\',\n\'jdz\',\n\'xinyu\',\n\'yingtan\',\n\'yxx\',\n\'sy\',\n\'dl\',\n\'as\',\n\'jinzhou\',\n\'fushun\',\n\'yk\',\n\'pj\',\n\'cy\',\n\'dandong\',\n\'liaoyang\',\n\'benxi\',\n\'hld\',\n\'tl\',\n\'fx\',\n\'pld\',\n\'wfd\',\n\'hrb\',\n\'dq\',\n\'qqhr\',\n\'mdj\',\n\'suihua\',\n\'jms\',\n\'jixi\',\n\'sys\',\n\'hegang\',\n\'heihe\',\n\'yich\',\n\'qth\',\n\'dxal\',\n\'cc\',\n\'jl\',\n\'sp\',\n\'yanbian\',\n\'songyuan\',\n\'bc\',\n\'th\',\n\'baishan\',\n\'liaoyuan\',\n\'cd\',\n\'mianyang\',\n\'deyang\',\n\'nanchong\',\n\'yb\',\n\'zg\',\n\'ls\',\n\'luzhou\',\n\'dazhou\',\n\'scnj\',\n\'suining\',\n\'panzhihua\',\n\'ms\',\n\'ga\',\n\'zy\',\n\'liangshan\',\n\'guangyuan\',\n\'ya\',\n\'bazhong\',\n\'ab\',\n\'ganzi\',\n\'km\',\n\'qj\',\n\'dali\',\n\'honghe\',\n\'yx\',\n\'lj\',\n\'ws\',\n\'cx\',\n\'bn\',\n\'zt\',\n\'dh\',\n\'pe\',\n\'bs\',\n\'lincang\',\n\'diqing\',\n\'nujiang\',\n\'gy\',\n\'zunyi\',\n\'qdn\',\n\'qn\',\n\'lps\',\n\'bijie\',\n\'tr\',\n\'anshun\',\n\'qxn\',\n\'lasa\',\n\'rkz\',\n\'sn\',\n\'linzhi\',\n\'changdu\',\n\'nq\',\n\'al\',\n\'sjz\',\n\'bd\',\n\'ts\',\n\'lf\',\n\'hd\',\n\'qhd\',\n\'cangzhou\',\n\'xt\',\n\'hs\',\n\'zjk\',\n\'chengde\',\n\'dingzhou\',\n\'gt\',\n\'zhangbei\',\n\'zx\',\n\'zd\',\n\'ty\',\n\'linfen\',\n\'dt\',\n\'yuncheng\',\n\'jz\',\n\'changzhi\',\n\'jincheng\',\n\'yq\',\n\'lvliang\',\n\'xinzhou\',\n\'shuozhou\',\n\'linyixian\',\n\'qingxu\',\n\'hu\',\n\'bt\',\n\'chifeng\',\n\'erds\',\n\'tongliao\',\n\'hlbe\',\n\'bycem\',\n\'wlcb\',\n\'xl\',\n\'xam\',\n\'wuhai\',\n\'alsm\',\n\'hlr\',\n\'xa\',\n\'xianyang\',\n\'baoji\',\n\'wn\',\n\'hanzhong\',\n\'yl\',\n\'yanan\',\n\'ankang\',\n\'sl\',\n\'tc\',\n\'xj\',\n\'changji\',\n\'bygl\',\n\'yili\',\n\'aks\',\n\'ks\',\n\'hami\',\n\'klmy\',\n\'betl\',\n\'tlf\',\n\'ht\',\n\'shz\',\n\'kzls\',\n\'ale\',\n\'wjq\',\n\'tmsk\',\n\'lz\',\n\'tianshui\',\n\'by\',\n\'qingyang\',\n\'pl\',\n\'jq\',\n\'zhangye\',\n\'wuwei\',\n\'dx\',\n\'jinchang\',\n\'ln\',\n\'linxia\',\n\'jyg\',\n\'gn\',\n\'yinchuan\',\n\'wuzhong\',\n\'szs\',\n\'zw\',\n\'guyuan\',\n\'xn\',\n\'hx\',\n\'haibei\',\n\'guoluo\',\n\'haidong\',\n\'huangnan\',\n\'ys\',\n\'hainan\',\n\'hk\',\n\'am\',\n\'tw\',\n\'diaoyudao\',\n\'cn\',\n]\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\'\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for city in city_list:\n            self.crawl(\'http://\'+city+\'.58.com/techang/\',save={\'city\':city}, callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#ObjectType > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            _dict[\'city\'] = response.save[\'city\']\n            for temp in nav:\n                if each.text() in temp:\n                    _dict[\'bread\'] = [temp[0], each.text()]\n                    \n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        if response.doc(\'#xiaolei > a\'):\n            for each in response.doc(\'#xiaolei > a\').items():\n                _dict = {}\n                _dict[\'city\'] = response.save[\'city\']\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n                else:\n                    _dict[\'title\'] = u\'附近哪里学\' + each.text() + u\'比较好\'\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'city\'] = response.save[\'city\']\n             _dict[\'bread\'] = response.save.get(\'bread\')\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n             self.crawl(response.url, save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=10 * 24 * 60 * 60)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            _dict[\'city\'] = response.save[\'city\']\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\n        #翻页\n        #for each in response.doc(\'.next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        #for each in response.doc(\'tr > .t\').items():\n         #   if \'adinfo\' in each.parent().attr[\'infotag\']:\n          #      continue\n           # content += each.find(\'a\').text().strip()+\'<br/>\'\n        #if len(content) < 2:\n         #   return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": \'\',\n            \"source\": u\'互联网\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,5.0000,5.0000,1473234434.6511),('cet46_inc','cet','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-21 14:58:56\n# Project: cet46com\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    page_dict = {\n        \"95\": u\'四级真题及答案\',\n        \"96\": u\'六级真题及答案\',\n        \'91\': u\'四级口语\',\n        \'97\': u\'模拟试题\',\n        \'86\': u\'六级作文\',\n        \'87\': u\'四级听力\',\n        \'88\': u\'六级听力\',\n        \'92\': u\'六级口语\',\n        \'78\': u\'名师指导\',\n        \'79\': u\'复习攻略\',\n        \'75\': u\'样题规定\',\n        \'83\': u\'四级阅读\',\n        \'84\': u\'六级阅读\',\n        \'85\': u\'四级作文\',\n        \'93\': u\'四级翻译\',\n        \'94\': u\'六级翻译\',\n    }\n    \n    @every(minutes=1 * 60)\n    def on_start(self):\n          for k, v in self.page_dict.iteritems():\n            self.crawl(\'http://www.cet-46.com/list%s.html\'%k,save={\'bread\': v}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\".left a\").items():\n             _dict = {}\n             url = each.attr.href  \n             _dict[\'bread\'] = [response.save[\'bread\'], ]\n             _dict[\'brief\'] = each.html()\n             try:\n                 date = each.parent().next(\'span\').find(\'font\').text().split(\'/\')\n                 _dict[\'date\'] = \'%s-%.2d-%2d\'%(date[0], int(date[1]), int(date[2]))\n             except:\n                 _dict[\'date\'] = \'2016-01-01\'\n             self.crawl(url, save=_dict, callback=self.detail_page) \n        \'\'\'\n        for each in response.doc(\".daohang a[href$=\'.html\']\").items(): \n             _dict = {}\n             _dict[\'bread\'] = response.save[\'bread\']\n             self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\n        \'\'\'\n        \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        res_dict[\'title\'] = response.doc(\'.xw_title > span\').html()\n        content_list = []\n        for info in response.doc(\'span > p\').items():\n            content = pyquery.PyQuery(info)\n            items = content.html()\n            content_list.append(items)\n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'source\'] = \'www.cet-46.com\'\n        res_dict[\'subject\'] = u\'四六级\'\n        res_dict[\'class\'] = 46\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,1.0000,3.0000,1471329965.7602),('changge_52jinshan_inc','changge','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:32:49\n# Project: changge_52jinshan_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=10 * 60)\n    def on_start(self):\n        self.crawl(\'http://52jinshan.cn/\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.excerpt\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h2 > a\').text().strip()\n            self.crawl(each.find(\'h2 > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.next-page > a\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n             \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'article.article-content > *\').items():\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        \n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 10:\n            return\n        if response.doc(\'.article-meta > li\').text():\n            date = response.doc(\'.article-meta > li\').eq(0).text().split()[-1].strip().replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n        else:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"52jinshan\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"唱歌\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1474940046.7639),('chengxuyuan_boke_inc','cxy','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-10 14:12:20\n# Project: chengxuyuan_boke_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nimport traceback\nfrom pyquery import PyQuery\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\nsub_items = u\'<div id=\"cate_content_block_108698\" onmouseover=\"cateShow(108698)\" onmouseout=\"cateHidden(108698)\" class=\"cate_content_block_wrapper\" style=\"top:30px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/beginner/\">.NET新手区(1)</a></li><li><a href=\"/cate/aspnet/\">ASP.NET(1)</a></li><li><a href=\"/cate/csharp/\">C#(1)</a></li><li><a href=\"/cate/dotnetcore/\">.NET Core(0)</a></li><li><a href=\"/cate/winform/\">WinForm(0)</a></li><li><a href=\"/cate/silverlight/\">Silverlight(0)</a></li><li><a href=\"/cate/wcf/\">WCF(0)</a></li><li><a href=\"/cate/clr/\">CLR(0)</a></li><li><a href=\"/cate/wpf/\">WPF(0)</a></li><li><a href=\"/cate/xna/\">XNA(0)</a></li><li><a href=\"/cate/vs2010/\">Visual Studio(0)</a></li><li><a href=\"/cate/mvc/\">ASP.NET MVC(2)</a></li><li><a href=\"/cate/control/\">控件开发(0)</a></li><li><a href=\"/cate/ef/\">Entity Framework(0)</a></li><li><a href=\"/cate/nhibernate/\">NHibernate(0)</a></li><li><a href=\"/cate/winrt_metro/\">WinRT/Metro(1)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_2\" onmouseover=\"cateShow(2)\" onmouseout=\"cateHidden(2)\" class=\"cate_content_block_wrapper\" style=\"top:58px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/java/\">Java(4)</a></li><li><a href=\"/cate/cpp/\">C++(1)</a></li><li><a href=\"/cate/php/\">PHP(1)</a></li><li><a href=\"/cate/delphi/\">Delphi(0)</a></li><li><a href=\"/cate/python/\">Python(2)</a></li><li><a href=\"/cate/ruby/\">Ruby(0)</a></li><li><a href=\"/cate/c/\">C语言(0)</a></li><li><a href=\"/cate/erlang/\">Erlang(0)</a></li><li><a href=\"/cate/go/\">Go(0)</a></li><li><a href=\"/cate/swift/\">Swift(0)</a></li><li><a href=\"/cate/scala/\">Scala(0)</a></li><li><a href=\"/cate/r/\">R语言(0)</a></li><li><a href=\"/cate/verilog/\">Verilog(0)</a></li><li><a href=\"/cate/otherlang/\">其它语言(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108701\" onmouseover=\"cateShow(108701)\" onmouseout=\"cateHidden(108701)\" class=\"cate_content_block_wrapper\" style=\"top:86px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/design/\">架构设计(0)</a></li><li><a href=\"/cate/108702/\">面向对象(0)</a></li><li><a href=\"/cate/dp/\">设计模式(0)</a></li><li><a href=\"/cate/ddd/\">领域驱动设计(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108703\" onmouseover=\"cateShow(108703)\" onmouseout=\"cateHidden(108703)\" class=\"cate_content_block_wrapper\" style=\"top:114px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/web/\">Html/Css(1)</a></li><li><a href=\"/cate/javascript/\">JavaScript(3)</a></li><li><a href=\"/cate/jquery/\">jQuery(1)</a></li><li><a href=\"/cate/html5/\">HTML5(1)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108704\" onmouseover=\"cateShow(108704)\" onmouseout=\"cateHidden(108704)\" class=\"cate_content_block_wrapper\" style=\"top:142px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/sharepoint/\">SharePoint(0)</a></li><li><a href=\"/cate/gis/\">GIS技术(0)</a></li><li><a href=\"/cate/sap/\">SAP(0)</a></li><li><a href=\"/cate/OracleERP/\">Oracle ERP(0)</a></li><li><a href=\"/cate/dynamics/\">Dynamics CRM(0)</a></li><li><a href=\"/cate/k2/\">K2 BPM(0)</a></li><li><a href=\"/cate/infosec/\">信息安全(0)</a></li><li><a href=\"/cate/3/\">企业信息化其他(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108705\" onmouseover=\"cateShow(108705)\" onmouseout=\"cateHidden(108705)\" class=\"cate_content_block_wrapper\" style=\"top:170px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/android/\">Android开发(1)</a></li><li><a href=\"/cate/ios/\">iOS开发(3)</a></li><li><a href=\"/cate/wp/\">Windows Phone(0)</a></li><li><a href=\"/cate/wm/\">Windows Mobile(0)</a></li><li><a href=\"/cate/mobile/\">其他手机开发(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108709\" onmouseover=\"cateShow(108709)\" onmouseout=\"cateHidden(108709)\" class=\"cate_content_block_wrapper\" style=\"top:198px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/agile/\">敏捷开发(0)</a></li><li><a href=\"/cate/pm/\">项目与团队管理(0)</a></li><li><a href=\"/cate/Engineering/\">软件工程其他(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108712\" onmouseover=\"cateShow(108712)\" onmouseout=\"cateHidden(108712)\" class=\"cate_content_block_wrapper\" style=\"top:226px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/sqlserver/\">SQL Server(0)</a></li><li><a href=\"/cate/oracle/\">Oracle(0)</a></li><li><a href=\"/cate/mysql/\">MySQL(1)</a></li><li><a href=\"/cate/nosql/\">NoSQL(0)</a></li><li><a href=\"/cate/bigdata/\">大数据(0)</a></li><li><a href=\"/cate/database/\">其它数据库(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108724\" onmouseover=\"cateShow(108724)\" onmouseout=\"cateHidden(108724)\" class=\"cate_content_block_wrapper\" style=\"top:254px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/win7/\">Windows(0)</a></li><li><a href=\"/cate/winserver/\">Windows Server(0)</a></li><li><a href=\"/cate/linux/\">Linux(0)</a></li><li><a href=\"/cate/osx/\">OS X(0)</a></li><li><a href=\"/cate/eos/\">嵌入式(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_4\" onmouseover=\"cateShow(4)\" onmouseout=\"cateHidden(4)\" class=\"cate_content_block_wrapper\" style=\"top:282px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/life/\">非技术区(0)</a></li><li><a href=\"/cate/testing/\">软件测试(0)</a></li><li><a href=\"/cate/software/\">代码与软件发布(0)</a></li><li><a href=\"/cate/cg/\">计算机图形学(0)</a></li><li><a href=\"/cate/google/\">Google开发(0)</a></li><li><a href=\"/cate/gamedev/\">游戏开发(1)</a></li><li><a href=\"/cate/codelife/\">程序人生(0)</a></li><li><a href=\"/cate/job/\">求职面试(0)</a></li><li><a href=\"/cate/book/\">读书区(0)</a></li><li><a href=\"/cate/quoted/\">转载区(0)</a></li><li><a href=\"/cate/wince/\">Windows CE(0)</a></li><li><a href=\"/cate/translate/\">翻译区(0)</a></li><li><a href=\"/cate/opensource/\">开源研究(0)</a></li><li><a href=\"/cate/flex/\">Flex(0)</a></li><li><a href=\"/cate/cloud/\">云计算(0)</a></li><li><a href=\"/cate/algorithm/\">算法与数据结构(0)</a></li><li><a href=\"/cate/misc/\">其他技术区(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div>\'\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.cnblogs.com/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        global sub_items\n        i = 0\n        for each in response.doc(\'#cate_item a\').items():\n            if u\'所有评论\' in each.text():\n                continue\n            sub_items = PyQuery(sub_items)\n            for each_sub in sub_items(\'.cate_content_block\').eq(i).find(\'li > a\').items():\n                _dict = {}\n                _dict[\'bread\'] = [each.text().split(\'(\')[0]]\n                _dict[\'bread\'].append(each_sub.text().split(\'(\')[0])\n                self.crawl(\'http://www.cnblogs.com\'+each_sub.attr.href, save = _dict, callback=self.list_page)\n            i += 1\n    \n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.post_item_body\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h3 > a\').text()\n            _dict[\'date\'] = [v for v in each.find(\'.post_item_foot\').remove(\'a\').text().split() if \'-\' in v][0]\n            self.crawl(each.find(\'h3 > a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页    \n        #for each in response.doc(\'#paging_block a\').items():\n         #   self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'#cnblogs_post_body\').items():\n            #print each.html()\n            if \'img\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif \'<pre>\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\'),\n            \"source\": u\"博客园\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"程序员\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1473651449.8860),('chinadance_inc','wudao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 09:55:12\n# Project: chinadance\n\nfrom pyspider.libs.base_handler import *\nimport re\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    page_dict = {\n\n        \'http://www.chinadance.cn/article/wudaozhishi/\':[u\'舞蹈知识\'],\n        \'http://www.chinadance.cn/article/wudaojiaoxue/\':[u\'舞蹈教学\'],\n        \'http://www.chinadance.cn/article/wudaoshangxi/\':[u\'舞蹈鉴赏\'],\n        \'http://www.chinadance.cn/article/wudaorensheng/\':[u\'舞蹈人生\'],\n        \'http://www.chinadance.cn/article/wudaoshi/\':[u\'舞蹈史论\'],\n        \'http://www.chinadance.cn/news/\':[u\'舞蹈资讯\'],\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'li.cl\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h3 > a\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            url = each.find(\'h3 > a\').attr.href\n            _dict[\'date\'] = each.find(\'p.info > span\').text().split(\' \')[0]\n            _dict[\'cover\'] = each.children(\'a\').find(\'img\').attr.src or \'\'\n            self.crawl(url, save = _dict,callback=self.detail_page)\n      \n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       res_dict[\'content\'] = response.doc(\'td#article_content\').remove(\'embed\').remove(\'script\').html()   \n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'舞蹈\'\n       res_dict[\'source\'] = \'chinadance\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict',NULL,1.0000,3.0000,1472611411.9489),('cxy_ithome_inc','cxy','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-12 14:39:05\n# Project: cxy_ithome\n\nfrom pyspider.libs.base_handler import *\nimport time\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.3\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.it-home.org/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'span > a\').items():\n            self.crawl(each.attr.href, callback=self.list_page)\n    \n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.xst\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text()\n            self.crawl(each.attr.href, save = _dict, callback=self.detail_page)\n        #翻页    \n        #for each in response.doc(\'.nxt\').items():\n         #   self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2,age=1 * 1)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.t_fsz .t_f\').items():\n            #print each.html()\n            if each.text().strip() != \'\':\n                if response.doc(\'.authi em span\').attr[\'title\']:\n                    date = response.doc(\'.authi em span\').attr[\'title\'].split()[0]\n                else:\n                    date = time.strftime(\'%Y-%m-%d\',time.localtime())\n                _list.append(\'<p>\'+each.remove(\'.pstatus\').html().strip()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'编程语言\'],\n            \"content\": content,\n            \"source\": \"it-home\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"程序员\",\n            \"date\": date,\n\n        }',NULL,1.0000,3.0000,1474961058.2478),('deyu_for68','delete','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 17:18:03\n# Project: deyu_for68\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://www.for68.com/deyu/cihui/\':[u\'德语基础\'],\n            \'http://www.for68.com/deyu/yufa/\':[u\'德语基础\'],\n            \'http://www.for68.com/deyu/kouyutingli/\':[u\'德语基础\'],\n            \'http://www.for68.com/deyu/yuedu/\':[u\'德语基础\'],\n            \'http://www.for68.com/deyu/fanyixiezuo/\':[u\'德语基础\'],\n            \'http://www.for68.com/deyu/shangwu/\':[u\'实用德语\'],\n            \'http://www.for68.com/deyu/zhongguo/\':[u\'实用德语\'],\n            \'http://www.for68.com/deyu/liuxue/\':[u\'德国留学\'],\n            \'http://www.for68.com/deyu/kaoshi/\':[u\'德语考试\']\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.list1_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'divnewslist > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\')\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.p1 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list1_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       _list = []\n       for each in response.doc(\'#fontzoom > *\').items():\n            if u\'文章导航：\' in each.text() or u\'阅读排行\' in each.text() or u\'返回专题\' in each.text() or u\'更多推荐\' in each.text():\n                break\n            if u\'外语教育网\' in each.text() or u\'小马在线专家\' in each.text():\n                continue\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n       content = \'\'.join(v for v in _list if v)\n       res_dict[\'content\'] = content\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'德语\'\n       res_dict[\'source\'] = u\'外语教育网\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1474596182.3394),('deyu_tingroom_inc','deyu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-23 09:59:29\n# Project: deyu_tingroom_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.tingroom.com/yuedu/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/yufa/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/cihui/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/kouyu/\':[u\'实用德语\'],\n            \'http://de.tingroom.com/gequ/\':[u\'德语文化\'],\n            \'http://de.tingroom.com/liuxue/\':[u\'德国留学\'],\n            \'http://de.tingroom.com/kaoshi/\':[u\'德语考试\']\n    }\n    @every(minutes=3 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            \n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n            \n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list1_page)\n            \n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'p > span\').eq(0).text().strip()\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pages > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\')\n            #self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list1_page)\n            \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'p > span\').eq(0).text().strip()\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pages > a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list1_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       \n       res_dict[\'content\'] = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.content > .content\').html().strip().replace(\'\\t\',\'  \').replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'德语\'\n       res_dict[\'source\'] = u\'德语学习网\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1474596214.4109),('dianzishangwu_ebrun_inc','dainzishangwu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 14:01:04\n# Project: dianzishangwu_ebrun_inc\nfrom pyspider.libs.base_handler import *\nimport re\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.ebrun.com/mec/more.php?page=1#m\', callback=self.list_page)\n        \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.dtleft > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.chanlDiv\').items():\n            #print each.html()\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.titleP a\').text().strip()\n            self.crawl(each.find(\'.titleP a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.next\').items():\n         #   self.crawl(each.attr.href,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.contdiv > p\').items():\n            if \'ybfirst_go\' in each.outerHtml():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 100:\n            return\n        date = response.doc(\'.zsp > span\').eq(-1).text().split()[0]\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"ebrun\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"电子商务\",\n            \"date\": date,\n\n        }',NULL,1.0000,3.0000,1474524153.4943),('dianzishangwu_exam8_inc','dianzishangwu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 14:03:18\n# Project: dianzishangwu_exam8_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=3 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.exam8.com/zige/dianshang/moni/\',save={\'bread\':[u\'模拟题\']}, callback=self.index_page)\n        self.crawl(\'http://www.exam8.com/zige/dianshang/dongtai/\',save={\'bread\':[u\'考试动态\']}, callback=self.list_page)\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.dtleft > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href, save = _dict,callback=self.list_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'[\',\'\').replace(\']\',\'\').strip()\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.lbpage a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save[\'bread\']\n           # self.crawl(each.attr.href, save = _dict,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.maincon > .left > div\').eq(5).find(\'p\').items():\n            if u\'相关推荐：\' in each.text() or u\'文章导航：\' in each.text() or u\'编辑推荐\' in each.text() or u\'返回专题\' in each.text() or u\'更多推荐\' in each.text() or \'showpage\' in each.outerHtml():\n                break\n            if u\'点击查看\' in each.text() or u\'查看汇总\' in each.text() or not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 100:\n            _list = []\n            for each in response.doc(\'.maincon > .left > div\').eq(6).find(\'*\').items():\n                if u\'相关推荐：\' in each.text() or u\'文章导航：\' in each.text() or u\'编辑推荐\' in each.text() or u\'返回专题\' in each.text() or u\'更多推荐\' in each.text() or \'showpage\' in each.outerHtml():\n                    break\n                if u\'点击查看\' in each.text() or u\'查看汇总\' in each.text() or not each.html():\n                    continue\n                _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n            content = \'\'.join(v for v in _list if v)\n        if len(content) < 100:\n            content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.maincon > .left > div\').eq(5).remove(\'.showpage\').html().strip()).replace(\'</a>\',\'\')+\'</p>\'\n        if len(content) < 100:\n            content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.maincon > .left > div\').eq(6).remove(\'.showpage\').html().strip()).replace(\'</a>\',\'\')+\'</p>\'\n        if len(content) < 100:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"exam8\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"电子商务\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474524313.2237),('dianzishangwu_wuwenyuan','dianzishangwu','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-20 14:46:03\n# Project: dianzishangwu_wuwenyuan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wuwenyuan.com/news\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.left_top .menu > li > a\').items():\n            if u\'资源下载\' in each.html():\n                continue\n            self.crawl(each.attr.href, callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.archive_title a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            self.crawl(each.attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'#pagenavi a\').items():\n            self.crawl(each.attr.href, callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        date = response.doc(\'.date\').text().strip().replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        content = response.doc(\'#content > .entry_box_s #entry\').remove(\'div\').html().strip().replace(\'\\n\',\'\')\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"wuwenyuan\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"电子商务\",\n            \"date\": date,\n\n        }',NULL,1.0000,3.0000,1474363489.6551),('dongman_missevan','dongman','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-18 15:50:19\n# Project: dongman_missevan\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport cPickle\nimport time\n\nmax_pageno = 1\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".newslist\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret:\n                a = P(url_node).find(\".newstitle\").find(\"a\")\n                ret.append(P(a))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        page_node = P(response.doc(\".pagelist\"))\n        try:\n            next_page = page_node.find(\".next\")\n            if next_page:\n                current_pageno = page_node.find(\".selected a\").text()\n                next_pageno = re.findall(r\"p=(\\d+)\", next_page.find(\"a\").attr.href)\n                if not next_pageno:\n                    return False\n                next_pageno = next_pageno[0]\n                    \n                if int(next_pageno) > int(current_pageno):\n                    # 限制只抓前20页\n                    if int(next_pageno) > max_pageno:\n                        return False\n                    return P(\'\'\'<a href=\"%s\">next</a>\'\'\' % next_page.find(\"a\").attr.href)\n                return False\n\n            else:\n                return False\n        except Exception as info:\n            return False\n    \n    @staticmethod\n    def parse_detail(response):\n        ret = {}\n        try:\n            content_node = response.doc(\"#articlebox\")\n            if not content_node:\n                return False\n            title = P(content_node).find(\"#articletitle\").text()\n            \n            detail = P(content_node).find(\"#articlecontent\").html().strip()\n            \n            article_info = P(content_node).find(\"#articleinfo\").text()\n            info = re.findall(u\"来源:\\s+([^\\s]+?)\\s+时间:\\s+(\\d{4}-\\d{2}-\\d{2})\", article_info)\n            source = False\n            date_string = time.strftime(\"%Y-%m-%d\", time.localtime(time.time()))\n            if info:\n                info = info[0]\n                source = info[0]\n                date_string = info[1]\n\n            bread = response.save.get(\"bread\")\n        except:\n            return False\n                \n        url = response.url\n        if not detail.strip():\n            return False\n        \n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n        return {\"title\": title,\n                \"url\": url,\n                \"content\": detail,\n                \"bread\": bread,\n                \"source\": source if source else u\"news.missevan.com\",\n                \"data_weight\": 0,\n                \"class\": 46,\n                \"subject\": u\"动漫\",\n                \"date\": date_string,\n                \"image_list\": image_list,\n                }\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\n        \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=2\",\n            \"bread\": [\"动画\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=3\",\n            \"bread\": [\"音乐\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=4\",\n            \"bread\": [\"游戏\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=5\",\n            \"bread\": [\"声优\"],\n        },\n\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=6\",\n            \"bread\": [\"小说\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=7\",\n            \"bread\": [\"漫画\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=8\",\n            \"bread\": [\"Cosplay\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=9\",\n            \"bread\": [\"杂志\"],\n        },\n\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=10\",\n            \"bread\": [\"周边\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=11\",\n            \"bread\": [\"展会\"],\n        },\n\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=12\",\n            \"bread\": [\"电影\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=13\",\n            \"bread\": [\"萌宅\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=14\",\n            \"bread\": [\"杂谈\"],\n        },\n\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=15\",\n            \"bread\": [\"DVD/BD\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=16\",\n            \"bread\": [\"其他\"],\n        },\n        \n    ]\n    \n    @every(minutes=5 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                           \'base_url\': crawler[\"url\"],\n                       },\n                       callback = self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n',NULL,1.0000,3.0000,1473303462.4193),('dongman_qiongkong','dongman','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-11 17:15:33\n# Project: dongman\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://qingkong.net/anime/dmzx/\', callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.nLink\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        #for each in response.doc(\'.p_current > a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        data = response.doc(\'.zxtitle td\').text()\n        source = data.split(u\'：\')[1].split(\'[\')[0].strip()\n        date = data.split(\'[\')[-1].split(\']\')[0]\n        content = response.doc(\'#content > div > div\').html()\n        if not content:\n            content = response.doc(\'#content\').html()\n        if not content:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.hzxnr\').text(),\n            \'content\': content,\n            \"bread\": [u\'动漫资讯\'],\n            \"source\": source if source else u\"qingkong.net\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"动漫\",\n            \"date\": date,\n        }\n',NULL,1.0000,3.0000,1471330343.5662),('english_yingyu_inc','english','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 09:30:50\n# Project: english_yingyu_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.yingyu.com/seyy/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.nav_main a\').items():\n            if not each.text().strip().endswith(u\'英语\'):\n                continue\n            _dict ={}\n            _dict[\'bread\'] = [each.text(),]\n            self.crawl(each.attr.href,save=_dict, callback=self.list_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.list_main1_tuijian li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'title\'] = each.find(\'.list_main1_tuijian_content > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.list_main1_tuijian_time > a\').text().strip()\n            self.crawl(each.find(\'.list_main1_tuijian_content > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.list_main1_tuijian_page > a\').items():\n            #self.crawl(each.attr.href,save =response.save, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.details_main1_left_content > p\').items():\n            if u\'相关推荐：\' in each.text() or u\'文章导航：\' in each.text() or u\'阅读排行\' in each.text() or u\'返回专题\' in each.text() or u\'更多推荐\' in each.text():\n                break\n            if u\'点击查看\' in each.text():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"yingyu\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"英语\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474508081.3520),('eoffcn_gongwuyuan_inc','gongwuyuan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-29 10:07:32\n# Project: eoffcn_tiku\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/zhenti/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/moniti/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/lxt/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        \n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/zhenti/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/moniti/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/lxt/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        \n        \n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/zhenti/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/moniti/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/lxt/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        \n        \n        \n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.xm_gwy_l_mainbottom li\').items():\n            _dict = {}\n            url = each.find(\'a\').attr.href\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'中公\',\'\').replace(\'网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save =_dict,callback=self.detail_page)\n        #for each in response.doc(\'#pages > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\')\n            #self.crawl(each.attr.href, save =_dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       res_dict[\'subject\'] = u\'公务员\'\n       res_dict[\'class\'] = 46\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'source\'] = \'eoffcn.com\'\n       res_dict[\'url\'] = response.url\n       res_dict[\'tdk_title\'] = u\'跟谁学 \'+response.doc(\'title\').text().replace(\'中公\',\'\').replace(\'网校\',\'\')\n       res_dict[\'tdk_desc\'] = response.doc(\'meta[name=\"description\"]\').attr.content.replace(\'中公\',\'\').replace(\'网校\',\'\')\n       res_dict[\'tdk_keywords\'] = response.doc(\'meta[name=\"keywords\"]\').attr.content.replace(\'中公\',\'\').replace(\'网校\',\'\')\n       content_list = []\n       for each in response.doc(\'.main_l_cont > p\').items():\n            if each and u\'相关推荐\' in each.text():\n                continue\n            if each and u\'中公\' in each.text():\n                continue\n            if each and u\'责任编辑\' in each.text():\n                break\n            if each:\n                content_list.append(each.remove(\'a\').html())\n            pass\n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%(s) for s in content_list if s])\n       return res_dict\n',NULL,1.0000,3.0000,1472611407.8003),('gangqin_new_163_inc','gangqin','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-25 15:04:32\n# Project: gangqin_new_163_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://edu.163.com/keywords/9/a/94a27434/1.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.titleBar > h3 > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content = response.doc(\'.post_text\').remove(\'.ep-source\').html()\n        if not content:\n            content = response.doc(\'.end-text\').remove(\'.ep-source\').html()\n        if not content:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"content\": content.replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'\\r\',\'\'),\n            \"subject\": u\'钢琴\',\n            \"source\": \'163.com\',\n            \"date\": response.doc(\'.post_time_source\').text()[:10],\n            \"bread\": [u\'钢琴资讯\',],\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471329973.5625),('gaokao_51test_inc','gaokao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-07 11:22:37\n# Project: gmat_51test\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.51test.net/gaokao/st/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.news-list-left-content li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'span\').text()\n            self.crawl(url, save={\'date\': date}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        date = response.save[\'date\']\n        \'\'\'\n        for each in response.doc(\'.show_content_next > a\').items():\n            self.crawl(each.attr.href,save={\'date\': date}, callback=self.detail_page)\n        \'\'\'\n\n        content = response.doc(\'.show_content\').remove(\'div\').html().replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'【无忧考网 - GMAT研究生管理考试试题】\',\'\')\n        if not content:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"date\": date,\n            \"subject\": \'高考\',\n            \"source\": \'51test\',\n            \"bread\": [u\'模拟真题\',],\n            \"class\": 26,\n            \"data_weight\": 0,\n            \"content\": content, #\'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n        }\n',NULL,1.0000,3.0000,1470815072.2272),('gmat_tiandao_inc','gmat','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-05 15:21:53\n# Project: gmat_tiandao_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    page_dict = {\n        \"advice\": u\'GMAT机经\',\n        \'read\': u\'GMAT阅读\',\n        \'syntax\': u\'GMAT语法\',\n        \'write\': u\'GMAT写作\',\n        \'math\': u\'GMAT数学\',\n        \'logic\': u\'GMAT逻辑\',\n        \'news\': u\'快讯动态\',\n        \'comments\': u\'冲刺宝典\',\n        \'experience\': u\'备考计划\',\n    }\n    @every(minutes=12 * 60)\n    def on_start(self):\n        for k, v in self.page_dict.iteritems():\n            self.crawl(\'http://gmat.tiandaoedu.com/%s/\'%k,save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://tiandaoedu.com/yyxx/syyd/\', save={\'bread\': u\'GMAT阅读\'}, callback=self.index_page)\n        self.crawl(\'http://tiandaoedu.com/yyxx/xcxy/\', save={\'bread\': u\'GMAT词汇\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.sty_two > .ptit > a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n        for each in response.doc(\'.rf > .ptit > a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n        #for each in response.doc(\'.pages > a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.wzlist > .yh\').text(),\n            \"date\": response.doc(\'.p_span > span\').text().split()[2].split(u\'：\')[-1],\n            \"brief\": response.doc(\'.zhw_p\').text(),\n            \"subject\": \'GMAT\',\n            \'source\': \'tiandao\',\n            \'content\': response.doc(\'.wzy_bot\').html().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\').strip(\' \').replace(u\'天道\',\'\'),\n            \"bread\": [response.save[\'bread\'],],\n            \"class\": 30,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471334123.6224),('gmat_zhan_inc','gmat','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 18:19:22\n# Project: gmat_xiaozhan_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    type_dict = {\n        \'yufa\': u\'GMAT语法\',\n        \'cihui\': u\'GMAT词汇\', \n        \'yuedu\': \'GMAT阅读\', \n        \'luoji\': u\'GMAT逻辑\',\n        \'tuili\': u\'GMAT逻辑\',\n        \'zuowen\': u\'GMAT作文\',\n        \'shuxue\': u\'GMAT数学\', \n        \'jihua\': u\'备考计划\', \n        \'tifen/beikao\': u\'备考计划\', \n        \'gaofen\': u\'高分心得\',\n        \'fuxi\': u\'复习攻略\',  \n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for type, v in self.type_dict.iteritems():\n            self.crawl(\'http://gmat.zhan.com/%s/\'%type, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gmat/kaoqian/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gmat/fukao/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gmat/beikao/\', save={\'bread\': u\'备考计划\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.experience-item\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.index-middle-info-3 > .pull-left\').text().split()[0]\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.index-middle-info-2\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        for each in response.doc(\'.things_list > .pull-right\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.padding_right_10\').text()\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.text\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \n        #for each in response.doc(\'nav a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        content_list = []\n        for each in  response.doc(\'.article-content > p\').items():\n            content_list.append((each.html().replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'小站\',\'\')))\n        content_list = content_list[:-1]\n        if not content_list:\n            return None\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'% v for v in content_list])\n        res_dict[\'subject\'] = u\'GMAT\'\n        res_dict[\'source\'] = u\'小站\'\n        res_dict[\'tag\'] = response.doc(\'.tag > a\').text().split(\' \')\n        bread = [res_dict[\'bread\'], ]\n        bread.extend(response.doc(\'.head-crumbs-a-active\').text().split(\' \'))\n        if res_dict[\'date\'][:3] != \'201\':\n            res_dict[\'date\'] = response.doc(\'.pull-left > span\').text().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        res_dict[\'bread\'] = list(set(bread))\n        res_dict[\'class\'] = 30\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471334127.3871),('gre_tiandao_inc','gre','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-05 15:21:53\n# Project: gre_tiandao_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    page_dict = {\n        \"advice\": u\'GRE机经\',\n        \"glossary\": u\'GRE词汇\',\n        \'read\': u\'GRE阅读\',\n        \'completion\': u\'GRE填空\',\n        \'write\': u\'GRE作文\',\n        \'math\': u\'GRE数学\',\n        \'news\': u\'快讯动态\',\n        \'comments\': u\'冲刺宝典\',\n        \'experience\': u\'备考计划\',\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in self.page_dict.iteritems():\n            self.crawl(\'http://gre.tiandaoedu.com/%s/\'%k,save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://tiandaoedu.com/yyxx/syyd/\', save={\'bread\': u\'GRE阅读\'}, callback=self.index_page)\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.sty_two > .ptit > a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n        for each in response.doc(\'.rf > .ptit > a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n        #for each in response.doc(\'.pages > a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.wzlist > .yh\').text(),\n            \"date\": response.doc(\'.p_span > span\').text().split()[2].split(u\'：\')[-1],\n            \"brief\": response.doc(\'.zhw_p\').text(),\n            \"subject\": \'GRE\',\n            \'source\': \'tiandao\',\n            \'content\': response.doc(\'.wzy_bot\').html().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\'),\n            \"bread\": [response.save[\'bread\'],],\n            \"class\": 29,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471334156.2028),('gre_xiaozhan_inc','gre','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 18:19:22\n# Project: gre_xiaozhan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    type_dict = {\n        \'zhenti\': u\'GRE机经\', \'cihui\': u\'GRE词汇\', \'yuedu\': \'GRE阅读\', \'tiankong\': u\'GRE填空\', \'zuowen\': u\'GRE作文\',\n        \'shuxue\': u\'GRE数学\', \'jihua\': u\'备考计划\', \'tifen/beikao\': u\'备考计划\', \'gaofen\': u\'高分心得\',\n        \'fuxi\': u\'复习攻略\',  \n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for type, v in self.type_dict.iteritems():\n            self.crawl(\'http://gre.zhan.com/%s/\'%type, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gre/kaoqian/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gre/fukao/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gre/beikao/\', save={\'bread\': u\'备考计划\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.experience-item\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.index-middle-info-3 > .pull-left\').text().split()[0]\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.index-middle-info-2\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        for each in response.doc(\'.things_list > .pull-right\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.padding_right_10\').text()\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.text\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.col-sm-9\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.move-top > .pull-left\').text().split()[0]\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.index-middle-info-5\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \'\'\'\n        #for each in response.doc(\'nav a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        content_list = []\n        for each in  response.doc(\'.article-content > p\').items():\n            content_list.append((each.html().replace(\'src=\"/uploadfile\',\'src=\"http://gre.zhan.com/uploadfile\').replace(\'\\t\',\'\').replace(\'\\r\',\'\').replace(\'\\n\',\'\')))\n        if not content_list:\n            return None\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'% v for v in content_list])\n        res_dict[\'subject\'] = u\'GRE\'\n        res_dict[\'source\'] = u\'小站\'\n        res_dict[\'tag\'] = response.doc(\'.tag > a\').text().split(\' \')\n        bread = [res_dict[\'bread\'], ]\n        bread.extend(response.doc(\'.head-crumbs-a-active\').text().split(\' \'))\n        if res_dict[\'date\'][:3] != \'201\':\n            res_dict[\'date\'] = response.doc(\'.pull-left > span\').text().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        res_dict[\'bread\'] = list(set(bread))\n        res_dict[\'class\'] = 29\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471334159.2649),('guolairen',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-31 14:54:50\n# Project: guolairen\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nimport datetime\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'v0.1\'\n    }\n\n    page_dict = {\n\n        \'http://www.guolairen.com/bishi/bishi-list.php?id=25\':[u\'求职须知\',u\'面试\'],\n        \'http://www.guolairen.com/mianshi/mianshi-list.php?id=32\':[u\'求职须知\',u\'面试\'],\n        \'http://www.guolairen.com/jianli/jianli-list.php?id=31\':[u\'求职须知\',u\'简历\'],\n        \'http://www.guolairen.com/jianli/jianli-list.php?id=30\':[u\'求职须知\',u\'简历\'],\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.alists > .font-blue\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text()\n            url = each.find(\'a\').attr.href\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'.bd-page > a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        try:\n            res_dict[\'date\']  = response.doc(\'.font-gray\').html().split(\'：\')[2].split()[0]\n        except:\n            res_dict[\'date\'] = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n        res_dict[\'content\'] = response.doc(\'.fn-left .fn-clear\').remove(\'script\').html()\n        res_dict[\'class\'] = 46\n        res_dict[\'subject\'] = u\'求职\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = u\'guolairen\'\n        if res_dict[\'content\']:\n                return res_dict\n',NULL,1.0000,3.0000,1472635785.6007),('guoxue_guoxue_inc','guoxue','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 14:06:54\n# Project: guoxue_guoxue_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=3 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.guoxue.com/?category_name=news\', callback=self.list_page)\n        \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.dtleft > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.border-brown > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(1).text().strip()\n            _dict[\'date\'] = each.find(\'.fg\').text().strip().split()[1]\n            self.crawl(each.find(\'a\').eq(1).attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pagenavi > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.entry-content > p\').items():\n            if u\'作者单位：\' in each.text():\n                break\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 100:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"guoxue\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"国学\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474524496.4831),('gu_drumchina','gu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-24 14:10:09\n# Project: gu_drumchina\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.2\'\n    }\n    \n    page_dict = {\n\n        \'http://news.drumchina.com/sort/1.html\':[u\'鼓资讯\'],\n        \'http://news.drumchina.com/sort/4.html\':[u\'鼓手志\'],\n        \'http://news.drumchina.com/sort/6.html\':[u\'鼓教室\'],\n        \'http://news.drumchina.com/sort/5.html\':[u\'鼓世界\'],\n        \'http://news.drumchina.com/sort/7.html\':[u\'鼓谱台\'],\n        \'http://news.drumchina.com/sort/9.html\':[u\'鼓服务\']\n\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k,save = {\'bread\':v}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'td td td td td td\').items():\n            if each.find(\'a\') and each.find(\'font\').text().split():\n                _dict = {}\n                _dict[\'title\'] = each.find(\'a\').text()\n                url = each.find(\'a\').attr.href\n                _dict[\'date\'] = each.find(\'font\').text().split()[0]\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'td[align=\"right\"] > .normalfont > a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict,callback=self.index_page)    \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'.content\').remove(\'script\').remove(\'embed\').html()\n        if \'</a>\' in res_dict[\'content\']:\n            res_dict[\'content\'] = removeLink(res_dict[\'content\'])\n        if res_dict[\'content\'] == \'<p/>\':\n            return\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'鼓\'\n        res_dict[\'source\'] = \'drumchina.com\'\n        res_dict[\'url\'] = response.url\n        return res_dict\n        \n',NULL,1.0000,3.0000,1472615979.1136),('gu_inc','gu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-24 14:10:09\n# Project: gu_drumchina\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.2\'\n    }\n    \n    page_dict = {\n\n        \'http://news.drumchina.com/sort/1.html\':[u\'鼓资讯\'],\n        \'http://news.drumchina.com/sort/4.html\':[u\'鼓手志\'],\n        \'http://news.drumchina.com/sort/6.html\':[u\'鼓教室\'],\n        \'http://news.drumchina.com/sort/5.html\':[u\'鼓世界\'],\n        \'http://news.drumchina.com/sort/7.html\':[u\'鼓谱台\'],\n        \'http://news.drumchina.com/sort/9.html\':[u\'鼓服务\']\n\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k,save = {\'bread\':v}, callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'td td td td td td\').items():\n            if each.find(\'a\') and each.find(\'font\').text().split():\n                _dict = {}\n                _dict[\'title\'] = each.find(\'a\').text()\n                url = each.find(\'a\').attr.href\n                _dict[\'date\'] = each.find(\'font\').text().split()[0]\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                self.crawl(url, save = _dict,callback=self.detail_page)\n       \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'.content\').remove(\'script\').remove(\'embed\').html()\n        if res_dict[\'content\'] == \'<p/>\':\n            return\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'鼓\'\n        res_dict[\'source\'] = \'drumchina.com\'\n        res_dict[\'url\'] = response.url\n        return res_dict\n        \n',NULL,1.0000,3.0000,1472615976.0705),('huihua_woaihuahua_inc','huihua','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 17:53:00\n# Project: huihua_woaihuahua_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nfrom pyquery import PyQuery as pq \nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.woaihuahua.com/meikao/gonglue/\', callback=self.index_page)\n        self.crawl(\'http://www.woaihuahua.com/baike/\', callback=self.list1_page)\n        self.crawl(\'http://www.woaihuahua.com/meikao/shijuan/\', callback=self.list1_page)\n           \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.right > a\').items():\n            if u\'优秀试卷\' in each.text() or u\'全部\' in each.text():\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.list > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.title\').text().strip()\n            _dict[\'date\'] = each.find(\'.time > span\').text().strip()\n            self.crawl(each.find(\'.title\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.page > a\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'#n_content > *\').items():\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\t\',\'\').replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        if response.doc(\'.page\').text().strip():\n            for each in response.doc(\'.page > a\').items():\n                if \'1\' in each.text() or u\'下一页\' in each.text():\n                    continue\n               \n                _list = []\n                for each in pq(url=each.attr[\'href\'],encoding=\'gbk\').find(\'#n_content > *\').items():\n                    if not each.html():\n                        continue\n                    _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\t\',\'\').replace(\'\\n\',\'\')).replace(\'</a>\',\'\').replace(\'/uploads/allimg\',\'http://www.woaihuahua.com/uploads/allimg\')+\'</p>\')\n                content += \'\'.join(v for v in _list if v)\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"woaihuahua\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"绘画\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474538068.0011),('hujiang_deyu_inc','deyu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 14:18:29\n# Project: hujiang_deyu\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.hujiang.com/new/rumen/\':[u\'德语基础\'],\n            \'http://de.hujiang.com/new/shiyong/\':[u\'实用德语\'],\n            \'http://de.hujiang.com/new/yule/\':[u\'德语文化\'],\n            \'http://de.hujiang.com/new/topic/627/\':[u\'德国留学\'],\n            \'http://de.hujiang.com/new/topic/1024/\':[u\'德语考试\']\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            _dict[\'date\'] = each.find(\'p.article_list_moreinfo\').find(\'span.green\').text().split()[0]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n      \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'德语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1472615768.5101),('hujiang_riyu_inc','riyu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 14:18:29\n# Project: hujiang_deyu\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://jp.hjenglish.com/new/c4010/\':[u\'日语基础\'],\n            \'http://jp.hjenglish.com/new/c4040/\':[u\'实用日语\'],\n            \'http://jp.hjenglish.com/new/c4020/\':[u\'日语考试\'],\n            \'http://jp.hjenglish.com/new/c4070/\':[u\'日语文化\'],\n            \'http://jp.hjenglish.com/new/c4090/\':[u\'商务日语\']\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            _dict[\'date\'] = each.find(\'p.article_list_moreinfo\').find(\'span.green\').text().split()[0]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').remove(\'script\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'德语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1472615761.9062),('inc_10_youku','guitar','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-30 16:44:48\n# Project: inc_10_youku\n\nfrom pyspider.libs.base_handler import *\nimport time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=5 * 60)\n    def on_start(self):\n        self.crawl(\'http://i.youku.com/i/UNDg1NzQyODA=/videos\', callback=self.index_page)\n        self.crawl(\'http://i.youku.com/u/UMjg2MDY1OTYxMg==\', callback=self.index_page)\n        self.crawl(\'http://www.soku.com/search_video/q_%E5%90%89%E4%BB%96_orderby_3_lengthtype_1_hd_7?site=14&_lg=10&limitdate=0\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.v\').items():\n            cover = each.find(\'img\').attr.src\n           # print cover\n            vlink_title = each.find(\'.v-link > a\').attr.title\n            if vlink_title == u\'该视频已被发布者加密\':\n                continue\n            url = each.find(\'.v-meta-title > a\').attr.href\n            title = each.find(\'.v-meta-title > a\').text()\n            if title.find(u\'吉他\') == -1 and title.find(u\'吉它\') == -1:\n                pass\n            else:\n                self.crawl(url, save={\'title\': title, \'cover\': cover},  callback=self.detail_page)\n\n   # @config(priority=2)\n    @config(age=1*1)\n    def detail_page(self, response):\n        try:\n            url = response.url\n        #http://v.youku.com/v_show/id_XMTUxNzMxODAyMA==.html?from=s1.8-1-1.2\n            id = url.split(\'id_\')[1].split(\'==\')[0]\n            video_url_element = \'http://player.youku.com/embed/%s\' %(id)\n            video_url = []\n            video_url.append(video_url_element)\n            title = response.save[\'title\']\n            cover = response.save[\'cover\']\n           # vlink_title = response.save[\'vlink_title\']\n           # title = response.doc(\'.base_info > .title\').text()\n            subject = u\'吉他\'\n            source = \'youku.com\'\n            publish_time = time.strftime(\'%Y-%m-%d\',time.localtime(time.time()))\n        except:\n            return None\n        return {\n            \"url\": url,\n            \"video_url\":video_url,\n            \"title\": title,\n            \"subject\": subject,\n            \"source\": source,\n            \"publish_time\": publish_time,\n            \"cover\": cover,\n            \"class\": 10,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471335003.5497),('inc_16_guitarworld','guitar','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-30 14:13:13\n# Project: guitar_news_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.guitarworld.com.cn/news\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'h3 > a\').items():\n            url = each.attr.href\n            if re.match(\'http://www.guitarworld.com.cn/news/\', url):\n                self.crawl(url, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        url = response.url\n        subject = u\'吉他\'\n        source = \'guitarworld.com.cn\'\n        title = response.doc(\'.news-title\').text()\n        content = response.doc(\'.news-content\').html()\n        info_arr = []\n        for item in  response.doc(\'.news-link > span\').items():\n            info_arr.append(item.text())\n        content = content.replace(\'news/data/attachment\', \'data/attachment\')\n        return {\n            \"url\": url,\n            \"subject\": subject,\n            \"source\": source,\n            \"content\": content,\n            \"title\": title,\n            \"date\": info_arr[1].split(\' \')[0],\n            \"class\": 16,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471329630.2764),('inc_gangqin_youku_6','gangqin','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-19 11:22:03\n# Project: inc_gangqin_youku_6\n\n\nfrom pyspider.libs.base_handler import *\nimport time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.soku.com/search_video/q_%E9%92%A2%E7%90%B4_limitdate_0?site=14&_lg=10&orderby=2\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.v\').items():\n            cover = each.find(\'img\').attr.src\n           # print cover\n            url = each.find(\'.v-meta-title > a\').attr.href\n            title = each.find(\'.v-meta-title > a\').text()\n            self.crawl(url, save={\'title\': title, \'cover\': cover},  callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        try:\n            url = response.url\n            id = url.split(\'id_\')[1].split(\'==\')[0]\n            video_url_element = \'http://player.youku.com/embed/%s\' %(id)\n            video_url = []\n            video_url.append(video_url_element)\n            title = response.save[\'title\']\n            cover = response.save[\'cover\']\n           # title = response.doc(\'.base_info > .title\').text()\n            subject = u\'钢琴\'\n            source = \'youku.com\'\n            publish_time = time.strftime(\'%Y-%m-%d\',time.localtime(time.time()))\n        except:\n            return None\n        return {\n            \"url\": url,\n            \"video_url\":video_url,\n            \"title\": title,\n            \"subject\": subject,\n            \"source\": source,\n            \"publish_time\": publish_time,\n            \"cover\": cover,\n            \"class\": 6,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471335046.7183),('jiajiao_58_m','jiajiao','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-31 17:02:09\n# Project: jiajiao_58_m\n\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\nfrom pyquery import PyQuery as pq\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\ncity_list = [\n\'bj\',\n\'sh\',\n\'gz\',\n\'sz\',\n\'cd\',\n\'hz\',\n\'nj\',\n\'tj\',\n\'wh\',\n\'cq\',\n\'qd\',\n\'jn\',\n\'yt\',\n\'wf\',\n\'linyi\',\n\'zb\',\n\'jining\',\n\'ta\',\n\'lc\',\n\'weihai\',\n\'zaozhuang\',\n\'dz\',\n\'rizhao\',\n\'dy\',\n\'heze\',\n\'bz\',\n\'lw\',\n\'zhangqiu\',\n\'kl\',\n\'zc\',\n\'shouguang\',\n\'su\',\n\'nj\',\n\'wx\',\n\'cz\',\n\'xz\',\n\'nt\',\n\'yz\',\n\'yancheng\',\n\'ha\',\n\'lyg\',\n\'taizhou\',\n\'suqian\',\n\'zj\',\n\'shuyang\',\n\'dafeng\',\n\'rugao\',\n\'qidong\',\n\'liyang\',\n\'haimen\',\n\'donghai\',\n\'yangzhong\',\n\'xinghuashi\',\n\'xinyishi\',\n\'taixing\',\n\'rudong\',\n\'pizhou\',\n\'xzpeixian\',\n\'jingjiang\',\n\'jianhu\',\n\'haian\',\n\'dongtai\',\n\'danyang\',\n\'hz\',\n\'nb\',\n\'wz\',\n\'jh\',\n\'jx\',\n\'tz\',\n\'sx\',\n\'huzhou\',\n\'lishui\',\n\'quzhou\',\n\'zhoushan\',\n\'yueqingcity\',\n\'ruiancity\',\n\'yiwu\',\n\'yuyao\',\n\'zhuji\',\n\'xiangshanxian\',\n\'wenling\',\n\'tongxiang\',\n\'cixi\',\n\'changxing\',\n\'jiashanx\',\n\'haining\',\n\'deqing\',\n\'hf\',\n\'wuhu\',\n\'bengbu\',\n\'fy\',\n\'hn\',\n\'anqing\',\n\'suzhou\',\n\'la\',\n\'huaibei\',\n\'chuzhou\',\n\'mas\',\n\'tongling\',\n\'xuancheng\',\n\'bozhou\',\n\'huangshan\',\n\'chizhou\',\n\'ch\',\n\'hexian\',\n\'hq\',\n\'tongcheng\',\n\'ningguo\',\n\'tianchang\',\n\'sz\',\n\'gz\',\n\'dg\',\n\'fs\',\n\'zs\',\n\'zh\',\n\'huizhou\',\n\'jm\',\n\'st\',\n\'zhanjiang\',\n\'zq\',\n\'mm\',\n\'jy\',\n\'mz\',\n\'qingyuan\',\n\'yj\',\n\'sg\',\n\'heyuan\',\n\'yf\',\n\'sw\',\n\'chaozhou\',\n\'taishan\',\n\'yangchun\',\n\'sd\',\n\'huidong\',\n\'boluo\',\n\'fz\',\n\'xm\',\n\'qz\',\n\'pt\',\n\'zhangzhou\',\n\'nd\',\n\'sm\',\n\'np\',\n\'ly\',\n\'wuyishan\',\n\'shishi\',\n\'jinjiangshi\',\n\'nananshi\',\n\'nn\',\n\'liuzhou\',\n\'gl\',\n\'yulin\',\n\'wuzhou\',\n\'bh\',\n\'gg\',\n\'qinzhou\',\n\'baise\',\n\'hc\',\n\'lb\',\n\'hezhou\',\n\'fcg\',\n\'chongzuo\',\n\'haikou\',\n\'sanya\',\n\'wzs\',\n\'sansha\',\n\'qh\',\n\'zz\',\n\'luoyang\',\n\'xx\',\n\'ny\',\n\'xc\',\n\'pds\',\n\'ay\',\n\'jiaozuo\',\n\'sq\',\n\'kaifeng\',\n\'puyang\',\n\'zk\',\n\'xy\',\n\'zmd\',\n\'luohe\',\n\'smx\',\n\'hb\',\n\'jiyuan\',\n\'mg\',\n\'yanling\',\n\'yuzhou\',\n\'changge\',\n\'wh\',\n\'yc\',\n\'xf\',\n\'jingzhou\',\n\'shiyan\',\n\'hshi\',\n\'xiaogan\',\n\'hg\',\n\'es\',\n\'jingmen\',\n\'xianning\',\n\'ez\',\n\'suizhou\',\n\'qianjiang\',\n\'tm\',\n\'xiantao\',\n\'snj\',\n\'yidou\',\n\'cs\',\n\'zhuzhou\',\n\'yiyang\',\n\'changde\',\n\'hy\',\n\'xiangtan\',\n\'yy\',\n\'chenzhou\',\n\'shaoyang\',\n\'hh\',\n\'yongzhou\',\n\'ld\',\n\'xiangxi\',\n\'zjj\',\n\'nc\',\n\'ganzhou\',\n\'jj\',\n\'yichun\',\n\'ja\',\n\'sr\',\n\'px\',\n\'fuzhou\',\n\'jdz\',\n\'xinyu\',\n\'yingtan\',\n\'yxx\',\n\'sy\',\n\'dl\',\n\'as\',\n\'jinzhou\',\n\'fushun\',\n\'yk\',\n\'pj\',\n\'cy\',\n\'dandong\',\n\'liaoyang\',\n\'benxi\',\n\'hld\',\n\'tl\',\n\'fx\',\n\'pld\',\n\'wfd\',\n\'hrb\',\n\'dq\',\n\'qqhr\',\n\'mdj\',\n\'suihua\',\n\'jms\',\n\'jixi\',\n\'sys\',\n\'hegang\',\n\'heihe\',\n\'yich\',\n\'qth\',\n\'dxal\',\n\'cc\',\n\'jl\',\n\'sp\',\n\'yanbian\',\n\'songyuan\',\n\'bc\',\n\'th\',\n\'baishan\',\n\'liaoyuan\',\n\'cd\',\n\'mianyang\',\n\'deyang\',\n\'nanchong\',\n\'yb\',\n\'zg\',\n\'ls\',\n\'luzhou\',\n\'dazhou\',\n\'scnj\',\n\'suining\',\n\'panzhihua\',\n\'ms\',\n\'ga\',\n\'zy\',\n\'liangshan\',\n\'guangyuan\',\n\'ya\',\n\'bazhong\',\n\'ab\',\n\'ganzi\',\n\'km\',\n\'qj\',\n\'dali\',\n\'honghe\',\n\'yx\',\n\'lj\',\n\'ws\',\n\'cx\',\n\'bn\',\n\'zt\',\n\'dh\',\n\'pe\',\n\'bs\',\n\'lincang\',\n\'diqing\',\n\'nujiang\',\n\'gy\',\n\'zunyi\',\n\'qdn\',\n\'qn\',\n\'lps\',\n\'bijie\',\n\'tr\',\n\'anshun\',\n\'qxn\',\n\'lasa\',\n\'rkz\',\n\'sn\',\n\'linzhi\',\n\'changdu\',\n\'nq\',\n\'al\',\n\'sjz\',\n\'bd\',\n\'ts\',\n\'lf\',\n\'hd\',\n\'qhd\',\n\'cangzhou\',\n\'xt\',\n\'hs\',\n\'zjk\',\n\'chengde\',\n\'dingzhou\',\n\'gt\',\n\'zhangbei\',\n\'zx\',\n\'zd\',\n\'ty\',\n\'linfen\',\n\'dt\',\n\'yuncheng\',\n\'jz\',\n\'changzhi\',\n\'jincheng\',\n\'yq\',\n\'lvliang\',\n\'xinzhou\',\n\'shuozhou\',\n\'linyixian\',\n\'qingxu\',\n\'hu\',\n\'bt\',\n\'chifeng\',\n\'erds\',\n\'tongliao\',\n\'hlbe\',\n\'bycem\',\n\'wlcb\',\n\'xl\',\n\'xam\',\n\'wuhai\',\n\'alsm\',\n\'hlr\',\n\'xa\',\n\'xianyang\',\n\'baoji\',\n\'wn\',\n\'hanzhong\',\n\'yl\',\n\'yanan\',\n\'ankang\',\n\'sl\',\n\'tc\',\n\'xj\',\n\'changji\',\n\'bygl\',\n\'yili\',\n\'aks\',\n\'ks\',\n\'hami\',\n\'klmy\',\n\'betl\',\n\'tlf\',\n\'ht\',\n\'shz\',\n\'kzls\',\n\'ale\',\n\'wjq\',\n\'tmsk\',\n\'lz\',\n\'tianshui\',\n\'by\',\n\'qingyang\',\n\'pl\',\n\'jq\',\n\'zhangye\',\n\'wuwei\',\n\'dx\',\n\'jinchang\',\n\'ln\',\n\'linxia\',\n\'jyg\',\n\'gn\',\n\'yinchuan\',\n\'wuzhong\',\n\'szs\',\n\'zw\',\n\'guyuan\',\n\'xn\',\n\'hx\',\n\'haibei\',\n\'guoluo\',\n\'haidong\',\n\'huangnan\',\n\'ys\',\n\'hainan\',\n\'hk\',\n\'am\',\n\'tw\',\n\'diaoyudao\',\n\'cn\',\n]\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'headers\':{\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'}\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for city in city_list:\n            self.crawl(\'http://\'+city+\'.58.com/jiajiao/\',headers=self.crawl_config[\'headers\'],save={\'city\':city}, callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#jieduan > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            _dict[\'city\'] = response.save[\'city\']\n            _dict[\'title\'] = each.text()\n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href,headers=self.crawl_config[\'headers\'], save = _dict, callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        if response.doc(\'#ObjectType > a\'):\n            for each in response.doc(\'#ObjectType > a\').items():\n                _dict = {}\n                _dict[\'city\'] = response.save[\'city\']\n                _dict[\'title\'] = response.save[\'title\']\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近的\'+_dict[\'title\']+\'家教\'\n                else:\n                    _dict[\'title\'] = u\'附近的\'+_dict[\'title\']+each.text()+\'家教\'\n                self.crawl(each.attr.href,headers=self.crawl_config[\'headers\'], save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'city\'] = response.save[\'city\']\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = response.save[\'title\']\n             _dict[\'title\'] = u\'附近的\'+_dict[\'title\']+\'家教\'\n             self.crawl(response.url,headers=self.crawl_config[\'headers\'], save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=10 * 24 * 60 * 60)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href,headers=self.crawl_config[\'headers\'], save = response.save, callback=self.list_page2)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'city\'] = response.save[\'city\']\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href,headers=self.crawl_config[\'headers\'], save = _dict, callback=self.list_page3)\n        #翻页\n        #for each in response.doc(\'.next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        _dict[\'content\'] = \'\'\n        i = 0\n        for each in response.doc(\'tr > .t\').items():\n            if \'adinfo\' in each.parent().attr[\'infotag\']:\n                continue\n            #print each.find(\'a\').outerHtml()\n            if \'http://jump\' in each.find(\'a\').attr.href:\n                url = \'http://m.58.com/\'+_dict[\'city\']+\'/jiajiao/\'+each.find(\'a\').attr[\'name\']+\'x.shtml\'\n            else:\n                url = \'http://m.58.com/\'+_dict[\'city\']+\'/jiajiao/\'+each.find(\'a\').attr[\'href\'].split(\'/\')[-1]\n            #print url\n            _dict[\'content\'] += \'<p>\'+pq(url).find(\'.tit_area h1\').eq(0).text()+\'</p>\'+\'<p>\'+pq(url).find(\'.article li\').eq(-2).text()+\'</p>\' +\'<p>\'+pq(url).find(\'.article li\').eq(-1).text()+\'</p>\'+\'<p>\'+pq(url).find(\'.firm_area h2\').eq(0).text()+\'</p>\'+\'<p>\'+pq(pq(url).find(\'.firm_area a\').eq(0).attr[\'href\']).find(\'.infoitembox li\').eq(-1).text()+\'</p>\'\n            _dict[\'content\'] += \'<br/>\'\n            i += 1\n            if i >= 10:\n                break\n        if not _dict[\'content\'].strip():\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":[u\'家教\'],\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'互联网\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1473741204.5981),('jingyan_120ask','jingyan','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-01 13:49:15\n# Project: jingyan_xuexila_inc\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\nurl_list = [\n\'http://www.xuexila.com/shenghuo/\',\n\'http://www.xuexila.com/naoli/\',\n\'http://www.xuexila.com/zhishi/\',\n]\n\nurl2_list = [\n\'http://www.xuexila.com/diannao/\',\n]\nurl3_list = [\n\'http://www.xuexila.com/tiyu/\',\n]\nbread_dict = {\n    u\'生活小常识\':[u\'生活技巧\'],\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n    \'itag\':\'3\',\n    \'headers\':{\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://tag.120ask.com/jingyan/list/aowvrf71yv-200.html\',headers = self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.p_allsortbtn p > a\').items():\n            #print each.text()\n            self.crawl(each.attr.href, headers = self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.p_topsick a\').items():\n            self.crawl(each.attr.href,headers = self.crawl_config[\'headers\'], callback=self.list1_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.p_topsick a\').items():\n            self.crawl(each.attr.href, headers = self.crawl_config[\'headers\'], callback=self.list2_page)\n\n        #翻页\n        #for each in response.doc(\'div > li > a\').items():\n        #    self.crawl(each.attr.href, save = response.save, headers = self.crawl_config[\'headers\'], callback=self.list_page\n        \n    @config(age=10 * 24 * 60 * 60)\n    def list2_page(self, response):\n        for each in response.doc(\'.p_listboxdlti > a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            self.crawl(each.attr.href, save = _dict,headers = self.crawl_config[\'headers\'], callback=self.detail_page)\n\n        #翻页\n        for each in response.doc(\'.p_pagenext\').items():\n            self.crawl(each.attr.href, save = response.save, headers = self.crawl_config[\'headers\'], callback=self.list2_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = False\n        flag_summary = False\n        flag_method = False\n        flag_first = True\n        steps = []\n        steps1 = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        #print pattern.match(str).group(0)\n        #print response.doc(\'.content div.left.d > div\').eq(-2).html()\n        \n        for each in response.doc(\'.w_article\').remove(\'.w_sharediv \').children().items():\n            #print each.text()\n            if \'<h3\' in each.outerHtml() and u\'概 述\' in each.text():\n                flag_abstract = True\n                continue\n            if \'<h3\' in each.outerHtml() and u\'注意事项\' in each.text():\n                flag_summary = True\n                continue\n            if \'<h3\' in each.outerHtml():\n                flag_abstract = False\n                flag_summary = False\n                flag_method = True\n                continue\n            if \'<img\' in each.html():\n                #print each.html()\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                    #print desc_img\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if not each.text():\n                continue\n            if flag_abstract:\n                steps.append(each.remove(\'i\').text().strip())\n            elif flag_summary:\n                steps1.append(each.remove(\'i\').text().strip())\n            else:\n                _list.append({\n                    \'img\': img,\n                    \'title\': each.remove(\'i\').text().strip(),\n                    \'substeps\': \'\',\n                })\n               \n        summary = {\n                \'title\': u\'注意事项\',\n                \'steps\': steps1,\n            }\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            abstract = {\n                \'title\': steps[0],\n                \'steps\': \'\',\n                \'img\': desc_img,\n            }\n        \n        \n        methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n        if len(steps) == 0:\n            steps = [\'\']\n            abstract = {\n\'title\': \'\',\n\'steps\': [steps[0]],\n\'img\': desc_img,\n        }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"summary\": summary,\n            \"date\": response.doc(\'.w_share_p1\').text().split(u\'：\')[1].split()[0],\n            \"bread\": [u\'健康养生\',],\n            \"source\": \"120ask\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,3.0000,5.0000,1474426276.0134),('jingyan_120ask_inc','jingyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-01 13:49:15\n# Project: jingyan_xuexila_inc\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\nurl_list = [\n\'http://www.xuexila.com/shenghuo/\',\n\'http://www.xuexila.com/naoli/\',\n\'http://www.xuexila.com/zhishi/\',\n]\n\nurl2_list = [\n\'http://www.xuexila.com/diannao/\',\n]\nurl3_list = [\n\'http://www.xuexila.com/tiyu/\',\n]\nbread_dict = {\n    u\'生活小常识\':[u\'生活技巧\'],\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n    \'itag\':\'3\',\n    \'headers\':{\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://tag.120ask.com/jingyan/list/aowvrf71yv-200.html\',headers = self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.p_allsortbtn p > a\').items():\n            #print each.text()\n            self.crawl(each.attr.href, headers = self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.p_topsick a\').items():\n            self.crawl(each.attr.href,headers = self.crawl_config[\'headers\'], callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.p_topsick a\').items():\n            self.crawl(each.attr.href, headers = self.crawl_config[\'headers\'], callback=self.list2_page)\n\n        #翻页\n        #for each in response.doc(\'div > li > a\').items():\n        #    self.crawl(each.attr.href, save = response.save, headers = self.crawl_config[\'headers\'], callback=self.list_page\n        \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        for each in response.doc(\'.p_listboxdlti > a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            self.crawl(each.attr.href, save = _dict,headers = self.crawl_config[\'headers\'], callback=self.detail_page)\n\n        #翻页\n        #for each in response.doc(\'.p_pagenext\').items():\n         #   self.crawl(each.attr.href, save = response.save, headers = self.crawl_config[\'headers\'], callback=self.list2_page)\n\n    @config(priority=2, age=1 * 1)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = False\n        flag_summary = False\n        flag_method = False\n        flag_first = True\n        steps = []\n        steps1 = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        #print pattern.match(str).group(0)\n        #print response.doc(\'.content div.left.d > div\').eq(-2).html()\n        \n        for each in response.doc(\'.w_article\').remove(\'.w_sharediv \').children().items():\n            #print each.text()\n            if \'<h3\' in each.outerHtml() and u\'概 述\' in each.text():\n                flag_abstract = True\n                continue\n            if \'<h3\' in each.outerHtml() and u\'注意事项\' in each.text():\n                flag_summary = True\n                continue\n            if \'<h3\' in each.outerHtml():\n                flag_abstract = False\n                flag_summary = False\n                flag_method = True\n                continue\n            if \'<img\' in each.html():\n                #print each.html()\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                    #print desc_img\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if not each.text():\n                continue\n            if flag_abstract:\n                steps.append(each.remove(\'i\').text().strip())\n            elif flag_summary:\n                steps1.append(each.remove(\'i\').text().strip())\n            else:\n                _list.append({\n                    \'img\': img,\n                    \'title\': each.remove(\'i\').text().strip(),\n                    \'substeps\': \'\',\n                })\n               \n        summary = {\n                \'title\': u\'注意事项\',\n                \'steps\': steps1,\n            }\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            abstract = {\n                \'title\': steps[0],\n                \'steps\': \'\',\n                \'img\': desc_img,\n            }\n        \n        \n        methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n        if len(steps) == 0:\n            steps = [\'\']\n            abstract = {\n\'title\': \'\',\n\'steps\': [steps[0]],\n\'img\': desc_img,\n        }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"summary\": summary,\n            \"date\": response.doc(\'.w_share_p1\').text().split(u\'：\')[1].split()[0],\n            \"bread\": [u\'健康养生\',],\n            \"source\": \"120ask\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1474352653.8189),('jingyan_sohu_gaokao_inc','jingyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 15:36:33\n# Project: jingyan_sohu_gaokao_inc\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://learning.sohu.com/tag/0313/000000313.shtml\',headers = self.header, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.published\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.content-title > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.time\').text().strip().split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            self.crawl(each.find(\'.content-title > a\').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\n        #翻页\n        #for i in range(263,362,1):\n            #self.crawl(\'http://learning.sohu.com/tag/0313/000000313_\'+str(i)+\'.shtml\',headers = self.header, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        pattern2 = re.compile(ur\'[0123456789]{1,2}[：].*\')\n        pattern3 = re.compile(r\'[0123456789]{2}.*\')\n        pattern4 = re.compile(r\'<strong>\')\n        for each in response.doc(\'#contentText > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break \n            \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\'))    \n        for each in response.doc(\'.text > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break  \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\')) \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        if len(methods[0][\'steps\']) == 0:\n           return \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'高考\',],\n            \"source\": u\"搜狐\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1473142302.0116),('jingyan_sohu_liuxue_inc','jingyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 15:38:51\n# Project: jingyan_sohu_liuxue_inc\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://learning.sohu.com/tag/0073/000000073.shtml\',save = {\'tag\':1}, headers = self.header, callback=self.index_page)\n        self.crawl(\'http://learning.sohu.com/tag/0249/000000249.shtml\',save = {\'tag\':2},headers = self.header, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.published\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.content-title > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.time\').text().strip().split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            self.crawl(each.find(\'.content-title > a\').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\n        #翻页\n        #if response.save.get(\'tag\') == 1:\n         #   for i in range(356,455,1):\n                #self.crawl(\'http://learning.sohu.com/tag/0073/000000073_\'+str(i)+\'.shtml\',save = response.save, headers = self.header, callback=self.index_page)\n #       if response.save.get(\'tag\') == 2:\n  #          for i in range(44,143,1):\n                #self.crawl(\'http://learning.sohu.com/tag/0249/000000249_\'+str(i)+\'.shtml\',save = response.save, headers = self.header, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        pattern2 = re.compile(ur\'[0123456789]{1,2}[：].*\')\n        pattern3 = re.compile(r\'[0123456789]{2}.*\')\n        pattern4 = re.compile(r\'<strong>\')\n        for each in response.doc(\'#contentText > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break \n            \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\'))    \n        for each in response.doc(\'.text > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break  \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\')) \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        if len(methods[0][\'steps\']) == 0:\n           return \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'留学\',],\n            \"source\": u\"搜狐\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1473142295.4072),('jingyan_xuexila_inc','jingyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-01 13:49:15\n# Project: jingyan_xuexila_inc\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\nurl_list = [\n\'http://www.xuexila.com/shenghuo/\',\n\'http://www.xuexila.com/naoli/\',\n\'http://www.xuexila.com/zhishi/\',\n]\n\nurl2_list = [\n\'http://www.xuexila.com/diannao/\',\n]\nurl3_list = [\n\'http://www.xuexila.com/tiyu/\',\n]\nbread_dict = {\n    u\'生活小常识\':[u\'生活技巧\'],\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n    \'itag\':\'2\',\n    \'headers\':{\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url in url_list:\n            bread = [u\'生活技巧\',]\n            self.crawl(url,headers = self.crawl_config[\'headers\'], save = {\'bread\':bread}, callback=self.index_page)\n        for url in url2_list:\n            bread = [u\'电脑数码\',]\n            self.crawl(url,headers = self.crawl_config[\'headers\'], save = {\'bread\':bread}, callback=self.index_page)\n        for url in url3_list:\n            bread = [u\'体育运动\',]\n            self.crawl(url,headers = self.crawl_config[\'headers\'], save = {\'bread\':bread}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.box > div\').items():\n            if u\'最强大脑\' in each.find(\'b > a\').text() or u\'一站到底\' in each.find(\'b > a\').text():\n                continue\n            self.crawl(each.find(\'.i_more > a\').attr.href, save = response.save,  headers = self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.r_list a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href, save = _dict,headers = self.crawl_config[\'headers\'], callback=self.detail_page)\n            \n        for each in response.doc(\'.box > div\').items():\n            self.crawl(each.find(\'.i_more > a\').attr.href, save = response.save,  headers = self.crawl_config[\'headers\'], callback=self.list1_page)\n        \n        #翻页\n        #for each in response.doc(\'div > li > a\').items():\n         #   self.crawl(each.attr.href, save = response.save, headers = self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.r_list a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href, save = _dict,headers = self.crawl_config[\'headers\'], callback=self.detail_page)\n\n        #翻页\n        #for each in response.doc(\'div > li > a\').items():\n        #    self.crawl(each.attr.href, save = response.save, headers = self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(priority=2, age=1 * 1)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        #print pattern.match(str).group(0)\n        #print response.doc(\'.content div.left.d > div\').eq(-2).html()\n        \n        for each in response.doc(\'#contentText\').children().items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if u\'高三网小编推荐你\' in each.text() or \'<hr />\'==each.outerHtml().strip() or u\'猜你喜欢\' in each.text() or \'BAIDU_CLB\' in each.text() or u\'点击进入\' in each.text() or u\'相关文章：\' in each.text() or u\'的人还看了：\' in each.text():\n                break \n            if each.text() == \'None\' or not each.html():\n                continue\n             \n            if u\'点击查看\' in each.text() or u\'查看更多\' in each.text() or u\'相关链接\' in each.text():\n                continue  \n            if pattern.match(each.text().strip()) or each.html().strip().startswith(\'<strong>\') or each.text().strip().startswith(u\'【\') or \'<h\' in each.outerHtml():\n                flag_abstract = False\n                flag_method = True\n            if \'<img\' in each.html():\n                #print each.html()\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                    #print desc_img\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if not each.text():\n                continue\n            if flag_abstract:\n                if \'<table\' in each.outerHtml():\n                    steps.append(each.outerHtml().replace(u\'高三网\',\'\'))\n                else:\n                    steps.append(each.text().replace(u\'高三网\',\'\'))\n            else:\n                if  pattern.match(each.text().strip()) or each.html().strip().startswith(\'<strong>\') or each.text().strip().startswith(u\'【\') or \'<h\' in each.outerHtml():\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'高三网\',\'\'))    \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.doc(\'.read_people_time\').text().split(u\'：\')[-1],\n            \"bread\": response.save.get(\'bread\'),\n            \"source\": u\"学习啦\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1473142298.7816),('jinrong_chinaacc_inc','jinrong','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 17:38:34\n# Project: jinrong_chinaacc_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nurls = {\n    \'http://www.chinaacc.com/acca/hyxw/\': [\'ACCA\'],\n    \'http://www.chinaacc.com/zhongjijingjishi/zhengcetiaojian/\': [u\'经济师\'],\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url,bread in urls.iteritems():\n            self.crawl(url, save = {\'bread\': bread}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'li > .fl\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            if u\'中华会计\' in each.find(\'a\').text():\n                continue\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        for each in response.doc(\'.nr li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            if u\'中华会计\' in each.find(\'a\').text():\n                continue\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'divpagestrcjzc a\').items():\n        #    self.crawl(each.attr.href, save = response.save, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        date = \'\'\n        source = u\'中华会计网校\'\n        if response.doc(\'.mark\'):\n            date = response.doc(\'.mark\').text().strip().split()[0]\n            source = response.doc(\'.mark\').text().strip().split(u\'来源：\')[1].split()[0]\n        \n        cover = \'\' \n        pattern = re.compile(r\'src=\".*?\"\')\n        _list = []\n        for each in response.doc(\'#fontzoom\').children().items():\n            if not each.html():\n                continue\n            #print each.html()\n            if \'<script>\' in each.html() or u\'中华会计网校\' in each.text():\n                continue\n            if u\'我要纠错\' in each.text() or u\'责任编辑\' in each.text() or u\'编辑推荐\' in each.text() or u\'点击阅读\' in each.text():\n                break\n            if \'<img\' in each.html():\n                if cover == \'\':\n                    cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                _list.append(\'<p>\'+ each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        \n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"cover\": cover,\n            \"content\": content,\n            \"source\": source,\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"金融\",\n            \"date\": date,\n\n        }',NULL,1.0000,3.0000,1474537260.4060),('jinrong_xinlang_inc','jinrong','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 17:41:23\n# Project: jinrong_xinlang_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nurls = {\n    \'http://roll.finance.sina.com.cn/finance/zq1/index_\': [u\'证券从业\'],\n    \'http://roll.finance.sina.com.cn/finance/yh/index_\': [u\'银行从业\'],\n    \'http://roll.finance.sina.com.cn/finance/jj4/index_\': [u\'基金从业\'],\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url,bread in urls.iteritems():\n            for i in range(1, 2):\n                self.crawl(url + str(i) +\'.shtml\', save = {\'bread\': bread}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.list_009 > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        date = \'\'\n        source = u\'新浪财经\'\n        if response.doc(\'#pub_date\'):\n            date = response.doc(\'#pub_date\').text().strip().split()[0].split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            source = response.doc(\'#media_name\').text().strip()\n        elif response.doc(\'.time-source\'):\n            date = response.doc(\'.time-source\').text().strip().split()[0].split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            source = response.doc(\'.time-source\').text().strip().split()[1]\n        elif response.doc(\'.articalTitle > .SG_txtc\'):\n            date = response.doc(\'.articalTitle > .SG_txtc\').text().strip().split()[0].replace(\'(\',\'\')\n        cover = \'\' \n        pattern = re.compile(r\'src=\".*?\"\')\n        _list = []\n        flag_first = False\n        for each in response.doc(\'.article_16\').children().items():\n            if not each.html():\n                continue\n            #print each.html()\n            flag_first = True\n            if \'<script>\' in each.html() or u\'新浪财经\' in each.text() or \'finance_app_zqtg\' in each.html():\n                continue\n            if u\'新浪声明\' in each.text() or u\'新浪财经股吧\' in each.text():\n                break\n            if \'<img\' in each.html():\n                if cover == \'\':\n                    cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                _list.append(\'<p>\'+ each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        if not flag_first:\n            for each in response.doc(\'#artibody\').children().items():\n                if not each.html():\n                    continue\n                flag_first = True\n                #print each.html()\n                if \'<script>\' in each.html() or u\'新浪财经\' in each.text() or \'finance_app_zqtg\' in each.html():\n                    continue\n                if u\'新浪声明\' in each.text() or u\'新浪财经股吧\' in each.text():\n                    break\n                if \'<img\' in each.html():\n                    if cover == \'\':\n                        cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                    _list.append(\'<p>\'+ each.html()+\'</p>\')\n                elif each.text().strip() != \'\':\n                    _list.append(\'<p>\'+each.text()+\'</p>\')\n        if not flag_first:\n            for each in response.doc(\'.newfont_family\').children().items():\n                if not each.html():\n                    continue\n                #print each.html()\n                if \'<script>\' in each.html() or u\'新浪财经\' in each.text() or \'finance_app_zqtg\' in each.html():\n                    continue\n                if u\'新浪声明\' in each.text() or u\'新浪财经股吧\' in each.text():\n                    break\n                if \'<img\' in each.html():\n                    if cover == \'\':\n                        cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                    _list.append(\'<p>\'+ each.html()+\'</p>\')\n                elif each.text().strip() != \'\':\n                    _list.append(\'<p>\'+each.text()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"cover\": cover,\n            \"content\": content,\n            \"source\": source,\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"金融\",\n            \"date\": date,\n\n        }',NULL,1.0000,3.0000,1474537357.5615),('jita_10_youku_base',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-19 14:57:23\n# Project: jita_10_youku_base\n\nfrom pyspider.libs.base_handler import *\nimport time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.soku.com/search_video/q_%E5%90%89%E4%BB%96_limitdate_0?site=14&_lg=10&orderby=3\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.sk_pager a\').items():\n            url = each.attr.href\n            self.crawl(url, callback=self.index_page)\n        for each in response.doc(\'.v\').items():\n            cover = each.find(\'img\').attr.src\n           # print cover\n            url = each.find(\'.v-meta-title > a\').attr.href\n            title = each.find(\'.v-meta-title > a\').text()\n            self.crawl(url, save={\'title\': title, \'cover\': cover},  callback=self.detail_page)\n\n   # @config(priority=2)\n    @config(age=1*1)\n    def detail_page(self, response):\n        try:\n            url = response.url\n        #http://v.youku.com/v_show/id_XMTUxNzMxODAyMA==.html?from=s1.8-1-1.2\n            id = url.split(\'id_\')[1].split(\'==\')[0]\n            video_url_element = \'http://player.youku.com/embed/%s\' %(id)\n            video_url = []\n            video_url.append(video_url_element)\n            title = response.save[\'title\']\n            cover = response.save[\'cover\']\n           # title = response.doc(\'.base_info > .title\').text()\n            subject = u\'吉他\'\n            source = \'youku.com\'\n            publish_time = time.strftime(\'%Y-%m-%d\',time.localtime(time.time()))\n        except:\n            return None\n        if len(video_url) == 0:\n            return None\n        return {\n            \"url\": url,\n            \"video_url\":video_url,\n            \"title\": title,\n            \"subject\": subject,\n            \"source\": source,\n            \"publish_time\": publish_time,\n            \"cover\": cover,\n            \"class\": 10,\n            \"data_weight\":0,\n        }\n',NULL,5.0000,3.0000,1468374380.6862),('kaoyan_chinakaoyan_inc','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-11 19:41:56\n# Project: kaoyan_chinakaoyan\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        page_list = {\'11\': [u\'政策新闻\'],\n                     \'12\': [u\'报考指南\'],\n                     \'14\': [u\'招生信息\'],\n                     \'13\': [u\'择选院校\'],\n                     \'91\': [u\'专业介绍\'],\n                     \'22\': [u\'考研分数线\'],\n                     \'24\': [u\'调剂指南\'],\n                     \'26\': [u\'考研英语\'],\n                     \'25\': [u\'考研政治\'],\n                     \'27\': [u\'考研数学\'],\n                     \'32\': [u\'计算机\',u\'专业课\'],\n                     \'33\': [u\'教育学\',u\'专业课\'],\n                     \'34\': [u\'心理学\',u\'专业课\'],\n                     \'35\': [u\'历史学\',u\'专业课\'],\n                     \'47\': [u\'其他\',u\'专业课\'],\n                     \'65\': [u\'大纲解析\'],\n                     \'64\': [u\'大纲解析\'],\n                     \'63\': [u\'大纲解析\'],\n                     \'62\': [u\'大纲解析\'],\n                     \'61\': [u\'大纲解析\'],\n                     \'49\': [u\'成绩查询\'],\n                     \'50\': [u\'研招动态\'],\n                     \'9\': [u\'研招动态\'],\n                     \'31\': [u\'研招动态\'],\n                     \'10\': [u\'研招动态\'],\n                     }\n        for k,v in page_list.iteritems():\n            self.crawl(\'http://www.chinakaoyan.com/info/list/ClassID/%s.shtml\'%k, save={\'bread\': v}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.uc_ulbox a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n\n        #for each in response.doc(\'.dajax > a\').items():\n        #    self.crawl(each.attr.href,  save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        for each in response.doc(\'.arcont > .cont\').items():\n            content = each.html().replace(\'\\t\',\'\').replace(\'\\r\',\'\').replace(\'\\n\',\'\').strip()\n        if not content:\n            return None\n        dates = response.doc(\'.time\').text().split()\n        for d in dates:\n            if d[:3] == \'201\':\n                break\n        #bread = set(response.doc(\'.change_con a\').text().split()[1:-1])\n        #bread.add(response.save[\'bread\'])\n        try:\n            source = response.doc(\'.time\').remove(\'a\').text().split()[0].split(u\'：\')[-1]\n        except:\n            source = u\'中国考研网\'\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"date\": d,\n            \"subject\": u\'考研\',\n            \"source\": source,\n            \"content\": content,\n            \"bread\": response.save[\'bread\'],\n            \"cover\": \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471332804.5242),('kaoyan_cnky_inc','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-11 14:19:24\n# Project: kaoyan_cnky_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    headers = {}\n    \n    page_dict = {\n\n        \'http://www.cnky.net/fuxi/zhengzhi/index.shtml\':[u\'公共课\',u\'政治\'],\n        \'http://www.cnky.net/fuxi/yingyu/index.shtml\':[u\'公共课\',u\'英语\'],\n        \'http://www.cnky.net/fuxi/shuxue/index.shtml\':[u\'公共课\',u\'数学\'],\n        \'http://www.cnky.net/fuxi/kaoyanzhuanyeke/index.shtml\':[u\'专业课\',u\'其他\'],\n        \'http://www.cnky.net/fuxi/zhuanyeke/index.shtml\':[u\'备考准备\',u\'备考经验\'],\n        \'http://www.cnky.net/fuxi/mingshi/index.shtml\':[u\'备考准备\',u\'备考经验\']\n    }\n    \n    @every(minutes=1*60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'div#listcontent li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a\').text()\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            url = each.find(\'h2 > a\').attr.href\n            self.crawl(url, save = _dict ,callback=self.detail_page)\n       \n            \n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.g-pct p\').items():\n            info = each.html()\n            if info:\n                content_list.append(info)\n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join(\'<p>%s</p>\'%k for k in content_list if k and k.strip())\n       res_dict[\'source\'] = \'cnky.net\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'subject\'] = u\'考研\'\n       res_dict[\'url\'] = response.url\n       res_dict[\'class\'] = 46\n       return res_dict\n',NULL,1.0000,3.0000,1471332836.1628),('kaoyan_eol_inc','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-11 19:54:16\n# Project: kaoyan_kaoyan\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        _dict = {\n            \'zhinan\': u\'备考经验\',           \n            \'zhengzhi\': u\'考研政治\',\n            \'yingyu\': u\'考研英语\',\n            \'shuxue\': u\'考研数学\',\n            \'zhuan_ye_ke\': u\'专业课\',\n        }\n        #for i in range(2):\n        for k, v in _dict.iteritems():\n            self.crawl(\'http://kaoyan.eol.cn/fu_xi/%s/\'%(k),  save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://kaoyan.eol.cn/nnews/\', save={\'bread\': u\'研招动态\'}, callback=self.index_page)\n        self.crawl(\'http://kaoyan.eol.cn/bao_kao/zheng_ce_bian_hua/\', save={\'bread\': u\'政策新闻\'}, callback=self.index_page)\n        self.crawl(\'http://kaoyan.eol.cn/bao_kao/re_men/\', save={\'bread\': u\'高校动态\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.page_left li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'span\').text()\n            self.crawl(url,save={\'bread\': bread, \'date\': date}, callback=self.detail_page)\n        #for each in response.doc(\'.tPage > a\').items():\n        #    self.crawl(each.attr.href,save={\'bread\': bread}, callback=self.detail_page)\n\n    def strip(self, _str):\n        if not _str:\n            return _str\n        return _str.replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\').strip(\' \').replace(u\'考研帮\',\'\').replace(u\'帮学堂刷视频\',\'\').replace(u\'帮学堂\',\'\')\n    \n    @config(priority=2)\n    def detail_page(self, response):\n        bread = response.save[\'bread\']\n        date = response.save[\'date\']\n        if not date:\n            date = response.doc(\'.articleInfo\').text()[:10]\n        content = self.strip(response.doc(\'.TRS_Editor\').remove(\'img\').remove(\'a\').html())\n        if not content:\n            return None\n        try:\n            source = response.doc(\'.page_time\').text().split()[1].strip()\n        except:\n            source = \'eol\'\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.page_title\').text(),\n            \"date\": date,\n            \"bread\": [bread,],\n            \"content\": content,\n            \"subject\": u\'考研\',\n            \"source\": source,\n            \"cover\": \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471332814.6296),('kaoyan_kaoyan_inc','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-11 19:54:16\n# Project: kaoyan_kaoyan\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        _dict = {\n            \'baokao/zhinan\': u\'报考指南\',\n            \'baokao/jingyan\': u\'报考经验\',\n            \'xinwen/zhengce/\': u\'政策新闻\',\n            \'zhaosheng\': u\'招生信息\',\n            \'baokao/zexiao/\': u\'择选院校\',\n            \'beikao/jingyan/\': u\'备考经验\',\n            \'fushi/jingyan/\': u\'复试攻略\',\n            \'yingyu/zhenti/\': u\'考研英语\',\n            \'zhengzhi/zhenti/\': u\'考研政治\',\n            \'zhengzhi/dagang/\': u\'考研政治\',\n            \'zhengzhi/jingyan/\': u\'考研政治\',\n            \'yingyu/dagang/\': u\'考研英语\',\n            \'yingyu/jingyan/\': u\'考研英语\',\n            \'shuxue/jingyan/\': u\'考研数学\',\n            \'shuxue/dagang/\': u\'考研数学\',\n            \'shuxue/zhenti/\': u\'考研数学\',\n            \'zhuanyeke/zhenti/\': u\'专业课\',\n            \'zhuanyeke/jingyan/\': u\'专业课\',\n            \'zhuanyeke/dagang/\': u\'专业课\',\n        }\n        for k, v in _dict.iteritems():\n            self.crawl(\'http://www.kaoyan.com/%s/\'%k, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://tiaoji.kaoyan.com/xinxi/\', save={\'bread\': u\'调剂指南\'}, callback=self.index_page)\n        self.crawl(\'http://mba.kaoyan.com/beikao/\', save={\'bread\': u\'MBA\'}, callback=self.index_page)\n        self.crawl(\'http://mba.kaoyan.com/zixun/\', save={\'bread\': u\'MBA\'}, callback=self.index_page)\n        self.crawl(\'http://mba.kaoyan.com/baokao/\', save={\'bread\': u\'MBA\'}, callback=self.index_page)\n        self.crawl(\'http://mpacc.kaoyan.com/tiaoji/\', save={\'bread\': u\'MPAcc\'}, callback=self.index_page)\n        self.crawl(\'http://mpacc.kaoyan.com/baokao/\', save={\'bread\': u\'MPAcc\'}, callback=self.index_page)\n        self.crawl(\'http://mpacc.kaoyan.com/beikao/\', save={\'bread\': u\'MPAcc\'}, callback=self.index_page)\n        self.crawl(\'http://mpacc.kaoyan.com/fushi/\', save={\'bread\': u\'MPAcc\'}, callback=self.index_page)\n        self.crawl(\'http://www.kaoyan.com/jianzhang/\', save={\'bread\': u\'招生简章\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.areaZslist > li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'.fr\').text()\n            self.crawl(url,save={\'bread\': bread, \'date\': date}, callback=self.detail_page)\n        #for each in response.doc(\'.tPage > a\').items():\n        #    self.crawl(each.attr.href,save={\'bread\': bread}, callback=self.index_page)\n\n    def strip(self, _str):\n        if not _str:\n            return _str\n        return _str.replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\').strip(\' \').replace(u\'考研帮\',\'\').replace(u\'帮学堂刷视频\',\'\').replace(u\'帮学堂\',\'\')\n\n    @config(priority=2)\n    def detail_page(self, response):\n        bread = response.save[\'bread\']\n        date = response.save[\'date\']\n        if not date:\n            date = response.doc(\'.articleInfo\').text()[:10]\n        content = self.strip(response.doc(\'.articleCon\').html())\n        if not content:\n            return None\n        title = response.doc(\'.articleTitle\').text()\n        if u\'汇总\' in title:\n            return None\n        try:\n            source = response.doc(\'.ml30\').text().split()[0].strip()\n        except:\n            source = u\'考研帮\'\n        if source == u\'本站原创\':\n            source = u\'考研帮\'\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"date\": date,\n            \"bread\": [bread,],\n            \"content\": content,\n            \"subject\": u\'考研\',\n            \"source\": source,\n            \"cover\": \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471332818.5245),('kaoyan_yuloo','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-24 16:11:21\n# Project: kaoyan_yuloo\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    dic = {\n        \'political/zhidao/\': u\'考研政治\',\n        \'political/jyfd/\': u\'考研政治\',\n        #\'political/zhenti/\': u\'政治\',\n        \'english/zhidao/\': u\'考研英语\',\n        \'english/jyfd/\': u\'考研英语\',\n        #\'english/zhenti/\': u\'英语\',\n        \'math/zhidao/\': u\'考研数学\',\n        \'math/jyfd/\': u\'考研数学\',\n        #\'math/zhenti/\': u\'数学\',\n        \'zyss/\': u\'专业课\',\n        \'dagang/zhuanyeke/\': u\'专业课\',\n        \'kyjy\': u\'备考经验\',\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k, v in self.dic.iteritems():\n            self.crawl(\'http://www.yuloo.com/kaoyan/%s\'%k, save={\'key\': v}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.left .clearfix li\').items():\n            key = response.save[\'key\']\n            url = each.find(\'a\').attr.href\n            date = each.find(\'span\').text()\n            if date[:4] >= \'2015\':\n                self.crawl(url, save={\'key\': key, \'date\': date}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        bread = response.save[\'key\']\n        content = response.doc(\'.jiathis_streak\').html().replace(\'\\n\',\'\')\n        if not content:\n            return None\n        source = response.doc(\'.top_h2 > p\').text().split(u\'发布时间\')[0].split(\':\')[-1].lstrip().rstrip()\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"date\": response.save[\'date\'],\n            \"bread\": [bread,],\n            \"source\": source if source else u\'育路考研网\',\n            \"subject\": u\'考研\',\n            \"class\": 46,\n            \"data_weight\": 0,\n            \"content\": content,\n            \'cover\': \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n        }\n',NULL,1.0000,3.0000,1471332826.2625),('keyword_monitor',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-05 10:17:49\n# Project: baidu_keyword_2\n\nfrom pyspider.libs.base_handler import *\nfrom urllib import quote, unquote\nimport redis\nimport urlparse\nimport json\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nclass Handler(BaseHandler):\n\n    crawl_config = {\n        \'itag\':\'0.1\',\n        \"headers\": {\n        \'Host\': \'www.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.84 Safari/537.36\'\n        }\n    }\n\n    def __init__(self):\n        self.r = redis.StrictRedis(host=\'localhost\', port=6379, db=13,charset=\'utf－8\')\n        \n        self.words = {}\n        for line in open(\'/apps/home/worker/xuzhihao/keyword2\'):\n            (word, info) = line.strip(\'\\n\').split(\'\\t\')\n            data = json.loads(info)\n            #self.words[word] = {k: {\'title\': v, \'find\': 0} for k, v in data.iteritems()}\n            self.words[u\'%s\'%word] = {}\n            for k, v in data.iteritems():\n                self.words[u\'%s\'%word][k] = {\'title\': v, \'find\': 0}\n        \'\'\'\n        self.words = {u\'成都MPA培训\':\n                          {\'http://www.genshuixue.com/bj/st--878_1116.html\':\n                               {\'title\': u\'【成都MPA培训|成都MPA辅导机构|成都MPA培训班费用】-跟谁学成都站\',\n                                \'find\': 0\n                                }\n                          },\n                    }\n        \'\'\'\n\n    @every(minutes=24*60)\n    def on_start(self):\n        for word, word_info in self.words.iteritems():\n            self.crawl(\'https://www.baidu.com/s?wd=%s&rsv_spt=1&rsv_iqid=0xffd5385a0000af37&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=5&rsv_sug1=3&rsv_sug7=100\'%(word), save={\"word\": word, \"word_info\": word_info,},  callback=self.index_page)\n\n    def judge(self, data):\n        for k, v in data.iteritems():\n            if v[\'find\'] == 0:\n                return False\n        return True\n\n    @config(priority=2)\n    def index_page(self, response):\n        try:\n            word = response.save[\'word\']\n            word_info = response.save[\'word_info\']\n        except Exception, e:\n            return\n\n        for each in response.doc(\'#page a\').items():\n            if self.r.get(word):\n                #print \'redis in\'\n                return\n            if \'pn\' in each.attr.href:\n                _parse = urlparse.urlparse(each.attr.href)\n                keys = urlparse.parse_qs(_parse.query)\n                try:\n                    pn = int(keys[\'pn\'][0])\n                except Exception, e:\n                    pn = -1\n                r_key = \'%s_%d\'%(word, pn)\n                #print r_key, self.r.get(r_key)\n                if not self.r.get(r_key):\n                    self.r.set(r_key, 1)\n                    self.r.expire(r_key, 10*60*60)\n                    self.crawl(each.attr.href, save={\"pn\": pn, \"word\": word, \"word_info\": word_info}, callback=self.index_page)\n\n        for index, each in enumerate(response.doc(\'.result\').items()):\n            target_url = each.find(\'.f13 a\').text()\n            #print target_url\n            res ={\n                    \"pn\": None,\n                    \"index\": index,\n                    \"word\": word,\n                    \"word_info\": None,\n                    \"target_title\": each.text(),\n                    \"url\":None\n                }\n\n            if \'genshuixue\' in target_url:\n                try:\n                    pn = response.save[\'pn\']\n                except Exception, e:\n                    pn = 0\n                bd_title = each.find(\'.t\').text().replace(\' \',\'\')\n                if bd_title[-3:] == \'...\':\n                    bd_title = bd_title[:-3]\n                for k, v in word_info.iteritems():\n                    if u\'%s\'%bd_title in v[\'title\']:\n                        #print bd_title\n                        res[\'pn\'] = pn\n                        res[\'word_info\'] = {k: v}\n                        #print self.words\n                        self.words[word][k][\'find\'] = 1\n                        res[\'url\'] = k\n                        if self.judge(self.words[word]):\n                            #print word\n                            self.r.set(word, 1)\n                            self.r.expire(word, 60)\n                        return res\n',NULL,10.0000,10.0000,1467701059.6946),('keyword_monitor_accurate',NULL,'STOP','# -*- encoding: utf-8 -*-\n# Created on 2016-07-04 15:44:10\n# Project: keyword_monitor_accurate\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport urlparse\nimport time\nimport json\nimport MySQLdb\n\nclass ConnectionUtil(object):\n\n    def __init__(self,connection):\n        self.connection = connection\n\n    def cursor(self):\n        if self.connection:\n            self.cursor  = self.connection.cursor()\n            return self.cursor\n        else:\n            return  None\n\n    def __enter__(self):\n        return self.cursor()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n            #print \'ok\'\n            if self.connection:\n                self.connection.commit()\n            if self.cursor:\n                self.cursor.close()\n            if self.connection:\n                self.connection.close()\n            if exc_type is not None:\n                #print \'errror\'\n                print exc_val\n                #traceback.print_exc()\n                return True\n\ndef qs(url):\n    query = urlparse.urlparse(url).query\n    return dict([(k,v[0]) for k,v in urlparse.parse_qs(query).items()])\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.1\',\n        \"headers\": {\n        \'Host\': \'www.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n\n    index_dict = {}\n    \n    @every(minutes=3 * 24 * 60)\n    def on_start(self):\n        with open(\'/apps/home/rd/hexing/data/keyword\',\'r\') as f:\n            for line in f:\n                if line :\n                    line = line.strip()\n                    arr = line.split(\'\\t\')\n                    for index in range(10):\n                        self.crawl(\'https://www.baidu.com/s?wd=%s&pn=%s&rsv_spt=1&rsv_iqid=0xa0eaa7930001b982&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=1&rsv_sug2=0&inputT=995&rsv_sug4=995\'%(arr[0],index*10),save = {\'query\':arr[0],\'info\':json.loads(arr[1])},priority = 100 - index * 10,callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n      if response.save.get(\'query\') not in self.index_dict:\n            #time.sleep(1)\n            for index , each in enumerate(response.doc(\'.result\').items()):\n                _dict = {}\n                _dict[\'title\'] = each.find(\'h3 a\').text()\n                _dict[\'query\'] = response.save.get(\'query\')\n                _dict[\'url\'] = each.find(\'.f13 a\').text()\n                _dict[\'info\'] = response.save.get(\'info\')\n                page_dict = qs(response.url)\n                if \'pn\' not in page_dict:\n                    pn = 0\n                else:\n                    pn = page_dict[\'pn\']\n                for k,v in response.save.get(\'info\').items():\n                    if each.find(\'h3 a\').text().replace(\'...\',\'\').replace(\' \',\'\') in v:\n\n                        rank = index + 1 + int(pn) \n                        self.index_dict[response.save.get(\'query\')] = rank\n                        _dict[\'rank\'] = rank\n                        _dict[\'accurate_url\'] =  k\n                        return _dict\n               \n\n    @config(priority=2)\n    def detail_page(self, response):\n        #res_dict = response.save\n        #res_dict[\'index\'] = self.index_dict[response.save.get(\'query\')]\n        #return res_dict\n        pass',NULL,5.0000,5.0000,1469408683.4581),('keyword_monitor_fuzzy','keyword','RUNNING','# -*- encoding: utf-8 -*-\n# Created on 2016-07-04 15:44:10\n# Project: keyword_monitor_m\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport urlparse\nimport time\nimport json\nfrom urllib import quote \n\n\ndef qs(url):\n    query = urlparse.urlparse(url).query\n    return dict([(k,v[0]) for k,v in urlparse.parse_qs(query).items()])\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'v0.1\',\n        \"headers\": {\n        \'Host\': \'www.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n\n    index_dict = {}\n    \n    @every(minutes=5*24*60)\n    def on_start(self):\n       with open(\'/apps/home/rd/hexing/data/keyword.txt\',\'r\') as f:\n            for line in f:\n                if line :\n                    line = line.strip()\n                    #arr = line.split(\'\\t\')\n                    for index in range(3):\n                        self.crawl(\'https://www.baidu.com/s?wd=%s&pn=%s&rsv_spt=1&rsv_iqid=0xa0eaa7930001b982&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=1&rsv_sug2=0&inputT=995&rsv_sug4=995\'%(line,index*10),save = {\'query\':line },priority = 100 - index * 10,callback=self.index_page)\n                      \n\n    @config(age=1*1)\n    def index_page(self, response):\n      if response.save.get(\'query\') not in self.index_dict:\n            for index , each in enumerate(response.doc(\'.result\').items()):\n                _dict = {}\n                _dict[\'title\'] = each.find(\'h3 a\').text()\n                _dict[\'query\'] = response.save.get(\'query\')\n                _dict[\'url\'] = each.find(\'.f13 a\').text()\n                page_dict = qs(response.url)\n                if \'pn\' not in page_dict:\n                    pn = 0\n                else:\n                    pn = page_dict[\'pn\']\n                if \'genshuixue\' in _dict[\'url\']:\n                    self.index_dict[response.save.get(\'query\')] = index + 1 + int(pn)      \n                    _dict[\'rank\'] = index + 1 +int(pn)                   \n                    return _dict\n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        pass',NULL,5.0000,5.0000,1473476865.5031),('korean_hujiang','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 18:49:43\n# Project: korean_hujiang\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.hujiang.com/new/rumen/\':[u\'德语基础\'],\n            \'http://de.hujiang.com/new/shiyong/\':[u\'实用德语\'],\n            \'http://de.hujiang.com/new/yule/\':[u\'德语文化\'],\n            \'http://de.hujiang.com/new/topic/627/\':[u\'德国留学\'],\n            \'http://de.hujiang.com/new/topic/1024/\':[u\'德语考试\']\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://kr.hujiang.com/\',callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.jnav_li_title > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页   \n        for each in response.doc(\'.page_list > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n         \n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n      \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = u\'韩语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'date\'] = response.doc(\'.page_tip > span\').eq(-1).text().strip().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n       return res_dict',NULL,1.0000,3.0000,1475139414.2915),('korean_hujiang_inc','korean','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 18:49:43\n# Project: korean_hujiang\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.hujiang.com/new/rumen/\':[u\'德语基础\'],\n            \'http://de.hujiang.com/new/shiyong/\':[u\'实用德语\'],\n            \'http://de.hujiang.com/new/yule/\':[u\'德语文化\'],\n            \'http://de.hujiang.com/new/topic/627/\':[u\'德国留学\'],\n            \'http://de.hujiang.com/new/topic/1024/\':[u\'德语考试\']\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://kr.hujiang.com/\',callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.jnav_li_title > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n    \n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页   \n        #for each in response.doc(\'.page_list > a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n         \n           # self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n      \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = u\'韩语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'date\'] = response.doc(\'.page_tip > span\').eq(-1).text().strip().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n       return res_dict',NULL,1.0000,3.0000,1475139420.6059),('korean_tingroom','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-29 09:58:40\n# Project: korean_tingroom\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.tingroom.com/yuedu/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/yufa/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/cihui/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/kouyu/\':[u\'实用德语\'],\n            \'http://de.tingroom.com/gequ/\':[u\'德语文化\'],\n            \'http://de.tingroom.com/liuxue/\':[u\'德国留学\'],\n            \'http://de.tingroom.com/kaoshi/\':[u\'德语考试\']\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://kr.tingroom.com/\',callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.topmenu li a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.index1_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def index1_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n            \n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'p > span\').eq(0).text().strip()\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.pages > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'p > span\').eq(0).text().strip()\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.pages > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       \n       res_dict[\'content\'] = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.content > .content\').html().strip().replace(\'\\t\',\'  \').replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = u\'韩语\'\n       res_dict[\'source\'] = u\'韩语学习网\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'bread\'] = [u\'文章资讯\']\n       return res_dict\n\n',NULL,1.0000,3.0000,1475198631.0400),('korean_tingroom_inc','korean','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-29 09:58:40\n# Project: korean_tingroom\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.tingroom.com/yuedu/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/yufa/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/cihui/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/kouyu/\':[u\'实用德语\'],\n            \'http://de.tingroom.com/gequ/\':[u\'德语文化\'],\n            \'http://de.tingroom.com/liuxue/\':[u\'德国留学\'],\n            \'http://de.tingroom.com/kaoshi/\':[u\'德语考试\']\n    }\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://kr.tingroom.com/\',callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.topmenu li a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.index1_page)\n    \n    @config(age=1)\n    def index1_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list_page)\n            \n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n            \n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'p > span\').eq(0).text().strip()\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pages > a\').items():\n         #   self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n            \n    @config(age=1)\n    def list1_page(self, response):\n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'p > span\').eq(0).text().strip()\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pages > a\').items():\n         #   self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       \n       res_dict[\'content\'] = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.content > .content\').html().strip().replace(\'\\t\',\'  \').replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = u\'韩语\'\n       res_dict[\'source\'] = u\'韩语学习网\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'bread\'] = [u\'文章资讯\']\n       return res_dict\n\n',NULL,1.0000,3.0000,1475198813.4844),('kuaiji_dongaokuaiji_inc','kuaiji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 16:06:51\n# Project: kuaiji_dongaokuaiji_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nkey_url =[\n    #\'http://chuji.dongao.com/zchjsdh/kstk/mryl/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/lnzt/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/tblx/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/mnst/\',\n    \'http://zhongji.dongao.com/ghzcdh/kstk/lnzt/\',\n    #\'http://zhongji.dongao.com/ghzcdh/kstk/mryl/\',\n    \'http://zhongji.dongao.com/ghzcdh/kstk/tblx/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/lnzt/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/mryl/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/tblx/\',\n    \'http://zhukuai.dongao.com/zchjsdh/kstk/lnzt/\',\n    #\'http://zhukuai.dongao.com/zchjsdh/kstk/mryl/\',\n    \'http://zhukuai.dongao.com/zchjsdh/kstk/tblx/\',\n    \'http://gaoji.dongao.com/ghzcdh/kstk/lnzt/\',\n    #\'http://gaoji.dongao.com/ghzcdh/kstk/mryl/\',\n    \'http://congye.dongao.com/qg/kstk/lnzt/\',\n    #\'http://congye.dongao.com/qg/kstk/mryl/\',\n    \'http://congye.dongao.com/qg/kstk/mnks/\',\n]\n\ntype_dict = {\n            \'chuji\':[u\'财会经济\',u\'初级会计师\'],\n            \'zhongji\':[u\'财会经济\',u\'中级会计师\'],\n            \'shiwushi\':[u\'财会经济\',u\'注册税务师\'],\n            \'zhukuai\':[u\'财会经济\',u\'注册会计师\'],\n            \'gaoji\':[u\'财会经济\',u\'高级会计师\'],\n            \'congye\':[u\'财会经济\',u\'会计从业\'],\n            u\'注册会计师\':[u\'财会经济\',u\'注册会计师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'初级会计职称\':[u\'财会经济\',u\'初级会计师\'],\n            u\'中级会计职称\':[u\'财会经济\',u\'中级会计师\'],\n            u\'税务师\':[u\'财会经济\',u\'注册税务师\'],\n            u\'统计师\':[u\'财会经济\',u\'统计师\'],\n            u\'审计师\':[u\'财会经济\',u\'审计师\'],\n            u\'高级经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'高级会计\':[u\'财会经济\',u\'高级会计师\'],\n            u\'理财规划师\':[u\'财会经济\',u\'理财规划师\'],\n\n\n            u\'英语四级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'英语六级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'雅思\':[u\'外语考试\',u\'雅思\'],\n            u\'托福\':[u\'外语考试\',u\'托福\'],\n            u\'职称英语\':[u\'外语考试\',u\'职称英语\'],\n            u\'商务英语\':[u\'外语考试\',u\'商务英语\'],\n            u\'公共英语\':[u\'外语考试\',u\'公共英语\'],\n            u\'日语\':[u\'外语考试\',u\'日语\'],\n            u\'GRE考试\':[u\'外语考试\',u\'GRE考试\'],\n            u\'专四专八\':[u\'外语考试\',u\'专四专八\'],\n            u\'口译笔译\':[u\'外语考试\',u\'口译笔译\'],\n\n            u\'一级建造师\':[u\'建筑工程\',u\'一级建造师\'],\n            u\'二级建造师\':[u\'建筑工程\',u\'二级建造师\'],\n            u\'咨询工程师\':[u\'建筑工程\',u\'咨询工程师\'],\n            u\'造价工程师\':[u\'建筑工程\',\'造价工程师\'],\n            u\'结构工程师\':[u\'建筑工程\',\'结构工程师\'],\n            u\'物业管理\':[u\'建筑工程\',u\'物业管理师\'],\n            u\'城市规划\':[u\'建筑工程\',u\'城市规划师\'],\n            u\'给排水工程\':[u\'建筑工程\',u\'给排水工程\'],\n            u\'电气工程\':[u\'建筑工程\',u\'电气工程师\'],\n            u\'公路监理师\':[u\'建筑工程\',u\'公路监理师\'],\n            u\'消防工程师\':[u\'建筑工程\',u\'消防工程师\'],\n\n            u\'物流师\':[u\'职业资格\',u\'物流师\'],\n            u\'人力资源\':[u\'职业资格\',u\'人力资源\'],\n            u\'心理咨询师\':[u\'职业资格\',u\'心理咨询师\'],\n            u\'公共营养师\':[u\'职业资格\',u\'公共营养师\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'证券从业资格\':[u\'职业资格\',u\'证券经纪人\'],\n            u\'电子商务师\':[u\'职业资格\',u\'电子商务\'],\n            u\'期货从业\':[u\'职业资格\',u\'期货从业\'],\n            u\'教师资格\':[u\'职业资格\',u\'教师资格\'],\n            u\'管理咨询师\':[u\'职业资格\',u\'管理咨询师\'],\n            u\'导游证\':[u\'职业资格\',u\'导游证\'],\n            \n            u\'英语\':[u\'学历教育\',u\'考研\'],\n            u\'数学\':[u\'学历教育\',u\'考研\'],\n            u\'政治\':[u\'学历教育\',u\'考研\'],\n            u\'专业课\':[u\'学历教育\',u\'考研\'],\n\n            u\'执业医师\':[u\'医学卫生\',u\'执业医师\'],\n            u\'执业药师\':[u\'医学卫生\',u\'执业药师\'],\n            u\'临床医师\':[u\'医学卫生\',u\'临床执业\'],\n            u\'中医医师\':[u\'医学卫生\',u\'中医执业\'],\n            u\'中西医医师\':[u\'医学卫生\',u\'中西医执业\'],\n            u\'中医助理\':[u\'医学卫生\',u\'中医助理\'],\n            u\'中西医助理\':[u\'医学卫生\',u\'中西医助理\'],\n            u\'主治\':[u\'医学卫生\',u\'主治\'],\n            u\'检验\':[u\'医学卫生\',u\'检验\'],\n            u\'执业护士资格\':[u\'医学卫生\',u\'执业护士\'],\n\n\n            u\'成人高考\':[u\'学历教育\',u\'成人高考\'],\n            u\'自学考试\':[u\'学历教育\',u\'自考\'],\n            u\'MBA考试\':[u\'学历教育\',u\'MBA\'],\n            u\'法律硕士\':[u\'学历教育\',u\'法律硕士\'],\n            u\'专升本\':[u\'学历教育\',u\'专升本\'],\n            #u\'\':[u\'学历教育\',u\'工程硕士\'],\n            u\'MPA考试\':[u\'学历教育\',u\'公共硕士\'],\n            #u\'\':[u\'学历教育\',u\'考研\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\': \'0.1\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for href in key_url:\n            _dict = {}\n            if href.split(\'//\')[1].split(\'.\')[0] in type_dict:\n                _dict[\'bread\'] = type_dict[href.split(\'//\')[1].split(\'.\')[0]]\n                self.crawl(href, save = _dict, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.column_list li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'/\', \'-\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        #for each in response.doc(\'.page_number a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n        #for each in response.doc(\'.showpage a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'.pabt-30\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'相关推荐\' in each1.text():\n                    continue\n                if each1.find(\'b\'):\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())  \n                    \n        for each in response.doc(\'.article\').items():\n            for each1 in each.find(\'.content > p\').items():\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'相关推荐\' in each1.text():\n                    continue\n                if each1.find(\'b\'):\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())   \n     \n                    \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'东奥\',\'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'), \n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',\'\').replace(\'None\',\'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',\'\').replace(\'None\',\'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'None\',\'\') + str(response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'None\',\'\'),\n                \"tdk_title\":response.doc(\'head > title\').eq(0).text().replace(u\'东奥会计在线\',\'\') + u\'跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"东奥会计在线\",\n\n            }\n',NULL,1.0000,3.0000,1472546254.4227),('kuaiji_zhengbao_inc','kuaiji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 16:11:21\n# Project: kuaiji_zhengbao_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ntype_dict = {\n            \'chuji\':[u\'财会经济\',u\'初级会计师\'],\n            \'chujizhicheng\':[u\'财会经济\',u\'初级会计师\'],\n            \'zhongji\':[u\'财会经济\',u\'中级会计师\'],\n            \'zhongjizhicheng\':[u\'财会经济\',u\'中级会计师\'],\n            \'shiwushi\':[u\'财会经济\',u\'注册税务师\'],\n            \'zhukuai\':[u\'财会经济\',u\'注册会计师\'],\n            \'gaoji\':[u\'财会经济\',u\'高级会计师\'],\n            \'congye\':[u\'财会经济\',u\'会计从业\'],\n            u\'注册会计师\':[u\'财会经济\',u\'注册会计师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'初级会计职称\':[u\'财会经济\',u\'初级会计师\'],\n            u\'中级会计职称\':[u\'财会经济\',u\'中级会计师\'],\n            u\'税务师\':[u\'财会经济\',u\'注册税务师\'],\n            u\'统计师\':[u\'财会经济\',u\'统计师\'],\n            u\'审计师\':[u\'财会经济\',u\'审计师\'],\n            u\'高级经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'高级会计\':[u\'财会经济\',u\'高级会计师\'],\n            u\'理财规划师\':[u\'财会经济\',u\'理财规划师\'],\n\n\n            u\'英语四级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'英语六级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'雅思\':[u\'外语考试\',u\'雅思\'],\n            u\'托福\':[u\'外语考试\',u\'托福\'],\n            u\'职称英语\':[u\'外语考试\',u\'职称英语\'],\n            u\'商务英语\':[u\'外语考试\',u\'商务英语\'],\n            u\'公共英语\':[u\'外语考试\',u\'公共英语\'],\n            u\'日语\':[u\'外语考试\',u\'日语\'],\n            u\'GRE考试\':[u\'外语考试\',u\'GRE考试\'],\n            u\'专四专八\':[u\'外语考试\',u\'专四专八\'],\n            u\'口译笔译\':[u\'外语考试\',u\'口译笔译\'],\n\n            \'zaojia\':[u\'建筑工程\',\'造价工程师\'],\n            u\'一级建造师\':[u\'建筑工程\',u\'一级建造师\'],\n            u\'二级建造师\':[u\'建筑工程\',u\'二级建造师\'],\n            u\'咨询工程师\':[u\'建筑工程\',u\'咨询工程师\'],\n            u\'造价工程师\':[u\'建筑工程\',\'造价工程师\'],\n            u\'结构工程师\':[u\'建筑工程\',\'结构工程师\'],\n            u\'物业管理\':[u\'建筑工程\',u\'物业管理师\'],\n            u\'城市规划\':[u\'建筑工程\',u\'城市规划师\'],\n            u\'给排水工程\':[u\'建筑工程\',u\'给排水工程\'],\n            u\'电气工程\':[u\'建筑工程\',u\'电气工程师\'],\n            u\'公路监理师\':[u\'建筑工程\',u\'公路监理师\'],\n            u\'消防工程师\':[u\'建筑工程\',u\'消防工程师\'],\n            u\'消防\':[u\'建筑工程\',u\'消防工程师\'],\n\n            u\'物流师\':[u\'职业资格\',u\'物流师\'],\n            u\'人力资源\':[u\'职业资格\',u\'人力资源\'],\n            u\'心理咨询师\':[u\'职业资格\',u\'心理咨询师\'],\n            u\'公共营养师\':[u\'职业资格\',u\'公共营养师\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'证券从业资格\':[u\'职业资格\',u\'证券经纪人\'],\n            u\'电子商务师\':[u\'职业资格\',u\'电子商务\'],\n            u\'期货从业\':[u\'职业资格\',u\'期货从业\'],\n            u\'教师资格\':[u\'职业资格\',u\'教师资格\'],\n            u\'管理咨询师\':[u\'职业资格\',u\'管理咨询师\'],\n            u\'导游证\':[u\'职业资格\',u\'导游证\'],\n            \n            u\'英语\':[u\'学历教育\',u\'考研\'],\n            u\'数学\':[u\'学历教育\',u\'考研\'],\n            u\'政治\':[u\'学历教育\',u\'考研\'],\n            u\'专业课\':[u\'学历教育\',u\'考研\'],\n\n            u\'执业医师\':[u\'医学卫生\',u\'执业医师\'],\n            u\'执业药师\':[u\'医学卫生\',u\'执业药师\'],\n            u\'临床医师\':[u\'医学卫生\',u\'临床执业\'],\n            u\'中医医师\':[u\'医学卫生\',u\'中医执业\'],\n            u\'中西医医师\':[u\'医学卫生\',u\'中西医执业\'],\n            u\'中医助理\':[u\'医学卫生\',u\'中医助理\'],\n            u\'中西医助理\':[u\'医学卫生\',u\'中西医助理\'],\n            u\'主治\':[u\'医学卫生\',u\'主治\'],\n            u\'检验\':[u\'医学卫生\',u\'检验\'],\n            u\'执业护士资格\':[u\'医学卫生\',u\'执业护士\'],\n\n\n            u\'成人高考\':[u\'学历教育\',u\'成人高考\'],\n            u\'自学考试\':[u\'学历教育\',u\'自考\'],\n            u\'MBA考试\':[u\'学历教育\',u\'MBA\'],\n            u\'法律硕士\':[u\'学历教育\',u\'法律硕士\'],\n            u\'专升本\':[u\'学历教育\',u\'专升本\'],\n            #u\'\':[u\'学历教育\',u\'工程硕士\'],\n            u\'MPA考试\':[u\'学历教育\',u\'公共硕士\'],\n            #u\'\':[u\'学历教育\',u\'考研\'],\n}\n\n\nsome_url =[\n    \'http://www.chinaacc.com/chujizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/chujizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/\',\n    \'http://www.chinaacc.com/zaojia/zt/\',\n    \'http://www.chinaacc.com/zaojia/mnst/\',    \n]\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url in some_url:\n            _dict = {}\n            _dict[\'bread\'] = type_dict[url.split(\'/\')[3]]\n            self.crawl(url, save = _dict, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.xinxi li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.fl > a\').text()\n            if \'报名\' in _dict[\'title\'] or \'名师\' in _dict[\'title\'] or \'汇总\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'.fr\').text().replace(\'[\',\'\').replace(\']\',\'\')\n            self.crawl(each.find(\'.fl > a\').attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页 \n        #for each in response.doc(\'divpagestrcjzc a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'#fontzoom\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'全网首发\' in each1.text():\n                    continue\n                if u\'估分更准确\' in each1.text():\n                    continue\n                if u\'独家\' in each1.text():\n                    continue\n                if u\'点击查看\' in each1.text():\n                    continue\n                if u\'点击参与\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text() or u\'推荐阅读\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'以上\' in each1.text() and u\'是中华会计网\' in each1.text():\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            \n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\', \'\').replace(\'\\n\', \'\').replace(\'\\t\', \'\').replace(u\'中华会计网校\', \'\').replace(\'【 】\', \'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'),\n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'中华会计网校\', \'\').replace(\n                    \'None\', \'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'中华会计网校\', \'\').replace(\n                    \'None\', \'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'中华会计网校\', u\'\').replace(\n                    \'建设网校\', \'\').replace(\'None\', \'\').replace(\'建设工程教育网\', \'\') + str(response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(\n                    u\'中华会计网校\', \'\').replace(\'建设网校\', \'\').replace(\'None\', \'\').replace(\'建设工程教育网\', \'\'),\n                \"tdk_title\": response.doc(\'head > title\').eq(0).text().replace(u\'中华会计网校\', \'\') + u\' 跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"中华会计网校\",\n            }\n\n',NULL,1.0000,3.0000,1472546258.5260),('kztk_qnr_inc','kztk','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 15:42:14\n# Project: kztk_qnr_inc\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-28 09:43:57\n# Project: kaozhengtiku_qingnianrenwang\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ndalei = [\n    u\'建筑工程\',\n    u\'财会经济\',\n    u\'医学卫生\',\n    u\'外语考试\',\n    u\'职业资格\',\n    u\'学历教育\',\n    u\'计算机考试\'\n]\nxiaolei = {\n            u\'一级建造师\':u\'一级建造师\',\n            u\'二级建造师\':u\'二级建造师\',\n            u\'监理工程师\':u\'监理工程师\',\n            u\'咨询工程师\':u\'咨询工程师\',\n            u\'造价工程师\':u\'造价工程师\',\n            u\'结构工程师\':u\'结构工程师\',\n            u\'电气工程师\':u\'电气工程师\',\n            u\'物业管理师\':u\'物业管理师\',\n            u\'经济师\':u\'经济师\',\n            u\'设计师\':u\'设计师\',\n            u\'初级会计\':u\'初级会计师\',\n            u\'中级会计师\':u\'中级会计师\',\n            u\'注册会计师\':u\'注册会计师\',\n            u\'统计师\':u\'统计师\',\n            u\'审计师\':u\'审计师\',\n            u\'注册税务师\':u\'注册税务师\',\n            u\'执业药师\':u\'执业药师\',\n            u\'执业药师\':u\'执业药师\',\n            u\'执业护士\':u\'执业护士\',\n            u\'临床执业\':u\'临床执业\',\n            u\'中西医执业\':u\'中西医执业\',\n            u\'中医执业\':u\'中医执业\',\n            u\'主治\':u\'主治\',\n            u\'检验\':u\'检验\',\n            u\'英语四级\':u\'英语四六级\',\n            u\'英语六级\':u\'英语四六级\',\n            u\'雅思\':u\'雅思\',\n            u\'托福\':u\'托福\',\n            u\'GRE考试\':u\'GRE考试\',\n            u\'职称英语\':u\'职称英语\',\n            u\'公共英语\':u\'公共英语\',\n            u\'商务英语\':u\'商务英语\',\n            u\'日语\':u\'日语\',\n            u\'人力资源\':u\'人力资源\',\n            u\'心理咨询师\':u\'心理咨询师\',\n            u\'物流师\':u\'物流师\',\n            u\'公共营养师\':u\'公共营养师\',\n            u\'秘书资格\':u\'秘书资格\',\n            u\'证券经纪人\':u\'证券经纪人\',\n            u\'电子商务\':u\'电子商务\',\n            u\'国家司法\':u\'国家司法\',\n            u\'成人高考\':u\'成人高考\',\n            u\'自考\':u\'自考\',\n            u\'MBA\':u\'MBA\',\n            u\'法律硕士\':u\'法律硕士\',\n            u\'会计硕士\':u\'会计硕士\',\n            u\'工程硕士\':u\'工程硕士\',\n            u\'MPA\':u\'公共硕士\',\n            u\'考研\':u\'考研\',\n            u\'计算机等级\':u\'计算机等级\',\n            u\'软件水平\':u\'软件水平\',\n            u\'微软认证\':u\'微软认证\',\n            u\'Cisco认证\':u\'Cisco认证\',\n            u\'Oracle认证\':u\'Oracle认证\',\n            u\'职称计算机\':u\'职称计算机\',\n            u\'Java认证\':u\'Java认证\',\n            u\'华为认证\':u\'华为认证\',\n    }\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.qnr.cn/zhenti/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'table\').items():\n            for each1 in each.find(\'a\').items():\n                _dict = {}\n                _dict[\'bread\'] = [each.find(\'td\').eq(0).text().split(\'(\')[0].strip(),\'\']\n                _dict[\'bread\'][1] = each1.text().strip()\n                self.crawl(each1.attr.href, save = _dict, callback=self.list_page)\n            \n    @config(age=1 * 1)\n    def list_page(self, response):\n        _dict = {}\n        for each in response.doc(\'.Right_last_lst\').items():\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'.R_date\').text().replace(\'/\',\'-\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n            \n        for each in response.doc(\'.P_Con\').items():\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'.time\').text().replace(\'/\',\'-\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []\n        for each in response.doc(\'.mar10 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'#manadona > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n        if len(list) == 0 and response.doc(\'#manadona\'):     \n            list.append(response.doc(\'#manadona\').remove(\'a\').remove(\'p\').html())\n            \n        for each in response.doc(\'#xx20 > div > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n                       \n            \n        for each in response.doc(\'#xx23 > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.remove(\'a\').html())\n            \n        for each in response.doc(\'#xx27 > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.remove(\'a\').html())\n\n        for each in response.doc(\'.hao > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'.mini > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n        if len(list) == 0 and response.doc(\'.mini\'):     \n            list.append(response.doc(\'.mini\').remove(\'a\').remove(\'p\').html())\n                       \n        \n        for each in response.doc(\'#shtdxlnews_4 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n         \n        for each in response.doc(\'#tb42 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'#gtsadfas > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'.nali\').items():\n            list.append(response.doc(\'.nali\').remove(\'a\').remove(\'p\').html())\n            \n        content = \'\'.join(\'<p>%s<p/>\' % s for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            bread = []\n            for val in dalei:\n                if val[0:2] in response.save.get(\'bread\')[0]:\n                    bread.append(val)\n                    for k, v in xiaolei.iteritems():\n                        if k in response.save.get(\'bread\')[1]:\n                            bread.append(v)\n            if len(bread) < 2:\n                bread = response.save.get(\'bread\')\n            return {\n                \"url\": response.url,\n                \"title\": response.doc(\'#tb41 > span\').text(),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'青年人网讯\',\'\').replace(\'青年人\',\'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": bread, \n                \"date\": response.save.get(\'date\'),\n                \"tdk_description\":response.doc(\'meta\').eq(1).attr.content.replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"tdk_keywords\":response.doc(\'meta\').eq(0).attr.content.replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"tdk_title\":response.doc(\'title\').eq(0).text().replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"青年人网\",\n\n            }\n',NULL,1.0000,3.0000,1472546284.3822),('licai_hexun_inc','licai','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-26 09:26:52\n# Project: licai_hexun_inc\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://money.hexun.com/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'#secnav11 > dl > dd > a\').items():\n            if u\'组图\' in each.text() or u\'专题\' in each.text() or u\'税务\' in each.text() or not each.text():\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n        self.crawl(\'http://tax.hexun.com/csdt/index.html\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.temp01 li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)\n        for each in response.doc(\'.ulspan > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)    \n        \n        pages = re.match(r\'(.*?)hxPage.maxPage = ([^;]+)\',response.doc(\'.listdh\').html(),re.S).group(2).strip()\n        #翻页\n        #for i in range(1,int(pages)):\n         #   self.crawl(response.url.replace(\'index.html\',\'index-%s.html\' % str(i)), callback=self.list2_page)\n    \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        for each in response.doc(\'.temp01 li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)\n        for each in response.doc(\'.ulspan > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)    \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.art_contextBox > *\').items():\n            if u\'相关推荐\' in each.text() or u\'文章导航：\' in each.text() or u\'阅读排行\' in each.text() or u\'返回专题\' in each.text() or u\'更多推荐\' in each.text():\n                break\n            if u\'百利天下\' in each.text() or u\'小马在线专家\' in each.text():\n                continue\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        for each in response.doc(\'.concent > p\').items():\n            if u\'相关推荐\' in each.text() or u\'文章导航：\' in each.text() or u\'阅读排行\' in each.text() or u\'返回专题\' in each.text() or u\'更多推荐\' in each.text():\n                break\n            if u\'百利天下\' in each.text() or u\'小马在线专家\' in each.text():\n                continue\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 100:\n            return\n        if response.doc(\'.pr20\').text():\n            date = response.doc(\'.pr20\').text().split()[0].strip()\n        elif response.doc(\'.box01 .gray\'):\n            date = re.sub(ur\'日.*\',\'\',response.doc(\'.box01 .gray\').eq(0).text().strip()).replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n        else:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"hexun\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"理财\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1474853411.6038),('licai_rong360_inc','licai','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:16:45\n# Project: licai_rong360_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.rong360.com/licai-news/\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'#secnav11 > dl > dd > a\').items():\n            if u\'组图\' in each.text() or u\'专题\' in each.text() or u\'税务\' in each.text() or not each.text():\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n        self.crawl(\'http://tax.hexun.com/csdt/index.html\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.content div\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.s_tit > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.from_date\').text().strip().split()[-2].split(u\'：\')[-1].replace(\'/\',\'-\')\n            self.crawl(each.find(\'.s_tit > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.next-page\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        for each in response.doc(\'.temp01 li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)\n        for each in response.doc(\'.ulspan > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)    \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.txt > *\').items():\n            if not each.html():\n                continue\n            if u\'：融360财秘微信\' in each.text() or \'http://static.rong360.com/gl/uploads/allimg/160926/0922532037-0.jpg\' == each.find(\'img\').attr[\'src\']:\n                break\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        \n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 100:\n            return\n        if response.doc(\'.bti_left > span\').text():\n            date = response.doc(\'.bti_left > span\').eq(-1).text().split(u\'：\')[-1].strip()\n        else:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"rong360\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"理财\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1474939148.9587),('licai_sina_inc','licai','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:21:35\n# Project: licai_sina_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nurls= [\n    \'http://licaishi.sina.com.cn/web/packageInfo?pkg_id=3358\',\n    \'http://licaishi.sina.com.cn/web/packageInfo?pkg_id=160&page=1\',\n    \'http://licaishi.sina.com.cn/web/packageInfo?pkg_id=1113\',\n    \'http://licaishi.sina.com.cn/web/packageInfo?pkg_id=1376\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url in urls:\n            self.crawl(url, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'#secnav11 > dl > dd > a\').items():\n            if u\'组图\' in each.text() or u\'专题\' in each.text() or u\'税务\' in each.text() or not each.text():\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n        self.crawl(\'http://tax.hexun.com/csdt/index.html\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.p_list > div > div > div\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.w_vp_h2 > a\').text().strip()\n            self.crawl(each.find(\'.w_vp_h2 > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.w_pages_next\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        for each in response.doc(\'.temp01 li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)\n        for each in response.doc(\'.ulspan > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)    \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.p_article > *\').remove(\'#_baidu_bookmark_start_4\').items():\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        \n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 10:\n            return\n        if response.doc(\'.p_info_time\').text():\n            date = response.doc(\'.p_info_time\').text().split(u\'日\')[0].strip().replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n        else:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"sina\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"理财\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1474939406.5384),('licai_zhongjin_inc','licai','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:26:07\n# Project: licai_zhongjin_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nurls= [\n    \'http://money.cnfol.com/licaizixun/\',\n    \'http://money.cnfol.com/licaigonglue/\',\n    \'http://money.cnfol.com/caishenghuo/\',\n    \'http://money.cnfol.com/shoucangjingcui/\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url in urls:\n            self.crawl(url, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'#secnav11 > dl > dd > a\').items():\n            if u\'组图\' in each.text() or u\'专题\' in each.text() or u\'税务\' in each.text() or not each.text():\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n        self.crawl(\'http://tax.hexun.com/csdt/index.html\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.NewsLstItem > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.Fl\').text().strip()\n            self.crawl(each.find(\'.Fl\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.NewsLstPage > a\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        for each in response.doc(\'.temp01 li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)\n        for each in response.doc(\'.ulspan > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(-1).text().strip()\n            self.crawl(each.find(\'a\').eq(-1).attr.href,save = _dict, callback=self.detail_page)    \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'#Content\').html().strip()).replace(\'</a>\',\'\')+\'</p>\'\n        content = re.sub(r\'<!--.*?-->\',\'\',content)\n        if len(content) < 10:\n            return\n        if response.doc(\'#pubtime_baidu\').text():\n            date = response.doc(\'#pubtime_baidu\').text().split()[0].strip()\n        else:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"zhongjin\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"理财\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1474939674.6841),('linux_idc',NULL,'RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-26 15:54:22\n# Project: linux_idc\n\nfrom pyspider.libs.base_handler import *\nimport time\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.linuxidc.com/it/\', callback=self.index_page)\n\n    @config(age=1 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a.nLink\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        #for each in response.doc(\'div.pager>ul>li>a\').items():\n        #    if each.attr.title == \'后页\':\n        #        self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        for each in response.doc(\'div#content\').items():\n            for p in response.doc(\'div#content  > p\').items():\n                if u\"引用来源\" in p.text() or u\"本文永久更新链接地址\" in p.text():\n                    continue\n                _list.append(\'<p>\'+p.html().strip()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1.aTitle\').text(),\n            \"bread\": [u\'linux\'],\n            \"content\": content,\n            \"source\": u\"互联网\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"程序员\",\n            \"date\": date,\n        }\n',NULL,1.0000,3.0000,1475128698.8180),('magic_magicyou','magic','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 09:28:16\n# Project: magic_magicyou\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nurls= [\n    \'http://www.magicyou.net/news/?list_35.html\',\n    \'http://www.magicyou.net/news/?list_7.html\',\n    \'http://www.magicyou.net/news/?list_21.html\',\n    \'http://www.magicyou.net/news/?list_16.html\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url in urls:\n            self.crawl(url, callback=self.list1_page)\n\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.mb20\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.newsTitle a\').text().strip()\n            _dict[\'date\'] = each.find(\'.date\').text().strip()\n            self.crawl(each.find(\'.newsTitle a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.fontNav_2\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.detailTxtCon > *\').items():\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        \n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 10:\n            return\n        date = response.save[\'date\']\n        if not date:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"magicyou\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"魔术\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1475137690.0150),('magic_magicyou_inc','delete','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-29 16:27:00\n# Project: magic_magicyou_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nurls= [\n    \'http://www.magicyou.net/news/?list_35.html\',\n    \'http://www.magicyou.net/news/?list_7.html\',\n    \'http://www.magicyou.net/news/?list_21.html\',\n    \'http://www.magicyou.net/news/?list_16.html\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        for url in urls:\n            self.crawl(url, callback=self.list1_page)\n\n\n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1)\n    def list1_page(self, response):\n        for each in response.doc(\'.mb20\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.newsTitle a\').text().strip()\n            _dict[\'date\'] = each.find(\'.date\').text().strip()\n            self.crawl(each.find(\'.newsTitle a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.fontNav_2\').items():\n            #self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.detailTxtCon > *\').items():\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        \n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 10:\n            return\n        date = response.save[\'date\']\n        if not date:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"magicyou\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"魔术\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1475137686.1203),('mba_mbachina','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 09:43:41\n# Project: mba_mbachina\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nurls= [\n    \'http://www.magicyou.net/news/?list_35.html\',\n    \'http://www.magicyou.net/news/?list_7.html\',\n    \'http://www.magicyou.net/news/?list_21.html\',\n    \'http://www.magicyou.net/news/?list_16.html\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.mbachina.com/html/news/\', callback=self.index_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.wf-nav a\').items():\n            if u\'最新\' in each.text():\n                continue\n            self.crawl(each.attr.href, callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.list-nav a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.list-item > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip()\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'#nextbtn\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.article-con > *\').items():\n            \n            if not each.html() or \'<h1\' in each.outerHtml() or \'<h6\' in each.outerHtml():\n                continue\n            if u\'版权声明\' in each.text() or u\'更多精彩内容关注\' in each.text():\n                break\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        \n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 10:\n            return\n        date = response.save[\'date\']\n        if not date:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"mbachina\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":\"MBA\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1475137795.4580),('mba_mbachina_inc','mba','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 09:43:41\n# Project: mba_mbachina\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nurls= [\n    \'http://www.magicyou.net/news/?list_35.html\',\n    \'http://www.magicyou.net/news/?list_7.html\',\n    \'http://www.magicyou.net/news/?list_21.html\',\n    \'http://www.magicyou.net/news/?list_16.html\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.mbachina.com/html/news/\', callback=self.index_page)\n    \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.wf-nav a\').items():\n            if u\'最新\' in each.text():\n                continue\n            self.crawl(each.attr.href, callback=self.list_page)\n\n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.list-nav a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n    \n    @config(age=1)\n    def list1_page(self, response):\n        for each in response.doc(\'.list-item > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip()\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'#nextbtn\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.article-con > *\').items():\n            \n            if not each.html() or \'<h1\' in each.outerHtml() or \'<h6\' in each.outerHtml():\n                continue\n            if u\'版权声明\' in each.text() or u\'更多精彩内容关注\' in each.text():\n                break\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        \n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 10:\n            return\n        date = response.save[\'date\']\n        if not date:\n            date = time.strftime(\'%Y-%m-%d\',time.localtime())\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"mbachina\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":\"MBA\",\n            \"date\":date ,\n\n        }',NULL,1.0000,3.0000,1475137789.1425),('mba_wangxiao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 14:32:47\n# Project: mba_wangxiao\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wangxiao.cn/mba/\',callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'li > .navlink\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n            \n                \n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pNext\').items():\n            self.crawl(each.attr.href,save = response.save,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pNext\').items():\n            self.crawl(each.attr.href,save = response.save,callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.newsCon > *\').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'编辑推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"wangxiao\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": \"MBA\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1475137883.0654),('mba_wangxiao_inc','mba','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 14:32:47\n# Project: mba_wangxiao\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wangxiao.cn/mba/\',callback=self.index_page)\n        \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'li > .navlink\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n            \n                \n\n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pNext\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list_page)\n            \n    @config(age=1)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pNext\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.newsCon > *\').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'编辑推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"wangxiao\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": \"MBA\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1475137885.4405),('ntce_inc','ntce','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-21 10:40:52\n# Project: ntce_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.ntce.cn/a/zuijinzixun/\', callback=self.index_page)\n\n    @config(age=1 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.ico_xie_02\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        #for each in response.doc(\'.dede_pages a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = {}\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h2\').text()\n        res_dict[\'content\'] = response.doc(\'.table_box\').html()\n        res_dict[\'subject\'] = u\'教师资格证\'\n        info = response.doc(\'.info\').text()\n        date = info.split(u\'来源:\')[0].split(u\':\')[1].strip(\' \')\n        res_dict[\'source\'] = info.split(u\'来源:\')[1].strip(\' \')\n        res_dict[\'date\'] = date\n        res_dict[\'bread\'] = [u\'报名时间\']\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        return res_dict\n',NULL,1.0000,3.0000,1474426960.5208),('pingpang_sohu_inc','pingpang','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:50:30\n# Project: pingpang_sohu_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://sports.sohu.com/s2005/pingpangqiudongtai.shtml\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.fr > a\').items():\n            if \'zt\' in each.attr.href or \'list\' in each.attr.href:\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.f14list a\').items():\n            #print each.text()\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            self.crawl(each.attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for i in range(640,739):\n            #self.crawl(\'http://sports.sohu.com/s2005/pingpangqiudongtai_\'+str(i)+\'.shtml\', callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'[itemprop=\"articleBody\"] > p\').remove(\'script\').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'学习推荐\' in each.text() or u\'学习交流平台\' in each.text() or u\'请关注233小学\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"sohu\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"乒乓球\",\n            \"date\": response.doc(\'[itemprop=\"datePublished\"]\').text().split()[0],\n\n        }',NULL,1.0000,3.0000,1474941114.0754),('proxy_monitor',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-19 17:37:58\n# Project: proxy_monitor\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        url = \'http://10.252.40.85:5000/tasks?project=gaokao_school_point_v2\'\n        self.crawl(url, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1459164249.8550),('putonghua_51test',NULL,'TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-26 11:33:49\n# Project: putonghua_51test\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=&search_key=%BF%BC%CA%D4%CA%B1%BC%E4&search_key2=\':[u\'考试动态\',u\'考试时间\'],\n    \'http://www.51test.net/pthsp/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=&search_key=%CC%E2%C4%BF&search_key2=\':[u\'学习资料\'],\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=&search_key=%CB%AE%C6%BD%B2%E2%CA%D4&search_key2=\':[u\'学习资料\'],\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=&search_key=%CB%B5%BB%B0&search_key2=\':[u\'学习资料\'],\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=&search_key=%C0%CA%B6%C1&search_key2=\':[u\'学习资料\'],\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=zt&search_key=&search_key2=\':[u\'学习资料\'],\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=st&search_key=&search_key2=\':[u\'学习资料\'],\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=&search_key=%C1%B7%CF%B0&search_key2=\':[u\'学习资料\'],\n    \'http://www.51test.net/channel_list_more.asp?page=2&classid=5&Nclassid=122&Key=fd&search_key=&search_key2=\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n    headers = {\n\'Host\':\'www.51test.net\',\n\'Connection\':\'keep-alive\',\n\'Referer\':\'http://www.51test.net/pthsp/baoming/\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36\'\n}\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k, v in urls.iteritems():\n            self.crawl(k, save={\'bread\':v},headers=self.headers,callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.showpagelist > a\').items():\n            self.crawl(each.attr.href,headers=self.headers,callback=self.list_page)\n            \n                \n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.lbpage a\').items():\n            self.crawl(each.attr.href,save = response.save,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.left > div\').eq(6).children().items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"exam8\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"普通话\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474861401.3022),('putonghua_exam8_inc','putonghua','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:38:35\n# Project: putonghua_exam8_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.exam8.com/zige/putonghua/zhinan/\':[u\'考试动态\',u\'考试资讯\'],\n    \'http://www.exam8.com/zige/putonghua/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.exam8.com/zige/putonghua/dongtai/\':[u\'考试动态\'],\n    \'http://www.exam8.com/zige/putonghua/zhenti/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/moni/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/tuijian/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/fudao/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/jingyan/\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in urls.iteritems():\n            self.crawl(k, save={\'bread\':v},callback=self.list_page)\n        \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.zx_bt > span > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.rz_bt > span > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n            \n                \n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.lbpage a\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.left > div\').eq(6).children().items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"exam8\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"普通话\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474940427.8102),('putonghua_wangxiao_inc','putonghua','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:40:59\n# Project: putonghua_wangxiao_inc\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.wangxiao.cn/jsz/putonghua/index.html\':[u\'考试动态\',u\'考试资讯\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in urls.iteritems():\n            self.crawl(k, save={\'bread\':v},callback=self.list_page)\n        \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.zx_bt > span > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.rz_bt > span > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n            \n                \n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pNext\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.newsCon > *\').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'编辑推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"wangxiao\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"普通话\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1474940570.3215),('qa_360',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-05 10:49:43\n# Project: qa_360\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'proxy_enable\': True,\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        \'\'\'\n        with open(\'/apps/home/worker/100zhan.txt\') as f:\n            for line in f:\n                subject = line.strip(\'\\n\')\n                self.crawl(\'http://wenda.so.com/search/?q=\' + subject + \'&filt=20\', save={\'subject\': subject}, callback=self.index_page)\n        \'\'\'\n        word = u\'考证\'\n        self.crawl(\'http://wenda.so.com/search/?q=\' + word + \'&filt=20\', save={\'subject\': word}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        subject = response.save[\'subject\']\n        for each in response.doc(\'.qa-i-hd a\').items():\n            title = each.text()\n            self.crawl(each.attr.href,save={\'title\': title, \'subject\': subject}, callback=self.detail_page)\n        for each in response.doc(\'.pagination a\').items():\n            self.crawl(each.attr.href, save={\'subject\': subject}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for info in response.doc(\'.mod-resolved-ans > .bd\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.text > div a\').text()\n            _dic[\'date\'] = info.find(\'.text > span\').text().split()[-1].replace(\'.\',\'-\')\n            _dic[\'content\'] = info.find(\'.resolved-cnt\').html()\n            _dic[\'avatar\'] = info.find(\'.info img\').attr.src\n            content_list.append((_dic))\n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'考证题库\',\n            \"answers\": content_list,\n            \"source\": \'360\',\n            \"question_detail\": response.doc(\'.q-cnt\').text(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1473145347.3661),('qa_sougou',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-02 14:39:04\n# Project: sogou_wenwen\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport pyquery\nimport json\n\nclass Handler(BaseHandler):\n   \n    crawl_config = {\n\n#\'proxy\': \'111.56.13.152:80\',\n            \"headers\":{\n\'Accept\':\'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\',\n\'Accept-Encoding\':\'gzip, deflate, sdch\',\n\'Accept-Language\':\'zh-CN,zh;q=0.8,en;q=0.6\',\n\'Cache-Control\':\'max-age=0\',\n\'Connection\':\'keep-alive\',\n\'Cookie\':\'ww_search_tips=nulln; ww_sTitle=%u6570%u5B66%u77E5%u8BC6%u624B%u5DE5%u5236%u4F5C*GMAT*%u4E2D%u8003*%u6258%u798F; ww_filter=1; CXID=E52AD3C93ACC10138699410B1397001C; IPLOC=CN4201; SUID=E105133A5EC90D0A0000000056FE42CD; ssuid=2208855100; ww_orig_ref=\"http%3A%2F%2Fwenwen.sogou.com%2Fs%2F%3Fw%3D%25E6%2595%25B0%25E5%25AD%25A6%25E7%259F%25A5%25E8%25AF%2586%25E6%2589%258B%25E5%25B7%25A5%25E5%2588%25B6%25E4%25BD%259C%26st%3D4\"; ld=blllllllll2glh9nlllllVtRwyclllllnPGVwZllll9lllllxylll5@@@@@@@@@@; MAIN_SESSIONID=n111i0ehc1fgbna386m03gin89ti.n11\',\n\'Host\':\'wenwen.sogou.com\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.76 Mobile Safari/537.36\',\n    }\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        \'\'\'\n        with open(\'/apps/home/worker/100zhan.txt\') as f:\n            for line in f:\n                self.crawl(\'http://wenwen.sogou.com/s/?w=\' + line.strip(\'\\n\') + \'&search=%E6%90%9C%E7%8B%97%E6%90%9C%E7%B4%A2&st=4&ch=11\', save={\'subject\': line.strip(\'\\n\')}, callback=self.index_page)\n        \'\'\'\n        self.crawl(\'http://wenwen.sogou.com/s/?w=%E6%89%98%E7%A6%8F&st=4&ch=11\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        subject = \'\' #response.save.get(\'subject\',\'\')\n        for each in response.doc(\'a[href^=\"http://wenwen.sogou.com/z/\"]\').items():\n            title = each.text()\n            self.crawl(each.attr.href, save={\'title\': title, \'subject\': subject}, callback=self.detail_page)\n        for each in response.doc(\'a[href^=\"http://wenwen.sogou.com/s/\"]\').items():\n            if \'&pg=\' in each.attr.href:\n                self.crawl(each.attr.href, save={\'subject\': subject}, callback=self.index_page)\n            \n        \n    @config(priority=2)\n    def detail_page(self, response):\n        title = response.save[\'title\'] #response.doc(\'#questionTitle\').text()        \n        \n        content_list = []\n        for info in response.doc(\'.satisfaction-answer\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.question-info > div\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.question-info span\').text()\n            _dic[\'content\'] = info.find(\'.answer-con\').html()\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic))\n        for info in response.doc(\'.default-answer\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.info-wrap\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.time\').text()\n            _dic[\'content\'] = info.find(\'.answer-con\').html()\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic))\n        try:\n            subject = response.save[\'subject\']\n        except:\n            subject = None\n\n        if not content_list:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"subject\": subject,\n            \"answers\": content_list,\n            \"question_detail\": response.doc(\'.question-con\').text(),\n        }',NULL,1.0000,3.0000,1473145139.7977),('qiuzhi_jianliwang','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-07 11:01:22\n# Project: qiuzhi_kuaiji\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/dianzijianlimoban/\', callback=self.index_page)\n\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/gerenjianlimoban/\', callback=self.index_page)\n\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/yingwenjianlimoban/\', callback=self.index_page)\n\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/qiuzhijianlimoban/\', callback=self.index_page)\n\n\n    @config(age=1* 60)\n    def index_page(self, response):\n        for each in response.doc(\'dt > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.pagelist a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n        \'\'\'\n        \n\n    @config(priority=2)\n    def detail_page(self, response):\n        try:\n            table = \'<table align=\"center\">\' +  response.doc(\'table\').html() + \'</table>\'\n        except:\n            table = \'\'\n        #intro = response.doc(\'.content > p\').text().split()[0]\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.title\').text(),\n            \"content\": response.doc(\'.content\').remove(\'script\').remove(\'div\').remove(\'a\').html().replace(\'jianli.yjbys.com\', \'\').replace(\'yjbys\', \'\').replace(\'\\n\',\'\'), #intro + table, \n            \"data_weight\": 0,\n            \"subject\": u\'求职\',\n            \"bread\": [u\'简历\',],\n            \"class\": 46,\n            \"source\": u\'简历网\',\n        }\n',NULL,1.0000,1.0000,1471329820.5476),('qiuzhi_shixisheng','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-16 10:50:19\n# Project: qiuzhi_shixisheng\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.yingjiesheng.com/commend-parttime-1.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.bg_0\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'title\'] = each.find(\'a\').text()\n            _save[\'date\'] = each.find(\'.date\').text()\n            #print _save[\'title\'], _save[\'date\']\n            self.crawl(url,save=_save, callback=self.detail_page)\n\n        for each in response.doc(\'.bg_1\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'title\'] = each.find(\'a\').text()\n            _save[\'date\'] = each.find(\'.date\').text()\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.page > a\').items():\n            url = each.attr.href\n            self.crawl(url, callback=self.index_page)\n        \'\'\'\n    \n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        res[\"content\"] = response.doc(\'.jobIntro\').html()\n        res[\"subject\"] = u\'求职\'\n        res[\"source\"] = u\'yingjiesheng\'\n        res[\'bread\'] = [u\'实习生\',]\n        if not res[\'content\']: \n            res[\'content\'] = response.doc(\'.job_list\').html()\n        res[\'class\'] = 46 \n        res[\'data_weight\'] = 0\n        return res\n',NULL,1.0000,3.0000,1471329837.9661),('qiuzhi_shixisheng_shixisheng','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-11 19:09:16\n# Project: qiuzhi_shixi_shixisheng\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    dic = {\n    \"opj_p0wwm34iuy3y\": [\n        {\n            \"name\": \"教育\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ydckmvemhhj9\",\n                    \"name\": \"教务\"\n                },\n                {\n                    \"uuid\": \"opj_i2atsrq7meb2\",\n                    \"name\": \"教师\"\n                },\n                {\n                    \"uuid\": \"opj_gsfxm6khk1mw\",\n                    \"name\": \"幼教\"\n                },\n                {\n                    \"uuid\": \"opj_anu5gkjal5cq\",\n                    \"name\": \"培训\"\n                },\n                {\n                    \"uuid\": \"opj_ovpebzlsbjbs\",\n                    \"name\": \"课程\"\n                }\n            ]\n        },\n        {\n            \"name\": \"咨询\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_v7lu487vtqcj\",\n                    \"name\": \"咨询/顾问\"\n                }\n            ]\n        }\n    ],\n    \"opj_4mun5l8wqssz\": [\n        {\n            \"name\": \"软件\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_twuuznrzmbdu\",\n                    \"name\": \"IOS\"\n                },\n                {\n                    \"uuid\": \"opj_frm2gpvkcpua\",\n                    \"name\": \"数据库\"\n                },\n                {\n                    \"uuid\": \"opj_worxhycw0gym\",\n                    \"name\": \"C#/.NET\"\n                },\n                {\n                    \"uuid\": \"opj_fwnc2trdffiw\",\n                    \"name\": \"Hadoop\"\n                },\n                {\n                    \"uuid\": \"opj_edurcbhia2n3\",\n                    \"name\": \"Android\"\n                },\n                {\n                    \"uuid\": \"opj_qc9afdwzavw9\",\n                    \"name\": \"算法\"\n                },\n                {\n                    \"uuid\": \"opj_dzsdsfwm0ntt\",\n                    \"name\": \"IT运维\"\n                },\n                {\n                    \"uuid\": \"opj_bgq6sdxqx1i8\",\n                    \"name\": \"Python\"\n                },\n                {\n                    \"uuid\": \"opj_rjdqlspswhli\",\n                    \"name\": \"云计算/大数据\"\n                },\n                {\n                    \"uuid\": \"opj_lsvexg8ehsjx\",\n                    \"name\": \"Node.js\"\n                },\n                {\n                    \"uuid\": \"opj_wbbl3ezi6ecv\",\n                    \"name\": \"数据挖掘\"\n                },\n                {\n                    \"uuid\": \"opj_bkbverpen0w3\",\n                    \"name\": \"PHP\"\n                },\n                {\n                    \"uuid\": \"opj_qpgp7xcerkex\",\n                    \"name\": \"Ruby/Perl\"\n                },\n                {\n                    \"uuid\": \"opj_ykka3ldcn21u\",\n                    \"name\": \"测试\"\n                },\n                {\n                    \"uuid\": \"opj_dxnurenognxo\",\n                    \"name\": \"Java\"\n                },\n                {\n                    \"uuid\": \"opj_jlm8stx4zz26\",\n                    \"name\": \"C/C++\"\n                },\n                {\n                    \"uuid\": \"opj_snjlgjshvfwn\",\n                    \"name\": \"前端\"\n                }\n            ]\n        },\n        {\n            \"name\": \"运营\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ielyo96oix78\",\n                    \"name\": \"新媒体\"\n                },\n                {\n                    \"uuid\": \"opj_okd5xorkjaks\",\n                    \"name\": \"内容运营\"\n                },\n                {\n                    \"uuid\": \"opj_uu6f6lasdshg\",\n                    \"name\": \"编辑\"\n                },\n                {\n                    \"uuid\": \"opj_ysojgsemw0fe\",\n                    \"name\": \"SEO\"\n                },\n                {\n                    \"uuid\": \"opj_cmp8uy9f6v2m\",\n                    \"name\": \"产品运营\"\n                }\n            ]\n        },\n        {\n            \"name\": \"硬件\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_rnbmq9wmvb54\",\n                    \"name\": \"嵌入式\"\n                },\n                {\n                    \"uuid\": \"opj_an2cblxq5b7w\",\n                    \"name\": \"集成电路\"\n                }\n            ]\n        },\n        {\n            \"name\": \"设计\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_mrxvzqkechw2\",\n                    \"name\": \"Flash\"\n                },\n                {\n                    \"uuid\": \"opj_0ic5vg9cusry\",\n                    \"name\": \"UI/UE\"\n                },\n                {\n                    \"uuid\": \"opj_uvbdobspakmu\",\n                    \"name\": \"特效\"\n                },\n                {\n                    \"uuid\": \"opj_5upcwefmaqhj\",\n                    \"name\": \"网页/美工\"\n                },\n                {\n                    \"uuid\": \"opj_uvhgcoyjkeyl\",\n                    \"name\": \"2D/3D\"\n                }\n            ]\n        },\n        {\n            \"name\": \"通信\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ppqp36lwiw1i\",\n                    \"name\": \"物联网\"\n                },\n                {\n                    \"uuid\": \"opj_gy6kzykxrpiq\",\n                    \"name\": \"射频\"\n                },\n                {\n                    \"uuid\": \"opj_unqfld9abm0b\",\n                    \"name\": \"通信\"\n                }\n            ]\n        },\n        {\n            \"name\": \"产品\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_zbmvshsqlpco\",\n                    \"name\": \"用户研究\"\n                },\n                {\n                    \"uuid\": \"opj_zs7hmkfz698q\",\n                    \"name\": \"产品助理\"\n                }\n            ]\n        }\n    ],\n    \"opj_32vfxjgfgkad\": [\n        {\n            \"name\": \"金融\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_tp0s0i4mg3jc\",\n                    \"name\": \"基金\"\n                },\n                {\n                    \"uuid\": \"opj_nrvscepwebo6\",\n                    \"name\": \"证券\"\n                },\n                {\n                    \"uuid\": \"opj_scgzknrfc9o8\",\n                    \"name\": \"风控\"\n                },\n                {\n                    \"uuid\": \"opj_pr9lpdarivfz\",\n                    \"name\": \"金融\"\n                }\n            ]\n        },\n        {\n            \"name\": \"投资\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_e7omu0xnhmi1\",\n                    \"name\": \"分析师\"\n                },\n                {\n                    \"uuid\": \"opj_swxgid8t2ylp\",\n                    \"name\": \"投资\"\n                }\n            ]\n        },\n        {\n            \"name\": \"法务\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_g3ufnzwf3ic7\",\n                    \"name\": \"合规\"\n                },\n                {\n                    \"uuid\": \"opj_5pv0h0gmmks4\",\n                    \"name\": \"律师\"\n                },\n                {\n                    \"uuid\": \"opj_d00vwoe9xk2f\",\n                    \"name\": \"法务\"\n                }\n            ]\n        },\n        {\n            \"name\": \"银行\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_yiffuqvioqk4\",\n                    \"name\": \"客户经理\"\n                },\n                {\n                    \"uuid\": \"opj_v8dzgkcdej8i\",\n                    \"name\": \"部门经理\"\n                },\n                {\n                    \"uuid\": \"opj_dfhiwt0i19fe\",\n                    \"name\": \"贷款\"\n                },\n                {\n                    \"uuid\": \"opj_a9ayetvpyzdw\",\n                    \"name\": \"大堂经理\"\n                }\n            ]\n        },\n        {\n            \"name\": \"保险\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_p2ifplcmars6\",\n                    \"name\": \"业务\"\n                },\n                {\n                    \"uuid\": \"opj_qww5tuazg0zs\",\n                    \"name\": \"保单\"\n                }\n            ]\n        },\n        {\n            \"name\": \"财会\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_a3hiwtqufpe2\",\n                    \"name\": \"审计\"\n                },\n                {\n                    \"uuid\": \"opj_hxlreh5cirnh\",\n                    \"name\": \"税务\"\n                },\n                {\n                    \"uuid\": \"opj_ydfam0kaxoa7\",\n                    \"name\": \"财务\"\n                },\n                {\n                    \"uuid\": \"opj_b38dn1wq26td\",\n                    \"name\": \"会计/出纳\"\n                }\n            ]\n        }\n    ],\n    \"opj_v7ygyfozpgy2\": [\n        {\n            \"name\": \"商务\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_jlsgwadhibpr\",\n                    \"name\": \"商务\"\n                },\n                {\n                    \"uuid\": \"opj_afjnklufvsfh\",\n                    \"name\": \"招投标\"\n                }\n            ]\n        },\n        {\n            \"name\": \"销售\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_qwrmbcrtzaud\",\n                    \"name\": \"推广\"\n                },\n                {\n                    \"uuid\": \"opj_dkhdshammijx\",\n                    \"name\": \"销售\"\n                }\n            ]\n        },\n        {\n            \"name\": \"公关\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_uuhho9meehxo\",\n                    \"name\": \"媒介\"\n                },\n                {\n                    \"uuid\": \"opj_fli2uh3axvam\",\n                    \"name\": \"公关\"\n                }\n            ]\n        },\n        {\n            \"name\": \"客服\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_eiy2odktulsm\",\n                    \"name\": \"客户服务\"\n                },\n                {\n                    \"uuid\": \"opj_jcvrwy2p6up9\",\n                    \"name\": \"销售支持\"\n                }\n            ]\n        },\n        {\n            \"name\": \"市场\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_xvmkicckmgan\",\n                    \"name\": \"渠道\"\n                },\n                {\n                    \"uuid\": \"opj_xtkcoq6vmcdr\",\n                    \"name\": \"分析/调研\"\n                },\n                {\n                    \"uuid\": \"opj_jf4xyp7jigdd\",\n                    \"name\": \"策划\"\n                },\n                {\n                    \"uuid\": \"opj_icis7su6fxju\",\n                    \"name\": \"品牌\"\n                },\n                {\n                    \"uuid\": \"opj_egxi3f7yog0h\",\n                    \"name\": \"市场\"\n                }\n            ]\n        }\n    ],\n    \"opj_g68s2gm09jdv\": [\n        {\n            \"name\": \"人力资源\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_acrvakvqyzik\",\n                    \"name\": \"人事/HR\"\n                },\n                {\n                    \"uuid\": \"opj_qtc4so3cydmo\",\n                    \"name\": \"招聘\"\n                },\n                {\n                    \"uuid\": \"opj_awun8u4ogl3m\",\n                    \"name\": \"企业文化\"\n                }\n            ]\n        },\n        {\n            \"name\": \"猎头\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ytjvc22kmoob\",\n                    \"name\": \"猎头\"\n                }\n            ]\n        },\n        {\n            \"name\": \"行政\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_y7p3orvwtr2v\",\n                    \"name\": \"行政\"\n                },\n                {\n                    \"uuid\": \"opj_m1mxy4ydijfc\",\n                    \"name\": \"前台\"\n                },\n                {\n                    \"uuid\": \"opj_89efrspgjjcn\",\n                    \"name\": \"助理\"\n                }\n            ]\n        }\n    ],\n    \"opj_cn9j3euwuvtl\": [\n        {\n            \"name\": \"外语\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_gs8yoyp0nefd\",\n                    \"name\": \"英语\"\n                },\n                {\n                    \"uuid\": \"opj_0hdaz9wznwv2\",\n                    \"name\": \"日语\"\n                },\n                {\n                    \"uuid\": \"opj_ys0eyzn5kkbt\",\n                    \"name\": \"翻译\"\n                }\n            ]\n        },\n        {\n            \"name\": \"外贸\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_b0ktbhlgvfnb\",\n                    \"name\": \"报关员\"\n                },\n                {\n                    \"uuid\": \"opj_z2vez1xgsf0x\",\n                    \"name\": \"外贸专员\"\n                }\n            ]\n        }\n    ],\n    \"opj_jxvy9fuotcvj\": [\n        {\n            \"name\": \"广告\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_l7utfoe88ylr\",\n                    \"name\": \"创意\"\n                },\n                {\n                    \"uuid\": \"opj_gojfv3twwx5r\",\n                    \"name\": \"策划\"\n                },\n                {\n                    \"uuid\": \"opj_x3tgzqznj1dj\",\n                    \"name\": \"AE\"\n                }\n            ]\n        },\n        {\n            \"name\": \"编辑\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_mspx4safdbwp\",\n                    \"name\": \"编辑/采编\"\n                },\n                {\n                    \"uuid\": \"opj_2672dgvnktgm\",\n                    \"name\": \"校对/排版\"\n                }\n            ]\n        },\n        {\n            \"name\": \"设计\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_fxel0nldueqw\",\n                    \"name\": \"美术设计\"\n                },\n                {\n                    \"uuid\": \"opj_zrgyhehcendi\",\n                    \"name\": \"工业设计\"\n                },\n                {\n                    \"uuid\": \"opj_izptkxcdrwdb\",\n                    \"name\": \"平面设计\"\n                },\n                {\n                    \"uuid\": \"opj_2xaf5axvltwq\",\n                    \"name\": \"视觉设计\"\n                }\n            ]\n        },\n        {\n            \"name\": \"媒体\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_mgnrscpzykac\",\n                    \"name\": \"记者\"\n                },\n                {\n                    \"uuid\": \"opj_rc5eetcarzu5\",\n                    \"name\": \"主持/播音\"\n                },\n                {\n                    \"uuid\": \"opj_vu4ra0k6bmiz\",\n                    \"name\": \"编导\"\n                }\n            ]\n        },\n        {\n            \"name\": \"艺术\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_bd9fqvuufgg6\",\n                    \"name\": \"演艺\"\n                },\n                {\n                    \"uuid\": \"opj_xziotarow0eq\",\n                    \"name\": \"摄影\"\n                }\n            ]\n        }\n    ],\n    \"opj_e4tvvgcavs2j\": [\n        {\n            \"name\": \"体育快消\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ilurdnsopa6g\",\n                    \"name\": \"快消\"\n                },\n                {\n                    \"uuid\": \"opj_z8dhzwpigjlk\",\n                    \"name\": \"体育\"\n                }\n            ]\n        },\n        {\n            \"name\": \"机械制造\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_vbauw4zsmsnn\",\n                    \"name\": \"质量\"\n                },\n                {\n                    \"uuid\": \"opj_oheplpt1dlow\",\n                    \"name\": \"机械设计\"\n                },\n                {\n                    \"uuid\": \"opj_urjflgxfv4dd\",\n                    \"name\": \"生产\"\n                },\n                {\n                    \"uuid\": \"opj_hf93dmd6ombh\",\n                    \"name\": \"安全\"\n                },\n                {\n                    \"uuid\": \"opj_327srkkqritm\",\n                    \"name\": \"设备\"\n                },\n                {\n                    \"uuid\": \"opj_un7mpmwbhwhs\",\n                    \"name\": \"自动化\"\n                }\n            ]\n        },\n        {\n            \"name\": \"物流采购\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_bcwafqijkvfc\",\n                    \"name\": \"采购\"\n                },\n                {\n                    \"uuid\": \"opj_bqhq5es6a7mk\",\n                    \"name\": \"供应链\"\n                },\n                {\n                    \"uuid\": \"opj_4zrljtoab9qg\",\n                    \"name\": \"物流\"\n                }\n            ]\n        },\n        {\n            \"name\": \"建筑房产\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_fvmfqadehva7\",\n                    \"name\": \"城规/市政\"\n                },\n                {\n                    \"uuid\": \"opj_itkittci96ka\",\n                    \"name\": \"工程造价\"\n                },\n                {\n                    \"uuid\": \"opj_kxy7uvay18ez\",\n                    \"name\": \"建筑\"\n                },\n                {\n                    \"uuid\": \"opj_waxyjpf3jy45\",\n                    \"name\": \"土木\"\n                },\n                {\n                    \"uuid\": \"opj_prf03bdanvzr\",\n                    \"name\": \"园林\"\n                },\n                {\n                    \"uuid\": \"opj_svfrapw9izjb\",\n                    \"name\": \"地产开发/策划\"\n                },\n                {\n                    \"uuid\": \"opj_0ajhcvn62omh\",\n                    \"name\": \"房产销售\"\n                },\n                {\n                    \"uuid\": \"opj_4bu0spgnss1c\",\n                    \"name\": \"给排水\"\n                },\n                {\n                    \"uuid\": \"opj_kj9bj8nck8n2\",\n                    \"name\": \"物业管理\"\n                }\n            ]\n        },\n        {\n            \"name\": \"生物医疗\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_a6wuiaia7zz7\",\n                    \"name\": \"医生\"\n                },\n                {\n                    \"uuid\": \"opj_fntj9r5yanty\",\n                    \"name\": \"医药\"\n                },\n                {\n                    \"uuid\": \"opj_cvbkzomriwym\",\n                    \"name\": \"生物\"\n                },\n                {\n                    \"uuid\": \"opj_jqoi0qrflscd\",\n                    \"name\": \"护理\"\n                }\n            ]\n        },\n        {\n            \"name\": \"能源环保\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_odmfoqvehqd0\",\n                    \"name\": \"矿产\"\n                },\n                {\n                    \"uuid\": \"opj_y5xs9hzcw8rw\",\n                    \"name\": \"能源\"\n                },\n                {\n                    \"uuid\": \"opj_ctqjqjhzzftu\",\n                    \"name\": \"环保\"\n                }\n            ]\n        },\n        {\n            \"name\": \"食品材料\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_grb2ruhukb8i\",\n                    \"name\": \"材料\"\n                },\n                {\n                    \"uuid\": \"opj_8mj8jkeniaev\",\n                    \"name\": \"食品\"\n                }\n            ]\n        },\n        {\n            \"name\": \"NGO公益\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ijdiapsz9edf\",\n                    \"name\": \"志愿者\"\n                }\n            ]\n        }\n    ],\n    \"opj_texrs8przurn\": [\n        {\n            \"name\": \"电子\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_mpcknopmcbsn\",\n                    \"name\": \"光电\"\n                },\n                {\n                    \"uuid\": \"opj_gdsqvqa3xrjf\",\n                    \"name\": \"半导体/芯片\"\n                },\n                {\n                    \"uuid\": \"opj_4ydypxlridqr\",\n                    \"name\": \"电子工程\"\n                }\n            ]\n        },\n        {\n            \"name\": \"电气\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_3zamopr7jxkd\",\n                    \"name\": \"电气设计\"\n                },\n                {\n                    \"uuid\": \"opj_ufpnyxgzxeyj\",\n                    \"name\": \"电气工程\"\n                }\n            ]\n        }\n    ]\n}\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k in self.dic:\n            for childs in self.dic[k]:\n                for child in childs[\'child\']:\n                    name = child[\'name\']\n                    for i in range(1, 5):\n                        self.crawl(\'http://www.shixiseng.com/interns?k=\' + name + \'&p=%d\'%i, callback=self.index_page)\n\n    @config(age=1 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.job_inf_inf\').items():\n            title = \'【%s】%s招聘%s实习生\'%(each.find(\'.addr_box > span\').text(), each.find(\'.company_name\').text() , each.find(\'a > h3\').text())\n            self.crawl(each.find(\'a\').attr.href, save={\'title\': title}, callback=self.detail_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.parent_job > li\').items():\n            self.crawl(each.attr.href, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        title = response.save[\'title\']\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"content\": response.doc(\'.dec_content\').html(),\n            \"date\": response.doc(\'.update_time\').text()[:10],\n            \"data_weight\": 0,\n            \"subject\": u\'求职\',\n            \"bread\": [u\'实习生\',],\n            \"class\": 46,\n            \"source\": u\'shixisheng\',\n        }\n',NULL,1.0000,3.0000,1471329830.4341),('qiuzhi_xuanjianghui_inc','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-15 17:25:49\n# Project: qiuzhi_yingjiesheng\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://my.yingjiesheng.com/xuanjianghui.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.bg0\').items():\n            tr = [info for info in  each.find(\'td\').items()]\n            _dict = {}\n            _dict[\'city\'] =  tr[0].text()\n            _dict[\'date\'] = tr[1].text()\n            _dict[\'hour\'] = tr[2].find(\'img\').attr.src\n            _dict[\'company\'] = tr[3].text()\n            url = tr[3].find(\'a\').attr.href\n            _dict[\'school\'] = tr[4].text()\n            _dict[\'address\'] = tr[5].text()\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        for each in response.doc(\'.bg1\').items():\n            tr = [info for info in  each.find(\'td\').items()]\n            _dict = {}\n            _dict[\'city\'] =  tr[0].text()\n            _dict[\'date\'] = tr[1].text()\n            _dict[\'hour\'] = tr[2].find(\'img\').attr.src\n            _dict[\'company\'] = tr[3].text()\n            url = tr[3].find(\'a\').attr.href\n            _dict[\'school\'] = tr[4].text()\n            _dict[\'address\'] = tr[5].text()\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        #for each in response.doc(\'.page > a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        res[\"title\"] = response.doc(\'.xjh_infos_h1\').text()\n        res[\"content\"] = response.doc(\'.colspan2\').html()\n        res[\'bread\'] = [u\'宣讲会\']\n        res[\'subject\'] = u\'求职\'\n        res[\'source\'] = u\'yingjiesheng\'\n        return res\n',NULL,1.0000,3.0000,1471329841.7512),('qiuzhi_zhaopin','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-16 10:50:19\n# Project: qiuzhi_shixi\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.yingjiesheng.com/commend-fulltime-1.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.bg_0\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'title\'] = each.find(\'a\').text()\n            _save[\'date\'] = each.find(\'.date\').text()\n            #print _save[\'title\'], _save[\'date\']\n            self.crawl(url,save=_save, callback=self.detail_page)\n\n        for each in response.doc(\'.bg_1\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'title\'] = each.find(\'a\').text()\n            _save[\'date\'] = each.find(\'.date\').text()\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.page > a\').items():\n            url = each.attr.href\n            self.crawl(url, callback=self.index_page)\n        \'\'\'\n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        res[\"content\"] = response.doc(\'.jobIntro\').html()\n        res[\"subject\"] = u\'求职\'\n        res[\"source\"] = u\'yingjiesheng\'\n        res[\'bread\'] = [u\'招聘信息\',]\n        if not res[\'content\']: \n            res[\'content\'] = response.doc(\'.job_list\').html()\n        res[\'class\'] = 46 \n        res[\'data_weight\'] = 0\n        return res\n',NULL,1.0000,3.0000,1471329846.4821),('sat_zhan_inc','sat','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 18:19:22\n# Project: sat_xiaozhan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    type_dict = {\n        \'yufa\': u\'SAT语法\',\n        \'yuedu\': u\'SAT阅读\',\n        \'xiezuo\': u\'SAT写作\',\n        \'shuxue\': u\'SAT数学\',\n        \'zonghe\': u\'SAT其他\',\n        \'jihua\': u\'备考计划\',\n        \'tifen/beikao\': u\'备考计划\',\n        \'gaofen\': u\'高分心得\',\n        \'fuxi\': u\'复习攻略\',\n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for type, v in self.type_dict.iteritems():\n            self.crawl(\'http://sat.zhan.com/%s/\'%type, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/sat/kaoqian/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/sat/kaohou/\', save={\'bread\': u\'包括指南\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/sat/fukao/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/sat/beikao/\', save={\'bread\': u\'备考计划\'}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.experience-item\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.index-middle-info-3 > .pull-left\').text().split()[0]\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.index-middle-info-2\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        for each in response.doc(\'.things_list > .pull-right\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.padding_right_10\').text()\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.text\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n\n        #for each in response.doc(\'nav a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        content_list = []\n        for each in  response.doc(\'.article-content > p\').items():\n            content_list.append((each.html().replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'小站\',\'\')))\n        content_list = content_list[:-1]\n        if not content_list:\n            return None\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'% v for v in content_list])\n        res_dict[\'subject\'] = u\'SAT\'\n        res_dict[\'source\'] = u\'小站\'\n        res_dict[\'tag\'] = response.doc(\'.tag > a\').text().split(\' \')\n        bread = [res_dict[\'bread\'], ]\n        bread.extend(response.doc(\'.head-crumbs-a-active\').text().split(\' \'))\n        if res_dict[\'date\'][:3] != \'201\':\n            res_dict[\'date\'] = response.doc(\'.pull-left > span\').text().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        res_dict[\'bread\'] = list(set(bread))\n        res_dict[\'class\'] = 31\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471334978.0441),('seo3',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-14 11:44:19\n# Project: seo_fenxi\n\nfrom pyspider.libs.base_handler import *\nimport sys,urllib,time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nfw = open(\'/apps/home/rd/zengsheng/query.done\',\'a\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'3\',\n        \"headers\":{\n\'Cookie\':\'__cfduid=d0c2292f9ed27f5c3550c60d3e06c24af1473823853; uid=oGflBHmAMcHIZ4tfB%2fxHCm3r9voqr9ZQXggMcFFDUeY%3d; Hm_lvt_295557bac3c4981f18b013f806da26d0=1473823915; Hm_lpvt_295557bac3c4981f18b013f806da26d0=1473823955; Hm_lvt_e51f41cefdaee205c99f313a1a7143f2=1473823946; Hm_lpvt_e51f41cefdaee205c99f313a1a7143f2=1473823955; ASP.NET_SessionId=53g1pwp3so32kera1obrufgj\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n    }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for line in open(\'/apps/home/rd/zengsheng/900query.txt\',\'r\'):\n            line = line.strip()\n            #line = unicode(line)\n            #print line\n            #print type(line)\n            #line = u\'初中\'\n            #print type(line)\n            #for i in range(1,11):\n            self.crawl(\'http://www.5118.com/seo/words/%s?isPager=true&pageIndex=%s&_=1473823913311\' % (line,str(1)),headers=self.crawl_config[\'headers\'],save = {\'query\':line}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        if \'captcha\' in response.url:\n            time.sleep(3000)\n        fw.write(response.save[\'query\']+\'\\n\')\n        fw.flush()\n        _list = []\n        for each in response.doc(\'.dig-list dl:gt(0)\').items():\n            _dict = {}\n            _dict[\'keyword\'] = each.find(\'.col2-7.word > span > a\').text().strip()\n            _dict[\'baidu_ex\'] = each.find(\'dd.center\').eq(0).text().strip()\n            _dict[\'search_res\'] = each.find(\'dd.center\').eq(1).text().strip()\n            _dict[\'recommend\'] = each.find(\'div > span em\').text().strip()\n            _list.append(_dict)\n        return {\n        \'url\':response.url,\n        \'res\':_list\n        }\n\n    @config(priority=2)\n    def detail_page(self, response):\n        pass\n        \n',NULL,0.3000,3.0000,1474265013.0972),('sheji_cndesign_inc','sheji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:58:13\n# Project: sheji_cndesign_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = [\n    \'http://www.uggd.com/news/qynews/list.php\',\n    \'http://www.uggd.com/news/hynews/\',\n    \'http://www.uggd.com/article/PDM/\',\n    \'http://www.uggd.com/article/CAD/\',\n    \'http://www.uggd.com/article/CAM/\',\n    \'http://www.uggd.com/article/CAE/\',\n    \'http://www.uggd.com/article/mujv/\',\n    \'http://www.uggd.com/article/mujv/\',\n    \'http://www.uggd.com/article/sk/\',\n    \'http://www.uggd.com/article/qc/\',\n    \'http://www.uggd.com/article/cl/\',\n    \'http://www.uggd.com/article/gl/yfgl/\',\n    \'http://www.uggd.com/article/gl/cxgl/\',\n    \'http://www.uggd.com/article/gl/xmgl/\',\n    \'http://www.uggd.com/article/gl/sbgl/\',\n    \'http://www.uggd.com/article/gl/zhlgl/\',\n    \'http://www.uggd.com/article/gl/scgl/\',\n]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'3\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://web.cndesign.com/news/0/1/0/0.html\',callback=self.list2_page)\n        \n    \n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.bb\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.title\').text().strip()\n            _dict[\'date\'] = each.find(\'p > .newLink\').eq(0).text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.title\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.page\').items():\n         #   self.crawl(each.attr.href,callback=self.list2_page)\n        \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.detail-mode\').html().strip().replace(\'\\n\',\'\').replace(\'\\t\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n        content = re.sub(r\'<!--.*?-->\',\'\',content)\n        if len(content) < 10:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"cndesign\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"设计\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474941772.1788),('sheji_shejinet_inc','sheji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:59:26\n# Project: sheji_shejinet_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = [\n    \'http://www.shejinet.com/news/huodong.html\',\n    \'http://www.shejinet.com/news/xinwen.html\',\n    \'http://www.shejinet.com/news/shinei.html\',\n    \'http://www.shejinet.com/news/pingmian.html\',\n    \'http://www.shejinet.com/news/wangzhan.html\',\n    \'http://www.shejinet.com/news/gongye.html\',\n    \'http://www.shejinet.com/news/baozhuang.html\',\n]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'3\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        for url in urls:\n            self.crawl(url,callback=self.list2_page)\n        \n    \n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.news-list\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h3 > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.tags\').text().strip().split()[-1].split(u\'：\')[-1].replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'h3 > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.next > a\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n        \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.content\').remove(\'.newspage\').html().strip().replace(\'\\n\',\'\').replace(\'\\t\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n        content = re.sub(r\'<!--.*?-->\',\'\',content)\n        if len(content) < 10:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"shejinet\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"设计\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474941774.6906),('sheji_uggd_inc','sheji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 10:00:22\n# Project: sheji_uggd_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = [\n    \'http://www.uggd.com/news/qynews/list.php\',\n    \'http://www.uggd.com/news/hynews/\',\n    \'http://www.uggd.com/article/PDM/\',\n    \'http://www.uggd.com/article/CAD/\',\n    \'http://www.uggd.com/article/CAM/\',\n    \'http://www.uggd.com/article/CAE/\',\n    \'http://www.uggd.com/article/mujv/\',\n    \'http://www.uggd.com/article/mujv/\',\n    \'http://www.uggd.com/article/sk/\',\n    \'http://www.uggd.com/article/qc/\',\n    \'http://www.uggd.com/article/cl/\',\n    \'http://www.uggd.com/article/gl/yfgl/\',\n    \'http://www.uggd.com/article/gl/cxgl/\',\n    \'http://www.uggd.com/article/gl/xmgl/\',\n    \'http://www.uggd.com/article/gl/sbgl/\',\n    \'http://www.uggd.com/article/gl/zhlgl/\',\n    \'http://www.uggd.com/article/gl/scgl/\',\n]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'3\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        for url in urls:\n            self.crawl(url,callback=self.list2_page)\n        \n    \n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.new_list_719_04 li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.new_list_719_04_a > a\').text().strip()\n            if not _dict[\'title\']:\n                _dict[\'title\'] = each.find(\'.new_list_719_04_a1 > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.new_list_719_04_c\').text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.new_list_719_04_a > a\').attr.href,save = _dict, callback=self.detail_page)\n            self.crawl(each.find(\'.new_list_719_04_a1 > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.new_list_719_03 > div > a\').items():\n          #  self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n        #翻页\n        #for each in response.doc(\'.pages > a\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'#zoom\').remove(\'.newspage\').html().strip().replace(\'\\n\',\'\').replace(\'\\t\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n        content = re.sub(r\'<!--.*?-->\',\'\',content)\n        if len(content) < 10:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"uggd\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"设计\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474941777.0592),('sheji_ui001_inc','sheji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:57:03\n# Project: sheji_ui001_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = [\n    \'http://www.ui001.com/information/\',\n    \'http://www.ui001.com/tutorial/\',\n]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'3\'\n    }\n\n    @every(minutes=4 * 60)\n    def on_start(self):\n           for url in urls: \n                self.crawl(url,callback=self.list2_page)\n        \n    \n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.left_area > .clearfix > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h3 > a\').text().strip()\n            _dict[\'date\'] = each.find(\'li .fr > span\').eq(-1).text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'h3 > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pagecode a\').items():\n         #   self.crawl(each.attr.href,callback=self.list2_page)\n        \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.detail_cont\').html().strip().replace(\'\\n\',\'\').replace(\'\\t\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n        content = re.sub(r\'<!--.*?-->\',\'\',content)\n        if len(content) < 10:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"ui001\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"设计\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474941780.8883),('shici_haoshiwen2',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 09:23:20\n# Project: shici_liuxue862\n\nfrom pyspider.libs.base_handler import *\nimport re,json\nfrom pyquery import PyQuery as pq\n\nshangxi_dict = {\n    u\'相关翻译\':\'translation_note\',\n    u\'相关赏析\':\'parse_appreciation\',\n    #u\'艺术特色\':\'art_appreciation\',\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'itag\':\'2\',\n\'headers\':{\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n}\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.haoshiwen.org/\',headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.son2Title2 > a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.sons\').items(): \n            #print each.html()\n            self.crawl(each.find(\'p a\').eq(0).attr.href, headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pages > a\').items(): \n            self.crawl(each.attr.href,save = response.save, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n        \n    \n    @config(priority=2)\n    def detail_page(self, response):\n        tags = []\n        shici_type = u\'其他\'\n        if u\'分类\' in response.doc(\'.sontitle > span\').text():\n            for each in response.doc(\'.shisonconttag > a\').items():\n                tags.append(each.text().strip())\n            tags = \',\'.join(v.replace(u\'的诗句\',\'\').replace(u\'诗句\' ,\'\') for v in tags)\n        _dict = {\n            \'translation_note\':\'\',\n            \'parse_appreciation\':\'\',\n            \'art_appreciation\':\'\',\n        }\n        \n        for each in response.doc(\'.son4\').items():\n            \n            if each.find(\'h2\').text() not in shangxi_dict.keys():\n                continue\n            each1 = each.next()\n            con = \'\'\n            #print each.html()\n            while each1.html() and \'son5\' in each1.outerHtml() and \'<h2>\' not in each1.html():\n                if not each1.find(\'a\').eq(0).attr.href:\n                    continue\n                con += pq(url=each1.find(\'a\').eq(0).attr.href,encoding=\'utf8\').find(\'.shileft .shangxicont\').remove(\'div\').remove(\'p:first\').remove(\'p:last\').html().strip()\n                each1 = each1.next()\n            _dict[shangxi_dict[each.find(\'h2\').text()]] = con\n        #print _dict\n        #for k,v in _dict.iteritems():\n         #   print k,v\n        #return    \n        res = {\n            \"url\": response.url,\n            \"name\":response.doc(\'.son1 > h1\').text().strip(),\n            \"type\":shici_type,\n            \"tags\":tags,    \n            \"author\":response.doc(\'.shileft .son2 > p\').eq(1).text().split(u\'：\')[1].strip(),\n            \"dynasty\":response.doc(\'.shileft .son2 > p\').eq(0).text().split(u\'：\')[1].strip(),\n            \"content\":response.doc(\'.shileft .son2\').remove(\'p:lt(3)\').remove(\'div\').html().strip(),\n            \"translation_note\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'translation_note\']).replace(\'</a>\',\'\'),\n            \"parse_appreciation\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'parse_appreciation\']).replace(\'</a>\',\'\'),\n            \"art_appreciation\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'art_appreciation\']).replace(\'</a>\',\'\'),\n        }\n        return res\n',NULL,5.0000,5.0000,1474941720.5229),('shufa_inc','shufa','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 10:22:48\n# Project: shufa_wzxx\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n    \n    \n    page_list = {\n\n        \'http://www.wzxx.org/shufalilun/shufashi/\':[u\'书法历史\'],\n        \'http://www.wzxx.org/shufalilun/gudaishujia/\':[u\'书法杂谈\'],\n        \'http://www.wzxx.org/shufalilun/lilunzhishi/\':[u\'书法知识\'],\n        \'http://www.wzxx.org/shufalilun/jibenjifa/\':[u\'基本技法\'],\n        \'http://www.wzxx.org/shufalilun/zhuanke/\':[u\'篆刻知识\'],\n        \'http://www.wzxx.org/shufalilun/yingbi/\':[u\'硬笔书法\'],\n        \'http://www.wzxx.org/shufalilun/diandi/\':[u\'书海点滴\']\n\n    }\n    \n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_list.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'div.vlist li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text()\n            _dict[\'date\'] = each.find(\'span\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            url = each.find(\'a\').attr.href\n            self.crawl(url, save = _dict,callback=self.detail_page)\n      \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'div#text\').remove(\'ins\').remove(\'script\').html()\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = u\'书法\'\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'wzxx.org\'\n        res_dict[\'data_weight\'] = 0\n        #res_dict[\'bread\'] = [u\'文章资讯\']\n        return res_dict\n',NULL,1.0000,3.0000,1472546336.4362),('shufa_sfrx_inc','shufa','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 18:04:38\n# Project: shufa_sfrx_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=3 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.sfrx.cn/index.html\', callback=self.index_page)\n        \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.zx_bt > span > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.rz_bt > span > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n            \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'span > a\').items():\n            #print each.parent().prev().text()\n            if u\'展讯\' in each.parent().prev().text():\n                continue\n            self.crawl(each.attr.href, callback=self.list_page)\n                \n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lb_xx > ul\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h2 > a\').text().strip()\n            _dict[\'date\'] = each.find(\'dd\').eq(-1).text().strip().split()[0] or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'h2 > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.lbfy a\').items():\n         #   self.crawl(each.attr.href,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.lb_wz_nrc  > *\').items():\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 100:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'书法杂谈\'],\n            \"content\": content,\n            \"source\": \"sfrx\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"书法\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474538757.8400),('spanish_hujiang','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 17:57:47\n# Project: spanish_hujiang\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 14:18:29\n# Project: hujiang_deyu\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.hujiang.com/new/rumen/\':[u\'德语基础\'],\n            \'http://de.hujiang.com/new/shiyong/\':[u\'实用德语\'],\n            \'http://de.hujiang.com/new/yule/\':[u\'德语文化\'],\n            \'http://de.hujiang.com/new/topic/627/\':[u\'德国留学\'],\n            \'http://de.hujiang.com/new/topic/1024/\':[u\'德语考试\']\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://es.hujiang.com/\',callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.es_nav_li_title > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页   \n        for each in response.doc(\'.page_list > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n         \n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n      \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = u\'西班牙语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'date\'] = response.doc(\'.page_tip > span\').eq(-1).text().strip().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n       return res_dict',NULL,1.0000,3.0000,1475139512.0124),('spanish_hujiang_inc','spanish','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 17:57:47\n# Project: spanish_hujiang\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.hujiang.com/new/rumen/\':[u\'德语基础\'],\n            \'http://de.hujiang.com/new/shiyong/\':[u\'实用德语\'],\n            \'http://de.hujiang.com/new/yule/\':[u\'德语文化\'],\n            \'http://de.hujiang.com/new/topic/627/\':[u\'德国留学\'],\n            \'http://de.hujiang.com/new/topic/1024/\':[u\'德语考试\']\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://es.hujiang.com/\',callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.es_nav_li_title > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n    \n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页   \n        #for each in response.doc(\'.page_list > a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n         \n           # self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.list_page)\n      \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = u\'西班牙语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'date\'] = response.doc(\'.page_tip > span\').eq(-1).text().strip().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n       return res_dict',NULL,1.0000,3.0000,1475139507.9530),('spanish_tingroom','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 17:45:17\n# Project: spanish_tingroom\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-23 09:59:29\n# Project: deyu_tingroom_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'2\'\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.tingroom.com/yuedu/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/yufa/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/cihui/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/kouyu/\':[u\'实用德语\'],\n            \'http://de.tingroom.com/gequ/\':[u\'德语文化\'],\n            \'http://de.tingroom.com/liuxue/\':[u\'德国留学\'],\n            \'http://de.tingroom.com/kaoshi/\':[u\'德语考试\']\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://sp.tingroom.com/\',callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#navMenu li a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.index1_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def index1_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n            \n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'.info\').text().strip().split()[1]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.pages > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'.info\').text().strip().split()[1]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.pages > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       \n       res_dict[\'content\'] = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.content > .content\').html().strip().replace(\'\\t\',\'  \').replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = u\'西班牙语\'\n       res_dict[\'source\'] = u\'西班牙语学习网\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'bread\'] = [u\'文章资讯\']\n       return res_dict\n\n',NULL,1.0000,3.0000,1475144280.6625),('spanish_tingroom_inc','spanish','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 17:45:17\n# Project: spanish_tingroom\n\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.tingroom.com/yuedu/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/yufa/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/cihui/\':[u\'德语基础\'],\n            \'http://de.tingroom.com/kouyu/\':[u\'实用德语\'],\n            \'http://de.tingroom.com/gequ/\':[u\'德语文化\'],\n            \'http://de.tingroom.com/liuxue/\':[u\'德国留学\'],\n            \'http://de.tingroom.com/kaoshi/\':[u\'德语考试\']\n    }\n    @every(minutes=4 * 60)\n    def on_start(self):\n        self.crawl(\'http://sp.tingroom.com/\',callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'#navMenu li a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.index1_page)\n    \n    @config(age=1)\n    def index1_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list_page)\n            \n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n            \n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'.info\').text().strip().split()[1]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pages > a\').items():\n         #   self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n            \n    @config(age=1)\n    def list1_page(self, response):\n        for each in response.doc(\'.e2 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            url =  each.find(\'a\').attr.href\n            _dict[\'date\'] = each.find(\'.info\').text().strip().split()[1]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pages > a\').items():\n         #   self.crawl(each.attr.href,headers = self.headers,callback=self.list1_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       \n       res_dict[\'content\'] = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.content > .content\').html().strip().replace(\'\\t\',\'  \').replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = u\'西班牙语\'\n       res_dict[\'source\'] = u\'西班牙语学习网\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'bread\'] = [u\'文章资讯\']\n       return res_dict\n\n',NULL,1.0000,3.0000,1475144372.0384),('toefl_taisha_inc','toefl','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 16:14:58\n# Project: toefl_taisha\n\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    type_dict = {\n        \'news\': u\'快讯动态\', \'guidance\': u\'复习攻略\', \'experience\': u\'复习攻略\', \'download\': u\'快讯动态\', \'guid\': u\'高分心得\', \'machine\': u\'托福机经\',\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in self.type_dict.iteritems(): \n            self.crawl(\'http://www.taisha.org/toefl/%s/\'%k, save={\'bread\': v}, callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.html_content dd\').items():\n            _dict = {}\n            url = each.find(\'.title > a\').attr.href\n            #_dict[\'url\'] = url\n            _dict[\'bread\'] = each.find(\'.bot > a\').text().split(\' \')\n            _dict[\'bread\'].append((bread))\n            _dict[\'brief\'] = self.replace(each.find(\'p\').text().replace(u\'[详情]\', \'\'))\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        #for each in response.doc(\'span > a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n    def replace(self, _str):\n        img1 = \'http://www.taisha.org/uploadfile/2015/1215/20151215035909345.jpg\'\n        if _str:\n            return _str.replace(u\'太傻留学\', u\'\').replace(u\'太傻\', \'\').replace(img1, \'\')\n        return _str\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        try:\n            date = response.doc(\'.txt_title span\').text()[-19:]\n        except:\n            date = \'\'\n        #content = response.doc(\'.txt_content\').html()\n        content_info = response.doc(\'.txt_content > p\')\n        content_list = []\n        for info in content_info:\n            answer = pyquery.PyQuery(info)\n            items = answer.html()\n            if \'20151215035909345.jpg\' not in items:\n                content_list.append((self.replace(items)))\n        content_list = content_list[:-7]\n        if not content_list:\n            return None\n        title = response.doc(\'.txt_title > h2\').text()\n        if u\'回忆版汇总\' in title:\n            return None\n        res_dict[\'title\'] = title\n        bread = res_dict[\'bread\']\n        if u\'托福机经\' in response.save[\'bread\']:\n            if u\'汇总\' in title:\n                return None\n            if u\'听力\' in title:\n                bread.append((u\'听力机经\'))\n            if u\'阅读\' in title:\n                bread.append((u\'阅读机经\'))\n            if u\'口语\' in title:\n                bread.append((u\'口语机经\'))\n            if u\'写作\' in title:\n                bread.append((u\'写作机经\'))\n        res_dict[\'bread\'] = list(set(bread)) \n        res_dict[\'url\'] = response.url\n        res_dict[\'date\'] = date[:10]\n        res_dict[\'source\'] = u\'太傻留学\'\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'subject\'] = u\'托福\'\n        res_dict[\'tag\'] = response.doc(\'#nav_location a\').text().split(\' \')[1:]\n        res_dict[\'class\'] = 27\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471329875.8495),('toefl_xdf_inc','toefl','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-26 14:19:35\n# Project: toefl_xdf_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://toefl.xdf.cn/list_9069_1.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.txt_lists01 > li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'.time\').text()\n            if u\'汇总\' not in each.find(\'a\').text():\n                self.crawl(url, save={\'date\': date}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for each in response.doc(\'.air_con > p\').items():\n            if u\'编辑推荐\' in each.text():\n                break\n            content_list.append((each.html().replace(\'\\n\',\'\').replace(\'\\r\',\'\').replace(\'\\t\',\'\')))\n        content_list = content_list[1:]\n        if not content_list:\n            return None\n        bread = [u\'托福机经\',]\n        title = response.doc(\'.title1\').text().replace(u\'新东方名师：\',\'\')\n        if u\'听力\' in title:\n            bread.append((u\'听力机经\'))\n        if u\'阅读\' in title:\n            bread.append((u\'阅读机经\'))\n        if u\'口语\' in title:\n            bread.append((u\'口语机经\'))\n        if u\'写作\' in title:\n            bread.append((u\'写作机经\'))\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"date\": response.save[\'date\'],\n            \"subject\": u\'托福\',\n            \"source\": \'xdf\',\n            \"class\": 27,\n            \"data_weight\":0,\n            \"bread\": bread,\n            \"content\": \'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n        }\n',NULL,1.0000,3.0000,1471329882.3647),('toefl_xiaozhan_inc','toefl','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 18:19:22\n# Project: toefl_xiaozhan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    type_dict = {\n        \'fuxi\': u\'复习攻略\', \'tpo\': u\'托福TPO\', \'tingli\': u\'托福听力\', \'kouyu\': u\'托福口语\', \n        \'yuedu\': \'托福阅读\', \'zonghe\': u\'冲刺宝典\', \'jihua\': u\'备考计划\', \'tifen/beikao\': u\'备考计划\',\n        \'xiezuo\': u\'托福写作\', \'tfziliao\': u\'托福综合\', \'gaofen\': u\'高分心得\', \n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for type, v in self.type_dict.iteritems():\n            self.crawl(\'http://toefl.zhan.com/%s/\'%type, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/toefl/kaoqian/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/toefl/fukao/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/toefl/beikao/\', save={\'bread\': u\'备考计划\'}, callback=self.index_page)\n\n    @config(age=1 * 60)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.things_list > .pull-right\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.padding_right_10\').text()\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.text\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        #for each in response.doc(\'nav a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    def resplace(self, _str):\n        if _str:\n            return _str.replace(u\'小站\',\'\').replace(u\'小站教育\',\'\')\n        return _str\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\"url\"] = response.url\n        brief = response.save[\'brief\']\n        res_dict[\'brief\'] = self.resplace(brief)\n        res_dict[\"title\"] = self.resplace(response.doc(\'h1\').text())\n        content_list = []\n        for each in  response.doc(\'.article-content > p\').items():\n            content_list.append((each.html().replace(\'\\r\',\'\').replace(\'\\t\',\'\').replace(\'\\n\',\'\')))\n        if not content_list:\n            return None\n        res_dict[\'content\'] = self.resplace(\'\'.join([\'<p>%s</p>\'% v for v in content_list]))\n        res_dict[\'subject\'] = u\'托福\'\n        res_dict[\'source\'] = u\'小站\'\n        res_dict[\'tag\'] = response.doc(\'.tag > a\').text().split(\' \')\n        bread = [res_dict[\'bread\'], ]\n        bread.extend(response.doc(\'.head-crumbs-a-active\').text().split(\' \'))\n        if res_dict[\'date\'][:3] != \'201\':\n            res_dict[\'date\'] = response.doc(\'.pull-left > span\').text().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        res_dict[\'bread\'] = list(set(bread))\n        res_dict[\'class\'] = 27\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471329885.7682),('weixin_sougou',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-11 12:14:31\n# Project: sheying_fsbus\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport cPickle\nimport time\n\ndetail_headers = {\n      \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n      \"Upgrade-Insecure-Requests\": \"1\",\n      \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"\n    }\n\nindex_headers = {\n      \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n      \"Cookie\": \"CXID=61CC362F8DB5AF26DB926FB349837DF7; SUV=00FA5F9E246E3FDC56C2B5484910E349; ssuid=2375315090; ad=zkllllllll2QZVNXq73CWONoohxQZrqfJpc@Tlllll9lllllVqxlw@@@@@@@@@@@; SUID=DC3F6E24516C860A56C0576700097B44; ABTEST=4|1466480404|v1; weixinIndexVisited=1; SNUID=4BA8F8B39693A2570288F98E972AD230; JSESSIONID=aaafIPdrNUcCPb_Tbeovv; ppinf=5|1466497966|1467707566|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZToyNzolRTUlQjAlOEYlRTUlQUUlODclRTUlQUUlOTl8Y3J0OjEwOjE0NjY0OTc5NjZ8cmVmbmljazoyNzolRTUlQjAlOEYlRTUlQUUlODclRTUlQUUlOTl8dXNlcmlkOjQ0OkM3NDBBQUFCMkM3RERCM0Y0Njg0NzFFRTUzMTYzQUJGQHFxLnNvaHUuY29tfA; pprdig=PMcHhapmHPZtX7WFM_2PIYC2cnXBsOAT_EIIh_UC9Z4kAqswRfsbRA3Mp2aFG1t9adYaCkjRQWP-dKMiNAxGJkHS9J6XJqRKV6s9h2iRMEFxkLGI05EGVIfa-bEO4KjhNrA3xzjoIDGqSwfmZizeN97d0f1wWxmM5nEzg-0nm5w; PHPSESSID=l3fu330sueq4uogqnavs6s5ot3; SUIR=4BA8F8B39693A2570288F98E972AD230; sct=14; IPLOC=CN3300; ppmdig=1466500977000000d7f8ecd6a79fbd98c17e6d636b5074b4\",\n      \"Host\": \"weixin.sogou.com\",\n      \"Upgrade-Insecure-Requests\": \"1\",\n      \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"\n}\n\nmax_pageno = 20\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".results\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\".txt-box h4\").find(\"a\"):\n                ret.append(P(url_node))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        page_node = P(response.doc(\"#pagebar_container\"))\n        try:\n            next_page = page_node.find(\"span\").next()\n            if next_page:\n                current_pageno = page_node.find(\"span\").text()\n                next_pageno = next_page.text()\n                if int(next_pageno) > int(current_pageno):\n                    # 限制只抓前20页\n                    if int(next_pageno) > max_pageno:\n                        return False\n                    return P(\'\'\'<a href=\"%s&page=%s\">next</a>\'\'\' % (response.save.get(\"base_url\"), next_pageno))\n                return False\n\n            else:\n                return False\n        except Exception as info:\n            return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        ret = {}\n        detail_node = response.doc(\"#img-content\")\n        title = P(detail_node).find(\".rich_media_title\").text()\n        ret[\"title\"] = title\n        ret[\"date\"] = P(detail_node).find(\"#post-date\").text()\n        ret[\"nickname\"] = P(detail_node).find(\"a.rich_media_meta_nickname\").text()\n        ret[\"author\"] = P(detail_node).find(\"#post-date\").next().text()\n        ret[\"content\"] = P(detail_node).find(\"#js_content\").html().strip()\n        ret[\"readnum\"] = P(detail_node).find(\"#sg_readNum3\").text()\n        ret[\"likenum\"] = P(detail_node).find(\"#sg_likeNum3\").text()\n        ret[\"isorg\"] = P(detail_node).find(\"#copyright_logo\").text()\n        return ret\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, fetch_type = \"js\", save = response.save, headers = detail_headers, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, headers = index_headers, callback = spider_handle.index_page)\n        \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(spider_handle, parser, response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://weixin.sogou.com/weixin?type=2&query=%E9%9B%85%E6%80%9D\",\n            \"bread\": [\"微信搜狗\"],\n        },\n    ]\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                           \'base_url\': crawler[\"url\"],\n                       },\n                       headers = index_headers,\n                       callback = self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n\n',NULL,1.0000,3.0000,1466571067.6703),('wenda_baidu_new',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-07 14:57:23\n# Project: wenda_baidu_new\n\n\nfrom pyspider.libs.base_handler import *\nimport MySQLdb\nimport traceback\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nfrom urllib import quote\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"querydb\", charset = \"utf8\")\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n\n        \"headers\": {\n        \'Host\': \'zhidao.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\', \n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n    \n\n    @every(minutes=1 * 30)\n    def on_start(self):\n        sql = \'select id, query from tb_new_query where flag = 0 limit 3000\'\n        try:\n            cursor.execute(sql)\n            for (query_id, query,) in cursor.fetchall():\n                #print type(query) \n                #query = query.encode(\"UTF-8\")\n                #print type(query)\n                #print query\n                #query = unicode(query,\'utf-8\')\n                #print type(query)\n                #print query\n                #word = \'good\'\n                query = query.replace(\' \',\'%20\')\n\n                for i in range(0,51,10):\n                    \n                    self.crawl(\'http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s\'%(i,query),save = {\'query\':query, \'id\':query_id}, headers=self.crawl_config[\'headers\'], callback=self.index_page)\n        except Exception,e:\n            print e\n            \n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.dl\').items():\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.line > a\').text()\n            url = each.find(\'.line > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'],callback=self.detail_page)\n        for each in response.doc(\'.mb-20 > dl\').items():\n            if u\'百度百科\' in each.text():\n                #print \'ok\'\n                continue\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.mb-8 > a\').text()\n            \n            url = each.find(\'.mb-8 > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pager-next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        query_id = response.save.get(\'id\')\n        sql = \"update tb_new_query set flag=1 where id=%s\" % (query_id)\n        try:           \n            cursor.execute(sql)\n            #print query_id\n            #print \'ok\'\n            conn.commit()\n        except Exception, e:\n            print e\n        #if response.doc(\'.word-replace\'):\n        #    return\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = \'baidu\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'category_id\'] = []\n        res_dict[\'class\'] = 34\n        #print response.doc(\'.q-content\').html()\n        res_dict[\'question_detail\'] = response.doc(\'.q-content\').text() if not response.doc(\'.q-content .word-replace\') else \'\'\n        \n        answers_list = []\n        if response.doc(\'.content > .mb-10\'):\n            if not response.doc(\'.content > .mb-10 .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'div.mb-15 > .pos-time\'):\n                    create_time = response.doc(\'div.mb-15 > .pos-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.content > .mb-10\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'div.mt-10\').text(),\n                })\n        if response.doc(\'.quality-content > .quality-content-detail\'):\n            if not response.doc(\'.quality-content > .quality-content-detail .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if response.doc(\'.quality-info > .reply-time\'):\n                    create_time = response.doc(\'.quality-info > .reply-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.quality-content > .quality-content-detail\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'.q-name > a\').text(),\n                })\n        if response.doc(\'.ec-answer\'):\n            #print u\'企业回答\'\n            if not response.doc(\'.ec-answer .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'.ec-time\'):\n                    create_time = response.doc(\'.ec-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.ec-answer\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": \'\',\n                })\n        if response.doc(\'.best-related dd > span\'):\n            if not response.doc(\'.best-related dd > span .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n                if  response.doc(\'.best-related i > span\'):\n                    create_time = response.doc(\'.best-related i\').eq(-1).text().split()[0]\n                    user_name = response.doc(\'.best-related i\').eq(0).text()\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                    user_name = \'\'\n                answers_list.append({\n                    \"content\":  response.doc(\'.best-related dd > span\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": user_name,\n                })\n        for each in response.doc(\'div.answer-text\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            if  each.parent().prev().find(\'.pos-time\'):\n                create_time = each.parent().prev().find(\'.pos-time\').text().split()[0]\n                if not each.parent().prev().find(\'.pos-time\').next():\n                    user_name = each.parent().prev().remove(\'.pos-time\').text()\n                else:\n                    user_name = each.parent().prev().find(\'.pos-time\').next().text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                user_name = \'\'\n            answers_list.append({\n                \"content\":  each.html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": user_name,\n            })\n        for each in response.doc(\'dl.other-answer > .answer\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n            if  each.find(\'.ext-info > i\'):\n                create_time = each.find(\'.ext-info > i\').eq(-1).text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  each.children().eq(0).html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'.ext-info > i\').eq(0).text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'create_user\'] = response.doc(\'.ask-info > span\').eq(1).text()\n        return res_dict',NULL,2.0000,4.0000,1473316412.8621),('wenda_baixing_inc',NULL,'RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-14 10:01:26\n# Project: wenda_baixing_inc\n\nfrom pyspider.libs.base_handler import *\nimport time,datetime,re\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天之前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u\'天之前\', \'\')))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'1\',\n        \"headers\": {\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://zhidao.baixing.com/page-1.html\', headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.questions-list article header\').items():\n            #print each.find(\'.title > a\').eq(0).text()\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.dwqa-title\').eq(0).text().strip()\n            _dict[\'category_id\'] = [each.find(\'span > a\').text()]\n            self.crawl(each.find(\'.dwqa-title\').eq(0).attr.href, headers=self.crawl_config[\'headers\'],save=_dict, callback=self.detail_page)\n         \n        #翻页\n        #for each in response.doc(\'.archive-question-footer a\').items():\n         #   self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.index_page)\n            \n            \n    @config(priority=2,age=1 * 1)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'create_time\'] = get_date(response.doc(\'.dwqa-question > .dwqa-header .dwqa-date\').text().split()[1])\n        res_dict[\'create_user\'] = response.doc(\'.dwqa-question > .dwqa-header .author > a\').text()\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = u\'百姓网\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'question_detail\'] = response.doc(\'.dwqa-content\').eq(0).html().strip()\n        answers_list = []\n        for each in response.doc(\'.dwqa-list-answers > div\').items():\n            if  each.find(\'.dwqa-date > a\'):\n                create_time = get_date(each.find(\'.dwqa-date > a\').text().split()[0].strip())\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  re.sub(r\'<a[^>]+>\',\'\',each.find(\'.dwqa-content > div\').html().strip()).replace(\'</a>\',\'\'),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'.author > a\').text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'category_id\'] = [\'0\',]\n        return res_dict',NULL,1.0000,3.0000,1473818905.7332),('wenda_cxy_iteye',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-12 11:48:40\n# Project: wenda_cxy_iteye\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"headers\": {\n            \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.iteye.com/problems/solved\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.summary\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h3 > a\').text().strip()\n            self.crawl(each.find(\'h3 > a\').attr.href,save=_dict,headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.pagination > a\').items():\n            self.crawl(each.attr.href,headers=self.crawl_config[\'headers\'], callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for info in response.doc(\'.for_sort\').items():\n            _dic = {}\n            \n            #print info.find(\'.solution_dd\').text()\n            _dic[\'name\'] = info.find(\'.user_blog > a\').text().strip()\n            _dic[\'date\'] = info.find(\'.ask_label > span\').text().strip().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n            _dic[\'content\'] = info.find(\'.solution_dd\').html().strip()\n            content_list.append((_dic))\n       \n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'程序员\',\n            \"answers\": content_list,\n            \"source\": \'iteye\',\n            \"question_detail\": response.doc(\'.new_content\').html().strip(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1473657325.2732),('wenda_cxy_segmentfault_inc',NULL,'RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-12 11:29:58\n# Project: wenda_cxy_segmentfault_inc\n\nfrom pyspider.libs.base_handler import *\nimport time\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'周前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'个月前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'年前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    return  time.strftime(\'%Y-%m-%d\',time.localtime())\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"headers\": {\n            \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'https://segmentfault.com/questions\',headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=1* 1)\n    def index_page(self, response):\n        for each in response.doc(\'.stream-list > .stream-list__item\').items():\n            if not each.find(\'.answered\'):\n                continue\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h2.title\').text().strip()\n            self.crawl(each.find(\'h2.title > a\').attr.href, save=_dict, headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.next\').items():\n            self.crawl(each.find(\'a\').attr.href,headers=self.crawl_config[\'headers\'], callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        #print response.doc(\'.question.fmt\').html().strip()\n        content_list = []\n        for info in response.doc(\'article.widget-answers__item\').items():\n            _dic = {}\n            #print info.find(\'.answer_author\').text()\n            _dic[\'name\'] = info.find(\'.answer__info--author-name\').eq(0).text().strip()\n            _dic[\'date\'] = get_date(info.find(\'ul.list-inline > li\').eq(0).text().strip())\n            _dic[\'content\'] = info.find(\'.answer.fmt\').html().strip()\n            content_list.append((_dic))\n       \n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'程序员\',\n            \"answers\": content_list,\n            \"source\": \'segmentfault\',\n            \"question_detail\": response.doc(\'.question.fmt\').html().strip(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1473651194.4657),('wenda_tgnet_inc',NULL,'RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-14 10:05:20\n# Project: wenda_tgnet_inc\n\nfrom pyspider.libs.base_handler import *\nimport time,datetime,re\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天之前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u\'天之前\', \'\')))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\',\n        \"headers\": {\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://wd.tgnet.com/QuestionList/0/1/0/0/0/1/\', headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.tableTopic tbody tr\').items():\n            #print each.find(\'.title > a\').eq(0).text()\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.title > a\').eq(0).text().strip()\n            _dict[\'create_time\'] = get_date(each.find(\'td > .cLGray\').text())\n            _dict[\'create_user\'] = each.find(\'td > a\').text()\n            self.crawl(each.find(\'.title > a\').eq(0).attr.href, headers=self.crawl_config[\'headers\'],save=_dict, callback=self.detail_page)\n         \n        #翻页\n        #for each in response.doc(\'.p > a\').items():\n         #   self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.index_page)\n            \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = u\'天工网\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'question_detail\'] = response.doc(\'#tblInfo .content > div\').eq(0).remove(\'.IsApp\').html().strip()\n        if \'strong\' in res_dict[\'question_detail\']:\n            res_dict[\'question_detail\'] = response.doc(\'#tblInfo .content > div\').eq(0).remove(\'.IsApp\').find(\'strong\').eq(0).html().strip()\n        res_dict[\'question_detail\'] = re.sub(r\'<a[^>]+>\',\'\',res_dict[\'question_detail\']).replace(\'</a>\',\'\')\n        answers_list = []\n        \n        for each in response.doc(\'table.tableReply\').items():\n            if \'tblInfo\' in each.outerHtml():\n                continue\n            if  each.find(\'.topInfo\'):\n                create_time = get_date(each.find(\'.topInfo\').remove(\'*\').text().split()[2].strip().replace(\'/\',\'-\'))\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  re.sub(r\'<a[^>]+>\',\'\',each.find(\'.subjectReply\').html().strip()).replace(\'</a>\',\'\'),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'strong > a\').text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'category_id\'] = [\'0\',]\n        return res_dict',NULL,1.0000,3.0000,1473819066.4113),('wushu_wushu','wushu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 11:15:04\n# Project: wushu_wushu\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wushu.com.cn/xwzx_xhxw.asp\', callback=self.list1_page)\n        self.crawl(\'http://www.wushu.com.cn/xwzx_zhxw.asp\', callback=self.list1_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.fr > a\').items():\n            if \'zt\' in each.attr.href or \'list\' in each.attr.href:\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'body > table\').eq(-2).find(\'table\').eq(0).find(\'table\').eq(-3).find(\'tr\').items():\n            #print each.text()\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.font12gray > .font12gray\').text().strip()\n            #print _dict\n            _dict[\'date\'] = each.find(\'.font12lan\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.font12gray > .font12gray\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.number > a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.font12gray\').eq(3).html().strip()).replace(\'</a>\',\'\')+\'</p>\'\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"wushu\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"武术\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474516521.1904),('wushu_wushu_inc','wushu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 17:56:17\n# Project: wushu_wushu_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=10 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wushu.com.cn/xwzx_xhxw.asp\', callback=self.list1_page)\n        self.crawl(\'http://www.wushu.com.cn/xwzx_zhxw.asp\', callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.fr > a\').items():\n            if \'zt\' in each.attr.href or \'list\' in each.attr.href:\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'body > table\').eq(-2).find(\'table\').eq(0).find(\'table\').eq(-3).find(\'tr\').items():\n            #print each.text()\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.font12gray > .font12gray\').text().strip()\n            #print _dict\n            _dict[\'date\'] = each.find(\'.font12lan\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.font12gray > .font12gray\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.number > a\').items():\n        #    self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.font12gray\').eq(3).html().strip()).replace(\'</a>\',\'\')+\'</p>\'\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"wushu\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"武术\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474538254.7509),('wushu_zhwsbl','wushu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 13:38:09\n# Project: wushu_zhwsbl\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 11:15:04\n# Project: wushu_wushu\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nimport time\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.zhwsbl.com/news/\', callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.fr > a\').items():\n            if \'zt\' in each.attr.href or \'list\' in each.attr.href:\n                continue\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.box_head > .f_r > a\').items(): \n            self.crawl(each.attr.href, callback=self.list1_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.catlist_li\').items():\n            #print each.text()\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'.f_gray\').text().strip().split()[0] or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pages > a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'#article > *\').items():\n            if \'lazy.gif\' in each.html():\n                continue\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip()).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 100:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"zhwsbl\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"武术\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1474523879.3433),('xiaoxue_233_inc','xiaoxue','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 13:05:04\n# Project: xiaoxue_233_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.exam8.com/zige/putonghua/zhinan/\':[u\'考试动态\',u\'考试资讯\'],\n    \'http://www.exam8.com/zige/putonghua/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.exam8.com/zige/putonghua/dongtai/\':[u\'考试动态\'],\n    \'http://www.exam8.com/zige/putonghua/zhenti/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/moni/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/tuijian/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/fudao/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/jingyan/\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.233.com/xiaoxue/yinianji/\',callback=self.index_page)\n        \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.nav\').find(\'a:gt(0):lt(10)\').items():\n            #print each.text()\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            if u\'小升初\' in each.text() or u\'作文\' in each.text() or u\'家长知道\' in each.text():\n                _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n            \n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'h2 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)          \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)\n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.column-bd dd\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.h4 > h1\').text().strip()\n            _dict[\'date\'] = each.find(\'.column-info > .f-f12\').text().strip().split()[0].replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.h4\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.but\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.news-body > *\').remove(\'script\').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'学习推荐\' in each.text() or u\'学习交流平台\' in each.text() or u\'请关注233小学\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"233\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"小学\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474965037.0186),('xiaoxue_51edu','xiaoxue','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-26 13:37:14\n# Project: xiaoxue_51edu\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.exam8.com/zige/putonghua/zhinan/\':[u\'考试动态\',u\'考试资讯\'],\n    \'http://www.exam8.com/zige/putonghua/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.exam8.com/zige/putonghua/dongtai/\':[u\'考试动态\'],\n    \'http://www.exam8.com/zige/putonghua/zhenti/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/moni/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/tuijian/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/fudao/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/jingyan/\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'3\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.51edu.com/xiaoxue/yinianji/\',callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.nav3 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n        for each in response.doc(\'.nav4 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n        for each in response.doc(\'.nav5 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n        for each in response.doc(\'.nav6 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.wdlbd1 li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'p > a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'p > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'#pages > a\').items():\n            self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)          \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.wdlbd1 li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'p > a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'p > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'#pages > a\').items():\n            self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.wdlbd1 li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'p > a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'p > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'#pages > a\').items():\n            self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'#endText > *\').remove(\'script \').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"51edu\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"小学\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,3.0000,5.0000,1474938856.3255),('xiaoxue_51edu_inc','xiaoxue','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 16:45:26\n# Project: xiaoxue_51edu_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.exam8.com/zige/putonghua/zhinan/\':[u\'考试动态\',u\'考试资讯\'],\n    \'http://www.exam8.com/zige/putonghua/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.exam8.com/zige/putonghua/dongtai/\':[u\'考试动态\'],\n    \'http://www.exam8.com/zige/putonghua/zhenti/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/moni/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/tuijian/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/fudao/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/jingyan/\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'3\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.51edu.com/xiaoxue/yinianji/\',callback=self.index_page)\n        \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.nav3 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n        for each in response.doc(\'.nav4 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n        for each in response.doc(\'.nav5 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n        for each in response.doc(\'.nav6 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n            \n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.wdlbd1 li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'p > a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'p > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'#pages > a\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list1_page)          \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.wdlbd1 li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'p > a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'p > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'#pages > a\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)\n            \n    @config(age=1 * 1)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.wdlbd1 li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'p > a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'p > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'#pages > a\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'#endText > *\').remove(\'script \').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"51edu\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"小学\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474966016.9179),('xingzuo123_inc','xinzuo','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 15:06:21\n# Project: xingzuo123\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n        \'http://www.xingzuo123.com/12xingzuo/\':[u\'星座\'],\n        \'http://www.xingzuo123.com/xingzuoyunshi/\':[u\'运势\'],\n        \'http://www.xingzuo123.com/htm/starlove/\':[u\'配对\'],\n        \'http://www.xingzuo123.com/12shengxiao/\':[u\'生肖\'],\n        \'http://www.xingzuo123.com/smdq/\':[u\'算命\'],\n        \'http://www.xingzuo123.com/mianxiang/\':[u\'面相\']\n\n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.listbox li\').items():\n            _dict = {}\n            _dict[\'title\']  = each.find(\'a\').eq(0).text() or each.find(\'a\').eq(1).text() or \'\'\n            _dict[\'cover\'] = each.find(\'a\').eq(0).find(\'img\').attr.src or \'\'\n            url =  each.find(\'a\').eq(0).attr.href\n            _dict[\'date\'] = each.find(\'small\').text().split(\'：\')[-1].split()[0]\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'星座\'\n        res_dict[\'source\'] = \'xingzuo123.com\'\n        res_dict[\'url\'] = response.url\n        res_dict[\'data_weight\'] = 0\n        content_list = []\n        for each in response.doc(\'div.article_content.info_area  p\').items():\n            info = each.html()\n            if info:\n                content_list.append(info)\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        return res_dict\n',NULL,1.0000,3.0000,1472546303.4206),('xingzuo_xzw','xingzuo','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 15:06:21\n# Project: xingzuo123\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n        \'http://www.xzw.com/astro/xiangguan/l_1.html\':[u\'星座\'],\n        \'http://www.xzw.com/shengxiao/zishu/l.html\':[u\'生肖\'],\n        \'http://www.xzw.com/astrolabe/\':[u\'算命\'],\n        \'http://www.xzw.com/fengshui/\':[u\'算命\'],\n        \'http://www.xzw.com/test/\':[u\'算命\']\n\n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            if v[0] == u\'生肖\':\n                self.crawl(k, save = {\'bread\':v},callback=self.list_page)\n            if \'astrolabe\' in k:\n                self.crawl(k, save = {\'bread\':v},callback=self.list1_page)\n            if \'fengshui\' in k or \'test\' in k:\n                self.crawl(k, save = {\'bread\':v},callback=self.list2_page)\n            else:\n                self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n                \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.l1\').items():\n            _dict = {}\n            url =  each.attr.href\n            #print url\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url+\'l.html\', save = _dict,callback=self.index_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            _dict = {}\n            url =  each.attr.href\n            #print url\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.index_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list2_page(self, response):\n        for each in response.doc(\'.al > h4 > a\').items():\n            _dict = {}\n            url =  each.attr.href\n            #print url\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'h3 > a\').items():\n            _dict = {}\n            _dict[\'title\']  = each.text()\n            url =  each.attr.href\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n            \n        #翻页\n        for each in response.doc(\'.pagelist > a\').items():\n            _dict = {}\n            url =  each.attr.href\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.index_page)\n        \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'星座\'\n        res_dict[\'source\'] = \'xzw\'\n        res_dict[\'url\'] = response.url\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'content\'] = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.sbody\').remove(\'.adb\').remove(\'.title\').remove(\'.info\').remove(\'script\').remove(\'span:last\').html().strip()).replace(\'</a>\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\')+\'</p>\'\n        return res_dict\n',NULL,1.0000,3.0000,1474422079.5569),('xmswim_inc','youyong','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 09:56:55\n# Project: xmswim\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n    content = content.replace(\'</a>\',\'\')\n    return  content\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n\n    \n    page_dict = {\n\n        \'http://www.xmswim.com/new/\':[u\'游泳新闻\'],\n        \'http://www.xmswim.com/jishu/\':[u\'游泳技术\'],\n        \'http://www.xmswim.com/jiankang/\':[u\'游泳健康\'],\n        \'http://www.xmswim.com/rcswim/\':[u\'水上救生\'],\n        \'http://www.xmswim.com/resource/\':[u\'游泳资源\'],\n        \'http://www.xmswim.com/wenxian/\':[u\'游泳文献\']\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'div.deanartice li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.deanarticername a\').text()\n            _dict[\'cover\'] = each.find(\'.deanarticel img\').attr.src.replace(\'/new\',\'\').replace(\'/jishu\',\'\').replace(\'/jiankang\',\'\').replace(\'/rcswim\',\'\').replace(\'/resource\',\'\').replace(\'/wenxian\',\'\')\n            if \'template\'  in _dict[\'cover\']:\n                _dict[\'cover\'] = \'\'\n            url = each.find(\'.deanarticername a\').attr.href\n            _dict[\'date\'] = each.find(\'span.deanfabushijian\').text().split()[0]\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'td#article_content\').remove(\'script\').remove(\'embed\').html()\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'游泳\'\n        res_dict[\'source\'] = u\'xmswim.com\'\n        res_dict[\'url\'] = response.url\n        return res_dict\n',NULL,1.0000,3.0000,1472546316.9926),('yasi_taisha_inc','ielts','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-25 17:44:23\n# Project: yasi_taisha\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    _dict = {\n    \'news\': \'快讯动态\', \'guidance\':\'分类资讯\', \'experience\':\'经验分享\', \'download\':\'分类资讯\', \'t-guide\':\'分类资讯\', \'machine\':\'雅思机经\'\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for page in self._dict: \n            self.crawl(\'http://www.taisha.org/ielts/%s/\'%page, save={\'type\': self._dict[page]}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        type = response.save[\'type\']\n        for each in response.doc(\'.html_content dd\').items():\n            _dict = {}\n            url = each.find(\'.title > a\').attr.href\n            #_dict[\'url\'] = url\n            _dict[\'tag\'] = each.find(\'.bot > a\').text().split(\' \')\n            _dict[\'brief\'] = each.find(\'p\').text().replace(u\'[详情]\', \'\')\n            _dict[\'type\'] = type\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        #for each in response.doc(\'span > a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        try:\n            date = response.doc(\'.txt_title span\').text()[-19:]\n        except:\n            date = \'\'\n        #content = response.doc(\'.txt_content\').html()\n        content_info = response.doc(\'.txt_content > p\')\n        content_list = []\n        for info in content_info:\n            answer = pyquery.PyQuery(info)\n            items = answer.html()\n            if answer.text() == u\'太傻网雅思频道精品推荐：\':\n                break\n            content_list.append((items.replace(u\'<a href=\"http://www.taisha.org/ielts/\" target=\"_blank\" class=\"keylink\">雅思</a>\',u\'雅思\').replace(u\'太傻\', u\'跟谁学\')))\n        content_list = content_list[:-1]\n        if not content_list:\n            return None\n        title = response.doc(\'.txt_title > h2\').text()\n        if u\'回忆版\' in title:\n            return None\n        res_dict[\'title\'] = title \n        res_dict[\'url\'] = response.url\n        res_dict[\'date\'] = date[0:10]\n        res_dict[\'source\'] = u\'太傻留学\'\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'subject\'] = u\'雅思\'\n        res_dict[\'bread\'] = response.doc(\'#nav_location a\').text().split(\' \')[1:]\n        res_dict[\'bread\'].extend(res_dict[\'tag\'])\n        res_dict[\'bread\'].append((response.save[\'type\']))\n        res_dict[\'bread\'] = list(set(res_dict[\'bread\']))\n        res_dict[\'class\'] = 17\n        res_dict.pop(\'type\')\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471329896.5925),('yasi_xiaozhan_inc','ielts','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-08 10:48:14\n# Project: yasi_xiaozhan_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for i in range(1,247):\n            url = \'http://ielts.zhan.com/\'\n            self.crawl(url, callback=self.index_page)\n            #self.crawl(url, fetch_type=\'js\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.pull-right > p > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        cnt = 0\n        for each in response.doc(\'.active > a\'):\n            cnt = cnt + 1\n        if cnt == 1:\n            return None\n        \n        url = response.url\n        brief = response.doc(\'.first-words\').text()\n        title = response.doc(\'title\').text()\n        title = title.replace(u\'小站\', u\'跟谁学\')\n        date = \'\'\n        for each in response.doc(\'.pull-left > span\').items():\n            #date =\'\'\n            date = each.text().replace(u\'年\', \'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n            break\n            \n        bread = []\n        for each in response.doc(\'.head-crumbs-a-active\').items():\n            bread.append(each.text())\n        \n        content_list = []\n        for info in response.doc(\'.article-content > p\'):\n            content_list.append(pyquery.PyQuery(info).html())\n        content = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        content = content.replace(\'img src=\\\"/uploadfile\', \'img src=\\\"http://ielts.zhan.com/uploadfile\')\n        content = content.replace(u\'小站\', u\'本站\')\n        brief = brief.replace(u\'小站\', u\'本站\')\n        if len(content) < 10:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"brief\": brief,\n            \"content\": content,\n            \"date\": date,\n            \"source\": \'zhan.com\',\n            \"subject\": u\'雅思\',\n            \"bread\": bread,\n            \"class\": 17,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471329900.4770),('yingyangshi_kaoshi110','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:23:00\n# Project: yingyangshi_kaoshi110\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://zige.kaoshi110.com/yingyangshi/ksdt/\':[u\'考试动态\'],\n    \'http://zige.kaoshi110.com/yingyangshi/ksdg/\':[u\'教材大纲\'],\n    \'http://zige.kaoshi110.com/yingyangshi/shiti/\':[u\'考试题库\'],\n    \'http://zige.kaoshi110.com/yingyangshi/fudao/\':[u\'科目辅导\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url,v in urls.iteritems():\n            self.crawl(url,save={\'bread\':v},callback=self.list2_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.nav\').find(\'a:gt(0):lt(10)\').items():\n            #print each.text()\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            if u\'小升初\' in each.text() or u\'作文\' in each.text() or u\'家长知道\' in each.text():\n                _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'h2 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)          \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.list-cont li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().split()[0].replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.list-pt a\').items():\n            self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.w-word > *\').remove(\'script\').items():\n            if u\'点击查看\' in each.text() or  u\'汇总\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'学习交流平台\' in each.text() or \'BAIDU_CLB_fillSlot\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"kaoshi110\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"营养师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1475138973.1271),('yingyangshi_kaoshi110_inc','yingyangshi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:23:00\n# Project: yingyangshi_kaoshi110\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://zige.kaoshi110.com/yingyangshi/ksdt/\':[u\'考试动态\'],\n    \'http://zige.kaoshi110.com/yingyangshi/ksdg/\':[u\'教材大纲\'],\n    \'http://zige.kaoshi110.com/yingyangshi/shiti/\':[u\'考试题库\'],\n    \'http://zige.kaoshi110.com/yingyangshi/fudao/\':[u\'科目辅导\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        for url,v in urls.iteritems():\n            self.crawl(url,save={\'bread\':v},callback=self.list2_page)\n        \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.nav\').find(\'a:gt(0):lt(10)\').items():\n            #print each.text()\n            _dict = {}\n            _dict[\'bread\'] = [each.text().strip()]\n            if u\'小升初\' in each.text() or u\'作文\' in each.text() or u\'家长知道\' in each.text():\n                _dict[\'bread\'] = [u\'相关信息\']\n            self.crawl(each.attr.href,save=_dict,callback=self.list_page)\n            \n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'h2 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)          \n    @config(age=1)\n    def list1_page(self, response):\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)\n            \n    @config(age=1)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.list-cont li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().split()[0].replace(\'/\',\'-\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.list-pt a\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list2_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.w-word > *\').remove(\'script\').items():\n            if u\'点击查看\' in each.text() or  u\'汇总\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'学习交流平台\' in each.text() or \'BAIDU_CLB_fillSlot\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"kaoshi110\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"营养师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1475138954.3677),('yingyangshi_wangxiao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:33:48\n# Project: yingyangshi_wangxiao\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wangxiao.cn/yys/\',callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'li > .navlink\').items():\n            #print each.text()\n            _dict = {}\n            if u\'辅导资料\' in each.text():\n                _dict[\'bread\'] = [u\'科目辅导\']\n            elif u\'模拟试题\' in each.text():\n                _dict[\'bread\'] = [u\'考试题库\']\n            elif u\'历年真题\' in each.text():\n                _dict[\'bread\'] = [u\'考试题库\']\n            elif u\'知识点\' in each.text():\n                _dict[\'bread\'] = [u\'考试题库\']\n            elif u\'在线考试\' in each.text():\n                continue\n            else:\n                _dict[\'bread\'] = [each.text().strip()]\n            \n            self.crawl(each.attr.href,save = _dict,callback=self.list_page)\n            \n                \n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict,callback=self.list_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pNext\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pNext\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.newsCon > *\').remove(\'script\').items():\n            #print each.text()\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'责任编辑\' in each.text() or u\'更多关注\' in each.text() or u\'相关推荐\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"wangxiao\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"营养师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1475138966.3645),('yingyangshi_wangxiao_inc','yingyangshi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:33:48\n# Project: yingyangshi_wangxiao\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wangxiao.cn/yys/\',callback=self.index_page)\n        \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'li > .navlink\').items():\n            #print each.text()\n            _dict = {}\n            if u\'辅导资料\' in each.text():\n                _dict[\'bread\'] = [u\'科目辅导\']\n            elif u\'模拟试题\' in each.text():\n                _dict[\'bread\'] = [u\'考试题库\']\n            elif u\'历年真题\' in each.text():\n                _dict[\'bread\'] = [u\'考试题库\']\n            elif u\'知识点\' in each.text():\n                _dict[\'bread\'] = [u\'考试题库\']\n            elif u\'在线考试\' in each.text():\n                continue\n            else:\n                _dict[\'bread\'] = [each.text().strip()]\n            \n            self.crawl(each.attr.href,save = _dict,callback=self.list_page)\n            \n                \n\n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href,save = _dict,callback=self.list_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pNext\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save[\'bread\']\n           # self.crawl(each.attr.href,save = _dict,callback=self.list_page)\n            \n    @config(age=1)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pNext\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save[\'bread\']\n           # self.crawl(each.attr.href,save = _dict,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.newsCon > *\').remove(\'script\').items():\n            #print each.text()\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'责任编辑\' in each.text() or u\'更多关注\' in each.text() or u\'相关推荐\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"wangxiao\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"营养师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1475138960.6485),('yishi_exam8','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 17:30:39\n# Project: yishi_exam8\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 10:04:07\n# Project: zhuanshengben_exam8\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.exam8.com/zige/putonghua/zhinan/\':[u\'考试动态\',u\'考试资讯\'],\n    \'http://www.exam8.com/zige/putonghua/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.exam8.com/zige/putonghua/dongtai/\':[u\'考试动态\'],\n    \'http://www.exam8.com/zige/putonghua/zhenti/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/moni/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/tuijian/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/fudao/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/jingyan/\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.exam8.com/yixue/yishi/\',callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.navxl01 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl02 > font\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl03 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl04 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl04 > font\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n                \n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.dtleft > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.lbpage a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            if u\'关注考试吧\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.lbpage a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.left > div\').eq(6).children().items():\n            if u\'>>>\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text() or u\'你想要的都有\' in each.text() or (u\'关注\' in each.text() and u\'微信\' in each.text()):\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        _list = []\n        for each in response.doc(\'.left > div\').eq(5).children().items():\n            if u\'>>>\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text() or u\'你想要的都有\' in each.text() or (u\'关注\' in each.text() and u\'微信\' in each.text()):\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content += \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"exam8\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"执业医师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1475139277.7599),('yishi_exam8_inc','yishi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 17:30:39\n# Project: yishi_exam8\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.exam8.com/zige/putonghua/zhinan/\':[u\'考试动态\',u\'考试资讯\'],\n    \'http://www.exam8.com/zige/putonghua/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.exam8.com/zige/putonghua/dongtai/\':[u\'考试动态\'],\n    \'http://www.exam8.com/zige/putonghua/zhenti/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/moni/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/tuijian/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/fudao/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/jingyan/\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.exam8.com/yixue/yishi/\',callback=self.index_page)\n        \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.navxl01 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl02 > font\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl03 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl04 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl04 > font\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n                \n\n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.dtleft > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.lbpage a\').items():\n         #   self.crawl(each.attr.href,callback=self.list_page)\n    \n    @config(age=1)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            if u\'关注考试吧\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.lbpage a\').items():\n         #   self.crawl(each.attr.href,callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.left > div\').eq(6).children().items():\n            if u\'>>>\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text() or u\'你想要的都有\' in each.text() or (u\'关注\' in each.text() and u\'微信\' in each.text()):\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        _list = []\n        for each in response.doc(\'.left > div\').eq(5).children().items():\n            if u\'>>>\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text() or u\'你想要的都有\' in each.text() or (u\'关注\' in each.text() and u\'微信\' in each.text()):\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content += \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"exam8\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"执业医师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1475139280.6618),('yishi_med66','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:57:47\n# Project: yishi_med66\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:23:00\n# Project: yingyangshi_kaoshi110\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.med66.com/yishizigekaoshi/?c=9160212\',callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.snbody a\').items():\n            #print each.text()\n            self.crawl(each.attr.href,callback=self.list2_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'h2 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)          \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.fl > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.fr\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.fl > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'divpagestr2016 a\').items():\n            self.crawl(each.attr.href,callback=self.list2_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'#fontzoom > *\').remove(\'script\').items():\n            if u\'点击查看\' in each.text() or  u\'汇总\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'教育网搜集整理，\' in each.text() or \'BAIDU_CLB_fillSlot\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"med66\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"执业医师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1475139284.5609),('yishi_med66_inc','yishi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 16:57:47\n# Project: yishi_med66\n\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=4 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.med66.com/yishizigekaoshi/?c=9160212\',callback=self.index_page)\n        \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.snbody a\').items():\n            #print each.text()\n            self.crawl(each.attr.href,callback=self.list2_page)\n            \n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'h2 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)          \n    @config(age=1)\n    def list1_page(self, response):\n        for each in response.doc(\'.wdlbt > p > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict, callback=self.list2_page)\n            \n    @config(age=1)\n    def list2_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.fl > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.fr\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'.fl > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'divpagestr2016 a\').items():\n         #   self.crawl(each.attr.href,callback=self.list2_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'#fontzoom > *\').remove(\'script\').items():\n            if u\'点击查看\' in each.text() or  u\'汇总\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'教育网搜集整理，\' in each.text() or \'BAIDU_CLB_fillSlot\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"med66\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"执业医师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1475139287.0211),('yishi_wangxiao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 17:16:16\n# Project: yishi_wangxiao\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'2\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wangxiao.cn/yishi/28102810617.html\',callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.mainnav a\').items():\n            #print each.text()\n            self.crawl(each.attr.href,callback=self.list_page)\n            \n                \n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pNext\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pNext\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.newsCon > *\').remove(\'script\').items():\n            #print each.text()\n            \n                \n                \n            if u\'责任编辑\' in each.text() or u\'更多关注\' in each.text() or u\'相关推荐：\' in each.text():\n                break\n            if not each.html():\n                continue\n            if \'http://img.wangxiao.cn/files/2013/9/29/ksjs.gif\' in each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"med66\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"执业医师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1475139294.9720),('yishi_wangxiao_inc','yishi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 17:16:16\n# Project: yishi_wangxiao\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'2\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.wangxiao.cn/yishi/28102810617.html\',callback=self.index_page)\n        \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.mainnav a\').items():\n            #print each.text()\n            self.crawl(each.attr.href,callback=self.list_page)\n            \n                \n\n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.more > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pNext\').items():\n         #   self.crawl(each.attr.href,callback=self.list_page)\n            \n    @config(age=1)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.newsList li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pNext\').items():\n         #   self.crawl(each.attr.href,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.newsCon > *\').remove(\'script\').items():\n            #print each.text()\n            \n                \n                \n            if u\'责任编辑\' in each.text() or u\'更多关注\' in each.text() or u\'相关推荐：\' in each.text():\n                break\n            if not each.html():\n                continue\n            if \'http://img.wangxiao.cn/files/2013/9/29/ksjs.gif\' in each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"med66\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"执业医师\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }',NULL,1.0000,3.0000,1475139298.7047),('yixue_zhengbao_inc','kztk','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 16:02:22\n# Project: yixue_zhengbao_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ntype_dict = {\n    \'chuji\': [u\'财会经济\', u\'初级会计师\'],\n    \'chujizhicheng\': [u\'财会经济\', u\'初级会计师\'],\n    \'zhongji\': [u\'财会经济\', u\'中级会计师\'],\n    \'zhongjizhicheng\': [u\'财会经济\', u\'中级会计师\'],\n    \'shiwushi\': [u\'财会经济\', u\'注册税务师\'],\n    \'zhukuai\': [u\'财会经济\', u\'注册会计师\'],\n    \'gaoji\': [u\'财会经济\', u\'高级会计师\'],\n    \'congye\': [u\'财会经济\', u\'会计从业\'],\n    u\'注册会计师\': [u\'财会经济\', u\'注册会计师\'],\n    u\'经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'初级会计职称\': [u\'财会经济\', u\'初级会计师\'],\n    u\'中级会计职称\': [u\'财会经济\', u\'中级会计师\'],\n    u\'税务师\': [u\'财会经济\', u\'注册税务师\'],\n    u\'统计师\': [u\'财会经济\', u\'统计师\'],\n    u\'审计师\': [u\'财会经济\', u\'审计师\'],\n    u\'高级经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'高级会计\': [u\'财会经济\', u\'高级会计师\'],\n    u\'理财规划师\': [u\'财会经济\', u\'理财规划师\'],\n\n    u\'英语四级\': [u\'外语考试\', u\'英语四六级\'],\n    u\'英语六级\': [u\'外语考试\', u\'英语四六级\'],\n    u\'雅思\': [u\'外语考试\', u\'雅思\'],\n    u\'托福\': [u\'外语考试\', u\'托福\'],\n    u\'职称英语\': [u\'外语考试\', u\'职称英语\'],\n    u\'商务英语\': [u\'外语考试\', u\'商务英语\'],\n    u\'公共英语\': [u\'外语考试\', u\'公共英语\'],\n    u\'日语\': [u\'外语考试\', u\'日语\'],\n    u\'GRE考试\': [u\'外语考试\', u\'GRE考试\'],\n    u\'专四专八\': [u\'外语考试\', u\'专四专八\'],\n    u\'口译笔译\': [u\'外语考试\', u\'口译笔译\'],\n\n    \'zaojia\': [u\'建筑工程\', \'造价工程师\'],\n    u\'一级建造师\': [u\'建筑工程\', u\'一级建造师\'],\n    u\'二级建造师\': [u\'建筑工程\', u\'二级建造师\'],\n    u\'咨询工程师\': [u\'建筑工程\', u\'咨询工程师\'],\n    u\'造价工程师\': [u\'建筑工程\', \'造价工程师\'],\n    u\'结构工程师\': [u\'建筑工程\', \'结构工程师\'],\n    u\'物业管理\': [u\'建筑工程\', u\'物业管理师\'],\n    u\'城市规划\': [u\'建筑工程\', u\'城市规划师\'],\n    u\'给排水工程\': [u\'建筑工程\', u\'给排水工程\'],\n    u\'电气工程\': [u\'建筑工程\', u\'电气工程师\'],\n    u\'公路监理师\': [u\'建筑工程\', u\'公路监理师\'],\n    u\'消防工程师\': [u\'建筑工程\', u\'消防工程师\'],\n    u\'消防\': [u\'建筑工程\', u\'消防工程师\'],\n\n    u\'物流师\': [u\'职业资格\', u\'物流师\'],\n    u\'人力资源\': [u\'职业资格\', u\'人力资源\'],\n    u\'心理咨询师\': [u\'职业资格\', u\'心理咨询师\'],\n    u\'公共营养师\': [u\'职业资格\', u\'公共营养师\'],\n    u\'秘书资格\': [u\'职业资格\', u\'秘书资格\'],\n    u\'秘书资格\': [u\'职业资格\', u\'秘书资格\'],\n    u\'证券从业资格\': [u\'职业资格\', u\'证券经纪人\'],\n    u\'电子商务师\': [u\'职业资格\', u\'电子商务\'],\n    u\'期货从业\': [u\'职业资格\', u\'期货从业\'],\n    u\'教师资格\': [u\'职业资格\', u\'教师资格\'],\n    u\'管理咨询师\': [u\'职业资格\', u\'管理咨询师\'],\n    u\'导游证\': [u\'职业资格\', u\'导游证\'],\n\n    u\'英语\': [u\'学历教育\', u\'考研\'],\n    u\'数学\': [u\'学历教育\', u\'考研\'],\n    u\'政治\': [u\'学历教育\', u\'考研\'],\n    u\'专业课\': [u\'学历教育\', u\'考研\'],\n\n    u\'执业医师\': [u\'医学卫生\', u\'执业医师\'],\n    u\'初级药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'初级药士\': [u\'医学卫生\', u\'执业药师\'],\n    u\'主管药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'主管中药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'中药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'中药士\': [u\'医学卫生\', u\'执业药师\'],\n    u\'临床执业\': [u\'医学卫生\', u\'临床执业\'],\n    u\'临床医师\': [u\'医学卫生\', u\'临床执业\'],\n    u\'中医医师\': [u\'医学卫生\', u\'中医执业\'],\n    u\'中医执业\': [u\'医学卫生\', u\'中医执业\'],\n    u\'中西医医师\': [u\'医学卫生\', u\'中西医执业\'],\n    u\'中西执业\': [u\'医学卫生\', u\'中西医执业\'],\n    u\'中医助理\': [u\'医学卫生\', u\'中医助理\'],\n    u\'中西医助理\': [u\'医学卫生\', u\'中西医助理\'],\n    u\'主治\': [u\'医学卫生\', u\'主治\'],\n    u\'全科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'内科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'外科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'儿科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'妇产科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'检验\': [u\'医学卫生\', u\'检验\'],\n    u\'检验士\': [u\'医学卫生\', u\'检验\'],\n    u\'检验师\': [u\'医学卫生\', u\'检验\'],\n    u\'主管检验师\': [u\'医学卫生\', u\'检验\'],\n    u\'执业护士资格\': [u\'医学卫生\', u\'执业护士\'],\n    u\'护士资格\': [u\'医学卫生\', u\'执业护士\'],\n\n    u\'成人高考\': [u\'学历教育\', u\'成人高考\'],\n    u\'自学考试\': [u\'学历教育\', u\'自考\'],\n    u\'MBA考试\': [u\'学历教育\', u\'MBA\'],\n    u\'法律硕士\': [u\'学历教育\', u\'法律硕士\'],\n    u\'专升本\': [u\'学历教育\', u\'专升本\'],\n    # u\'\':[u\'学历教育\',u\'工程硕士\'],\n    u\'MPA考试\': [u\'学历教育\', u\'公共硕士\'],\n    # u\'\':[u\'学历教育\',u\'考研\'],\n}\n\nsome_url =[\n    \'http://www.chinaacc.com/chujizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/chujizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/\',\n    \'http://www.chinaacc.com/zaojia/zt/\',\n    \'http://www.chinaacc.com/zaojia/mnst/\',    \n]\n\nremove_name = [\n    u\'卫生网校\',\n    u\'医学书店\',\n    u\'定期考核\',\n    u\'医学会议\',\n    u\'继续教育\',\n    u\'学习卡\',\n    u\'首页\',\n    u\'论坛\',\n    u\'邮箱\',\n]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.med66.com/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.navItem > div li\').items():\n            if each.text().replace(\' \', \'\') in remove_name:\n                continue\n            _dict = {}\n            if each.text().replace(\' \', \'\') in type_dict.keys():\n                _dict[\'bread\'] = type_dict[each.text().replace(\' \', \'\')]\n            else:\n                _dict[\'bread\'] = [\'医学卫生\', each.text().replace(\' \', \'\')]\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.list_page)\n         \n        \n    @config(age=1*1)\n    def list_page(self, response):\n        for each in response.doc(\'.snbody > .subnav\').items():\n            if \'复习指导\' not in each.text():\n                continue\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            for each1 in each.find(\'a\').items():\n                self.crawl(each1.attr.href, save = _dict, callback=self.list_page1)\n        \n    @config(age=1*1)\n    def list_page1(self, response):\n        for each in response.doc(\'.xinxi li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.fl > a\').text()\n            if \'报名\' in _dict[\'title\'] or \'名师\' in _dict[\'title\'] or \'汇总\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'.fr\').text().replace(\'[\',\'\').replace(\']\',\'\')\n            self.crawl(each.find(\'.fl > a\').attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页 \n        #for each in response.doc(\'divpagestr2016 a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'#fontzoom\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'全网首发\' in each1.text():\n                    continue\n                if u\'估分更准确\' in each1.text():\n                    continue\n                if u\'独家\' in each1.text():\n                    continue\n                if u\'点击\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text() or u\'推荐阅读\' in each1.text() or u\'相关推荐\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'以上\' in each1.text() and u\'是中华会计网\' in each1.text():\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            \n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\', \'\').replace(\'\\n\', \'\').replace(\'\\t\', \'\').replace(u\'医学教育网\', \'\').replace(\'【 】\', \'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'),\n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'医学教育网\', \'\').replace(\n                    \'None\', \'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'医学教育网\', \'\').replace(\n                    \'None\', \'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'医学教育网\', u\'\').replace(\n                    \'建设网校\', \'\').replace(\'None\', \'\') + str(\n                    response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(\n                    u\'医学教育网\', \'\').replace(\'建设网校\', \'\').replace(\'None\', \'\'),\n                \"tdk_title\": response.doc(\'head > title\').eq(0).text().replace(u\'医学教育网\', \'\') + u\' 跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"医学教育网\",\n            }\n\n',NULL,1.0000,3.0000,1471334954.4903),('youshengxiao_ysxiao_zhongdianxiaoxue','youshengxiao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-30 10:41:03\n# Project: youshengxiao_jiaoliu\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    page_dict = {\n    \'http://www.ysxiao.cn/xiaoxue-haidianqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-xichengqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-chaoyangqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-dongchengqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-fengtaiqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-qitaqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/zhongdianxiaoxue/xiaoxue-shijingshanqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/beijingyoushengxiao/youshengxiaoxianjie/\': u\'经验交流\'\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in self.page_dict.iteritems():\n            self.crawl(k, save={\'label\': v}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        label = response.save[\'label\']\n        for each in response.doc(\'.desc\').items():\n            _dict = {}\n            url = each.find(\'a\').attr.href\n            _dict[\'label\'] = label\n            _dict[\'date\'] = each.find(\'p\').text()\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        \n        #for each in response.doc(\'.pages a\').items():\n        #    self.crawl(each.attr.href, save={\'label\': label}, callback=self.index_page)\n            \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for each in response.doc(\'p\'):\n            _cont = pyquery.PyQuery(each).html().replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\')\n            if _cont:\n                content_list.append((_cont))\n        \n        res_dict = response.save\n        #if res_dict[\'label\'] == u\'北京幼升小招生简章\':\n        content_list = content_list[:-3]\n        if not content_list:\n            return None\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        res_dict[\"content\"] = \'\'.join([\'<p>%s</p>\'%v for v in content_list])\n        #res_dict[\"tags\"] = response.doc(\'#tags > a\').text().split()\n        res_dict[\'subject\'] = u\'幼升小\'\n        res_dict[\'bread\'] = response.doc(\'#position a\').text().split()\n        res_dict[\'bread\'][0] = res_dict[\'label\']\n        res_dict[\'bread\'] = list(set(res_dict[\'bread\']))\n        res_dict.pop(\'label\')\n        res_dict[\'brief\'] = response.doc(\'#desc\').text()\n        res_dict[\'class\'] = 23\n        res_dict[\'source\'] = \'ysxiao.com\'\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,1.0000,3.0000,1471332766.8603),('youshengxiao_ysxiao_zixun_inc','youshengxiao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-17 15:15:26\n# Project: youshengxiao_ysxiao_zixun_inc\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    zhengce_map = {\'haidianqu-youshengxiao\': u\'幼升小海淀区政策\', \n                   \'xichengqu-youshengxiao\': u\'幼升小西城区政策\', \n                   \'chaoyangqu-youshengxiao\': u\'幼升小朝阳区政策\', \n                   \'dongchengqu-youshengxiao\': u\'幼升小东城区政策\', \n                   \'qita\': u\'幼升小其他城区政策\', \n                   \'youshengxiao-huapian\': u\'北京幼升小招生划片\',\n                   \'zhaoshengjianzhang\': u\'北京幼升小招生简章\',\n                   \'youshengxiao-xuequfang\': u\'北京幼升小学区房\',\n                   \'youshengxiao-zexiao\': u\'北京幼升小择校技巧\',\n                   \'ruxuekaoshizhenti\': u\'北京幼升小名校试题\',\n                   \'youshengxiao-mianshi\': u\'北京幼升小面试辅导\',\n                   \'youshengxiao-jianli\': u\'北京幼升小简历\',\n                   \'beijing-youeryuan/youeryuanruyuanjiaoliu\': u\'北京幼升小经验交流\',\n                   \'zhaoshengjianzhang\': u\'招生简章\',\n                   }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for page in self.zhengce_map:\n            self.crawl(\'http://www.ysxiao.cn/%s/\'%page, save={\'label\': self.zhengce_map[page]}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        label = response.save[\'label\']\n        for each in response.doc(\'.desc\').items():\n            _dict = {}\n            url = each.find(\'a\').attr.href\n            img = each.find(\'.hasimg img\').attr.src\n            if img:\n                _dict[\'img\'] = \'http://www.ysxiao.cn\' + img\n            _dict[\'date\'] = each.find(\'p\').text()\n            _dict[\'label\'] = label\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.pages a\').items():\n            self.crawl(each.attr.href, save={\'label\': label}, callback=self.index_page)\n        \'\'\' \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for each in response.doc(\'p\'):\n            _cont =pyquery.PyQuery(each).html()\n            if _cont:\n                _cont = _cont.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'src=\"/fup/\',\'src=\"http://www.ysxiao.cn/fup/\')\n                content_list.append((_cont))\n        \n        res_dict = response.save\n        if res_dict[\'label\'] in (u\'北京幼升小招生简章\', u\'幼升小海淀区政策\', u\'幼升小西城区政策\', u\'幼升小朝阳区政策\', u\'幼升小东城区政策\', u\'幼升小其他城区政策\', u\'北京幼升小经验交流\'):\n            content_list = content_list[:-2]\n        if not content_list:\n            return None\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        \n        res_dict[\"content\"] = \'\'.join([\'<p>%s</p>\'%v for v in content_list])\n        res_dict[\"tag\"] = response.doc(\'#tags > a\').text().split()\n        res_dict[\'subject\'] = u\'幼升小\'\n        res_dict[\'bread\'] = response.doc(\'#position a\').text().split()\n        res_dict[\'bread\'][0] = res_dict[\'label\']\n        res_dict.pop(\'label\')\n        res_dict[\'bread\'] = list(set(res_dict[\'bread\']))\n        res_dict[\'source\'] = \'ysxiao\'\n        res_dict[\'class\'] = 23\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,1.0000,3.0000,1472441331.6511),('youyong_pclady_inc','youyong','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-27 09:45:32\n# Project: youyong_pclady_inc\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://g.pclady.com.cn/baike/fitness/swimming/\':[u\'游泳资源\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=5 * 60)\n    def on_start(self):\n        for k, v in urls.iteritems():\n            self.crawl(k, save={\'bread\':v},callback=self.list_page)\n        \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.zx_bt > span > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.rz_bt > span > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n            \n                \n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.mb15 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.text().strip()\n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href,save = _dict, callback=self.detail_page)\n            \n        for each in response.doc(\'.topnews > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.text().strip()\n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pclady_page > a\').items():\n         #   self.crawl(each.attr.href,save = response.save,callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.m-txt > *\').items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\'bread\'),\n            \"content\": content,\n            \"source\": \"pclady\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"游泳\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1474940800.0618),('yujia_jianfei_inc','yujia','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 17:48:29\n# Project: yujia_jianfei_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nfrom pyquery import PyQuery as pq \nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nurls = [\n    \'http://jianfei.fh21.com.cn/yd/yujia/jc/\',\n    \'http://jianfei.fh21.com.cn/yd/yujia/dz/\',\n    \'http://jianfei.fh21.com.cn/yd/yujia/hc/\',\n    \'http://jianfei.fh21.com.cn/yd/yujia/xg/\',\n    \'http://jianfei.fh21.com.cn/yd/yujia/ch/\',\n    \'http://jianfei.fh21.com.cn/yd/yujia/fx/\',\n    \'http://jianfei.fh21.com.cn/yd/yujia/sy/\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'2\',\n    }\n\n    @every(minutes=3 * 60)\n    def on_start(self):\n        for url in urls:\n            self.crawl(url, callback=self.list1_page)\n           \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'#menu_true > a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'#content > ul\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'p > a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().split()[0]\n            self.crawl(each.find(\'p > a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pageStyle > a\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        #print response.doc(\'html\').html()\n        \n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.detailc\').html().strip().replace(\'\\t\',\'\').replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n        \n                \n        if len(content) < 100:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"jianfei\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"瑜伽\",\n            \"date\": response.save[\'date\'],\n\n        }',NULL,1.0000,3.0000,1474537836.3041),('yujia_yujia_inc','yujia','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-22 17:46:18\n# Project: yujia_yujia_inc\n\n#!/usr/bin/env python\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nfrom pyquery import PyQuery as pq \nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'3\',\n    }\n\n    @every(minutes=3 * 60)\n    def on_start(self):\n        self.crawl(\'http://news.yujia.com/index.php/home/test/so_list/class_id/14.html\', callback=self.index_page)\n           \n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'#menu_true > a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.tag1 a\').items():\n            self.crawl(each.attr.href, callback=self.list1_page)\n    \n    @config(age=1 * 1)\n    def list1_page(self, response):\n        for each in response.doc(\'.Listcont > h2 a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            self.crawl(each.attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.next\').items():\n         #   self.crawl(each.attr.href, callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content = \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',response.doc(\'.Newscontent\').html().replace(response.doc(\'.pagnation\').outerHtml(),\'\').strip().replace(\'\\t\',\'\').replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\'\n        if response.doc(\'.pagnation\').text().strip():\n            for each in response.doc(\'.pagnation a\').items():\n                if \'1\' in each.text():\n                    continue\n                content += \'<p>\'+re.sub(r\'<a[^>]+>\',\'\',pq(url=each.attr[\'href\'],encoding=\'utf8\').find(\'.Newscontent\').html().replace(pq(url=each.attr[\'href\'],encoding=\'utf8\').find(\'.pagnation\').outerHtml(),\'\').strip().replace(\'\\t\',\'\').replace(\'\\n\',\'\')).replace(\'</a>\',\'\').replace(\'/Public/attached/image/\',\'http://news.yujia.com/Public/attached/image/\')+\'</p>\'\n        if len(content) < 100:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"yujia\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\":u\"瑜伽\",\n            \"date\": response.doc(\'.explan > span\').eq(2).text().strip().split()[0].split(u\'：\')[1],\n\n        }',NULL,1.0000,3.0000,1474537687.3672),('zhongkaocom_inc','zhongkao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-03 18:29:37\n# Project: zhongkaocom\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n       \n    }\n    \n    page_config = {\n       \n         \'http://www.zhongkao.com/baokao/zkzx/\':[\'政策资讯\',\'中考资讯\'],\n         \'http://www.zhongkao.com/baokao/zkzc/\':[\'政策资讯\',\'中考政策\'],\n\n         \'http://www.zhongkao.com/beikao/jyjl/\':[\'复习备考\',\'高分经验\'],\n         \'http://www.zhongkao.com/beikao/zy/zyjy/\':[\'复习备考\',\'高分经验\'],\n\n         \'http://www.zhongkao.com/beikao/ys/kqys/\':[\'考前调整\',\'中考饮食\'],\n         \'http://www.zhongkao.com/beikao/jz/jzbd/\':[\'考前调整\',\'中考家长\'],\n         \'http://www.zhongkao.com/beikao/xlzd/zkxl/\':[\'考前调整\',\'心理辅导\'],\n\n         \'http://www.zhongkao.com/beikao/zkfx/ywfx/\':[\'科目指导\',\'中考语文\'],\n         \'http://www.zhongkao.com/beikao/zkfx/sxfx/\':[\'科目指导\',\'中考数学\'],\n         \'http://www.zhongkao.com/beikao/zkfx/yyfx/\':[\'科目指导\',\'中考英语\'],\n         \'http://www.zhongkao.com/beikao/zkfx/wlfx/\':[\'科目指导\',\'中考物理\'],\n         \'http://www.zhongkao.com/beikao/zkfx/hxfx/\':[\'科目指导\',\'中考化学\'],\n         \'http://www.zhongkao.com/beikao/zkfx/zzfx/\':[\'科目指导\',\'中考政治\'],\n\n         \'http://www.zhongkao.com/beikao/zkzw/zkmf/\':[\'真题作文\',\'满分作文\'],\n         \'http://www.zhongkao.com/beikao/zkzw/zwdp/\':[\'真题作文\',\'作文指导\'],\n         \'http://www.zhongkao.com/beikao/zkzw/zkyy/\':[\'真题作文\',\'英语作文\'],\n         \'http://www.zhongkao.com/beikao/zkzw/15zkzw/\':[\'真题作文\',\'作文题目\'],\n         \'http://www.zhongkao.com/beikao/zkzw/14zkzw/\':[\'真题作文\',\'作文题目\']\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_config.items():\n            self.crawl(k,save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n       for each in response.doc(\'.bk-item\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            url = each.find(\'h3\').find(\'a\').attr.href\n            _dict[\'title\'] = each.find(\'h3\').find(\'a\').html()\n            _dict[\'date\'] =  each.find(\'.c-b9\').text().split()[0]\n            self.crawl(url,save = _dict ,callback=self.detail_page)         \n       #for each in response.doc(\'.pages > a[href]\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #self.crawl(each.attr.href,save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'.ft14 > p\').items():\n               content = pyquery.PyQuery(info)           \n               items = content.remove(\'a\').html()\n               if content.find(\'img\'):\n                    continue\n               if items and items.find(\'推荐\')>=0 :\n                    break\n               if items and items.strip():\n                    content_list.append(items.replace(\'中考网\',\'\'))        \n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list if v and v.strip()])\n        res_dict[\'source\'] = \'zhongkao.com\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'title\'] = res_dict[\'title\'].replace(\'中考网\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1471343989.9680),('zhongkaosian_inc','zhongkao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-25 17:44:23\n# Project: zhongkao_sina\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    page_dict = {\n    	\'http://roll.edu.sina.com.cn/lm/zk/zkzx/index.shtml\':[\'政策资讯\',\'中考资讯\'],\n        \'http://roll.edu.sina.com.cn/more/zk/fxjq/index.shtml\':[\'复习备考\',\'复习技巧\'],\n        \'http://roll.edu.sina.com.cn/more/zk/gfjy/index.shtml\':[\'复习备考\',\'高分经验\'],\n        \'http://roll.edu.sina.com.cn/lm/zk/zkzy/index.shtml\':[\'复习备考\',\'高分经验\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czyw/index.shtml\':[\'科目指导\',\'中考语文\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czsx/index.shtml\':[\'科目指导\',\'中考数学\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czyy/index.shtml\':[\'科目指导\',\'中考英文\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkst/index.shtml\':[\'中考试题\',\'模拟试题\'],\n        \'http://roll.edu.sina.com.cn/lm/zk/fxjq/index.shtml\':[\'复习备考\',\'复习技巧\'],\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n          for k,v in self.page_dict.iteritems():\n            self.crawl(k,save={\'bread\': v}, callback = self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\"li\").items():\n             _dict = {}\n             url = each.find(\'a\').attr.href \n             _dict[\'bread\'] = response.save.get(\'bread\',[])\n             _dict[\'title\'] = each.find(\'a\').html()\n             _dict[\'date\'] = each.find(\'span\').html().split()[0].replace(\'(\',\'\').replace(\'年\',\'－\').replace(\'月\',\'－\').replace(\'日\',\'\')\n             self.crawl(url, save=_dict, callback=self.detail_page)     \n        #for each in response.doc(\".pagebox_num > a[href] , .pagebox_next > a[href]\").items():  \n             #_dict = {}\n             #_dict[\'bread\'] = response.save.get(\'bread\',[])\n             #self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'#artibody > p\').items():\n            content = pyquery.PyQuery(info)\n            items = content.remove(\'a\').html()\n            if content.find(\'img\'):\n                continue\n            if items and items.find(\'推荐\')>=0 :\n                break\n            if  items and (items.find(\'来源：\')>=0 or items.find(\'更多信息请访问\')>=0):\n                break\n            content_list.append(items.replace(\'新浪\',\'\'))\n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list if v and v.strip()])\n        res_dict[\'source\'] = \'edu.sina.com.cn\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'class\'] = 33\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'title\']  = res_dict[\'title\'].replace(\'新浪\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1471343960.4557),('zhongkaoxdfcn_inc','zhongkao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-25 17:44:23\n# Project: zhongkaoxdfcn\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    page_dict = {\n    	\'http://zhongkao.xdf.cn/list_7995_1.html\':[\'政策资讯\',\'志愿填报\'],\n        \'http://zhongkao.xdf.cn/list_4512_1.html\':[\'政策资讯\',\'中考资讯\'],\n        \'http://zhongkao.xdf.cn/list_1015_1.html\':[\'科目指导\',\'中考历史\'],\n        \'http://zhongkao.xdf.cn/list_1014_1.html\':[\'科目指导\',\'中考地理\'],\n        \'http://zhongkao.xdf.cn/list_1011_1.html\':[\'科目指导\',\'中考生物\'],\n\n        \'http://zhongkao.xdf.cn/list_1016_1.html\':[\'中考试题\',\'模拟试题\'],\n        \'http://zhongkao.xdf.cn/list_6132_1.html\':[\'中考试题\',\'期中末试题\'],\n        \'http://zhongkao.xdf.cn/list_1017_1.html\':[\'中考试题\',\'历年真题\'],\n\n\n        \'http://zhongkao.xdf.cn/list_1507_1.html\':[\'考前调整\',\'中考家长\'],\n        \'http://zhongkao.xdf.cn/list_1509_1.html\':[\'考前调整\',\'心理辅导\'],\n        \'http://zhongkao.xdf.cn/list_1510_1.html\':[\'考前调整\',\'中考饮食\'],\n\n        \'http://zhongkao.xdf.cn/list_1515_1.html\':[\'复习备考\',\'高分经验\'],\n        \n        \'http://zhongkao.xdf.cn/list_4809_1.html\':[\'真题作文\',\'英语作文\'],\n        \'http://zhongkao.xdf.cn/list_1512_1.html\':[\'真题作文\',\'作文题目\'],\n\n        \'http://zhongkao.xdf.cn/list_1000_1.html\':[\'复习备考\',\'备考策略\']\n    }\n\n    @every(minutes=1*60)\n    def on_start(self):\n          for k,v in self.page_dict.iteritems():\n            self.crawl(k,save={\'bread\': v}, callback = self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\".txt_lists01 > li\").items():\n             _dict = {}\n             url = each.find(\'a\').attr.href \n             _dict[\'bread\'] = response.save.get(\'bread\',[])\n             _dict[\'title\'] = each.find(\'a\').html()\n             _dict[\'date\'] = each.find(\'cite\').html()\n             self.crawl(url, save=_dict, callback=self.detail_page)     \n        #for each in response.doc(\".ch_conpage a[href$=\'.html\']\").items():  \n             #_dict = {}\n             #_dict[\'bread\'] = response.save.get(\'bread\',[])\n             #self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'.air_con p\').items():\n            content = pyquery.PyQuery(info)\n            items = content.remove(\'a\').html()\n            if content.find(\'img\'):\n                continue\n            if items and (items.find(\'推荐\')>=0 or items.find(\'编辑\')>=0):\n                    break\n            content_list.append(items.replace(\'新东方\',\'\'))\n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list if v and v.strip()])\n        res_dict[\'source\'] = \'zhongkao.xdf.cn\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'class\'] = 33\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'title\']  = res_dict[\'title\'].replace(\'新东方\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1471343991.9025),('zhuanshengben_exam8','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 10:04:07\n# Project: zhuanshengben_exam8\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.exam8.com/zige/putonghua/zhinan/\':[u\'考试动态\',u\'考试资讯\'],\n    \'http://www.exam8.com/zige/putonghua/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.exam8.com/zige/putonghua/dongtai/\':[u\'考试动态\'],\n    \'http://www.exam8.com/zige/putonghua/zhenti/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/moni/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/tuijian/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/fudao/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/jingyan/\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.exam8.com/xueli/zsb/?agent=bdopen\',callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.navxl01 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl02 > font\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl03 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl04 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl04 > font\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n                \n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.dtleft > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.lbpage a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.lbpage a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.left > div\').eq(6).children().items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        _list = []\n        for each in response.doc(\'.left > div\').eq(5).children().items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content += \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"exam8\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"专升本\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1475137971.1816),('zhuanshengben_exam8_inc','zhuanshengben','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-28 10:04:07\n# Project: zhuanshengben_exam8\n\n\nfrom pyspider.libs.base_handler import *\nimport re,time\n\nurls = {\n    \'http://www.exam8.com/zige/putonghua/zhinan/\':[u\'考试动态\',u\'考试资讯\'],\n    \'http://www.exam8.com/zige/putonghua/baoming/\':[u\'考试动态\',u\'考试报名\'],\n    \'http://www.exam8.com/zige/putonghua/dongtai/\':[u\'考试动态\'],\n    \'http://www.exam8.com/zige/putonghua/zhenti/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/moni/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/tuijian/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/fudao/\':[u\'学习资料\'],\n    \'http://www.exam8.com/zige/putonghua/jingyan/\':[u\'学习资料\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'1\'\n    }\n\n    @every(minutes=2 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.exam8.com/xueli/zsb/?agent=bdopen\',callback=self.index_page)\n        \n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.navxl01 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl02 > font\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl03 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl04 > a\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n        for each in response.doc(\'.navxl04 > font\').items():\n            self.crawl(each.attr.href,callback=self.list_page)\n                \n\n    @config(age=1)\n    def list_page(self, response):\n        for each in response.doc(\'.dtleft > a\').items():\n            self.crawl(each.attr.href,callback=self.list1_page)\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.lbpage a\').items():\n         #   self.crawl(each.attr.href,callback=self.list_page)\n    \n    @config(age=1)\n    def list1_page(self, response):\n        #print response.doc(\'.chanlDiv\').html()\n        for each in response.doc(\'.lbneirong\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip().replace(\'[\',\'\').replace(\']\',\'\') or time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.find(\'a\').attr.href,save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.lbpage a\').items():\n         #   self.crawl(each.attr.href,callback=self.list1_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        \n        _list = []\n        for each in response.doc(\'.left > div\').eq(6).children().items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content = \'\'.join(v for v in _list if v)\n        _list = []\n        for each in response.doc(\'.left > div\').eq(5).children().items():\n            if u\'点击查看\' in each.text():\n                continue\n                \n                \n            if u\'相关推荐\' in each.text() or u\'长按二维码\' in each.text() or u\'下一页\' in each.text():\n                break\n            if not each.html():\n                continue\n            _list.append(\'<p>\'+re.sub(r\'<a[^>]+>\',\'\',each.html().strip().replace(\'\\n\',\'\')).replace(\'</a>\',\'\')+\'</p>\')\n        content += \'\'.join(v for v in _list if v)\n        \n        if len(content) < 50:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [u\'文章资讯\'],\n            \"content\": content,\n            \"source\": \"exam8\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"专升本\",\n            \"date\": response.save.get(\'date\').strip(),\n\n        }\n',NULL,1.0000,3.0000,1475137983.6137),('zuowen_zgxcc_inc','zuowen','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-09-18 17:03:16\n# Project: zuowen_zgxcc\n\nfrom pyspider.libs.base_handler import *\n\nzuowen_dict = {\n\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.zgxcc.com/xiaoxue/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.nav ul\').items():\n            if u\'小学英语\' in each.text():\n                continue\n            #print each.html()\n            _dict = {}\n            if u\'一年级\' in each.text():\n                _dict[\'bread\'] = u\'小学作文\'\n            if u\'初一\' in each.text():\n                _dict[\'bread\'] = u\'初中作文\'\n            if u\'高一\' in each.text():\n                _dict[\'bread\'] = u\'高中作文\'\n            if u\'大学\' in each.text():\n                _dict[\'bread\'] = u\'作文素材\'\n            for each1 in each.find(\'li a\').items():\n                self.crawl(each1.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.lb_list a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'title\'] = each.text().strip()\n            self.crawl(each.attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页\n        #for each in response.doc(\'.pages a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save[\'bread\']\n           # self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2,age=1 * 1)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": [response.save[\'bread\'],],\n            \"content\": \'\'.join(\'<p>\'+v.text().strip()+\'</p>\' for v in response.doc(\'.art_content *\').items() if v.text().strip() and \'<a\' not in v.outerHtml()),\n            \"source\": u\"甲虫英文作文网\",\n            \"data_weight\": 0,\n            \"class\": 32,\n            \"subject\": u\"作文\",\n            \"date\": response.doc(\'.info > span\').eq(0).text().split()[0].split(u\'：\')[-1],\n\n        }\n',NULL,1.0000,3.0000,1474251308.6491);
/*!40000 ALTER TABLE `projectdb` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2016-09-30 10:07:32
