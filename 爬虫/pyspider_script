21cn_org$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-17 15:01:32\
# Project: 21cn_org\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://edu.21cn.com/news/list_train/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('div.cl_right ul li').items():\
            _dict = {}\
            _dict['title'] = each.find('a').eq(1).text()\
            _dict['date'] = each.find('span').text().replace('/','-')\
            url = each.find('a').eq(1).attr.href\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('div.page a').items():\
            self.crawl(each.attr.href,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       content_list = []\
       for each in response.doc('div.clt_cont > p').items():\
            info = each.html()\
            if info:\
                content_list.append(info)\
       for each in response.doc('div#p_content > p').items():\
            info = each.html()\
            if info:\
                content_list.append(info)\
       if not content_list:\
            return\
       res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
       res_dict['data_weight'] = 0\
       res_dict['class'] = 46\
       res_dict['subject'] = '机构'\
       res_dict['source'] = '21cn.com'\
       res_dict['bread'] = [u'文章资讯']\
       res_dict['url'] = response.url\
       return res_dict\
$$$$$1471417904.0839
233jzs$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-15 16:41:08\
# Project: 233kuaiji_inc\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import re\
\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    page_dict = {\
       'http://www.233.com/jzs2/sggl/fudao/':['建筑工程','辅导资料'],\
       'http://www.233.com/jzs2/sggl/moniti/':['建筑工程','模拟试题'],\
       'http://www.233.com/jzs2/sggl/zhenti/':['建筑工程','历年真题'],\
       'http://www.233.com/jzs2/law/fudao/':['工程法规','辅导资料'],\
       'http://www.233.com/jzs2/law/moniti/':['工程法规','模拟试题'],\
       'http://www.233.com/jzs2/law/zhenti/':['工程法规','历年真题'],\
       'http://www.233.com/jzs2/jzgc/':['管理实务','建筑工程'],\
       'http://www.233.com/jzs2/szfgc/':['管理实务','市政工程'],\
       'http://www.233.com/jzs2/jdgc/':['管理实务','机电工程'],\
       'http://www.233.com/jzs2/glgc/':['管理实务','公路工程'],\
       'http://www.233.com/jzs2/kygc/':['管理实务','矿业工程'],\
       'http://www.233.com/jzs2/slgc/':['管理实务','水电工程']\
        \
\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('ul.fl li').items():\
            url = each.find('a').attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').text().replace('233网校','')\
            _dict['date'] = each.find('span').text().replace('年','-').replace('月','-').replace('日','')\
            self.crawl(url,save = _dict ,callback=self.detail_page)\
        for each in response.doc('dl > dd').items():\
            url = each.find('h4 > a').attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h4 > a').text().replace('233网校','')\
            _dict['date'] = each.find('.column-info span.fl').text().replace('年','-').replace('月','-').replace('日','')\
            self.crawl(url,save = _dict ,callback=self.detail_page)\
        for each in response.doc('div.pagebox > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        content_list = []\
        for each in response.doc('div.news-body > p').items():\
            info = each.html()\
            if info and (u'编辑推荐' in info or u'手机用户' in info or u'小编' in info):\
                break\
            if info and (u'二维码' in info):\
                break\
            if info and (u'233网校' in info):\
                continue         \
            if info:\
                content_list.append(removeLink(info.replace('233网校','')))\
        if not content_list:\
            return\
        res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
        res_dict['class'] = 33\
        res_dict['source'] = '233.com'\
        res_dict['data_weight'] = 0\
        res_dict['url'] = response.url\
        res_dict['subject'] = '二级建造师'\
        return res_dict\
$$$$$1471327773.8286
233jzs_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-15 16:41:08\
# Project: 233kuaiji_inc\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import re\
\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    page_dict = {\
       'http://www.233.com/jzs2/sggl/fudao/':['建筑工程','辅导资料'],\
       'http://www.233.com/jzs2/sggl/moniti/':['建筑工程','模拟试题'],\
       'http://www.233.com/jzs2/sggl/zhenti/':['建筑工程','历年真题'],\
       'http://www.233.com/jzs2/law/fudao/':['工程法规','辅导资料'],\
       'http://www.233.com/jzs2/law/moniti/':['工程法规','模拟试题'],\
       'http://www.233.com/jzs2/law/zhenti/':['工程法规','历年真题'],\
       'http://www.233.com/jzs2/jzgc/':['管理实务','建筑工程'],\
       'http://www.233.com/jzs2/szfgc/':['管理实务','市政工程'],\
       'http://www.233.com/jzs2/jdgc/':['管理实务','机电工程'],\
       'http://www.233.com/jzs2/glgc/':['管理实务','公路工程'],\
       'http://www.233.com/jzs2/kygc/':['管理实务','矿业工程'],\
       'http://www.233.com/jzs2/slgc/':['管理实务','水电工程']\
        \
\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        for each in response.doc('ul.fl li').items():\
            url = each.find('a').attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').text().replace('233网校','')\
            _dict['date'] = each.find('span').text().replace('年','-').replace('月','-').replace('日','')\
            self.crawl(url,save = _dict ,callback=self.detail_page)\
        for each in response.doc('dl > dd').items():\
            url = each.find('h4 > a').attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h4 > a').text().replace('233网校','')\
            _dict['date'] = each.find('.column-info span.fl').text().replace('年','-').replace('月','-').replace('日','')\
            self.crawl(url,save = _dict ,callback=self.detail_page)\
        #for each in response.doc('div.pagebox > a').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread')\
            #self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        content_list = []\
        for each in response.doc('div.news-body > p').items():\
            info = each.html()\
            if info and (u'编辑推荐' in info or u'手机用户' in info or u'小编' in info):\
                break\
            if info and (u'二维码' in info):\
                break\
            if info and (u'233网校' in info):\
                continue         \
            if info:\
                content_list.append(removeLink(info.replace('233网校','')))\
        if not content_list:\
            return\
        res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
        res_dict['class'] = 33\
        res_dict['source'] = '233.com'\
        res_dict['data_weight'] = 0\
        res_dict['url'] = response.url\
        res_dict['subject'] = '二级建造师'\
        return res_dict\
$$$$$1471327845.9828
233kuaiji$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-15 16:41:08\
# Project: 233kuaiji_inc\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import re\
\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    page_dict = {\
        'http://www.233.com/cy/zhidao/jichu/fudao/':['会计基础','考试辅导'],\
        'http://www.233.com/cy/moniti/jichu/':['会计基础','模拟试题'],\
        'http://www.233.com/cy/zhenti/jichu/':['会计基础','历年真题'],\
        'http://www.233.com/cy/zhidao/daode/fudao/':['财经法规','考试辅导'],\
        'http://www.233.com/cy/moniti/law/':['财经法规','模拟试题'],\
        'http://www.233.com/cy/zhenti/law/':['财经法规','历年真题'],\
        'http://www.233.com/cy/zhidao/diansuanhua/fudao/':['会计电算化','考试辅导'],\
        'http://www.233.com/cy/moniti/diansuanhua/':['会计电算化','模拟试题'],\
        'http://www.233.com/cy/zhenti/diansuanhua/':['会计电算化','历年真题'],\
\
\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('ul.column-list-bd li').items():\
            url = each.find('a').attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').text().replace('233网校','')\
            _dict['date'] = each.find('span').text().replace('年','-').replace('月','-').replace('日','')\
            self.crawl(url,save = _dict ,callback=self.detail_page)\
        for each in response.doc('dl > dd').items():\
            url = each.find('h4 > a').attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h4 > a').text().replace('233网校','')\
            _dict['date'] = each.find('.column-info span.fl').text().replace('年','-').replace('月','-').replace('日','')\
            self.crawl(url,save = _dict ,callback=self.detail_page)\
        #for each in response.doc('div.pagebox > a').items():\
        #    _dict = {}\
        #    _dict['bread'] = response.save.get('bread')\
        #    self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        content_list = []\
        for each in response.doc('div.news-body > p').items():\
            info = each.html()\
            if u'编辑推荐' in info or u'手机用户' in info or u'小编' in info:\
                break\
            if info:\
                content_list.append(removeLink(info.replace('233网校','')))\
        if not content_list:\
            return\
        res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
        res_dict['class'] = 33\
        res_dict['source'] = '233.com'\
        res_dict['data_weight'] = 0\
        res_dict['url'] = response.url\
        res_dict['subject'] = '会计'\
        return res_dict\
$$$$$1471254448.1247
233kuaiji_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-15 16:41:08\
# Project: 233kuaiji_inc\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import re\
\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    page_dict = {\
        'http://www.233.com/cy/zhidao/jichu/fudao/':['会计基础','考试辅导'],\
        'http://www.233.com/cy/moniti/jichu/':['会计基础','模拟试题'],\
        'http://www.233.com/cy/zhenti/jichu/':['会计基础','历年真题'],\
        'http://www.233.com/cy/zhidao/daode/fudao/':['财经法规','考试辅导'],\
        'http://www.233.com/cy/moniti/law/':['财经法规','模拟试题'],\
        'http://www.233.com/cy/zhenti/law/':['财经法规','历年真题'],\
        'http://www.233.com/cy/zhidao/diansuanhua/fudao/':['会计电算化','考试辅导'],\
        'http://www.233.com/cy/moniti/diansuanhua/':['会计电算化','模拟试题'],\
        'http://www.233.com/cy/zhenti/diansuanhua/':['会计电算化','历年真题'],\
\
\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('ul.column-list-bd li').items():\
            url = each.find('a').attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').text().replace('233网校','')\
            _dict['date'] = each.find('span').text().replace('年','-').replace('月','-').replace('日','')\
            self.crawl(url,save = _dict ,callback=self.detail_page)\
        for each in response.doc('dl > dd').items():\
            url = each.find('h4 > a').attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h4 > a').text().replace('233网校','')\
            _dict['date'] = each.find('.column-info span.fl').text().replace('年','-').replace('月','-').replace('日','')\
            self.crawl(url,save = _dict ,callback=self.detail_page)\
        for each in response.doc('div.pagebox > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        content_list = []\
        for each in response.doc('div.news-body > p').items():\
            info = each.html()\
            if u'编辑推荐' in info or u'手机用户' in info or u'小编' in info:\
                break\
            if info:\
                content_list.append(removeLink(info.replace('233网校','')))\
        if not content_list:\
            return\
        res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
        res_dict['class'] = 33\
        res_dict['source'] = '233.com'\
        res_dict['data_weight'] = 0\
        res_dict['url'] = response.url\
        res_dict['subject'] = '会计'\
        return res_dict\
$$$$$1471254367.7785
360_img$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-29 15:24:09\
# Project: 360_img\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    headers = {\
\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        with open('/apps/home/rd/hexing/data/zxk') as f:\
            for line in f:\
                if line:\
                    line = line.strip()\
                    self.crawl('http://image.so.com/j?q='+line+'&src=sr&sn=0&pn=10',save = {'school_name':line} ,headers = self.headers,callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
       res_list =  response.json['list']\
       res_dict = {}\
       _list = []\
       for each in res_list:          \
            if each.get('img',''):\
                _list.append(each.get('img'))\
       if _list:\
                res_dict['school_name'] = response.save.get('school_name')\
                res_dict['imgs'] = _list\
                return res_dict\
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
        }\
$$$$$1472456271.1003
51job_shixisheng$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-14 14:43:24\
# Project: qiuzhi_51job\
\
from pyspider.libs.base_handler import *\
import traceback\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for i in range(1, 28):\
            self.crawl('http://search.51job.com/list/' + '%.2d'%i + '0000,000000,0000,00,4,99,%2B,2,1.html?lang=c&stype=1&postchannel=0100&workyear=99&cotype=99&degreefrom=99&jobterm=03&companysize=99&lonlat=0%2C0&radius=-1&ord_field=0&confirmdate=9&fromType=&dibiaoid=0&address=&line=&specialarea=00&from=', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.dw_table > .el').items():\
            title = '【%s】招聘%s实习生'%(each.find('.t2').text(), each.find('.tg a').text())\
            #print title\
            date = '2016-' + each.find('.t5').text()\
            self.crawl(each.find('.tg a').attr.href,save={'title': title, 'date': date}, callback=self.detail_page)\
        #for each in response.doc('.dw_page a').items():\
        #    self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res = response.save\
        res["url"] = response.url\
        try:\
            job_msg = response.doc('.job_msg').remove('.i_share').remove('.i_note').html()\
            tmsg = response.doc('.tmsg').html()\
            content = '%s<p>%s</p>'%(job_msg, tmsg)\
            content = content.replace('\\t','').replace('\\n','').replace('\\r','')\
        except Exception, e:\
            traceback.print_exc()\
            return None\
        #print job_msg\
        #print tmsg\
        res['content'] = content\
        res["data_weight"] = 0\
        res["subject"] = u'求职'\
        res["bread"] = [u'实习生',]\
        res["class"] = 46\
        res["source"] = u'51job'\
        return res$$$$$1471333013.9522
51sxue_xiaoxue_youshengxiao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-15 10:58:27\
# Project: 51sxue_xinwen\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://news.51sxue.com/newsList/classId_65.html', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.newslistcon li a').items():\
            \
            self.crawl(each.attr.href, callback=self.detail_page)\
        '''\
        for each in response.doc('.page_link').items():\
            \
            self.crawl(each.attr.href, callback=self.index_page)\
        '''\
    @config(priority=2)\
    @config(age=1 * 1)\
    def detail_page(self, response):\
        content_list = [v.text() for v in response.doc('.news_main > p').items()]\
        if not content_list:\
            return None\
        return {\
            "url": response.url,\
            "title": response.doc('.news > h3').text(),\
            "date": response.doc('.news_list > li').eq(1).text()[:10],\
            "bread": [u'小升初',],\
            "content": ''.join(['<p>%s</p>'%v for v in content_list]),\
            "subject": u'小学',\
            "class": 33,\
            "source": "51sxue",\
            "data_weight": 0,\
        }\
$$$$$1472440787.3509
51sxue_zhongxue$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-23 09:28:49\
# Project: 51sxue_zhongxue\
\
from pyspider.libs.base_handler import *\
import requests\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
from pyquery import PyQuery as pq\
import urllib2\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://xuexiao.51sxue.com/slist/?o=&t=3&areaCodeS=&level=&sp=&score=&order=&areaS=%C8%AB%B9%FA&searchKey=', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.reply_box').items():\
            _dict = {}\
            _dict['school_logo'] = each.find('.school_m_img img').attr.src\
            _dict['school_name'] = each.find('.school_m_img a').attr.title\
            for info in each.find('.school_m_main li').items():\
                 content = info.text()                 \
                 if content and u': ' not in content:\
                        _dict['school_name'] = content\
                 elif content and u': '  in content:\
                      arr = content.split(':')\
                      if u'地区' in arr[0]:\
                          _dict['province'] = arr[1].split()[0]\
                          _dict['city'] = arr[1].split()[1].replace('市','')\
                      if u'属性' in arr[0]:\
                          _dict['school_type'] = _dict.get('school_type','') + arr[1]\
                      if u'性质' in arr[0]:\
                          _dict['school_type'] = _dict.get('school_type','') + arr[1]  \
                          _dict['school_type'] = _dict['school_type'].replace('类型','').strip()\
            _dict['address'] = each.find('.school_m_lx .school_dz b').text()\
            _dict['phone'] = each.find('.school_m_lx .school_telephone b').text()\
            url = each.find('.school_m_main h3 a').attr.href\
            self.crawl(url, save = _dict,callback=self.detail_page)\
            \
        for each in response.doc('a.page_link').items():\
            self.crawl(each.attr.href, save = _dict,callback=self.index_page)    \
   \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save    \
        school_summary =  pq(urllib2.urlopen(response.url.replace('detail','content')).read())\
        res_dict['school_summary'] = school_summary.find('.nr_m').remove('img').remove('div').html()  or ''\
        enrollment_condition = pq(urllib2.urlopen(response.url.replace('detail','zhaosheng')).read())\
        res_dict['enrollment_condition'] = enrollment_condition.find('.nr_m').remove('img').remove('div').html() or ''\
        res_dict['school_url'] = response.url\
        res_dict['site'] = response.doc('#webSite').text() or ''\
        res_dict['charge_situation'] = response.doc('.school_y_sf').text() or ''\
        res_dict['feature_course'] = response.doc('.school_y_kc').text() or ''\
        school_environment = []    \
        for each in response.doc('.school_img_main').eq(0).find('img').items():\
                school_environment.append(each.attr.src)\
        res_dict['school_environment'] = school_environment\
        #print len(response.doc('.school_t'))\
        #res_dict['academic_achievement']  = response.doc('.school_t').eq(len(response.doc('.school_t')) -1).html() or ''\
        return res_dict$$$$$1469501749.7366
acca_accazhongguo$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-18 17:38:49\
# Project: acca_accazhongguo\
\
from pyspider.libs.base_handler import *\
import re\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.1',\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://cn.accaglobal.com/news/news.html', save = {'bread': ['ACCA']}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.newsMContentRMenu > li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').eq(1).text()\
            _dict['date'] = each.find('.mainList1letter').eq(0).text()\
            _dict['cover'] = each.find('a > img').attr.src\
            self.crawl(each.find('a').eq(1).attr.href, save = _dict, callback=self.detail_page)\
        #翻页\
        #for each in response.doc('.next > a').items():\
         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        cover = response.save.get('cover') \
        pattern = re.compile(r'src=".*?"')\
        _list = []\
        for each in response.doc('.mainList2RCont pre').children().items():\
            if not each.html():\
                continue\
            #print each.html()\
            if '<script>' in each.html():\
                continue\
            if u'我要纠错' in each.text() or u'责任编辑' in each.text() or u'编辑推荐' in each.text() or u'点击阅读' in each.text():\
                break\
            if '<img' in each.html():\
                if cover == '':\
                    cover = pattern.search(each.html()).group(0).replace('src="','').replace('"','')\
                _list.append('<p>'+ each.html()+'</p>')\
            elif each.text().strip() != '':\
                _list.append('<p>'+each.text()+'</p>')\
        \
        content = ''.join([v for v in _list if v])\
        #print content\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread": response.save.get("bread"),\
            "cover": cover,\
            "content": content,\
            "source": u'ACCA中国',\
            "data_weight": 0,\
            "class": 33,\
            "subject": u"金融",\
            "date": response.save.get('date'),\
\
        }$$$$$1469013096.4271
allgwy_zhonggong$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-08 18:37:37\
# Project: allgwy_zhonggong\
\
from pyspider.libs.base_handler import *\
\
add_words = [\
    'bj',\
    'sh',\
    'sd',\
    'js',\
    'zj',\
    'ah',\
    'jl',\
    'fj',\
    'gd',\
    'gx',\
    'hn',\
    'tj',\
    'hb',\
    'hlj',\
    'sx',\
    'gs',\
    'hu',\
    'hn',\
    'he',\
    'sc',\
    'cq',\
    'yn',\
    'gz',\
    'xz',\
    'nx',\
    'xj',\
    'qh',\
    'sx',\
    'ln',\
    'jx',\
    'nm',\
]\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for word in add_words:\
            self.crawl('http://www.offcn.com/'+word+'gwy/ziliao/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.zg_lm_list > li').items():\
            _dict = {}\
            _dict['title'] = each.find('.zg_lm_2').text()\
            _dict['date'] = each.find('font').text()\
            self.crawl(each.find('.zg_lm_2').attr.href, save = _dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.a1').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):        \
        return {\
            "url": response.url,\
            'bread': [u'公务员'],\
            'title': response.save.get('title'),\
            'date': response.save.get('date'),\
            "html": response.doc('.zg_show_word').html(),\
            'source': u'中公',\
            'class': 36,\
            'subject': u'经验',\
            'data_weight': 0,\
        }\
$$$$$1468413445.5348
baidu_index_tiku$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-22 19:59:25\
# Project: baidu_index_tiku\
\
from pyspider.libs.base_handler import *\
import MySQLdb\
import datetime\
\
conn = MySQLdb.connect(host="127.0.0.1",user="root",passwd="123",port=3306)\
cursor = conn.cursor()\
\
class Handler(BaseHandler):\
\
    crawl_config = {\
        'headers': {\
            'Cookie': 'PSTM=1464750941; BIDUPSID=E86D6D1C5EB675F2CB9CF17431C6D0F8; BDUSS=1h6VmViVUNZcVdVMUk1fnVCV1IzR1ktVWFCQ0ZFUi1LcENJZWRVeXlHWm52MzFYQVFBQUFBJCQAAAAAAAAAAAEAAACa-JFPsNm80rulwaq4-sut0acAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGcyVldnMlZXa; BDSFRCVID=Er4sJeCCxG3CHUcRDzRi_30n8YUjwFHt67Bc3J; H_BDCLCKID_SF=JbIDVCIytDvWe6rxMtTJ-tCE-fTMetJyaR3Wh45bWJ5TMCoLKbOhy5twQh6NqPviWmnNXnAbafP-ShPC-tPV0PDj3n6r2nbCtaRtKUjy3l02VbLRhhQ2Wf3DMMRfatRMW23r0h7mWU5GVKFCe5-Kjj5QepJf-K6hKCoM0n-8Kb7V-P5mhqn5hnLX-U__5to05TTdQbn7KtoHDR-I5JOVBPu8bMrt26ra-D6X0U7tKRTffjQG5-cr-P4H5MoX-Tra2C6yLnIQ34_MqpcNLUbWQTtdybo2Qf3ELD5doKJ8fUTlj43Dyb8-jUDSDPCE5bj2qRFHoC0M3J; lsv=globalTjs_e63380f-wwwTcss_941ce39-routejs_6ede3cf-activityControllerjs_b6f8c66-wwwBcss_cd6b841-framejs_3109ba6-globalBjs_1d5cdae-sugjs_93b1335-wwwjs_609a8a4; H_WISE_SIDS=104381_103996_107047_100615_100040_106465_102431_107196_100098_107290_107285_106666_104340_107065_107185_103759_103999_106926_106890_104671_107325_107116_107042_104613_104638_107044_107060_107092_106795_100458; MSA_WH=320_568; MSA_PBT=92; MSA_ZOOM=1000; BAIDUID=12455DCFF0A2EEB24A1668FB3AC77E9E:FG=1; BD_HOME=1; BDRCVFR[feWj1Vr5u3D]=mk3SLVN4HKm; BD_CK_SAM=1; H_PS_PSSID=20145_18286_1453_20318_18280_20368_20388_19690_20417_19861_15142_11478; BD_UPN=123253; sug=3; sugstore=1; ORIGIN=2; bdime=0; H_PS_645EC=e22ao6ItMqNOF%2BuE7PqRhAQIo36xqOHl9e4ph0bKrJAbiYbAH8SYeru51SMIZV6tCJ%2Bk',\
        'Host':'www.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1',\
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36',\
        }\
    }\
\
    @every(minutes=1 * 3)\
    def on_start(self):\
        sql = "select url_id from zhanqundb.baidu_recruit_info where spider=0 limit 10000"\
        try:\
            cursor.execute(sql)\
            for (url_id, ) in cursor.fetchall():\
                url = 'http://www.genshuixue.com/tiku/%s.html'%(url_id)\
                baidu_url = 'https://www.baidu.com/s?wd=' + url + '&rsv_spt=1&rsv_iqid=0xdd34d799000450a7&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=7&rsv_sug1=4&rsv_sug7=100&rsv_sug2=0&inputT=1411&rsv_sug4=1412'\
                self.crawl(baidu_url, save={'url': url, 'id': url_id}, callback=self.detail_page)\
        except Exception, e:\
            print e\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        num = 0\
        for each in response.doc('#content_left').items():\
            num += 1\
        res = response.save\
        \
        if num > 0:\
            sql = "update zhanqundb.baidu_recruit_info set spider=1, recruit=1, recruit_date='%s' where url_id='%s'"%(	datetime.datetime.now().strftime("%Y-%m-%d"), res['id'])\
        else:\
            sql = "update zhanqundb.baidu_recruit_info set spider=1 where url_id='%s'"%res['id']\
        print sql\
        \
        cursor.execute(sql)\
        conn.commit()\
        res['num'] = num\
        return res\
$$$$$1468039519.2148
baidu_jingyan$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-25 16:12:34\
# Project: jingyan\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "headers":{\
        'Host':'jingyan.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1',\
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36',\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for line in open('/apps/home/worker/xuzhihao/jingyan/tags'):\
            tag = line.strip('\\n')\
            self.crawl('http://jingyan.baidu.com/tag?tagName='+tag, save={'tag': tag},  callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('dt > a').items():\
            self.crawl(each.attr.href, save={'tag': response.save['tag']}, callback=self.detail_page)\
        for each in response.doc('.pager a').items():\
            self.crawl(each.attr.href, save={'tag': response.save['tag']}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        answers = []\
        all_content = [v for v in response.doc('.exp-content-block').items()]\
        if len(all_content) == 1:\
            '''\
            title = [v.text() for v in all_content[0].find('h2').items()]\
            data = [v.text() for v in all_content[0].find('p').items() if v.text().replace(u'&nbsp; &nbsp;&nbsp;','')]\
            if len(data) > len(title):\
                data = data[1:]\
            answers = [{'title': t, "steps": [v,]} for t in title for v in data]\
            print len(title),len(data)\
            abstract = {}\
            print all_content[0].html()\
            '''\
            answers = {'title': '', 'data': [all_content[0].find('.content-listblock-text').remove('img').html(),]}\
            abstract = {}\
        else:\
            for each in all_content[1:]:\
                title = each.find('.exp-content-head').text()\
                _list = []\
                for step in each.find('li').items():\
                    try:\
                        img = step.find('a').html().split('data-src="')[-1].split('"')[0]\
                    except:\
                        img = ''\
                    _list.append({\
                        "img": img,\
                        "title": step.text(),\
                        "steps": [],\
                    })\
                if not _list:\
                    _list.append((each.find('.content-listblock-text').text()))\
                answers.append({"title": title, "data": _list})\
            try:\
                question_desc_img = all_content[0].find('.content-listblock-image').html().split('data-src="')[-1].split('"')[0]\
            except:\
                question_desc_img = ''\
            abstract = {'title': '', \
                        'data': all_content[0].find('p').text(),\
                        'img': question_desc_img\
                        }\
        return {\
            "url": response.url,\
            "title": response.doc('h1').text(),\
            "methods": answers,\
            "abstract": abstract,  \
            "date": response.doc('time').text(),\
            "bread": [response.save['tag'],],\
            "source": "baidu",\
            "class": 36,\
            "subject": '经验',\
            "data_weight": 0,\
        }\
$$$$$1467339072.2474
buxiban_58$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-15 15:04:24\
# Project: buxiban_58\
from pyspider.libs.base_handler import *\
import time\
import sys\
reload(sys)\
sys.setdefaultencoding='utf-8'\
\
\
nav_yishu = [u'艺术',u'舞蹈',u'乐器',u'美术',u'声乐',u'表演',u'艺考']\
nav_xiqu = [u'兴趣',u'摄影',u'DJ',u'魔术',u'书法',u'风水',u'国学']\
nav_shenghuo = [u'生活',u'礼仪',u'茶艺',u'插花',u'烹饪',u'形体',u'园艺']\
nav = [nav_yishu, nav_xiqu, nav_shenghuo]\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.1'\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1', callback=self.index_page)\
        \
    @config(age=24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('#ObjectType > a').items():\
            if each.text() == u'全部':\
                continue\
            _dict = {}\
            for temp in nav:\
                if each.text() in temp:\
                    _dict['bread'] = [temp[0], each.text()]\
                    \
            _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(age=24 * 60 * 60)\
    def list_page(self, response):\
        if response.doc('#xiaolei > a'):\
            for each in response.doc('#xiaolei > a').items():\
                _dict = {}\
                _dict['bread'] = response.save.get('bread')\
                _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
                if each.text() == u'全部':\
                    _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
                else:\
                    _dict['title'] = u'附近哪里学' + each.text() + u'比较好'\
                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
        else:\
             _dict = {}\
             _dict['bread'] = response.save.get('bread')\
             _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
             _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
             self.crawl(response.url, save = _dict, callback=self.list_page1)       \
            \
        \
    @config(age=10*24*60*60)\
    def list_page1(self, response):\
        for each in response.doc('#local > a').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread',[])\
            #_dict['date'] = response.save.get('date')\
            #_dict['title'] = each.text() + response.save.get('title')\
            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=10*24*60*60)\
    def list_page2(self, response):\
        global flag\
        global cishu\
        for each in response.doc('.subarea > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            _dict['date'] = response.save.get('date')\
            _dict['title'] = each.text() + response.save.get('title')\
            _dict['content'] = ''\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\
        #翻页\
        for each in response.doc('.next').items():\
            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=10*24*60*60)\
    def list_page3(self, response):\
        global flag\
        global cishu\
        _dict = response.save\
        for each in response.doc('.tdiv').items():\
            _dict['content'] += each.find('a').eq(0).text() + '<br/>' + each.find('div').text()+ '<br/>' + each.find('p').text() + '<br/>'\
        \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }\
    \
    @config(priority=3)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }$$$$$1472120342.9155
buxiban_58_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-16 15:25:42\
# Project: buxiban_58_inc\
from pyspider.libs.base_handler import *\
import time\
import sys\
reload(sys)\
sys.setdefaultencoding='utf-8'\
\
\
nav_yishu = [u'艺术',u'舞蹈',u'乐器',u'美术',u'声乐',u'表演',u'艺考']\
nav_xiqu = [u'兴趣',u'摄影',u'DJ',u'魔术',u'书法',u'风水',u'国学']\
nav_shenghuo = [u'生活',u'礼仪',u'茶艺',u'插花',u'烹饪',u'形体',u'园艺']\
nav = [nav_yishu, nav_xiqu, nav_shenghuo]\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.1'\
    }\
    \
    @every(minutes=3 * 60)\
    def on_start(self):\
        self.crawl('http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1', callback=self.index_page)\
        \
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('#ObjectType > a').items():\
            if each.text() == u'全部':\
                continue\
            _dict = {}\
            for temp in nav:\
                if each.text() in temp:\
                    _dict['bread'] = [temp[0], each.text()]\
                    \
            _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(age=1*1)\
    def list_page(self, response):\
        if response.doc('#xiaolei > a'):\
            for each in response.doc('#xiaolei > a').items():\
                _dict = {}\
                _dict['bread'] = response.save.get('bread')\
                _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
                if each.text() == u'全部':\
                    _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
                else:\
                    _dict['title'] = u'附近哪里学' + each.text() + u'比较好'\
                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
        else:\
             _dict = {}\
             _dict['bread'] = response.save.get('bread')\
             _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
             _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
             self.crawl(response.url, save = _dict, callback=self.list_page1)       \
            \
        \
    @config(age=1*1)\
    def list_page1(self, response):\
        for each in response.doc('#local > a').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread',[])\
            #_dict['date'] = response.save.get('date')\
            #_dict['title'] = each.text() + response.save.get('title')\
            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=1*1)\
    def list_page2(self, response):\
        global flag\
        global cishu\
        for each in response.doc('.subarea > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            _dict['date'] = response.save.get('date')\
            _dict['title'] = each.text() + response.save.get('title')\
            _dict['content'] = ''\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\
        #翻页\
        #for each in response.doc('.next').items():\
         #   self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=1*1)\
    def list_page3(self, response):\
        global flag\
        global cishu\
        _dict = response.save\
        for each in response.doc('tr > .t').items():\
            _dict['content'] += '<p>'+each.find('a').eq(0).text() + '</p><p>' + each.find('div').text()+ '</p><p>' + each.find('p').text() + '</p>'\
            _dict['content'] += '<br/>'\
        \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }\
    \
    @config(priority=3)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }$$$$$1472120339.5478
buxiban_58_m$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-25 16:44:08\
# Project: buxiban_58_m\
\
from pyspider.libs.base_handler import *\
import time\
import sys\
reload(sys)\
sys.setdefaultencoding='utf-8'\
from pyquery import PyQuery as pq\
\
nav_yishu = [u'艺术',u'舞蹈',u'乐器',u'美术',u'声乐',u'表演',u'艺考']\
nav_xiqu = [u'兴趣',u'摄影',u'DJ',u'魔术',u'书法',u'风水',u'国学']\
nav_shenghuo = [u'生活',u'礼仪',u'茶艺',u'插花',u'烹饪',u'形体',u'园艺']\
nav = [nav_yishu, nav_xiqu, nav_shenghuo]\
city_list = [\
'bj',\
'sh',\
'gz',\
'sz',\
'cd',\
'hz',\
'nj',\
'tj',\
'wh',\
'cq',\
'qd',\
'jn',\
'yt',\
'wf',\
'linyi',\
'zb',\
'jining',\
'ta',\
'lc',\
'weihai',\
'zaozhuang',\
'dz',\
'rizhao',\
'dy',\
'heze',\
'bz',\
'lw',\
'zhangqiu',\
'kl',\
'zc',\
'shouguang',\
'su',\
'nj',\
'wx',\
'cz',\
'xz',\
'nt',\
'yz',\
'yancheng',\
'ha',\
'lyg',\
'taizhou',\
'suqian',\
'zj',\
'shuyang',\
'dafeng',\
'rugao',\
'qidong',\
'liyang',\
'haimen',\
'donghai',\
'yangzhong',\
'xinghuashi',\
'xinyishi',\
'taixing',\
'rudong',\
'pizhou',\
'xzpeixian',\
'jingjiang',\
'jianhu',\
'haian',\
'dongtai',\
'danyang',\
'hz',\
'nb',\
'wz',\
'jh',\
'jx',\
'tz',\
'sx',\
'huzhou',\
'lishui',\
'quzhou',\
'zhoushan',\
'yueqingcity',\
'ruiancity',\
'yiwu',\
'yuyao',\
'zhuji',\
'xiangshanxian',\
'wenling',\
'tongxiang',\
'cixi',\
'changxing',\
'jiashanx',\
'haining',\
'deqing',\
'hf',\
'wuhu',\
'bengbu',\
'fy',\
'hn',\
'anqing',\
'suzhou',\
'la',\
'huaibei',\
'chuzhou',\
'mas',\
'tongling',\
'xuancheng',\
'bozhou',\
'huangshan',\
'chizhou',\
'ch',\
'hexian',\
'hq',\
'tongcheng',\
'ningguo',\
'tianchang',\
'sz',\
'gz',\
'dg',\
'fs',\
'zs',\
'zh',\
'huizhou',\
'jm',\
'st',\
'zhanjiang',\
'zq',\
'mm',\
'jy',\
'mz',\
'qingyuan',\
'yj',\
'sg',\
'heyuan',\
'yf',\
'sw',\
'chaozhou',\
'taishan',\
'yangchun',\
'sd',\
'huidong',\
'boluo',\
'fz',\
'xm',\
'qz',\
'pt',\
'zhangzhou',\
'nd',\
'sm',\
'np',\
'ly',\
'wuyishan',\
'shishi',\
'jinjiangshi',\
'nananshi',\
'nn',\
'liuzhou',\
'gl',\
'yulin',\
'wuzhou',\
'bh',\
'gg',\
'qinzhou',\
'baise',\
'hc',\
'lb',\
'hezhou',\
'fcg',\
'chongzuo',\
'haikou',\
'sanya',\
'wzs',\
'sansha',\
'qh',\
'zz',\
'luoyang',\
'xx',\
'ny',\
'xc',\
'pds',\
'ay',\
'jiaozuo',\
'sq',\
'kaifeng',\
'puyang',\
'zk',\
'xy',\
'zmd',\
'luohe',\
'smx',\
'hb',\
'jiyuan',\
'mg',\
'yanling',\
'yuzhou',\
'changge',\
'wh',\
'yc',\
'xf',\
'jingzhou',\
'shiyan',\
'hshi',\
'xiaogan',\
'hg',\
'es',\
'jingmen',\
'xianning',\
'ez',\
'suizhou',\
'qianjiang',\
'tm',\
'xiantao',\
'snj',\
'yidou',\
'cs',\
'zhuzhou',\
'yiyang',\
'changde',\
'hy',\
'xiangtan',\
'yy',\
'chenzhou',\
'shaoyang',\
'hh',\
'yongzhou',\
'ld',\
'xiangxi',\
'zjj',\
'nc',\
'ganzhou',\
'jj',\
'yichun',\
'ja',\
'sr',\
'px',\
'fuzhou',\
'jdz',\
'xinyu',\
'yingtan',\
'yxx',\
'sy',\
'dl',\
'as',\
'jinzhou',\
'fushun',\
'yk',\
'pj',\
'cy',\
'dandong',\
'liaoyang',\
'benxi',\
'hld',\
'tl',\
'fx',\
'pld',\
'wfd',\
'hrb',\
'dq',\
'qqhr',\
'mdj',\
'suihua',\
'jms',\
'jixi',\
'sys',\
'hegang',\
'heihe',\
'yich',\
'qth',\
'dxal',\
'cc',\
'jl',\
'sp',\
'yanbian',\
'songyuan',\
'bc',\
'th',\
'baishan',\
'liaoyuan',\
'cd',\
'mianyang',\
'deyang',\
'nanchong',\
'yb',\
'zg',\
'ls',\
'luzhou',\
'dazhou',\
'scnj',\
'suining',\
'panzhihua',\
'ms',\
'ga',\
'zy',\
'liangshan',\
'guangyuan',\
'ya',\
'bazhong',\
'ab',\
'ganzi',\
'km',\
'qj',\
'dali',\
'honghe',\
'yx',\
'lj',\
'ws',\
'cx',\
'bn',\
'zt',\
'dh',\
'pe',\
'bs',\
'lincang',\
'diqing',\
'nujiang',\
'gy',\
'zunyi',\
'qdn',\
'qn',\
'lps',\
'bijie',\
'tr',\
'anshun',\
'qxn',\
'lasa',\
'rkz',\
'sn',\
'linzhi',\
'changdu',\
'nq',\
'al',\
'sjz',\
'bd',\
'ts',\
'lf',\
'hd',\
'qhd',\
'cangzhou',\
'xt',\
'hs',\
'zjk',\
'chengde',\
'dingzhou',\
'gt',\
'zhangbei',\
'zx',\
'zd',\
'ty',\
'linfen',\
'dt',\
'yuncheng',\
'jz',\
'changzhi',\
'jincheng',\
'yq',\
'lvliang',\
'xinzhou',\
'shuozhou',\
'linyixian',\
'qingxu',\
'hu',\
'bt',\
'chifeng',\
'erds',\
'tongliao',\
'hlbe',\
'bycem',\
'wlcb',\
'xl',\
'xam',\
'wuhai',\
'alsm',\
'hlr',\
'xa',\
'xianyang',\
'baoji',\
'wn',\
'hanzhong',\
'yl',\
'yanan',\
'ankang',\
'sl',\
'tc',\
'xj',\
'changji',\
'bygl',\
'yili',\
'aks',\
'ks',\
'hami',\
'klmy',\
'betl',\
'tlf',\
'ht',\
'shz',\
'kzls',\
'ale',\
'wjq',\
'tmsk',\
'lz',\
'tianshui',\
'by',\
'qingyang',\
'pl',\
'jq',\
'zhangye',\
'wuwei',\
'dx',\
'jinchang',\
'ln',\
'linxia',\
'jyg',\
'gn',\
'yinchuan',\
'wuzhong',\
'szs',\
'zw',\
'guyuan',\
'xn',\
'hx',\
'haibei',\
'guoluo',\
'haidong',\
'huangnan',\
'ys',\
'hainan',\
'hk',\
'am',\
'tw',\
'diaoyudao',\
'cn',\
]\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.1'\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        for city in city_list:\
            self.crawl('http://'+city+'.58.com/techang/',save={'city':city}, callback=self.index_page)\
        \
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('#ObjectType > a').items():\
            if each.text() == u'全部':\
                continue\
            _dict = {}\
            _dict['city'] = response.save['city']\
            for temp in nav:\
                if each.text() in temp:\
                    _dict['bread'] = [temp[0], each.text()]\
                    \
            _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        if response.doc('#xiaolei > a'):\
            for each in response.doc('#xiaolei > a').items():\
                _dict = {}\
                _dict['city'] = response.save['city']\
                _dict['bread'] = response.save.get('bread')\
                _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
                if each.text() == u'全部':\
                    _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
                else:\
                    _dict['title'] = u'附近哪里学' + each.text() + u'比较好'\
                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
        else:\
             _dict = {}\
             _dict['city'] = response.save['city']\
             _dict['bread'] = response.save.get('bread')\
             _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
             _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
             self.crawl(response.url, save = _dict, callback=self.list_page1)       \
            \
        \
    @config(age=10 * 24 * 60 * 60)\
    def list_page1(self, response):\
        for each in response.doc('#local > a').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread',[])\
            #_dict['date'] = response.save.get('date')\
            #_dict['title'] = each.text() + response.save.get('title')\
            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page2(self, response):\
        global flag\
        global cishu\
        for each in response.doc('.subarea > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            _dict['city'] = response.save['city']\
            _dict['date'] = response.save.get('date')\
            _dict['title'] = each.text() + response.save.get('title')\
            _dict['content'] = ''\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\
        #翻页\
        #for each in response.doc('.next').items():\
         #   self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page3(self, response):\
        global flag\
        global cishu\
        _dict = response.save\
        _dict['content'] = ''\
        for each in response.doc('tr > .t').items():\
            #print each.find('a').outerHtml()\
            if 'http://jump' in each.find('a').attr.href:\
                url = 'http://m.58.com/'+_dict['city']+'/techang/'+each.find('a').attr['name']+'x.shtml'\
            else:\
                url = 'http://m.58.com/'+_dict['city']+'/techang/'+each.find('a').attr['href'].split('/')[-1]\
            #print url\
            _dict['content'] += '<p>'+pq(url).find('.tit_area h1').eq(0).text()+'</p>'+'<p>'+pq(url).find('.article li').eq(-2).text()+'</p>' +'<p>'+pq(url).find('.article li').eq(-1).text()+'</p>'+'<p>'+pq(url).find('.firm_area h2').eq(0).text()+'</p>'+'<p>'+pq(pq(url).find('.firm_area a').eq(0).attr['href']).find('.infoitembox li').eq(-1).text()+'</p>'\
            _dict['content'] += '<br/>'\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'互联网',\
            "data_weight": 0,\
        }\
    \
    @config(priority=3)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }$$$$$1472176733.8101
bxb_other$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-15 15:04:24\
# Project: buxiban_58\
from pyspider.libs.base_handler import *\
import time\
import sys\
reload(sys)\
sys.setdefaultencoding='utf-8'\
\
\
nav_yishu = [u'艺术',u'舞蹈',u'乐器',u'美术',u'声乐',u'表演',u'艺考']\
nav_xiqu = [u'兴趣',u'摄影',u'DJ',u'魔术',u'书法',u'风水',u'国学']\
nav_shenghuo = [u'生活',u'礼仪',u'茶艺',u'插花',u'烹饪',u'形体',u'园艺']\
nav = [nav_yishu, nav_xiqu, nav_shenghuo]\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.1'\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1', callback=self.index_page)\
        \
    @config(age=24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('#ObjectType > a').items():\
            if each.text() == u'全部':\
                continue\
            _dict = {}\
            for temp in nav:\
                if each.text() in temp:\
                    _dict['bread'] = [temp[0], each.text()]\
                    \
            _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(age=24 * 60 * 60)\
    def list_page(self, response):\
        if response.doc('#xiaolei > a'):\
            for each in response.doc('#xiaolei > a').items():\
                _dict = {}\
                _dict['bread'] = response.save.get('bread')\
                _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
                if each.text() == u'全部':\
                    _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
                else:\
                    _dict['title'] = u'附近哪里学' + each.text() + u'比较好'\
                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
        else:\
             _dict = {}\
             _dict['bread'] = response.save.get('bread')\
             _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
             _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
             self.crawl(response.url, save = _dict, callback=self.list_page1)       \
            \
        \
    @config(age=10*24*60*60)\
    def list_page1(self, response):\
        for each in response.doc('#local > a').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread',[])\
            #_dict['date'] = response.save.get('date')\
            #_dict['title'] = each.text() + response.save.get('title')\
            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=10*24*60*60)\
    def list_page2(self, response):\
        global flag\
        global cishu\
        for each in response.doc('.subarea > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            _dict['date'] = response.save.get('date')\
            _dict['title'] = each.text() + response.save.get('title')\
            _dict['content'] = ''\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\
        #翻页\
        for each in response.doc('.next').items():\
            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=10*24*60*60)\
    def list_page3(self, response):\
        global flag\
        global cishu\
        _dict = response.save\
        for each in response.doc('.tdiv').items():\
            _dict['content'] += each.find('a').eq(0).text() + '<br/>' + each.find('div').text()+ '<br/>' + each.find('p').text() + '<br/>'\
        \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }\
    \
    @config(priority=3)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }$$$$$1472113261.5521
cet46_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-21 14:58:56\
# Project: cet46com\
\
from pyspider.libs.base_handler import *\
import pyquery\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    page_dict = {\
        "95": u'四级真题及答案',\
        "96": u'六级真题及答案',\
        '91': u'四级口语',\
        '97': u'模拟试题',\
        '86': u'六级作文',\
        '87': u'四级听力',\
        '88': u'六级听力',\
        '92': u'六级口语',\
        '78': u'名师指导',\
        '79': u'复习攻略',\
        '75': u'样题规定',\
        '83': u'四级阅读',\
        '84': u'六级阅读',\
        '85': u'四级作文',\
        '93': u'四级翻译',\
        '94': u'六级翻译',\
    }\
    \
    @every(minutes=1 * 60)\
    def on_start(self):\
          for k, v in self.page_dict.iteritems():\
            self.crawl('http://www.cet-46.com/list%s.html'%k,save={'bread': v}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc(".left a").items():\
             _dict = {}\
             url = each.attr.href  \
             _dict['bread'] = [response.save['bread'], ]\
             _dict['brief'] = each.html()\
             try:\
                 date = each.parent().next('span').find('font').text().split('/')\
                 _dict['date'] = '%s-%.2d-%2d'%(date[0], int(date[1]), int(date[2]))\
             except:\
                 _dict['date'] = '2016-01-01'\
             self.crawl(url, save=_dict, callback=self.detail_page) \
        '''\
        for each in response.doc(".daohang a[href\$='.html']").items(): \
             _dict = {}\
             _dict['bread'] = response.save['bread']\
             self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\
        '''\
        \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save     \
        res_dict['title'] = response.doc('.xw_title > span').html()\
        content_list = []\
        for info in response.doc('span > p').items():\
            content = pyquery.PyQuery(info)\
            items = content.html()\
            content_list.append(items)\
        if not content_list:\
            return None\
        res_dict['url'] = response.url\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list])\
        res_dict['source'] = 'www.cet-46.com'\
        res_dict['subject'] = u'四六级'\
        res_dict['class'] = 46\
        res_dict['data_weight'] = 0\
        return res_dict\
$$$$$1471329965.7602
chazidian_tiku$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-29 17:05:44\
# Project: chazidian_tiku\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1',\
        'headers':{\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
\
        }\
    }\
\
    dict_map = {\
\
            u'注册会计师':[u'财会经济',u'注册会计师'],\
            u'经济师':[u'财会经济',u'经济师'],\
            u'初级会计职称':[u'财会经济',u'初级会计师'],\
            u'中级会计职称':[u'财会经济',u'中级会计师'],\
            u'税务师':[u'财会经济',u'注册税务师'],\
            u'统计师':[u'财会经济',u'统计师'],\
            u'审计师':[u'财会经济',u'审计师'],\
            u'高级经济师':[u'财会经济',u'经济师'],\
            u'高级会计':[u'财会经济',u'高级会计师'],\
            u'理财规划师':[u'财会经济',u'理财规划师'],\
\
\
            u'英语四级':[u'外语考试',u'英语四六级'],\
            u'英语六级':[u'外语考试',u'英语四六级'],\
            u'雅思':[u'外语考试',u'雅思'],\
            u'托福':[u'外语考试',u'托福'],\
            u'职称英语':[u'外语考试',u'职称英语'],\
            u'商务英语':[u'外语考试',u'商务英语'],\
            u'公共英语':[u'外语考试',u'公共英语'],\
            u'日语':[u'外语考试',u'日语'],\
            u'GRE考试':[u'外语考试',u'GRE考试'],\
            u'专四专八':[u'外语考试',u'专四专八'],\
            u'口译笔译':[u'外语考试',u'口译笔译'],\
\
            u'一级建造师':[u'建筑工程',u'一级建造师'],\
            u'二级建造师':[u'建筑工程',u'二级建造师'],\
            u'咨询工程师':[u'建筑工程',u'咨询工程师'],\
            u'造价工程师':[u'建筑工程','造价工程师'],\
            u'结构工程师':[u'建筑工程','结构工程师'],\
            u'物业管理':[u'建筑工程',u'物业管理师'],\
            u'城市规划':[u'建筑工程',u'城市规划师'],\
            u'给排水工程':[u'建筑工程',u'给排水工程'],\
            u'电气工程':[u'建筑工程',u'电气工程师'],\
            u'公路监理师':[u'建筑工程',u'公路监理师'],\
            u'消防工程师':[u'建筑工程',u'消防工程师'],\
\
            u'物流师':[u'职业资格',u'物流师'],\
            u'人力资源':[u'职业资格',u'人力资源'],\
            u'心理咨询师':[u'职业资格',u'心理咨询师'],\
            u'公共营养师':[u'职业资格',u'公共营养师'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'证券从业资格':[u'职业资格',u'证券经纪人'],\
            u'电子商务师':[u'职业资格',u'电子商务'],\
            u'期货从业':[u'职业资格',u'期货从业'],\
            u'教师资格':[u'职业资格',u'教师资格'],\
            u'管理咨询师':[u'职业资格',u'管理咨询师'],\
            u'导游证':[u'职业资格',u'导游证'],\
            \
            u'英语':[u'学历教育',u'考研'],\
            u'数学':[u'学历教育',u'考研'],\
            u'政治':[u'学历教育',u'考研'],\
            u'专业课':[u'学历教育',u'考研'],\
\
            u'执业医师':[u'医学卫生',u'执业医师'],\
            u'执业药师':[u'医学卫生',u'执业药师'],\
            u'临床医师':[u'医学卫生',u'临床执业'],\
            u'中医医师':[u'医学卫生',u'中医执业'],\
            u'中西医医师':[u'医学卫生',u'中西医执业'],\
            u'中医助理':[u'医学卫生',u'中医助理'],\
            u'中西医助理':[u'医学卫生',u'中西医助理'],\
            u'主治':[u'医学卫生',u'主治'],\
            u'检验':[u'医学卫生',u'检验'],\
            u'执业护士资格':[u'医学卫生',u'执业护士'],\
\
\
            u'成人高考':[u'学历教育',u'成人高考'],\
            u'自学考试':[u'学历教育',u'自考'],\
            u'MBA考试':[u'学历教育',u'MBA'],\
            u'法律硕士':[u'学历教育',u'法律硕士'],\
            u'专升本':[u'学历教育',u'专升本'],\
            #u'':[u'学历教育',u'工程硕士'],\
            u'MPA考试':[u'学历教育',u'公共硕士'],\
            #u'':[u'学历教育',u'考研'],\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://tiku.chazidian.com/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('li dd').items():         \
                _dict = {}\
                url = each.find('a').attr.href\
                _dict['type'] = each.find('a').text()\
                self.crawl(url, save = _dict,callback=self.list_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.list_sj_xx > a').items():\
            _dict = {}\
            _dict['title'] = each.text()\
            _dict['type'] = response.save.get('type')\
            self.crawl(each.attr.href, save = _dict,callback=self.detail_page)\
        for each in response.doc('.page_link').items():\
            _dict = {}\
            _dict['type'] = response.save.get('type')\
            self.crawl(each.attr.href, save = _dict,callback=self.list_page)\
        \
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       res_dict['tdk_title'] = '跟谁学 '+response.doc('title').text().replace('查字典','').replace('题库网','')\
       res_dict['tdk_desc'] = response.doc('meta[name="description"]').attr.content.replace('查字典','').replace('题库网','')\
       res_dict['tdk_keywords'] =  response.doc('meta[name="keywords"]').attr.content.replace('查字典','').replace('题库网','')\
       content_list = []\
       for each in response.doc('p').items():\
            info = each.remove('a').html()\
            if info and  info.find('查字典') <0 and info.find('题库网') <0:\
                content_list.append(info)\
       if not content_list:\
            return\
       res_dict['content'] = ''.join(['<p>%s</p>'%(s) for s in content_list if s and s.strip()])\
       res_dict['class'] = 33\
       res_dict['subject'] = u'考证题库'\
       res_dict['source'] = 'chazidian.com'\
       res_dict['data_weight'] = 0\
       res_dict['url'] = response.url\
       res_dict['date'] = u'2016-06-29'\
       if res_dict['type'] in self.dict_map:\
            res_dict['bread'] = self.dict_map[res_dict['type']]\
            del res_dict['type']\
       else:\
            return \
       return res_dict\
\
$$$$$1468033286.7487
chengxuyuan_bkyshengqu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-13 16:09:10\
# Project: chengxuyuan_bkyshengqu\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.cnblogs.com/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('#cate_item a').items():\
            if u'所有评论' in each.text():\
                continue\
            _dict = {}\
            _dict['bread'] = [each.text().split('(')[0]]\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
    \
    def list_page(self, response):\
        for each in response.doc('.post_item_body').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h3 > a').text()\
            _dict['date'] = [v for v in each.find('.post_item_foot').remove('a').text().split() if '-' in v][0]\
            self.crawl(each.find('h3 > a').attr.href, save = _dict, callback=self.detail_page)\
        #翻页    \
        for each in response.doc('#paging_block a').items():\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        _list = []\
        for each in response.doc('#cnblogs_post_body').items():\
            #print each.html()\
            if 'img' in each.html():\
                _list.append('<p>'+each.html()+'</p>')\
            elif '<pre>' in each.html():\
                _list.append('<p>'+each.html()+'</p>')\
            elif each.text().strip() != '':\
                _list.append('<p>'+each.text()+'</p>')\
        content = ''.join([v for v in _list if v])\
        #print content\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread": response.save.get("bread"),\
            "content": content.replace('\\r','').replace('\\n',''),\
            "source": u"博客园",\
            "data_weight": 0,\
            "class": 33,\
            "subject": u"程序员",\
            "date": response.save.get('date'),\
\
        }$$$$$1468481651.8452
chengxuyuan_bkyuan$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-13 14:37:27\
# Project: chengxuyuan_bkyuan\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.4'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://kb.cnblogs.com/zt/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('span > a').items():\
            if u'知识库' in each.text() or u'专题' in each.text() or u'最新文章' in each.text():\
                continue\
            _dict = {}\
            _dict['bread'] = [each.text()]\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
    \
    def list_page(self, response):\
        for each in response.doc('#list_block > div').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('.list_title').text()\
            _dict['date'] = [v for v in each.find('.listfooter').remove('a').text().split() if '-' in v][0]\
            self.crawl(each.find('.list_title > a').attr.href, save = _dict, callback=self.detail_page)\
        #翻页    \
        for each in response.doc('#pager_block_top a').items():\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        _list = []\
        for each in response.doc('.contents_main > div').items(): \
            if 'img' in each.html():\
                _list.append('<p>'+each.html()+'</p>')\
            elif '<pre>' in each.html():\
                _list.append('<p>'+each.html()+'</p>')\
            elif each.text().strip() != '':\
                _list.append('<p>'+each.text()+'</p>')\
        content = ''.join([v for v in _list if v])\
        #print content\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread": response.save.get("bread"),\
            "content": content,\
            "source": u"博客园",\
            "data_weight": 0,\
            "class": 33,\
            "subject": u"程序员",\
            "date": response.save.get('date'),\
\
        }$$$$$1468481637.5180
chengxuyuan_boke$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-14 11:27:09\
# Project: chengxuyuan_boke\
\
from pyspider.libs.base_handler import *\
import sys\
import traceback\
from pyquery import PyQuery\
reload(sys)\
sys.setdefaultencoding("utf-8")\
sub_items = u'<div id="cate_content_block_108698" onmouseover="cateShow(108698)" onmouseout="cateHidden(108698)" class="cate_content_block_wrapper" style="top:30px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/beginner/">.NET新手区(1)</a></li><li><a href="/cate/aspnet/">ASP.NET(1)</a></li><li><a href="/cate/csharp/">C#(1)</a></li><li><a href="/cate/dotnetcore/">.NET Core(0)</a></li><li><a href="/cate/winform/">WinForm(0)</a></li><li><a href="/cate/silverlight/">Silverlight(0)</a></li><li><a href="/cate/wcf/">WCF(0)</a></li><li><a href="/cate/clr/">CLR(0)</a></li><li><a href="/cate/wpf/">WPF(0)</a></li><li><a href="/cate/xna/">XNA(0)</a></li><li><a href="/cate/vs2010/">Visual Studio(0)</a></li><li><a href="/cate/mvc/">ASP.NET MVC(2)</a></li><li><a href="/cate/control/">控件开发(0)</a></li><li><a href="/cate/ef/">Entity Framework(0)</a></li><li><a href="/cate/nhibernate/">NHibernate(0)</a></li><li><a href="/cate/winrt_metro/">WinRT/Metro(1)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_2" onmouseover="cateShow(2)" onmouseout="cateHidden(2)" class="cate_content_block_wrapper" style="top:58px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/java/">Java(4)</a></li><li><a href="/cate/cpp/">C++(1)</a></li><li><a href="/cate/php/">PHP(1)</a></li><li><a href="/cate/delphi/">Delphi(0)</a></li><li><a href="/cate/python/">Python(2)</a></li><li><a href="/cate/ruby/">Ruby(0)</a></li><li><a href="/cate/c/">C语言(0)</a></li><li><a href="/cate/erlang/">Erlang(0)</a></li><li><a href="/cate/go/">Go(0)</a></li><li><a href="/cate/swift/">Swift(0)</a></li><li><a href="/cate/scala/">Scala(0)</a></li><li><a href="/cate/r/">R语言(0)</a></li><li><a href="/cate/verilog/">Verilog(0)</a></li><li><a href="/cate/otherlang/">其它语言(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108701" onmouseover="cateShow(108701)" onmouseout="cateHidden(108701)" class="cate_content_block_wrapper" style="top:86px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/design/">架构设计(0)</a></li><li><a href="/cate/108702/">面向对象(0)</a></li><li><a href="/cate/dp/">设计模式(0)</a></li><li><a href="/cate/ddd/">领域驱动设计(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108703" onmouseover="cateShow(108703)" onmouseout="cateHidden(108703)" class="cate_content_block_wrapper" style="top:114px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/web/">Html/Css(1)</a></li><li><a href="/cate/javascript/">JavaScript(3)</a></li><li><a href="/cate/jquery/">jQuery(1)</a></li><li><a href="/cate/html5/">HTML5(1)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108704" onmouseover="cateShow(108704)" onmouseout="cateHidden(108704)" class="cate_content_block_wrapper" style="top:142px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/sharepoint/">SharePoint(0)</a></li><li><a href="/cate/gis/">GIS技术(0)</a></li><li><a href="/cate/sap/">SAP(0)</a></li><li><a href="/cate/OracleERP/">Oracle ERP(0)</a></li><li><a href="/cate/dynamics/">Dynamics CRM(0)</a></li><li><a href="/cate/k2/">K2 BPM(0)</a></li><li><a href="/cate/infosec/">信息安全(0)</a></li><li><a href="/cate/3/">企业信息化其他(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108705" onmouseover="cateShow(108705)" onmouseout="cateHidden(108705)" class="cate_content_block_wrapper" style="top:170px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/android/">Android开发(1)</a></li><li><a href="/cate/ios/">iOS开发(3)</a></li><li><a href="/cate/wp/">Windows Phone(0)</a></li><li><a href="/cate/wm/">Windows Mobile(0)</a></li><li><a href="/cate/mobile/">其他手机开发(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108709" onmouseover="cateShow(108709)" onmouseout="cateHidden(108709)" class="cate_content_block_wrapper" style="top:198px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/agile/">敏捷开发(0)</a></li><li><a href="/cate/pm/">项目与团队管理(0)</a></li><li><a href="/cate/Engineering/">软件工程其他(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108712" onmouseover="cateShow(108712)" onmouseout="cateHidden(108712)" class="cate_content_block_wrapper" style="top:226px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/sqlserver/">SQL Server(0)</a></li><li><a href="/cate/oracle/">Oracle(0)</a></li><li><a href="/cate/mysql/">MySQL(1)</a></li><li><a href="/cate/nosql/">NoSQL(0)</a></li><li><a href="/cate/bigdata/">大数据(0)</a></li><li><a href="/cate/database/">其它数据库(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108724" onmouseover="cateShow(108724)" onmouseout="cateHidden(108724)" class="cate_content_block_wrapper" style="top:254px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/win7/">Windows(0)</a></li><li><a href="/cate/winserver/">Windows Server(0)</a></li><li><a href="/cate/linux/">Linux(0)</a></li><li><a href="/cate/osx/">OS X(0)</a></li><li><a href="/cate/eos/">嵌入式(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_4" onmouseover="cateShow(4)" onmouseout="cateHidden(4)" class="cate_content_block_wrapper" style="top:282px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/life/">非技术区(0)</a></li><li><a href="/cate/testing/">软件测试(0)</a></li><li><a href="/cate/software/">代码与软件发布(0)</a></li><li><a href="/cate/cg/">计算机图形学(0)</a></li><li><a href="/cate/google/">Google开发(0)</a></li><li><a href="/cate/gamedev/">游戏开发(1)</a></li><li><a href="/cate/codelife/">程序人生(0)</a></li><li><a href="/cate/job/">求职面试(0)</a></li><li><a href="/cate/book/">读书区(0)</a></li><li><a href="/cate/quoted/">转载区(0)</a></li><li><a href="/cate/wince/">Windows CE(0)</a></li><li><a href="/cate/translate/">翻译区(0)</a></li><li><a href="/cate/opensource/">开源研究(0)</a></li><li><a href="/cate/flex/">Flex(0)</a></li><li><a href="/cate/cloud/">云计算(0)</a></li><li><a href="/cate/algorithm/">算法与数据结构(0)</a></li><li><a href="/cate/misc/">其他技术区(0)</a></li></ul></div><div class="cate_content_bottom"></div></div>'\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1'\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.cnblogs.com/', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        global sub_items\
        i = 0\
        for each in response.doc('#cate_item a').items():\
            if u'所有评论' in each.text():\
                continue\
            sub_items = PyQuery(sub_items)\
            for each_sub in sub_items('.cate_content_block').eq(i).find('li > a').items():\
                _dict = {}\
                _dict['bread'] = [each.text().split('(')[0]]\
                _dict['bread'].append(each_sub.text().split('(')[0])\
                self.crawl('http://www.cnblogs.com'+each_sub.attr.href, save = _dict, callback=self.list_page)\
            i += 1\
    \
    def list_page(self, response):\
        for each in response.doc('.post_item_body').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h3 > a').text()\
            _dict['date'] = [v for v in each.find('.post_item_foot').remove('a').text().split() if '-' in v][0]\
            self.crawl(each.find('h3 > a').attr.href, save = _dict, callback=self.detail_page)\
        #翻页    \
        #for each in response.doc('#paging_block a').items():\
         #   self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        _list = []\
        for each in response.doc('#cnblogs_post_body').items():\
            #print each.html()\
            if 'img' in each.html():\
                _list.append('<p>'+each.html()+'</p>')\
            elif '<pre>' in each.html():\
                _list.append('<p>'+each.html()+'</p>')\
            elif each.text().strip() != '':\
                _list.append('<p>'+each.text()+'</p>')\
        content = ''.join([v for v in _list if v])\
        #print content\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread": response.save.get("bread"),\
            "content": content.replace('\\r','').replace('\\n',''),\
            "source": u"博客园",\
            "data_weight": 0,\
            "class": 33,\
            "subject": u"程序员",\
            "date": response.save.get('date'),\
\
        }$$$$$1470809547.0914
chengxuyuan_boke_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-10 14:12:20\
# Project: chengxuyuan_boke_inc\
\
from pyspider.libs.base_handler import *\
import sys\
import traceback\
from pyquery import PyQuery\
reload(sys)\
sys.setdefaultencoding("utf-8")\
sub_items = u'<div id="cate_content_block_108698" onmouseover="cateShow(108698)" onmouseout="cateHidden(108698)" class="cate_content_block_wrapper" style="top:30px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/beginner/">.NET新手区(1)</a></li><li><a href="/cate/aspnet/">ASP.NET(1)</a></li><li><a href="/cate/csharp/">C#(1)</a></li><li><a href="/cate/dotnetcore/">.NET Core(0)</a></li><li><a href="/cate/winform/">WinForm(0)</a></li><li><a href="/cate/silverlight/">Silverlight(0)</a></li><li><a href="/cate/wcf/">WCF(0)</a></li><li><a href="/cate/clr/">CLR(0)</a></li><li><a href="/cate/wpf/">WPF(0)</a></li><li><a href="/cate/xna/">XNA(0)</a></li><li><a href="/cate/vs2010/">Visual Studio(0)</a></li><li><a href="/cate/mvc/">ASP.NET MVC(2)</a></li><li><a href="/cate/control/">控件开发(0)</a></li><li><a href="/cate/ef/">Entity Framework(0)</a></li><li><a href="/cate/nhibernate/">NHibernate(0)</a></li><li><a href="/cate/winrt_metro/">WinRT/Metro(1)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_2" onmouseover="cateShow(2)" onmouseout="cateHidden(2)" class="cate_content_block_wrapper" style="top:58px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/java/">Java(4)</a></li><li><a href="/cate/cpp/">C++(1)</a></li><li><a href="/cate/php/">PHP(1)</a></li><li><a href="/cate/delphi/">Delphi(0)</a></li><li><a href="/cate/python/">Python(2)</a></li><li><a href="/cate/ruby/">Ruby(0)</a></li><li><a href="/cate/c/">C语言(0)</a></li><li><a href="/cate/erlang/">Erlang(0)</a></li><li><a href="/cate/go/">Go(0)</a></li><li><a href="/cate/swift/">Swift(0)</a></li><li><a href="/cate/scala/">Scala(0)</a></li><li><a href="/cate/r/">R语言(0)</a></li><li><a href="/cate/verilog/">Verilog(0)</a></li><li><a href="/cate/otherlang/">其它语言(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108701" onmouseover="cateShow(108701)" onmouseout="cateHidden(108701)" class="cate_content_block_wrapper" style="top:86px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/design/">架构设计(0)</a></li><li><a href="/cate/108702/">面向对象(0)</a></li><li><a href="/cate/dp/">设计模式(0)</a></li><li><a href="/cate/ddd/">领域驱动设计(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108703" onmouseover="cateShow(108703)" onmouseout="cateHidden(108703)" class="cate_content_block_wrapper" style="top:114px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/web/">Html/Css(1)</a></li><li><a href="/cate/javascript/">JavaScript(3)</a></li><li><a href="/cate/jquery/">jQuery(1)</a></li><li><a href="/cate/html5/">HTML5(1)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108704" onmouseover="cateShow(108704)" onmouseout="cateHidden(108704)" class="cate_content_block_wrapper" style="top:142px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/sharepoint/">SharePoint(0)</a></li><li><a href="/cate/gis/">GIS技术(0)</a></li><li><a href="/cate/sap/">SAP(0)</a></li><li><a href="/cate/OracleERP/">Oracle ERP(0)</a></li><li><a href="/cate/dynamics/">Dynamics CRM(0)</a></li><li><a href="/cate/k2/">K2 BPM(0)</a></li><li><a href="/cate/infosec/">信息安全(0)</a></li><li><a href="/cate/3/">企业信息化其他(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108705" onmouseover="cateShow(108705)" onmouseout="cateHidden(108705)" class="cate_content_block_wrapper" style="top:170px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/android/">Android开发(1)</a></li><li><a href="/cate/ios/">iOS开发(3)</a></li><li><a href="/cate/wp/">Windows Phone(0)</a></li><li><a href="/cate/wm/">Windows Mobile(0)</a></li><li><a href="/cate/mobile/">其他手机开发(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108709" onmouseover="cateShow(108709)" onmouseout="cateHidden(108709)" class="cate_content_block_wrapper" style="top:198px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/agile/">敏捷开发(0)</a></li><li><a href="/cate/pm/">项目与团队管理(0)</a></li><li><a href="/cate/Engineering/">软件工程其他(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108712" onmouseover="cateShow(108712)" onmouseout="cateHidden(108712)" class="cate_content_block_wrapper" style="top:226px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/sqlserver/">SQL Server(0)</a></li><li><a href="/cate/oracle/">Oracle(0)</a></li><li><a href="/cate/mysql/">MySQL(1)</a></li><li><a href="/cate/nosql/">NoSQL(0)</a></li><li><a href="/cate/bigdata/">大数据(0)</a></li><li><a href="/cate/database/">其它数据库(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_108724" onmouseover="cateShow(108724)" onmouseout="cateHidden(108724)" class="cate_content_block_wrapper" style="top:254px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/win7/">Windows(0)</a></li><li><a href="/cate/winserver/">Windows Server(0)</a></li><li><a href="/cate/linux/">Linux(0)</a></li><li><a href="/cate/osx/">OS X(0)</a></li><li><a href="/cate/eos/">嵌入式(0)</a></li></ul></div><div class="cate_content_bottom"></div></div><div id="cate_content_block_4" onmouseover="cateShow(4)" onmouseout="cateHidden(4)" class="cate_content_block_wrapper" style="top:282px"><div class="cate_content_top"></div><div class="cate_content_block"><ul><li><a href="/cate/life/">非技术区(0)</a></li><li><a href="/cate/testing/">软件测试(0)</a></li><li><a href="/cate/software/">代码与软件发布(0)</a></li><li><a href="/cate/cg/">计算机图形学(0)</a></li><li><a href="/cate/google/">Google开发(0)</a></li><li><a href="/cate/gamedev/">游戏开发(1)</a></li><li><a href="/cate/codelife/">程序人生(0)</a></li><li><a href="/cate/job/">求职面试(0)</a></li><li><a href="/cate/book/">读书区(0)</a></li><li><a href="/cate/quoted/">转载区(0)</a></li><li><a href="/cate/wince/">Windows CE(0)</a></li><li><a href="/cate/translate/">翻译区(0)</a></li><li><a href="/cate/opensource/">开源研究(0)</a></li><li><a href="/cate/flex/">Flex(0)</a></li><li><a href="/cate/cloud/">云计算(0)</a></li><li><a href="/cate/algorithm/">算法与数据结构(0)</a></li><li><a href="/cate/misc/">其他技术区(0)</a></li></ul></div><div class="cate_content_bottom"></div></div>'\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1'\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.cnblogs.com/', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        global sub_items\
        i = 0\
        for each in response.doc('#cate_item a').items():\
            if u'所有评论' in each.text():\
                continue\
            sub_items = PyQuery(sub_items)\
            for each_sub in sub_items('.cate_content_block').eq(i).find('li > a').items():\
                _dict = {}\
                _dict['bread'] = [each.text().split('(')[0]]\
                _dict['bread'].append(each_sub.text().split('(')[0])\
                self.crawl('http://www.cnblogs.com'+each_sub.attr.href, save = _dict, callback=self.list_page)\
            i += 1\
    \
    @config(age=1 * 1)\
    def list_page(self, response):\
        for each in response.doc('.post_item_body').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h3 > a').text()\
            _dict['date'] = [v for v in each.find('.post_item_foot').remove('a').text().split() if '-' in v][0]\
            self.crawl(each.find('h3 > a').attr.href, save = _dict, callback=self.detail_page)\
        #翻页    \
        #for each in response.doc('#paging_block a').items():\
         #   self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        _list = []\
        for each in response.doc('#cnblogs_post_body').items():\
            #print each.html()\
            if 'img' in each.html():\
                _list.append('<p>'+each.html()+'</p>')\
            elif '<pre>' in each.html():\
                _list.append('<p>'+each.html()+'</p>')\
            elif each.text().strip() != '':\
                _list.append('<p>'+each.text()+'</p>')\
        content = ''.join([v for v in _list if v])\
        #print content\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread": response.save.get("bread"),\
            "content": content.replace('\\r','').replace('\\n',''),\
            "source": u"博客园",\
            "data_weight": 0,\
            "class": 33,\
            "subject": u"程序员",\
            "date": response.save.get('date'),\
\
        }$$$$$1471334415.9426
chinadance$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 09:55:12\
# Project: chinadance\
\
from pyspider.libs.base_handler import *\
import re\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    page_dict = {\
\
        'http://www.chinadance.cn/article/wudaozhishi/':[u'舞蹈知识'],\
        'http://www.chinadance.cn/article/wudaojiaoxue/':[u'舞蹈教学'],\
        'http://www.chinadance.cn/article/wudaoshangxi/':[u'舞蹈鉴赏'],\
        'http://www.chinadance.cn/article/wudaorensheng/':[u'舞蹈人生'],\
        'http://www.chinadance.cn/article/wudaoshi/':[u'舞蹈史论'],\
        'http://www.chinadance.cn/news/':[u'舞蹈资讯'],\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('li.cl').items():\
            _dict = {}\
            _dict['title'] = each.find('h3 > a').text()\
            _dict['bread'] = response.save.get('bread')\
            url = each.find('h3 > a').attr.href\
            _dict['date'] = each.find('p.info > span').text().split(' ')[0]\
            _dict['cover'] = each.children('a').find('img').attr.src or ''\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('div.pg > a[href]').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       if '</a>' in response.doc('td#article_content').remove('embed').remove('script').html():   \
            res_dict['content'] = removeLink(response.doc('td#article_content').remove('embed').remove('script').html())\
       else:\
             res_dict['content'] = response.doc('td#article_content').remove('embed').remove('script').html()   \
       res_dict['class'] = 33\
       res_dict['subject'] = u'舞蹈'\
       res_dict['source'] = 'chinadance'\
       res_dict['data_weight'] = 0\
       res_dict['url'] = response.url\
       return res_dict$$$$$1471844120.2141
chinadance_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 09:55:12\
# Project: chinadance\
\
from pyspider.libs.base_handler import *\
import re\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    page_dict = {\
\
        'http://www.chinadance.cn/article/wudaozhishi/':[u'舞蹈知识'],\
        'http://www.chinadance.cn/article/wudaojiaoxue/':[u'舞蹈教学'],\
        'http://www.chinadance.cn/article/wudaoshangxi/':[u'舞蹈鉴赏'],\
        'http://www.chinadance.cn/article/wudaorensheng/':[u'舞蹈人生'],\
        'http://www.chinadance.cn/article/wudaoshi/':[u'舞蹈史论'],\
        'http://www.chinadance.cn/news/':[u'舞蹈资讯'],\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('li.cl').items():\
            _dict = {}\
            _dict['title'] = each.find('h3 > a').text()\
            _dict['bread'] = response.save.get('bread')\
            url = each.find('h3 > a').attr.href\
            _dict['date'] = each.find('p.info > span').text().split(' ')[0]\
            _dict['cover'] = each.children('a').find('img').attr.src or ''\
            self.crawl(url, save = _dict,callback=self.detail_page)\
      \
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       res_dict['content'] = response.doc('td#article_content').remove('embed').remove('script').html()   \
       res_dict['class'] = 33\
       res_dict['subject'] = u'舞蹈'\
       res_dict['source'] = 'chinadance'\
       res_dict['data_weight'] = 0\
       res_dict['url'] = response.url\
       return res_dict$$$$$1472437711.5546
cidian2_haici$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-15 09:34:36\
# Project: cidian2_haici\
\
from pyspider.libs.base_handler import *\
import sys\
import MySQLdb\
import traceback\
reload(sys)\
sys.setdefaultencoding("utf-8")\
\
conn = MySQLdb.connect(host = "127.0.0.1", user = "root", passwd = "123", db = "cidiandb", charset = "utf8")\
cursor = conn.cursor()\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 30)\
    def on_start(self):\
        sql = 'select id, word from tb_word where final_flag = 0 limit 10000'\
        try:\
            cursor.execute(sql)\
            for (ci_id, word,) in cursor.fetchall():\
                #word = 'good'\
                self.crawl('http://dict.cn/'+word, save = {'ci': word, 'id': ci_id}, callback=self.detail_page)\
        except:\
            traceback.print_exc()\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        word = response.save.get('ci').strip()\
        ci_id = response.save.get('id')\
        sql = "update tb_word set final_flag= 1 where id= %s" % (ci_id)\
        try:           \
            cursor.execute(sql)\
            conn.commit()\
        except Exception, e:\
            print e\
        paraphrase = {}\
        example = {}\
        annotation = {}\
        related_word = {}\
        recommend = {}\
        \
        for each in response.doc('.sent > h3').items():\
            if u'词汇搭配' in each.text():\
                related_word[each.text().strip()] = {}\
                _dict = {}\
\
                for each_span in each.next().find('div').items():\
                    #_dict = {}\
                    temp = each_span.find('b').text()\
                    _dict[temp] = []\
                    _dict2 = {}\
                    while each_span.next():\
                        \
                        if '<b>' in each_span.next().html():\
                            break\
                        if '<li>' not in each_span.next().html():\
                            _dict2['title'] = each_span.next().text()\
                        else:\
                            _dict2['words'] = {}\
                            for each_li in each_span.next().find('li').items():\
                                #这两句话不能放一行！！，不然remove('a')会先执行\
                                k = each_li.find('a').text()\
                                v = each_li.remove('a').text()\
                                _dict2['words'][k] = v\
                                #print _dict2    \
                            _dict[temp].append(_dict2)\
                            _dict2 = {}\
                        #print _dict2        \
                        each_span = each_span.next()\
                        #_dict[temp].append(_dict2)\
                if not each.next().find('b'):\
                    _dict['all'] = []\
                    _dict1 ={}\
                    for each_li in each.next().find('li').items():\
                        k = each_li.find('a').text()\
                        v = each_li.remove('a').html()\
                        _dict1[k] = v\
                    _dict['all'].append(_dict1)\
                related_word[each.text().strip()] = _dict\
                #continue\
            elif u'例句' in each.text():    \
                example[each.text().strip()] = {}\
                _dict = {}\
\
                for each_span in each.next().find('div').items():\
                    #_dict = {}\
                    if not each_span.find('b'):\
                        continue\
                    _dict[each_span.find('b').text()] = []\
                    \
                    for each_li in each_span.next().find('li').items():\
                        _dict1 = {}\
                        _dict1["chinese"] = each_li.remove('i').html().split('<br/>')[1].replace('\\r','').replace('\\n','').replace('\\t','').replace('&#13;','').strip()\
                        _dict1["english"] = each_li.remove('i').html().split('<br/>')[0].replace('\\r','').replace('\\n','').replace('\\t','').replace('&#13;','').strip()\
                        #print _dict1\
                        _dict[each_span.find('b').text()].append(_dict1)\
                example[each.text().strip()] = _dict\
            elif u'常见句型' in each.text():\
                example[each.text().strip()] = {}\
                _dict = {}\
\
                for each_span in each.next().find('div').items():\
                    #_dict = {}\
                    if not each_span.find('b'):\
                        continue\
                    temp = each_span.find('b').text()\
                    _dict[temp] = []\
                    #_dict1 = {}\
                    while each_span.next():\
                        if '<b>' in each_span.next().html():\
                            break\
                        if '<li>' in each_span.next().html():\
                            for each_li in each_span.next().find('li').items():\
                                _dict1 = {}\
                                #print each_li.remove('i').html().split('<br/>')[0]\
                                _dict1["chinese"] = each_li.remove('i').html().split('<br/>')[1].replace('\\r','').replace('\\n','').replace('\\t','').replace('&#13;','').strip()\
                                _dict1["english"] = each_li.remove('i').html().split('<br/>')[0].replace('\\r','').replace('\\n','').replace('\\t','').replace('&#13;','').strip()\
                                _dict[temp].append(_dict1)     \
                        each_span = each_span.next()\
                example[each.text().strip()] = _dict\
            elif u'经典引文' in each.text():    \
                example[each.text().strip()] = []\
                for each_li in each.next().find('li').items():\
                    _dict1 = {}\
                    _dict1["source"] = each_li.find('b').text().strip()\
                    _dict1["sentence"] = each_li.find('p').text().strip()\
                    #print _dict1\
                    example[each.text().strip()].append(_dict1)\
        for each in response.doc('.rel > h3').items():\
            if u'近反义词' in each.text():\
                related_word[each.text().strip()] = {}\
                _dict = {}\
\
                for each_span in each.next().find('div').items():\
                    #_dict = {}\
                    _dict[each_span.text()] = {}\
                    _dict1 = {}\
                    #_list = []\
                    for each_li in each_span.next().find('li').items():\
                        if each_li.find('span'):\
                            _dict1[each_li.find('span').text()] = [v.text() for v in each_li.find('a').items() if v]\
                        #else:\
                         #   _list.append(each_li.text())\
                    #if not each.next().find('div').find('span'):\
                     #   _dict1['all'] = _list\
                    _dict[each_span.text()] = _dict1\
                if not each.next().find('span'):\
                    _list = []\
                    for each_li in each.next().find('li').items():\
                         _list.append(each_li.text())\
                    _dict1['all'] = _list\
                    _dict[each_span.text()] = _dict1\
                related_word[each.text().strip()] = _dict\
                continue\
            \
        \
        for each in response.doc('.def > h3').items():\
            if u'行业释义' in each.text():\
                continue\
            if u'英英释义' in each.text():\
                paraphrase[each.text().strip()] = []\
                #_dict = {}\
\
                for each_span in each.next().find('span').items():\
                    _dict = {}\
                    meanings = {}\
                    _dict["name"] = each_span.find('bdo').text()\
                    _dict["english_name"] = each_span.remove('bdo').text()\
                    for each_li in each_span.next().find('li').items():\
                        val = each_li.find('p').html()\
                        k = each_li.remove('p').text()\
                        meanings[k] = [v for v in val.replace('\\r','').replace('\\n','').replace('\\t','').replace('&#13;','').split('<br/>') if v]\
                    _dict["meanings"] = meanings\
                    paraphrase[each.text().strip()].append(_dict)\
                continue\
                \
            paraphrase[each.text().strip()] = []\
            #_dict = {}\
            \
            for each_span in each.next().find('span').items():\
                _dict = {}\
                meanings = []\
                _dict["name"] = each_span.find('bdo').text()\
                _dict["english_name"] = each_span.remove('bdo').text()\
                for each_li in each_span.next().find('li').items():\
                    meanings.append(each_li.text())\
                _dict["meanings"] = meanings\
                paraphrase[each.text().strip()].append(_dict)\
        \
        for each in response.doc('.learn > h3').items():\
            if u'词义辨析' in each.text():\
                annotation[each.text().strip()] = []\
                #_dict = {}\
\
                for each_span in each.next().find('span').items():\
                    phrase_detail = []\
                    _dict = {}\
                    _dict["name"] = each_span.find('bdo').text()\
                    #print each_spann.find('bdo').text()\
                    _dict["english_name"] = each_span.remove('bdo').text()\
                    #print each_span.next().next().html()\
                    #break\
                    while each_span.next():\
                        if 'dd' not in each_span.next().html():\
                            break\
                        _dict1 = {}\
                        _dict1['phrase'] = each_span.next().find('dt').text()\
                        _dict1['sentences'] = []\
                        for each_div in each_span.next().find('dd > div').items():\
                            _dict1['sentences'].append(each_div.text())\
                        for each_div in each_span.next().find('dd li').items():\
                            _dict1['sentences'].append(each_div.text())\
                        each_span = each_span.next()\
                        #print each_span.html()\
                        phrase_detail.append(_dict1)\
                    _dict["phrase_detail"] = phrase_detail\
                    annotation[each.text().strip()].append(_dict)\
            elif u'词语用法' in each.text():\
                annotation[each.text().strip()] = []\
                #_dict = {}\
\
                for each_span in each.next().find('span').items():\
                    _dict = {}\
                    _dict["name"] = each_span.find('bdo').text()\
                    _dict["english_name"] = each_span.remove('bdo').text()\
                    _dict["sentences"] = []\
                    for each_div in each_span.next().find('li').items():\
                        _dict['sentences'].append(each_div.text())\
                    annotation[each.text().strip()].append(_dict)\
                    \
        \
        \
        for each in response.doc('.rel > h3').items():\
            if u'缩略词' in each.text():\
                recommend[each.text().strip()] = {}\
                _dict = {}\
                _dict[each.next().find('div').text().split(u'，')[0]] = []\
                for each_li in each.next().find('div').next().find('li').items():\
                    _dict[each.next().find('div').text().split(u'，')[0]].append(each_li.text())\
                recommend[each.text().strip()] = _dict\
            elif u'临近单词' in each.text():\
                recommend[each.text().strip()] = []\
                #_dict = {}\
\
                for each_a in each.next().find('a').items():\
                    recommend[each.text().strip()].append(each_a.text())\
\
    \
            \
        return {\
            "url": response.url,\
            "title": response.doc('title').text().replace(u'海词',u'跟谁学'),\
            "word": response.save.get('ci'),\
            "paraphrase": paraphrase,\
            "example": example,\
            "annotation": annotation,\
            "related_word": related_word,\
            "recommend": recommend,\
        }$$$$$1468912645.7863
cidian_haici$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-12 09:36:09\
# Project: cidian_haici\
\
from pyspider.libs.base_handler import *\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        i = 0\
        with open('/apps/home/rd/zengsheng/ciku.txt') as f:\
            for line in f:\
                #print line\
                ci = line.strip().replace('\\n','')\
                #ci = 'good'\
                #i += 1\
                #if i<44:\
                self.crawl('http://dict.cn/'+ci, save = {'ci': ci}, callback=self.detail_page)\
                #else:\
                 #  break\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        paraphrase = {}\
        example = {}\
        annotation = {}\
        related_word = {}\
        for each in response.doc('.def > h3').items():\
            if u'行业释义' in each.text():\
                continue\
            paraphrase[each.text()] = each.next().remove('.sound').html().replace('\\t','').replace('\\n','').replace('\\r','')\
        \
        for each in response.doc('.sent > h3').items():\
            if u'常用短语' in each.text():\
                annotation[each.text()] = each.next().remove('.sound').html().replace('\\t','').replace('\\n','').replace('\\r','')\
                continue\
            if u'词汇搭配' in each.text():\
                related_word[each.text()] = each.next().remove('.sound').text().replace('\\t','').replace('\\n','').replace('\\r','')\
                continue\
            example[each.text()] = each.next().remove('.sound').remove('.more').html().replace('\\t','').replace('\\n','').replace('\\r','')\
        \
        for each in response.doc('.learn > h3').items():\
            annotation[each.text()] = each.next().remove('.sound').html().replace('\\t','').replace('\\n','').replace('\\r','')\
        \
        for each in response.doc('.rel > h3').items():\
            if u'近反义词' in each.text():\
                related_word[each.text()] = each.next().remove('.sound').text().replace('\\t','').replace('\\n','').replace('\\r','')\
            \
        return {\
            "url": response.url,\
            "title": response.doc('title').text().replace(u'海词',u'跟谁学'),\
            "word": response.save.get('ci'),\
            "paraphrase": paraphrase,\
            "example": example,\
            "annotation": annotation,\
            "related_word": related_word,\
        }$$$$$1468461575.5630
cidian_iciba$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-11 14:20:25\
# Project: cidian_iciba\
\
from pyspider.libs.base_handler import *\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        i = 0\
        with open('/apps/home/rd/zengsheng/ciku.txt') as f:\
            for line in f:\
                #print line\
                ci = line.strip().replace('\\n','')\
                #ci = 'good'\
                #i += 1\
                #if i<44:\
                self.crawl('http://www.iciba.com/'+ci, save = {'ci': ci}, callback=self.detail_page)\
                #else:\
                 #   break\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        level = response.doc('.word-rate > p > i').size()\
        flag_first = True\
        other_meaning = {}\
        character = {}\
        pronunciation = {}\
        for each in response.doc('.base-speak span').items():\
            pronunciation[each.text()] = each.next().attr.onmouseover.split("('")[1].split("')")[0]\
        for each in response.doc('.base-list').items():\
            if flag_first:\
                flag_first = False\
                for each_li in each.find('li').items():\
                    character[each_li.find('span').text()] = each_li.find('p').text()\
            else:\
                other_meaning[each.prev().text()] = each.text()\
        if len(character) == 0:\
            character[''] = response.doc('.base-word-long').text()\
        return {\
            "url": response.url,\
            "title": response.doc('title').text().replace(u'爱词霸',u'跟谁学'),\
            "word": response.save.get('ci'),\
            "type": response.doc('.base-level > p').text(),\
            "pronunciation": pronunciation,\
            "character":character,\
            "other_meaning": other_meaning,\
            "change": response.doc('.change').find('p').text(),\
            "level": level,\
        }$$$$$1468310786.3896
cizu2_youdao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-20 16:54:44\
# Project: cizu2_youdao\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    header = {\
    'Accept': '*/*',\
    'Accept-Encoding':'gzip, deflate, sdch',\
    'Accept-Language':'zh-CN,zh;q=0.8',\
    'Cache-Control':'max-age=0',\
    'Connection':'keep-alive',\
    'Cookie':'_ntes_nnid=0bcf1ec80a7f728f778add2cf07877d5,1466150794390; OUTFOX_SEARCH_USER_ID_NCOO=1559550182.3308268; YOUDAO_EAD_UUID=bb83f57c-3189-401e-a9d5-6b6f45759154; search-popup-show=-1; tabRecord.webTrans=%23tEETrans; tabRecord.examples=%23authority; OUTFOX_SEARCH_USER_ID=-550718429@113.57.47.225; JSESSIONID=abcC4xkadz5D_5SeoFhyv; PICUGC_FLASH=; PICUGC_SESSION=90fbe28fda40b5ff04f343a261160ce83d3439fc-%00_TS%3Asession%00',\
    'Host':'dict.youdao.com',\
    'Referer':'http://dict.youdao.com/w/p_guest_professor/',\
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36',\
    'X-Requested-With':'XMLHttpRequest'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://dict.youdao.com/map/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('h2').eq(0).next().find('a').items():\
            self.crawl(each.attr.href,headers = self.header,  callback=self.list_page)\
    \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('li > a').items():\
            self.crawl(each.attr.href, headers = self.header, callback=self.list2_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list2_page(self, response):\
        for each in response.doc('td > a').items():\
            self.crawl(each.attr.href, save = {'word': each.text()}, headers = self.header, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        paraphrase = {}\
        example = {}\
        annotation = {}\
        related_word = {}\
        recommend = {}\
        \
        for each in response.doc('#webTrans > h3').items():\
            if u'网络释义' in each.text():\
                paraphrase[u'网络释义'] = []\
                for each_div in each.next().next().find('#tWebTrans > div').items():\
                    _dict = {}\
                    #print '<p>'+each_div.html()+'</p>'\
                    if u'短语' in each_div.text():\
                        annotation[u'常用短语'] = {}\
                        for each_p in each_div.find('p').items():\
                            k = each_p.find('span').text().replace('  ', '').replace('\\n','')\
                            v = each_p.remove('span').text().replace('  ', '').replace('\\n','')\
                            _dict[k] = v    \
                        annotation[u'常用短语'] = _dict\
                    else:\
                        _dict['name'] = each_div.find('span').text().strip().split()[0]\
                        _dict['meanings'] = [each_div.find('p').eq(0).text().replace('  ', '').replace('\\n','')]\
                        _dict['english_name'] = ''\
                        paraphrase[u'网络释义'].append(_dict)\
            if u'英英释义' in each.text():\
                paraphrase[u'英英释义'] = []\
                for each_div in each.next().next().find('#tEETrans > div').items():\
                    _dict = {}\
                    _dict['name'] = ''\
                    _dict['english_name'] = each_div.find('.ol').prev().text().replace('  ', '').replace('\\n','')\
                    _dict1 = {}\
                    for each_li in each_div.find('.ol > li').items():\
                        _dict1[each_li.find('span').text().replace('\\n','')] = [each_li.find('p').text().replace('\\n','')]\
                    _dict['meanings'] = _dict1\
                    paraphrase[u'英英释义'].append(_dict)\
        for each in response.doc('#examples > h3').items():\
            \
            if u'双语例句' in each.text():\
                example[u'双语例句'] = []\
                for each_div in each.next().find('#bilingual li').items():\
                    _dict = {}\
                    _dict['chinese'] = each_div.find('p').eq(1).text().replace(' ', '').replace('\\n','')\
                    _dict['english'] =  each_div.find('p').eq(0).text().replace('  ', '').replace('\\n','')\
                    example[u'双语例句'].append(_dict)\
                    \
            if u'原声例句' in each.text():\
                example[u'经典引文'] = []\
                for each_div in each.next().find('#originalSound li').items():\
                    _dict = {}\
                    _dict['source'] = each_div.find('p').eq(-1).text().replace('  ', '').replace('\\n','')\
                    _dict['sentence'] =  each_div.find('p').eq(0).text().replace('  ', '').replace('\\n','')\
                    example[u'经典引文'].append(_dict)\
                for each_div in each.next().find('#authority li').items():\
                    _dict = {}\
                    _dict['source'] = each_div.find('p').eq(-1).text().replace('  ', '').replace('\\n','')\
                    _dict['sentence'] =  each_div.find('p').eq(0).text().replace('  ', '').replace('\\n','')\
                    example[u'经典引文'].append(_dict)\
                    \
        for each in response.doc('#rel-search').items():\
            \
            if u'相关搜索' in each.text():\
                recommend[u'相关搜索'] = []\
                for each_a in each.find('a').items():\
                    recommend[u'相关搜索'].append(each_a.text())\
              \
        ci_character = {}\
        if response.doc('.clearfix li'):\
            ci_character[u'词义'] = response.doc('.clearfix li').text()\
        return {\
            "url": response.url,\
            "title": response.doc('title').text().replace(u'有道词典',u'跟谁学'),\
            "paraphrase": paraphrase,\
            "example": example,\
            "annotation": annotation,\
            "related_word": related_word,\
            "recommend": recommend,  \
            "dict": {},\
            "question": {},\
            "ci_type": {},\
            "ci_character": ci_character,\
            "ci_change": {},\
            "word": response.save.get('word'),\
            "ci_level": 0,\
            "pronunciation": {},\
            "other_meaning": {},\
        }$$$$$1469011834.0120
cizu_youdao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-19 16:40:25\
# Project: cizu_youdao\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    header = {\
    'Accept': '*/*',\
    'Accept-Encoding':'gzip, deflate, sdch',\
    'Accept-Language':'zh-CN,zh;q=0.8',\
    'Cache-Control':'max-age=0',\
    'Connection':'keep-alive',\
    'Cookie':'_ntes_nnid=0bcf1ec80a7f728f778add2cf07877d5,1466150794390; OUTFOX_SEARCH_USER_ID_NCOO=1559550182.3308268; YOUDAO_EAD_UUID=bb83f57c-3189-401e-a9d5-6b6f45759154; search-popup-show=-1; tabRecord.webTrans=%23tEETrans; tabRecord.examples=%23authority; OUTFOX_SEARCH_USER_ID=-550718429@113.57.47.225; JSESSIONID=abcC4xkadz5D_5SeoFhyv; PICUGC_FLASH=; PICUGC_SESSION=90fbe28fda40b5ff04f343a261160ce83d3439fc-%00_TS%3Asession%00',\
    'Host':'dict.youdao.com',\
    'Referer':'http://dict.youdao.com/w/p_guest_professor/',\
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36',\
    'X-Requested-With':'XMLHttpRequest'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://dict.youdao.com/map/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('h2').eq(0).next().find('a').items():\
            self.crawl(each.attr.href,headers = self.header,  callback=self.list_page)\
    \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('li > a').items():\
            self.crawl(each.attr.href, headers = self.header, callback=self.list2_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list2_page(self, response):\
        for each in response.doc('td > a').items():\
            self.crawl(each.attr.href, save = {'word': each.text()}, headers = self.header, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        paraphrase = {}\
        example = {}\
        annotation = {}\
        related_word = {}\
        recommend = {}\
        \
        for each in response.doc('#webTrans > h3').items():\
            if u'网络释义' in each.text():\
                paraphrase[u'网络释义'] = []\
                for each_div in each.next().next().find('#tWebTrans > div').items():\
                    _dict = {}\
                    #print '<p>'+each_div.html()+'</p>'\
                    if u'短语' in each_div.text():\
                        annotation[u'常用短语'] = {}\
                        for each_p in each_div.find('p').items():\
                            k = each_p.find('span').text().replace('  ', '').replace('\\n','')\
                            v = each_p.remove('span').text().replace('  ', '').replace('\\n','')\
                            _dict[k] = v    \
                        annotation[u'常用短语'] = _dict\
                    else:\
                        _dict['name'] = each_div.find('span').text().strip().split()[0]\
                        _dict['meanings'] = [each_div.find('p').eq(0).text().replace('  ', '').replace('\\n','')]\
                        _dict['english_name'] = ''\
                        paraphrase[u'网络释义'].append(_dict)\
            if u'英英释义' in each.text():\
                paraphrase[u'英英释义'] = []\
                for each_div in each.next().next().find('#tEETrans > div').items():\
                    _dict = {}\
                    _dict['name'] = ''\
                    _dict['english_name'] = each_div.find('.ol').prev().text().replace('  ', '').replace('\\n','')\
                    _dict1 = {}\
                    for each_li in each_div.find('.ol > li').items():\
                        _dict1[each_li.find('span').text().replace('\\n','')] = [each_li.find('p').text().replace('\\n','')]\
                    _dict['meanings'] = _dict1\
                    paraphrase[u'英英释义'].append(_dict)\
        for each in response.doc('#examples > h3').items():\
            \
            if u'双语例句' in each.text():\
                example[u'双语例句'] = []\
                for each_div in each.next().find('#bilingual li').items():\
                    _dict = {}\
                    _dict['chinese'] = each_div.find('p').eq(1).text().replace(' ', '').replace('\\n','')\
                    _dict['english'] =  each_div.find('p').eq(0).text().replace('  ', '').replace('\\n','')\
                    example[u'双语例句'].append(_dict)\
                    \
            if u'原声例句' in each.text():\
                example[u'经典引文'] = []\
                for each_div in each.next().find('#originalSound li').items():\
                    _dict = {}\
                    _dict['source'] = each_div.find('p').eq(-1).text().replace('  ', '').replace('\\n','')\
                    _dict['sentence'] =  each_div.find('p').eq(0).text().replace('  ', '').replace('\\n','')\
                    example[u'经典引文'].append(_dict)\
                for each_div in each.next().find('#authority li').items():\
                    _dict = {}\
                    _dict['source'] = each_div.find('p').eq(-1).text().replace('  ', '').replace('\\n','')\
                    _dict['sentence'] =  each_div.find('p').eq(0).text().replace('  ', '').replace('\\n','')\
                    example[u'经典引文'].append(_dict)\
                    \
        for each in response.doc('#rel-search').items():\
            \
            if u'相关搜索' in each.text():\
                recommend[u'相关搜索'] = []\
                for each_a in each.find('a').items():\
                    recommend[u'相关搜索'].append(each_a.text())\
              \
        ci_character = {}\
        if response.doc('.clearfix li'):\
            ci_character[u'词义'] = response.doc('.clearfix li').text()\
        return {\
            "url": response.url,\
            "title": response.doc('title').text().replace(u'有道词典',u'跟谁学'),\
            "paraphrase": paraphrase,\
            "example": example,\
            "annotation": annotation,\
            "related_word": related_word,\
            "recommend": recommend,  \
            "dict": {},\
            "question": {},\
            "ci_type": {},\
            "ci_character": ci_character,\
            "ci_change": {},\
            "word": response.save.get('word'),\
            "ci_level": 0,\
            "pronunciation": {},\
            "other_meaning": {},\
        }$$$$$1469005032.4482
csdn_proxy$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 10:14:04\
# Project: csdn_proxy\
\
\
\
from pyspider.libs.base_handler import *\
\
import json,re\
menu = '''{\
    "forumNodes": [{"name":"\\u79fb\\u52a8\\u5f00\\u53d1","url":"/forums/Mobile","children":[{"name":"iOS","url":"/forums/ios"},{"name":"Android","url":"/forums/Android"},{"name":"Swift","url":"/forums/swift"},{"name":"Windows\\u5ba2\\u6237\\u7aef\\u5f00\\u53d1","url":"/forums/WindowsMobile"},{"name":"Symbian","url":"/forums/Symbian"},{"name":"BlackBerry","url":"/forums/BlackBerry"},{"name":"Qt","url":"/forums/Qt"},{"name":"\\u79fb\\u52a8\\u652f\\u4ed8","url":"/forums/PaypalCommunity"},{"name":"\\u79fb\\u52a8\\u5e7f\\u544a","url":"/forums/MobileAD"},{"name":"\\u5fae\\u4fe1\\u5f00\\u53d1","url":"/forums/weixin"},{"name":"\\u79fb\\u52a8\\u5f00\\u53d1\\u5176\\u4ed6\\u95ee\\u9898","url":"/forums/Mobile_Other"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/MobileNonTechnical"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"},{"name":"\\u8054\\u901aWO+\\u5f00\\u653e\\u5e73\\u53f0","url":"/forums/chinaunicom"}]},{"name":"\\u4e91\\u8ba1\\u7b97","url":"/forums/CloudComputing","children":[{"name":"IaaS","children":[{"name":"OpenStack","url":"/forums/OpenStack"}]},{"name":"PaaS/SaaS","children":[{"name":"Cloud Foundry","url":"/forums/CloudFoundry"},{"name":"GAE","url":"/forums/GAE"}]},{"name":"\\u6570\\u636e\\u4e2d\\u5fc3\\u8fd0\\u7ef4","children":[{"name":"\\u670d\\u52a1\\u5668","url":"/forums/server"},{"name":"\\u7f51\\u7edc","url":"/forums/network"},{"name":"\\u865a\\u62df\\u5316","url":"/forums/virtual"}]},{"name":"AWS","url":"/forums/AWS"},{"name":"\\u534e\\u4e3a\\u4e91\\u8ba1\\u7b97","url":"/forums/fusioncloud"},{"name":"\\u5f00\\u653e\\u5e73\\u53f0","url":"/forums/OpenAPI"},{"name":"\\u4e91\\u5b89\\u5168","url":"/forums/ST_Security"},{"name":"\\u5206\\u5e03\\u5f0f\\u8ba1\\u7b97/Hadoop","url":"/forums/hadoop"},{"name":"\\u4e91\\u5b58\\u50a8","url":"/forums/CloudStorage"},{"name":"Docker","url":"/forums/docker"},{"name":"Spark","url":"/forums/spark"},{"name":"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0","url":"/forums/DE"}]},{"name":"\\u4f01\\u4e1aIT","url":"/forums/Enterprise","children":[{"name":"\\u4e2d\\u95f4\\u4ef6","children":[{"name":"\\u4e2d\\u95f4\\u4ef6","url":"/forums/Middleware"},{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"JBoss","url":"/forums/JBoss"}]},{"name":"\\u4f01\\u4e1a\\u7ba1\\u7406\\u8f6f\\u4ef6","children":[{"name":"\\u6d88\\u606f\\u534f\\u4f5c","url":"/forums/ExchangeServer"},{"name":"SharePoint","url":"/forums/SharePoint"}]},{"name":"Atlassian\\u6280\\u672f\\u8bba\\u575b","url":"/forums/atlassian"},{"name":"JetBrains\\u6280\\u672f\\u8bba\\u575b","url":"/forums/JetBrains"},{"name":"\\u5730\\u7406\\u4fe1\\u606f\\u7cfb\\u7edf","url":"/forums/GIS"},{"name":"\\u4f01\\u4e1a\\u4fe1\\u606f\\u5316","url":"/forums/Enterprise_Information"},{"name":"ERP/CRM","url":"/forums/ERP"},{"name":"\\u5176\\u4ed6","url":"/forums/Enterprise_Other"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"},{"name":"Enterprise Architect\\u6280\\u672f\\u8bba\\u575b","url":"/forums/EA"}]},{"name":".NET\\u6280\\u672f","url":"/forums/DotNET","children":[{"name":"C#","url":"/forums/CSharp"},{"name":"ASP.NET","url":"/forums/ASPDotNET"},{"name":".NET Framework","url":"/forums/DotNETFramework"},{"name":"Web Services","url":"/forums/DotNETWebServices"},{"name":"VB.NET","url":"/forums/VBDotNET"},{"name":"VC.NET","url":"/forums/VCDotNet"},{"name":"\\u56fe\\u8868\\u533a","url":"/forums/DotNETReport"},{"name":".NET\\u6280\\u672f\\u524d\\u77bb","url":"/forums/DotNET_NewTech"},{"name":".NET\\u5206\\u6790\\u4e0e\\u8bbe\\u8ba1","url":"/forums/DotNETAnalysisAndDesign"},{"name":"\\u7ec4\\u4ef6/\\u63a7\\u4ef6\\u5f00\\u53d1","url":"/forums/DotNET_Controls"},{"name":"SharePoint","url":"/forums/SharePoint"},{"name":"WPF/Silverlight","url":"/forums/Silverlight"},{"name":"LINQ","url":"/forums/LINQ"},{"name":"VSTS","url":"/forums/VSTS"},{"name":"\\u5176\\u4ed6","url":"/forums/DotNET_Other"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DotNETNonTechnical"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"}]},{"name":"Java \\u6280\\u672f","url":"/forums/Java","children":[{"name":"Java SE","url":"/forums/J2SE"},{"name":"J2ME","url":"/forums/J2ME"},{"name":"Java Web \\u5f00\\u53d1","url":"/forums/Java_WebDevelop"},{"name":"Java EE","url":"/forums/J2EE"},{"name":"Eclipse","url":"/forums/Eclipse"},{"name":"Java\\u5176\\u4ed6\\u76f8\\u5173","url":"/forums/JavaOther"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/JavaNonTechnical"},{"name":"JBoss","url":"/forums/JBoss"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"}]},{"name":"Web \\u5f00\\u53d1","url":"/forums/WebDevelop","children":[{"name":"ASP","url":"/forums/ASP"},{"name":"ASP.NET","url":"/forums/ASPDotNET"},{"name":"JSP","url":"/forums/Java_WebDevelop"},{"name":"PHP","url":"/forums/PHP","children":[{"name":"\\u5f00\\u6e90\\u8d44\\u6e90","url":"/forums/PHPOpenSource"},{"name":"\\u57fa\\u7840\\u7f16\\u7a0b","url":"/forums/PHPBase"},{"name":"Framework","url":"/forums/PHPFramework"}]},{"name":"JavaScript","url":"/forums/JavaScript"},{"name":"\\u641c\\u7d22\\u5f15\\u64ce\\u6280\\u672f","url":"/forums/SearchEngine"},{"name":"Ajax \\u6280\\u672f","url":"/forums/Ajax"},{"name":"VBScript","url":"/forums/vbScript"},{"name":"CGI","url":"/forums/CGI"},{"name":"XML/XSL","url":"/forums/XMLSOAP"},{"name":"IIS","url":"/forums/IIS"},{"name":"Apache","url":"/forums/Apache"},{"name":"HTML(CSS)","url":"/forums/HTMLCSS"},{"name":"ColdFusion","url":"/forums/ColdFusion"},{"name":"Ruby/Rails","url":"/forums/ROR"},{"name":"\\u8de8\\u6d4f\\u89c8\\u5668\\u5f00\\u53d1","url":"/forums/CrossBrowser"},{"name":"\\u5176\\u4ed6","url":"/forums/WebDevelop_Other"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/WebNonTechnical"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"HTML5","url":"/forums/HTML5"}]},{"name":"\\u5f00\\u53d1\\u8bed\\u8a00/\\u6846\\u67b6","children":[{"name":"VC/MFC","url":"/forums/VC","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/VC_Basic"},{"name":"\\u754c\\u9762","url":"/forums/VC_UI"},{"name":"\\u7f51\\u7edc\\u7f16\\u7a0b","url":"/forums/VC_Network"},{"name":"\\u8fdb\\u7a0b/\\u7ebf\\u7a0b/DLL","url":"/forums/VC_Process"},{"name":"ATL/ActiveX/COM","url":"/forums/VC_ActiveX"},{"name":"\\u6570\\u636e\\u5e93","url":"/forums/VC_Database"},{"name":"\\u786c\\u4ef6/\\u7cfb\\u7edf","url":"/forums/VC_Hardware"},{"name":"HTML/XML","url":"/forums/VC_HTML"},{"name":"\\u56fe\\u5f62\\u5904\\u7406/\\u7b97\\u6cd5","url":"/forums/VC_ImageProcessing"},{"name":"\\u8d44\\u6e90","url":"/forums/VCResources"},{"name":"\\u975e\\u6280\\u672f\\u7c7b","url":"/forums/VC_NonTechnical"}]},{"name":"VB","url":"/forums/VB","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/VB_Basic"},{"name":"\\u975e\\u6280\\u672f\\u7c7b","url":"/forums/VB_NonTechnical"},{"name":"\\u63a7\\u4ef6","url":"/forums/VB_Controls"},{"name":"API","url":"/forums/VB_API"},{"name":"\\u6570\\u636e\\u5e93(\\u5305\\u542b\\u6253\\u5370\\uff0c\\u5b89\\u88c5\\uff0c\\u62a5\\u8868)","url":"/forums/VB_Database"},{"name":"\\u591a\\u5a92\\u4f53","url":"/forums/VB_Multimedia"},{"name":"\\u7f51\\u7edc\\u7f16\\u7a0b","url":"/forums/VB_Network"},{"name":"VBA","url":"/forums/VBA"},{"name":"COM/DCOM/COM+","url":"/forums/VB_COM"},{"name":"\\u8d44\\u6e90","url":"/forums/VBResources"}]},{"name":"Delphi","url":"/forums/Delphi","children":[{"name":"VCL\\u7ec4\\u4ef6\\u5f00\\u53d1\\u53ca\\u5e94\\u7528","url":"/forums/DelphiVCL"},{"name":"\\u6570\\u636e\\u5e93\\u76f8\\u5173","url":"/forums/DelphiDB"},{"name":"Windows SDK/API","url":"/forums/DelphiAPI"},{"name":"\\u7f51\\u7edc\\u901a\\u4fe1/\\u5206\\u5e03\\u5f0f\\u5f00\\u53d1","url":"/forums/DelphiNetwork"},{"name":"\\u8bed\\u8a00\\u57fa\\u7840/\\u7b97\\u6cd5/\\u7cfb\\u7edf\\u8bbe\\u8ba1","url":"/forums/DelphiBase"},{"name":"GAME\\uff0c\\u56fe\\u5f62\\u5904\\u7406/\\u591a\\u5a92\\u4f53","url":"/forums/DelphiMultimedia"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DelphiNonTechnical"}]},{"name":"C++ Builder","url":"/forums/BCB","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/BCBBase"},{"name":"\\u6570\\u636e\\u5e93\\u53ca\\u76f8\\u5173\\u6280\\u672f","url":"/forums/BCBDB"},{"name":"VCL\\u7ec4\\u4ef6\\u4f7f\\u7528\\u548c\\u5f00\\u53d1","url":"/forums/BCBVCL"},{"name":"Windows SDK/API","url":"/forums/BCBAPI"},{"name":"\\u7f51\\u7edc\\u53ca\\u901a\\u8baf\\u5f00\\u53d1","url":"/forums/BCBNetwork"},{"name":"ActiveX/COM/DCOM","url":"/forums/BCBCOM"},{"name":"\\u8336\\u9986","url":"/forums/BCBTeaHouses"}]},{"name":"C/C++","url":"/forums/Cpp","children":[{"name":"\\u65b0\\u624b\\u4e50\\u56ed","url":"/forums/Cpp_Freshman"},{"name":"C\\u8bed\\u8a00","url":"/forums/C"},{"name":"C++ \\u8bed\\u8a00","url":"/forums/CPPLanguage"},{"name":"\\u5de5\\u5177\\u5e73\\u53f0\\u548c\\u7a0b\\u5e8f\\u5e93","url":"/forums/Cpp_ToolsPlatform"},{"name":"\\u6a21\\u5f0f\\u53ca\\u5b9e\\u73b0","url":"/forums/Cpp_Model"},{"name":"\\u5176\\u4ed6\\u6280\\u672f\\u95ee\\u9898","url":"/forums/Cpp_Other"},{"name":"Qt","url":"/forums/Qt"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Cpp_NonTechnical"}]},{"name":"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00","url":"/forums/OtherLanguage","open":true,"children":[{"name":"OpenCL\\u548c\\u5f02\\u6784\\u7f16\\u7a0b","url":"/forums/Heterogeneous"},{"name":"Go\\u8bed\\u8a00","url":"/forums/golang"},{"name":"JBoss\\u6280\\u672f\\u4ea4\\u6d41","url":"/forums/JBoss"},{"name":"\\u6c47\\u7f16\\u8bed\\u8a00","url":"/forums/ASM"},{"name":"\\u811a\\u672c\\u8bed\\u8a00\\uff08Perl/Python\\uff09","url":"/forums/OL_Script"},{"name":"Office\\u5f00\\u53d1/ VBA","url":"/forums/OfficeDevelopment"},{"name":"VFP","url":"/forums/VFP"},{"name":"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00","url":"/forums/OtherLanguage_Other"}]}]},{"name":"\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":null,"children":[{"name":"\\u5927\\u6570\\u636e","children":[{"name":"Hadoop","url":"/forums/hadoop"}]},{"name":"MS-SQL Server","url":"/forums/MSSQL","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/MSSQL_Basic"},{"name":"\\u5e94\\u7528\\u5b9e\\u4f8b","url":"/forums/MSSQL_Cases"},{"name":"\\u7591\\u96be\\u95ee\\u9898","url":"/forums/MSSQL_DifficultProblems"},{"name":"\\u65b0\\u6280\\u672f\\u524d\\u6cbf","url":"/forums/MSSQL_NewTech"},{"name":"SQL Server BI","url":"/forums/SQLSERVERBI"},{"name":"\\u975e\\u6280\\u672f\\u7248","url":"/forums/MSSQL_NonTechnical"}]},{"name":"PowerBuilder","url":"/forums/PowerBuilder","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/PB_Basic"},{"name":"Pb\\u811a\\u672c\\u8bed\\u8a00","url":"/forums/PBScript"},{"name":"DataWindow","url":"/forums/PB_DataWindow"},{"name":"API \\u8c03\\u7528","url":"/forums/PB_API"},{"name":"\\u63a7\\u4ef6\\u4e0e\\u754c\\u9762","url":"/forums/PB_Controls"},{"name":"Pb Web \\u5e94\\u7528","url":"/forums/PB_WEB"},{"name":"\\u6570\\u636e\\u5e93\\u76f8\\u5173","url":"/forums/PB_Database"},{"name":"\\u9879\\u76ee\\u7ba1\\u7406","url":"/forums/PB_ProjectManagement"},{"name":"\\u975e\\u6280\\u672f\\u7248","url":"/forums/PB_NonTechnical"}]},{"name":"Oracle","url":"/forums/Oracle","children":[{"name":"\\u57fa\\u7840\\u548c\\u7ba1\\u7406","url":"/forums/Oracle_Management"},{"name":"\\u5f00\\u53d1","url":"/forums/Oracle_Develop"},{"name":"\\u9ad8\\u7ea7\\u6280\\u672f","url":"/forums/Oracle_Technology"},{"name":"\\u8ba4\\u8bc1\\u4e0e\\u8003\\u8bd5","url":"/forums/Oracle_Certificate"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Oracle_NonTechnical"}]},{"name":"Informatica","url":"/forums/Informatica"},{"name":"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/OtherDatabase","open":true,"children":[{"name":"IBM DB2","url":"/forums/DB2"},{"name":"MongoDB","url":"/forums/MongoDB"},{"name":"\\u6570\\u636e\\u4ed3\\u5e93","url":"/forums/DataWarehouse"},{"name":"VFP","url":"/forums/VFP"},{"name":"Access","url":"/forums/Access"},{"name":"Sybase","url":"/forums/Sybase"},{"name":"Informix","url":"/forums/Informix"},{"name":"MySQL","url":"/forums/MySQL"},{"name":"PostgreSQL","url":"/forums/PostgreSQL"},{"name":"\\u6570\\u636e\\u5e93\\u62a5\\u8868","url":"/forums/DatabaseReport"},{"name":"\\u5176\\u4ed6\\u6570\\u636e\\u5e93","url":"/forums/OtherDatabase_Other"},{"name":"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/HPDatabase"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DatabaseNonTechnical"}]}]},{"name":"Linux/Unix\\u793e\\u533a","url":"/forums/Linux","children":[{"name":"\\u7cfb\\u7edf\\u7ef4\\u62a4\\u4e0e\\u4f7f\\u7528\\u533a","url":"/forums/Linux_System"},{"name":"\\u5e94\\u7528\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a","url":"/forums/Linux_Development"},{"name":"\\u5185\\u6838\\u6e90\\u4ee3\\u7801\\u7814\\u7a76\\u533a","url":"/forums/Linux_Kernel"},{"name":"\\u9a71\\u52a8\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a","url":"/forums/Linux_Driver"},{"name":"CPU\\u548c\\u786c\\u4ef6\\u533a","url":"/forums/Linux_Hardware"},{"name":"\\u4e13\\u9898\\u6280\\u672f\\u8ba8\\u8bba\\u533a","url":"/forums/Linux_SpecialTopic"},{"name":"\\u5b9e\\u7528\\u8d44\\u6599\\u53d1\\u5e03\\u533a","url":"/forums/Linux_Information"},{"name":"UNIX\\u6587\\u5316","url":"/forums/Unix_Culture"},{"name":"Solaris","url":"/forums/Solaris"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/LinuxNonTechnical"}]},{"name":"Windows\\u4e13\\u533a","url":"/forums/Windows","children":[{"name":"Windows\\u5ba2\\u6237\\u7aef\\u4f7f\\u7528","url":"/forums/Windows7"},{"name":"Windows Server","url":"/forums/WinNT2000XP2003"},{"name":"\\u7f51\\u7edc\\u7ba1\\u7406\\u4e0e\\u914d\\u7f6e","url":"/forums/NetworkConfiguration"},{"name":"\\u5b89\\u5168\\u6280\\u672f/\\u75c5\\u6bd2","url":"/forums/WindowsSecurity"},{"name":"\\u4e00\\u822c\\u8f6f\\u4ef6\\u4f7f\\u7528","url":"/forums/WindowsBase"},{"name":"Microsoft Office\\u5e94\\u7528","url":"/forums/OfficeBase"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/WindowsNonTechnical"}]},{"name":"\\u786c\\u4ef6/\\u5d4c\\u5165\\u5f00\\u53d1","url":"/forums/Embedded","children":[{"name":"\\u5d4c\\u5165\\u5f00\\u53d1(WinCE)","url":"/forums/WinCE"},{"name":"\\u6c47\\u7f16\\u8bed\\u8a00","url":"/forums/ASM"},{"name":"\\u786c\\u4ef6\\u8bbe\\u8ba1","url":"/forums/Embedded_hardware"},{"name":"\\u9a71\\u52a8\\u5f00\\u53d1/\\u6838\\u5fc3\\u5f00\\u53d1","url":"/forums/Embedded_driver"},{"name":"\\u5355\\u7247\\u673a/\\u5de5\\u63a7","url":"/forums/Embedded_SCM"},{"name":"\\u65e0\\u7ebf","url":"/forums/Embedded_wireless"},{"name":"\\u5176\\u4ed6\\u786c\\u4ef6\\u5f00\\u53d1","url":"/forums/Embedded_Other"},{"name":"VxWorks\\u5f00\\u53d1","url":"/forums/VxWorks"},{"name":"Qt\\u5f00\\u53d1","url":"/forums/Qt"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/EmbeddedNonTechnical"},{"name":"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97","url":"/forums/HPC"},{"name":"\\u667a\\u80fd\\u786c\\u4ef6","url":"/forums/SmartHardware"}]},{"name":"\\u6e38\\u620f\\u5f00\\u53d1","url":"/forums/GameDevelop","children":[{"name":"Cocos2d-x","url":"/forums/GD_Cocos2d-x"},{"name":"Unity3D","url":"/forums/GD_Unity3D"},{"name":"\\u5176\\u4ed6\\u6e38\\u620f\\u5f15\\u64ce","url":"/forums/Othergameengines"},{"name":"\\u6e38\\u620f\\u7b56\\u5212\\u4e0e\\u8fd0\\u8425","url":"/forums/Gdesignoperation"}]},{"name":"\\u7f51\\u7edc\\u4e0e\\u901a\\u4fe1","url":"/forums/network_communication","children":[{"name":"\\u7f51\\u7edc\\u534f\\u8bae\\u4e0e\\u914d\\u7f6e","url":"/forums/IP_Protocolconfiguration"},{"name":"\\u7f51\\u7edc\\u7ef4\\u62a4\\u4e0e\\u7ba1\\u7406","url":"/forums/maintainmanage"},{"name":"\\u4ea4\\u6362\\u53ca\\u8def\\u7531\\u6280\\u672f","url":"/forums/Hardware_SwitchRouter"},{"name":"CDN","url":"/forums/NetworkC_CDN"},{"name":"\\u901a\\u4fe1\\u6280\\u672f","url":"/forums/ST_Network"},{"name":"VOIP\\u6280\\u672f\\u63a2\\u8ba8","url":"/forums/voip"}]},{"name":"\\u6269\\u5145\\u8bdd\\u9898","url":"/forums/Other","children":[{"name":"\\u704c\\u6c34\\u4e50\\u56ed","url":"/forums/FreeZone"},{"name":"\\u7a0b\\u5e8f\\u4eba\\u751f","url":"/forums/ProgrammerStory"},{"name":"\\u7a0b\\u5e8f\\u5a9b\\u4e16\\u754c","url":"/forums/ProgramGirls"},{"name":"\\u7a0b\\u5e8f\\u5458\\u4ea4\\u53cb","url":"/forums/ProgramFriends"},{"name":"\\u4e09\\u5341\\u800c\\u7acb","url":"/forums/30Plus"},{"name":"\\u6e38\\u620f\\u4e13\\u533a","url":"/forums/Game"},{"name":"\\u4e1a\\u754c\\u65b0\\u95fb","url":"/forums/ITnews"},{"name":"\\u7a0b\\u5e8f\\u5458\\u82f1\\u8bed","url":"/forums/English"},{"name":"\\u6c42\\u804c\\u4e0e\\u62db\\u8058","url":"/forums/CAREER"},{"name":"\\u8ba1\\u7b97\\u673a\\u56fe\\u4e66","url":"/forums/Book"},{"name":"\\u5927\\u5b66\\u65f6\\u4ee3","url":"/forums/CollegeTime"},{"name":"\\u8df3\\u86a4\\u5e02\\u573a","url":"/forums/Trade"},{"name":"\\u8f6f\\u4ef6\\u6c42\\u52a9","url":"/forums/Shareware"}]},{"name":"\\u6328\\u8e22\\u804c\\u6daf","url":"/forums/CAREER","children":[{"name":"\\u6c42\\u804c\\u9762\\u8bd5","url":"/forums/WorkplaceCommunication"},{"name":"\\u4f01\\u4e1a\\u70b9\\u8bc4","url":"/forums/TECHHUNT"},{"name":"\\u804c\\u573a\\u8bdd\\u9898","url":"/forums/OFFICELIFE"},{"name":"JOB \\u9a7f\\u7ad9","url":"/forums/jobservice"}]},{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u793e\\u533a","url":"/forums/eSDK","children":[{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b","url":"/forums/DevChallenge2016"},{"name":"\\u4e91\\u8ba1\\u7b97","url":"/forums/hwfsdeveloper"},{"name":"\\u4f01\\u4e1a\\u901a\\u4fe1","url":"/forums/hwucdeveloper"},{"name":"BYOD","url":"/forums/hwbyoddeveloper"},{"name":"\\u5927\\u6570\\u636e","children":[{"name":"FusionInsight HD","url":"/forums/fusioninsightdeveloper"},{"name":"FusionInsight Universe","url":"/forums/hwuniversedeveloper"}]},{"name":"Digital inCloud","url":"/forums/hwswdeveloper"},{"name":"CaaS","url":"/forums/hwcndeveloper"},{"name":"SDN","url":"/forums/hwsdndeveloper"},{"name":"\\u4f01\\u4e1a\\u7f51\\u7edc\\u5f00\\u53d1","url":"/forums/hwendeveloper"},{"name":"\\u654f\\u6377\\u7f51\\u7edc","url":"/forums/hwesightdeveloper"},{"name":"eLTE","url":"/forums/hwbbtdeveloper"},{"name":"\\u7269\\u8054\\u7f51\\u5f00\\u53d1","url":"/forums/hwiotdeveloper"},{"name":"\\u79fb\\u52a8\\u5f00\\u653e\\u5de5\\u573a","url":"/forums/hwwldeveloper"},{"name":"OpenLife\\u667a\\u6167\\u5bb6\\u5ead","url":"/forums/OpenLife"},{"name":"HUAWEI Code Craft","url":"/forums/hwcodecraft"}]},{"name":"IBM \\u6280\\u672f\\u793e\\u533a","children":[{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"DB2","url":"/forums/DB2"},{"name":"Rational","url":"/forums/Rational"},{"name":"Lotus","url":"/forums/Lotus"},{"name":"IBM\\u4e91\\u8ba1\\u7b97","url":"/forums/ibmcloud"},{"name":"IBM \\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"Tivoli","url":"/forums/Tivoli"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"}]},{"name":"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a","children":[{"name":"\\u82f1\\u7279\\u5c14\\u6280\\u672f","url":"/forums/intel"}]},{"name":"Qualcomm\\u5f00\\u53d1\\u8bba\\u575b","children":[{"name":"Qualcomm\\u5f00\\u53d1","url":"/forums/qualcomm"}]},{"name":"\\u4f01\\u4e1a\\u6280\\u672f","children":[{"name":"IBM \\u6280\\u672f\\u793e\\u533a","children":[{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"DB2","url":"/forums/DB2"},{"name":"Rational","url":"/forums/Rational"},{"name":"Lotus","url":"/forums/Lotus"},{"name":"IBM\\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"Tivoli","url":"/forums/Tivoli"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"}]},{"name":"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a","children":[{"name":"\\u82f1\\u7279\\u5c14\\u6280\\u672f","url":"/forums/intel"}]},{"name":"T\\u5ba2\\u8bba\\u575b","url":"/forums/tcl"},{"name":"Paypal\\u5f00\\u53d1\\u8005\\u793e\\u533a","url":"/forums/PaypalCommunity"},{"name":"CUDA","url":"/forums/CUDA","children":[{"name":"CUDA\\u7f16\\u7a0b","url":"/forums/CUDA_Dev"},{"name":"CUDA\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\\u8ba8\\u8bba","url":"/forums/CUDA_Compute"},{"name":"CUDA on Linux","url":"/forums/CUDA_Linux"},{"name":"CUDA on Windows XP","url":"/forums/CUDA_WinXP"}]},{"name":"Google\\u6280\\u672f\\u793e\\u533a","children":[{"name":"Google\\u6280\\u672f\\u793e\\u533a","url":"/forums/GoogleCommunity"},{"name":"Android","url":"/forums/Android"}]},{"name":"Microsoft Office \\u5e94\\u7528\\u4e8e\\u5f00\\u53d1","children":[{"name":"Office\\u5f00\\u53d1","url":"/forums/OfficeDevelopment"},{"name":"Office\\u4f7f\\u7528","url":"/forums/OfficeBase"}]}]},{"name":"\\u5176\\u4ed6\\u6280\\u672f\\u8bba\\u575b","children":[{"name":"\\u8f6f\\u4ef6\\u5de5\\u7a0b/\\u7ba1\\u7406","url":"/forums/SE","children":[{"name":"\\u8f6f\\u4ef6\\u6d4b\\u8bd5","url":"/forums/SE_Quality"},{"name":"\\u7814\\u53d1\\u7ba1\\u7406","url":"/forums/SE_Management"},{"name":"\\u654f\\u6377\\u5f00\\u53d1","url":"/forums/Agile"},{"name":"\\u7248\\u672c\\u63a7\\u5236","url":"/forums/CVS_SVN"},{"name":"\\u8bbe\\u8ba1\\u6a21\\u5f0f","url":"/forums/DesignPatterns"}]},{"name":"\\u9ad8\\u6027\\u80fd\\u5f00\\u53d1","url":"/forums/HPDevelopment","children":[{"name":"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97","url":"/forums/HPC"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/HPDatabase"},{"name":"\\u6d77\\u91cf\\u6570\\u636e\\u5904\\u7406/\\u641c\\u7d22\\u6280\\u672f","url":"/forums/SearchEngine"},{"name":"\\u6570\\u636e\\u7ed3\\u6784\\u4e0e\\u7b97\\u6cd5","url":"/forums/ST_Arithmetic"}]},{"name":"\\u4e13\\u9898\\u5f00\\u53d1/\\u6280\\u672f/\\u9879\\u76ee","url":"/forums/SpecialTopic","children":[{"name":"OpenAPI","url":"/forums/OpenAPI"},{"name":"OpenStack","url":"/forums/OpenStack"},{"name":"\\u673a\\u5668\\u89c6\\u89c9","url":"/forums/ST_Image"},{"name":"OpenCV","url":"/forums/OpenCV"},{"name":"\\u4fe1\\u606f/\\u7f51\\u7edc\\u5b89\\u5168","url":"/forums/ST_Security"},{"name":"\\u4eba\\u5de5\\u667a\\u80fd\\u6280\\u672f","url":"/forums/AI"},{"name":"\\u8d28\\u91cf\\u7ba1\\u7406/\\u8f6f\\u4ef6\\u6d4b\\u8bd5","url":"/forums/SE_Quality"}]},{"name":"\\u591a\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/MediaAndFlash","children":[{"name":"\\u591a\\u5a92\\u4f53/\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/Multimedia"},{"name":"\\u56fe\\u8c61\\u5de5\\u5177\\u4f7f\\u7528","url":"/forums/ImageTools"},{"name":"Flash\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/FlashDevelop"},{"name":"\\u4ea4\\u4e92\\u5f0f\\u8bbe\\u8ba1","url":"/forums/InteractiveDesign"},{"name":"WPF/Silverlight","url":"/forums/Silverlight"},{"name":"Flex","url":"/forums/Flex"}]},{"name":"\\u786c\\u4ef6\\u4f7f\\u7528","url":"/forums/HardwareUse","children":[{"name":"\\u6570\\u7801\\u8bbe\\u5907","url":"/forums/Hardware_Digital"},{"name":"\\u7535\\u8111\\u6574\\u673a\\u53ca\\u914d\\u4ef6","url":"/forums/Hardware_Computer"},{"name":"\\u5916\\u8bbe\\u53ca\\u529e\\u516c\\u8bbe\\u5907","url":"/forums/Hardware_Peripheral"},{"name":"\\u88c5\\u673a\\u4e0e\\u5347\\u7ea7\\u53ca\\u5176\\u4ed6","url":"/forums/Hardware_DIY"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Hardware_NonTechnical"}]},{"name":"\\u4ea7\\u54c1/\\u5382\\u5bb6","url":"/forums/ADS","children":[{"name":"IBM \\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"\\u5fae\\u521b\\u8f6f\\u4ef6\\u5f00\\u53d1\\u7ba1\\u7406","url":"/forums/WeiChuang"},{"name":"\\u5176\\u4ed6","url":"/forums/ADSOther"}]}]},{"name":"\\u57f9\\u8bad\\u8ba4\\u8bc1","url":"/forums/Trainning","children":[{"name":"IT\\u57f9\\u8bad","url":"/forums/ITCertificate"}]},{"name":"\\u7ad9\\u52a1\\u4e13\\u533a","url":"/forums/Support","children":[{"name":"\\u793e\\u533a\\u516c\\u544a","url":"/forums/placard"},{"name":"\\u6d3b\\u52a8\\u4e13\\u533a","url":"/forums/Activity"},{"name":"\\u5ba2\\u670d\\u4e13\\u533a","url":"/forums/Service"},{"name":"\\u7248\\u4e3b\\u4e13\\u533a","url":"/forums/Moderator"},{"name":"\\u300a\\u7a0b\\u5e8f\\u5458\\u300b\\u6742\\u5fd7","url":"/forums/Programmer"}]}],\
    "isLogined": "false",\
    "isModerator": "false",\
    "favoriteForumUrls": [],\
    "lastForumNodes": [{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b ","url":"/forums/DevChallenge2016"},{"name":"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\\u8bba\\u575b","url":"/forums/DE"},{"name":"OpenCV","url":"/forums/OpenCV"},{"name":"FusionInsight HD","url":"/forums/fusioninsightdeveloper"},{"name":"HUAWEI Code Craft","url":"/forums/hwcodecraft"},{"name":"JetBrains\\u6280\\u672f\\u8bba\\u575b","url":"/forums/JetBrains"},{"name":"Enterprise Architect","url":"/forums/EA"}]\
  }'''\
menu = json.loads(menu)\
\
\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'周前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'个月前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'年前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
\
class Handler(BaseHandler):\
    crawl_config = {\
'headers':{\
#'proxy':'123.161.133.18:63574',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
},\
'proxy':'171.38.166.177:8123',\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for each in menu['forumNodes']:\
            if each.has_key('children'):\
                for ea in each['children']:\
                    if ea.has_key('url'):\
                            url = ea['url']\
                            self.crawl('http://bbs.csdn.net'+url+'/closed',headers=self.crawl_config['headers'],proxy=self.crawl_config['proxy'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.title > a').items():\
            _dict = {}\
            _dict['title'] = each.text()\
            self.crawl(each.attr.href, save = _dict,headers=self.crawl_config['headers'],proxy=self.crawl_config['proxy'], callback=self.detail_page)\
        #翻页\
        for each in response.doc('.next').items():\
            self.crawl(each.attr.href, headers=self.crawl_config['headers'],proxy=self.crawl_config['proxy'],callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        #print len(response.doc('table.post.topic'))\
        content_list = []\
        for info in response.doc('.post').items():\
            if 'topic' in info.attr['class']:\
                continue\
            if u'被管理员删除' in info.find('.post_body').remove('fieldset').html().strip():\
                continue\
            _dic = {}\
            #print info.find('.answer_author').text()\
            _dic['name'] = info.find('.username > a').text()\
            _dic['date'] = info.find('.time').text().split()[-2]\
            _dic['content'] = info.find('.post_body').remove('fieldset').html().strip()\
            content_list.append((_dic))\
        \
        if not content_list:\
            return None\
        return {\
            "url": response.url,\
            "title": response.save['title'],\
            #"subject": response.save['subject'],\
            "subject": u'程序员',\
            "answers": content_list,\
            "source": u'csdn',\
            "question_detail": re.sub('<!--.*','',response.doc('.topic > .post_body').remove('*').html()).strip(),\
            "class": 47,\
            "data_weight": 0,\
        }\
$$$$$1471940080.0747
csdn_proxy2$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 10:14:04\
# Project: csdn_proxy2\
\
\
\
from pyspider.libs.base_handler import *\
\
import json,re\
menu = '''{\
    "forumNodes": [{"name":"\\u79fb\\u52a8\\u5f00\\u53d1","url":"/forums/Mobile","children":[{"name":"iOS","url":"/forums/ios"},{"name":"Android","url":"/forums/Android"},{"name":"Swift","url":"/forums/swift"},{"name":"Windows\\u5ba2\\u6237\\u7aef\\u5f00\\u53d1","url":"/forums/WindowsMobile"},{"name":"Symbian","url":"/forums/Symbian"},{"name":"BlackBerry","url":"/forums/BlackBerry"},{"name":"Qt","url":"/forums/Qt"},{"name":"\\u79fb\\u52a8\\u652f\\u4ed8","url":"/forums/PaypalCommunity"},{"name":"\\u79fb\\u52a8\\u5e7f\\u544a","url":"/forums/MobileAD"},{"name":"\\u5fae\\u4fe1\\u5f00\\u53d1","url":"/forums/weixin"},{"name":"\\u79fb\\u52a8\\u5f00\\u53d1\\u5176\\u4ed6\\u95ee\\u9898","url":"/forums/Mobile_Other"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/MobileNonTechnical"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"},{"name":"\\u8054\\u901aWO+\\u5f00\\u653e\\u5e73\\u53f0","url":"/forums/chinaunicom"}]},{"name":"\\u4e91\\u8ba1\\u7b97","url":"/forums/CloudComputing","children":[{"name":"IaaS","children":[{"name":"OpenStack","url":"/forums/OpenStack"}]},{"name":"PaaS/SaaS","children":[{"name":"Cloud Foundry","url":"/forums/CloudFoundry"},{"name":"GAE","url":"/forums/GAE"}]},{"name":"\\u6570\\u636e\\u4e2d\\u5fc3\\u8fd0\\u7ef4","children":[{"name":"\\u670d\\u52a1\\u5668","url":"/forums/server"},{"name":"\\u7f51\\u7edc","url":"/forums/network"},{"name":"\\u865a\\u62df\\u5316","url":"/forums/virtual"}]},{"name":"AWS","url":"/forums/AWS"},{"name":"\\u534e\\u4e3a\\u4e91\\u8ba1\\u7b97","url":"/forums/fusioncloud"},{"name":"\\u5f00\\u653e\\u5e73\\u53f0","url":"/forums/OpenAPI"},{"name":"\\u4e91\\u5b89\\u5168","url":"/forums/ST_Security"},{"name":"\\u5206\\u5e03\\u5f0f\\u8ba1\\u7b97/Hadoop","url":"/forums/hadoop"},{"name":"\\u4e91\\u5b58\\u50a8","url":"/forums/CloudStorage"},{"name":"Docker","url":"/forums/docker"},{"name":"Spark","url":"/forums/spark"},{"name":"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0","url":"/forums/DE"}]},{"name":"\\u4f01\\u4e1aIT","url":"/forums/Enterprise","children":[{"name":"\\u4e2d\\u95f4\\u4ef6","children":[{"name":"\\u4e2d\\u95f4\\u4ef6","url":"/forums/Middleware"},{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"JBoss","url":"/forums/JBoss"}]},{"name":"\\u4f01\\u4e1a\\u7ba1\\u7406\\u8f6f\\u4ef6","children":[{"name":"\\u6d88\\u606f\\u534f\\u4f5c","url":"/forums/ExchangeServer"},{"name":"SharePoint","url":"/forums/SharePoint"}]},{"name":"Atlassian\\u6280\\u672f\\u8bba\\u575b","url":"/forums/atlassian"},{"name":"JetBrains\\u6280\\u672f\\u8bba\\u575b","url":"/forums/JetBrains"},{"name":"\\u5730\\u7406\\u4fe1\\u606f\\u7cfb\\u7edf","url":"/forums/GIS"},{"name":"\\u4f01\\u4e1a\\u4fe1\\u606f\\u5316","url":"/forums/Enterprise_Information"},{"name":"ERP/CRM","url":"/forums/ERP"},{"name":"\\u5176\\u4ed6","url":"/forums/Enterprise_Other"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"},{"name":"Enterprise Architect\\u6280\\u672f\\u8bba\\u575b","url":"/forums/EA"}]},{"name":".NET\\u6280\\u672f","url":"/forums/DotNET","children":[{"name":"C#","url":"/forums/CSharp"},{"name":"ASP.NET","url":"/forums/ASPDotNET"},{"name":".NET Framework","url":"/forums/DotNETFramework"},{"name":"Web Services","url":"/forums/DotNETWebServices"},{"name":"VB.NET","url":"/forums/VBDotNET"},{"name":"VC.NET","url":"/forums/VCDotNet"},{"name":"\\u56fe\\u8868\\u533a","url":"/forums/DotNETReport"},{"name":".NET\\u6280\\u672f\\u524d\\u77bb","url":"/forums/DotNET_NewTech"},{"name":".NET\\u5206\\u6790\\u4e0e\\u8bbe\\u8ba1","url":"/forums/DotNETAnalysisAndDesign"},{"name":"\\u7ec4\\u4ef6/\\u63a7\\u4ef6\\u5f00\\u53d1","url":"/forums/DotNET_Controls"},{"name":"SharePoint","url":"/forums/SharePoint"},{"name":"WPF/Silverlight","url":"/forums/Silverlight"},{"name":"LINQ","url":"/forums/LINQ"},{"name":"VSTS","url":"/forums/VSTS"},{"name":"\\u5176\\u4ed6","url":"/forums/DotNET_Other"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DotNETNonTechnical"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"}]},{"name":"Java \\u6280\\u672f","url":"/forums/Java","children":[{"name":"Java SE","url":"/forums/J2SE"},{"name":"J2ME","url":"/forums/J2ME"},{"name":"Java Web \\u5f00\\u53d1","url":"/forums/Java_WebDevelop"},{"name":"Java EE","url":"/forums/J2EE"},{"name":"Eclipse","url":"/forums/Eclipse"},{"name":"Java\\u5176\\u4ed6\\u76f8\\u5173","url":"/forums/JavaOther"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/JavaNonTechnical"},{"name":"JBoss","url":"/forums/JBoss"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"}]},{"name":"Web \\u5f00\\u53d1","url":"/forums/WebDevelop","children":[{"name":"ASP","url":"/forums/ASP"},{"name":"ASP.NET","url":"/forums/ASPDotNET"},{"name":"JSP","url":"/forums/Java_WebDevelop"},{"name":"PHP","url":"/forums/PHP","children":[{"name":"\\u5f00\\u6e90\\u8d44\\u6e90","url":"/forums/PHPOpenSource"},{"name":"\\u57fa\\u7840\\u7f16\\u7a0b","url":"/forums/PHPBase"},{"name":"Framework","url":"/forums/PHPFramework"}]},{"name":"JavaScript","url":"/forums/JavaScript"},{"name":"\\u641c\\u7d22\\u5f15\\u64ce\\u6280\\u672f","url":"/forums/SearchEngine"},{"name":"Ajax \\u6280\\u672f","url":"/forums/Ajax"},{"name":"VBScript","url":"/forums/vbScript"},{"name":"CGI","url":"/forums/CGI"},{"name":"XML/XSL","url":"/forums/XMLSOAP"},{"name":"IIS","url":"/forums/IIS"},{"name":"Apache","url":"/forums/Apache"},{"name":"HTML(CSS)","url":"/forums/HTMLCSS"},{"name":"ColdFusion","url":"/forums/ColdFusion"},{"name":"Ruby/Rails","url":"/forums/ROR"},{"name":"\\u8de8\\u6d4f\\u89c8\\u5668\\u5f00\\u53d1","url":"/forums/CrossBrowser"},{"name":"\\u5176\\u4ed6","url":"/forums/WebDevelop_Other"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/WebNonTechnical"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"HTML5","url":"/forums/HTML5"}]},{"name":"\\u5f00\\u53d1\\u8bed\\u8a00/\\u6846\\u67b6","children":[{"name":"VC/MFC","url":"/forums/VC","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/VC_Basic"},{"name":"\\u754c\\u9762","url":"/forums/VC_UI"},{"name":"\\u7f51\\u7edc\\u7f16\\u7a0b","url":"/forums/VC_Network"},{"name":"\\u8fdb\\u7a0b/\\u7ebf\\u7a0b/DLL","url":"/forums/VC_Process"},{"name":"ATL/ActiveX/COM","url":"/forums/VC_ActiveX"},{"name":"\\u6570\\u636e\\u5e93","url":"/forums/VC_Database"},{"name":"\\u786c\\u4ef6/\\u7cfb\\u7edf","url":"/forums/VC_Hardware"},{"name":"HTML/XML","url":"/forums/VC_HTML"},{"name":"\\u56fe\\u5f62\\u5904\\u7406/\\u7b97\\u6cd5","url":"/forums/VC_ImageProcessing"},{"name":"\\u8d44\\u6e90","url":"/forums/VCResources"},{"name":"\\u975e\\u6280\\u672f\\u7c7b","url":"/forums/VC_NonTechnical"}]},{"name":"VB","url":"/forums/VB","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/VB_Basic"},{"name":"\\u975e\\u6280\\u672f\\u7c7b","url":"/forums/VB_NonTechnical"},{"name":"\\u63a7\\u4ef6","url":"/forums/VB_Controls"},{"name":"API","url":"/forums/VB_API"},{"name":"\\u6570\\u636e\\u5e93(\\u5305\\u542b\\u6253\\u5370\\uff0c\\u5b89\\u88c5\\uff0c\\u62a5\\u8868)","url":"/forums/VB_Database"},{"name":"\\u591a\\u5a92\\u4f53","url":"/forums/VB_Multimedia"},{"name":"\\u7f51\\u7edc\\u7f16\\u7a0b","url":"/forums/VB_Network"},{"name":"VBA","url":"/forums/VBA"},{"name":"COM/DCOM/COM+","url":"/forums/VB_COM"},{"name":"\\u8d44\\u6e90","url":"/forums/VBResources"}]},{"name":"Delphi","url":"/forums/Delphi","children":[{"name":"VCL\\u7ec4\\u4ef6\\u5f00\\u53d1\\u53ca\\u5e94\\u7528","url":"/forums/DelphiVCL"},{"name":"\\u6570\\u636e\\u5e93\\u76f8\\u5173","url":"/forums/DelphiDB"},{"name":"Windows SDK/API","url":"/forums/DelphiAPI"},{"name":"\\u7f51\\u7edc\\u901a\\u4fe1/\\u5206\\u5e03\\u5f0f\\u5f00\\u53d1","url":"/forums/DelphiNetwork"},{"name":"\\u8bed\\u8a00\\u57fa\\u7840/\\u7b97\\u6cd5/\\u7cfb\\u7edf\\u8bbe\\u8ba1","url":"/forums/DelphiBase"},{"name":"GAME\\uff0c\\u56fe\\u5f62\\u5904\\u7406/\\u591a\\u5a92\\u4f53","url":"/forums/DelphiMultimedia"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DelphiNonTechnical"}]},{"name":"C++ Builder","url":"/forums/BCB","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/BCBBase"},{"name":"\\u6570\\u636e\\u5e93\\u53ca\\u76f8\\u5173\\u6280\\u672f","url":"/forums/BCBDB"},{"name":"VCL\\u7ec4\\u4ef6\\u4f7f\\u7528\\u548c\\u5f00\\u53d1","url":"/forums/BCBVCL"},{"name":"Windows SDK/API","url":"/forums/BCBAPI"},{"name":"\\u7f51\\u7edc\\u53ca\\u901a\\u8baf\\u5f00\\u53d1","url":"/forums/BCBNetwork"},{"name":"ActiveX/COM/DCOM","url":"/forums/BCBCOM"},{"name":"\\u8336\\u9986","url":"/forums/BCBTeaHouses"}]},{"name":"C/C++","url":"/forums/Cpp","children":[{"name":"\\u65b0\\u624b\\u4e50\\u56ed","url":"/forums/Cpp_Freshman"},{"name":"C\\u8bed\\u8a00","url":"/forums/C"},{"name":"C++ \\u8bed\\u8a00","url":"/forums/CPPLanguage"},{"name":"\\u5de5\\u5177\\u5e73\\u53f0\\u548c\\u7a0b\\u5e8f\\u5e93","url":"/forums/Cpp_ToolsPlatform"},{"name":"\\u6a21\\u5f0f\\u53ca\\u5b9e\\u73b0","url":"/forums/Cpp_Model"},{"name":"\\u5176\\u4ed6\\u6280\\u672f\\u95ee\\u9898","url":"/forums/Cpp_Other"},{"name":"Qt","url":"/forums/Qt"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Cpp_NonTechnical"}]},{"name":"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00","url":"/forums/OtherLanguage","open":true,"children":[{"name":"OpenCL\\u548c\\u5f02\\u6784\\u7f16\\u7a0b","url":"/forums/Heterogeneous"},{"name":"Go\\u8bed\\u8a00","url":"/forums/golang"},{"name":"JBoss\\u6280\\u672f\\u4ea4\\u6d41","url":"/forums/JBoss"},{"name":"\\u6c47\\u7f16\\u8bed\\u8a00","url":"/forums/ASM"},{"name":"\\u811a\\u672c\\u8bed\\u8a00\\uff08Perl/Python\\uff09","url":"/forums/OL_Script"},{"name":"Office\\u5f00\\u53d1/ VBA","url":"/forums/OfficeDevelopment"},{"name":"VFP","url":"/forums/VFP"},{"name":"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00","url":"/forums/OtherLanguage_Other"}]}]},{"name":"\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":null,"children":[{"name":"\\u5927\\u6570\\u636e","children":[{"name":"Hadoop","url":"/forums/hadoop"}]},{"name":"MS-SQL Server","url":"/forums/MSSQL","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/MSSQL_Basic"},{"name":"\\u5e94\\u7528\\u5b9e\\u4f8b","url":"/forums/MSSQL_Cases"},{"name":"\\u7591\\u96be\\u95ee\\u9898","url":"/forums/MSSQL_DifficultProblems"},{"name":"\\u65b0\\u6280\\u672f\\u524d\\u6cbf","url":"/forums/MSSQL_NewTech"},{"name":"SQL Server BI","url":"/forums/SQLSERVERBI"},{"name":"\\u975e\\u6280\\u672f\\u7248","url":"/forums/MSSQL_NonTechnical"}]},{"name":"PowerBuilder","url":"/forums/PowerBuilder","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/PB_Basic"},{"name":"Pb\\u811a\\u672c\\u8bed\\u8a00","url":"/forums/PBScript"},{"name":"DataWindow","url":"/forums/PB_DataWindow"},{"name":"API \\u8c03\\u7528","url":"/forums/PB_API"},{"name":"\\u63a7\\u4ef6\\u4e0e\\u754c\\u9762","url":"/forums/PB_Controls"},{"name":"Pb Web \\u5e94\\u7528","url":"/forums/PB_WEB"},{"name":"\\u6570\\u636e\\u5e93\\u76f8\\u5173","url":"/forums/PB_Database"},{"name":"\\u9879\\u76ee\\u7ba1\\u7406","url":"/forums/PB_ProjectManagement"},{"name":"\\u975e\\u6280\\u672f\\u7248","url":"/forums/PB_NonTechnical"}]},{"name":"Oracle","url":"/forums/Oracle","children":[{"name":"\\u57fa\\u7840\\u548c\\u7ba1\\u7406","url":"/forums/Oracle_Management"},{"name":"\\u5f00\\u53d1","url":"/forums/Oracle_Develop"},{"name":"\\u9ad8\\u7ea7\\u6280\\u672f","url":"/forums/Oracle_Technology"},{"name":"\\u8ba4\\u8bc1\\u4e0e\\u8003\\u8bd5","url":"/forums/Oracle_Certificate"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Oracle_NonTechnical"}]},{"name":"Informatica","url":"/forums/Informatica"},{"name":"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/OtherDatabase","open":true,"children":[{"name":"IBM DB2","url":"/forums/DB2"},{"name":"MongoDB","url":"/forums/MongoDB"},{"name":"\\u6570\\u636e\\u4ed3\\u5e93","url":"/forums/DataWarehouse"},{"name":"VFP","url":"/forums/VFP"},{"name":"Access","url":"/forums/Access"},{"name":"Sybase","url":"/forums/Sybase"},{"name":"Informix","url":"/forums/Informix"},{"name":"MySQL","url":"/forums/MySQL"},{"name":"PostgreSQL","url":"/forums/PostgreSQL"},{"name":"\\u6570\\u636e\\u5e93\\u62a5\\u8868","url":"/forums/DatabaseReport"},{"name":"\\u5176\\u4ed6\\u6570\\u636e\\u5e93","url":"/forums/OtherDatabase_Other"},{"name":"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/HPDatabase"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DatabaseNonTechnical"}]}]},{"name":"Linux/Unix\\u793e\\u533a","url":"/forums/Linux","children":[{"name":"\\u7cfb\\u7edf\\u7ef4\\u62a4\\u4e0e\\u4f7f\\u7528\\u533a","url":"/forums/Linux_System"},{"name":"\\u5e94\\u7528\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a","url":"/forums/Linux_Development"},{"name":"\\u5185\\u6838\\u6e90\\u4ee3\\u7801\\u7814\\u7a76\\u533a","url":"/forums/Linux_Kernel"},{"name":"\\u9a71\\u52a8\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a","url":"/forums/Linux_Driver"},{"name":"CPU\\u548c\\u786c\\u4ef6\\u533a","url":"/forums/Linux_Hardware"},{"name":"\\u4e13\\u9898\\u6280\\u672f\\u8ba8\\u8bba\\u533a","url":"/forums/Linux_SpecialTopic"},{"name":"\\u5b9e\\u7528\\u8d44\\u6599\\u53d1\\u5e03\\u533a","url":"/forums/Linux_Information"},{"name":"UNIX\\u6587\\u5316","url":"/forums/Unix_Culture"},{"name":"Solaris","url":"/forums/Solaris"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/LinuxNonTechnical"}]},{"name":"Windows\\u4e13\\u533a","url":"/forums/Windows","children":[{"name":"Windows\\u5ba2\\u6237\\u7aef\\u4f7f\\u7528","url":"/forums/Windows7"},{"name":"Windows Server","url":"/forums/WinNT2000XP2003"},{"name":"\\u7f51\\u7edc\\u7ba1\\u7406\\u4e0e\\u914d\\u7f6e","url":"/forums/NetworkConfiguration"},{"name":"\\u5b89\\u5168\\u6280\\u672f/\\u75c5\\u6bd2","url":"/forums/WindowsSecurity"},{"name":"\\u4e00\\u822c\\u8f6f\\u4ef6\\u4f7f\\u7528","url":"/forums/WindowsBase"},{"name":"Microsoft Office\\u5e94\\u7528","url":"/forums/OfficeBase"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/WindowsNonTechnical"}]},{"name":"\\u786c\\u4ef6/\\u5d4c\\u5165\\u5f00\\u53d1","url":"/forums/Embedded","children":[{"name":"\\u5d4c\\u5165\\u5f00\\u53d1(WinCE)","url":"/forums/WinCE"},{"name":"\\u6c47\\u7f16\\u8bed\\u8a00","url":"/forums/ASM"},{"name":"\\u786c\\u4ef6\\u8bbe\\u8ba1","url":"/forums/Embedded_hardware"},{"name":"\\u9a71\\u52a8\\u5f00\\u53d1/\\u6838\\u5fc3\\u5f00\\u53d1","url":"/forums/Embedded_driver"},{"name":"\\u5355\\u7247\\u673a/\\u5de5\\u63a7","url":"/forums/Embedded_SCM"},{"name":"\\u65e0\\u7ebf","url":"/forums/Embedded_wireless"},{"name":"\\u5176\\u4ed6\\u786c\\u4ef6\\u5f00\\u53d1","url":"/forums/Embedded_Other"},{"name":"VxWorks\\u5f00\\u53d1","url":"/forums/VxWorks"},{"name":"Qt\\u5f00\\u53d1","url":"/forums/Qt"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/EmbeddedNonTechnical"},{"name":"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97","url":"/forums/HPC"},{"name":"\\u667a\\u80fd\\u786c\\u4ef6","url":"/forums/SmartHardware"}]},{"name":"\\u6e38\\u620f\\u5f00\\u53d1","url":"/forums/GameDevelop","children":[{"name":"Cocos2d-x","url":"/forums/GD_Cocos2d-x"},{"name":"Unity3D","url":"/forums/GD_Unity3D"},{"name":"\\u5176\\u4ed6\\u6e38\\u620f\\u5f15\\u64ce","url":"/forums/Othergameengines"},{"name":"\\u6e38\\u620f\\u7b56\\u5212\\u4e0e\\u8fd0\\u8425","url":"/forums/Gdesignoperation"}]},{"name":"\\u7f51\\u7edc\\u4e0e\\u901a\\u4fe1","url":"/forums/network_communication","children":[{"name":"\\u7f51\\u7edc\\u534f\\u8bae\\u4e0e\\u914d\\u7f6e","url":"/forums/IP_Protocolconfiguration"},{"name":"\\u7f51\\u7edc\\u7ef4\\u62a4\\u4e0e\\u7ba1\\u7406","url":"/forums/maintainmanage"},{"name":"\\u4ea4\\u6362\\u53ca\\u8def\\u7531\\u6280\\u672f","url":"/forums/Hardware_SwitchRouter"},{"name":"CDN","url":"/forums/NetworkC_CDN"},{"name":"\\u901a\\u4fe1\\u6280\\u672f","url":"/forums/ST_Network"},{"name":"VOIP\\u6280\\u672f\\u63a2\\u8ba8","url":"/forums/voip"}]},{"name":"\\u6269\\u5145\\u8bdd\\u9898","url":"/forums/Other","children":[{"name":"\\u704c\\u6c34\\u4e50\\u56ed","url":"/forums/FreeZone"},{"name":"\\u7a0b\\u5e8f\\u4eba\\u751f","url":"/forums/ProgrammerStory"},{"name":"\\u7a0b\\u5e8f\\u5a9b\\u4e16\\u754c","url":"/forums/ProgramGirls"},{"name":"\\u7a0b\\u5e8f\\u5458\\u4ea4\\u53cb","url":"/forums/ProgramFriends"},{"name":"\\u4e09\\u5341\\u800c\\u7acb","url":"/forums/30Plus"},{"name":"\\u6e38\\u620f\\u4e13\\u533a","url":"/forums/Game"},{"name":"\\u4e1a\\u754c\\u65b0\\u95fb","url":"/forums/ITnews"},{"name":"\\u7a0b\\u5e8f\\u5458\\u82f1\\u8bed","url":"/forums/English"},{"name":"\\u6c42\\u804c\\u4e0e\\u62db\\u8058","url":"/forums/CAREER"},{"name":"\\u8ba1\\u7b97\\u673a\\u56fe\\u4e66","url":"/forums/Book"},{"name":"\\u5927\\u5b66\\u65f6\\u4ee3","url":"/forums/CollegeTime"},{"name":"\\u8df3\\u86a4\\u5e02\\u573a","url":"/forums/Trade"},{"name":"\\u8f6f\\u4ef6\\u6c42\\u52a9","url":"/forums/Shareware"}]},{"name":"\\u6328\\u8e22\\u804c\\u6daf","url":"/forums/CAREER","children":[{"name":"\\u6c42\\u804c\\u9762\\u8bd5","url":"/forums/WorkplaceCommunication"},{"name":"\\u4f01\\u4e1a\\u70b9\\u8bc4","url":"/forums/TECHHUNT"},{"name":"\\u804c\\u573a\\u8bdd\\u9898","url":"/forums/OFFICELIFE"},{"name":"JOB \\u9a7f\\u7ad9","url":"/forums/jobservice"}]},{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u793e\\u533a","url":"/forums/eSDK","children":[{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b","url":"/forums/DevChallenge2016"},{"name":"\\u4e91\\u8ba1\\u7b97","url":"/forums/hwfsdeveloper"},{"name":"\\u4f01\\u4e1a\\u901a\\u4fe1","url":"/forums/hwucdeveloper"},{"name":"BYOD","url":"/forums/hwbyoddeveloper"},{"name":"\\u5927\\u6570\\u636e","children":[{"name":"FusionInsight HD","url":"/forums/fusioninsightdeveloper"},{"name":"FusionInsight Universe","url":"/forums/hwuniversedeveloper"}]},{"name":"Digital inCloud","url":"/forums/hwswdeveloper"},{"name":"CaaS","url":"/forums/hwcndeveloper"},{"name":"SDN","url":"/forums/hwsdndeveloper"},{"name":"\\u4f01\\u4e1a\\u7f51\\u7edc\\u5f00\\u53d1","url":"/forums/hwendeveloper"},{"name":"\\u654f\\u6377\\u7f51\\u7edc","url":"/forums/hwesightdeveloper"},{"name":"eLTE","url":"/forums/hwbbtdeveloper"},{"name":"\\u7269\\u8054\\u7f51\\u5f00\\u53d1","url":"/forums/hwiotdeveloper"},{"name":"\\u79fb\\u52a8\\u5f00\\u653e\\u5de5\\u573a","url":"/forums/hwwldeveloper"},{"name":"OpenLife\\u667a\\u6167\\u5bb6\\u5ead","url":"/forums/OpenLife"},{"name":"HUAWEI Code Craft","url":"/forums/hwcodecraft"}]},{"name":"IBM \\u6280\\u672f\\u793e\\u533a","children":[{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"DB2","url":"/forums/DB2"},{"name":"Rational","url":"/forums/Rational"},{"name":"Lotus","url":"/forums/Lotus"},{"name":"IBM\\u4e91\\u8ba1\\u7b97","url":"/forums/ibmcloud"},{"name":"IBM \\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"Tivoli","url":"/forums/Tivoli"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"}]},{"name":"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a","children":[{"name":"\\u82f1\\u7279\\u5c14\\u6280\\u672f","url":"/forums/intel"}]},{"name":"Qualcomm\\u5f00\\u53d1\\u8bba\\u575b","children":[{"name":"Qualcomm\\u5f00\\u53d1","url":"/forums/qualcomm"}]},{"name":"\\u4f01\\u4e1a\\u6280\\u672f","children":[{"name":"IBM \\u6280\\u672f\\u793e\\u533a","children":[{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"DB2","url":"/forums/DB2"},{"name":"Rational","url":"/forums/Rational"},{"name":"Lotus","url":"/forums/Lotus"},{"name":"IBM\\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"Tivoli","url":"/forums/Tivoli"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"}]},{"name":"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a","children":[{"name":"\\u82f1\\u7279\\u5c14\\u6280\\u672f","url":"/forums/intel"}]},{"name":"T\\u5ba2\\u8bba\\u575b","url":"/forums/tcl"},{"name":"Paypal\\u5f00\\u53d1\\u8005\\u793e\\u533a","url":"/forums/PaypalCommunity"},{"name":"CUDA","url":"/forums/CUDA","children":[{"name":"CUDA\\u7f16\\u7a0b","url":"/forums/CUDA_Dev"},{"name":"CUDA\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\\u8ba8\\u8bba","url":"/forums/CUDA_Compute"},{"name":"CUDA on Linux","url":"/forums/CUDA_Linux"},{"name":"CUDA on Windows XP","url":"/forums/CUDA_WinXP"}]},{"name":"Google\\u6280\\u672f\\u793e\\u533a","children":[{"name":"Google\\u6280\\u672f\\u793e\\u533a","url":"/forums/GoogleCommunity"},{"name":"Android","url":"/forums/Android"}]},{"name":"Microsoft Office \\u5e94\\u7528\\u4e8e\\u5f00\\u53d1","children":[{"name":"Office\\u5f00\\u53d1","url":"/forums/OfficeDevelopment"},{"name":"Office\\u4f7f\\u7528","url":"/forums/OfficeBase"}]}]},{"name":"\\u5176\\u4ed6\\u6280\\u672f\\u8bba\\u575b","children":[{"name":"\\u8f6f\\u4ef6\\u5de5\\u7a0b/\\u7ba1\\u7406","url":"/forums/SE","children":[{"name":"\\u8f6f\\u4ef6\\u6d4b\\u8bd5","url":"/forums/SE_Quality"},{"name":"\\u7814\\u53d1\\u7ba1\\u7406","url":"/forums/SE_Management"},{"name":"\\u654f\\u6377\\u5f00\\u53d1","url":"/forums/Agile"},{"name":"\\u7248\\u672c\\u63a7\\u5236","url":"/forums/CVS_SVN"},{"name":"\\u8bbe\\u8ba1\\u6a21\\u5f0f","url":"/forums/DesignPatterns"}]},{"name":"\\u9ad8\\u6027\\u80fd\\u5f00\\u53d1","url":"/forums/HPDevelopment","children":[{"name":"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97","url":"/forums/HPC"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/HPDatabase"},{"name":"\\u6d77\\u91cf\\u6570\\u636e\\u5904\\u7406/\\u641c\\u7d22\\u6280\\u672f","url":"/forums/SearchEngine"},{"name":"\\u6570\\u636e\\u7ed3\\u6784\\u4e0e\\u7b97\\u6cd5","url":"/forums/ST_Arithmetic"}]},{"name":"\\u4e13\\u9898\\u5f00\\u53d1/\\u6280\\u672f/\\u9879\\u76ee","url":"/forums/SpecialTopic","children":[{"name":"OpenAPI","url":"/forums/OpenAPI"},{"name":"OpenStack","url":"/forums/OpenStack"},{"name":"\\u673a\\u5668\\u89c6\\u89c9","url":"/forums/ST_Image"},{"name":"OpenCV","url":"/forums/OpenCV"},{"name":"\\u4fe1\\u606f/\\u7f51\\u7edc\\u5b89\\u5168","url":"/forums/ST_Security"},{"name":"\\u4eba\\u5de5\\u667a\\u80fd\\u6280\\u672f","url":"/forums/AI"},{"name":"\\u8d28\\u91cf\\u7ba1\\u7406/\\u8f6f\\u4ef6\\u6d4b\\u8bd5","url":"/forums/SE_Quality"}]},{"name":"\\u591a\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/MediaAndFlash","children":[{"name":"\\u591a\\u5a92\\u4f53/\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/Multimedia"},{"name":"\\u56fe\\u8c61\\u5de5\\u5177\\u4f7f\\u7528","url":"/forums/ImageTools"},{"name":"Flash\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/FlashDevelop"},{"name":"\\u4ea4\\u4e92\\u5f0f\\u8bbe\\u8ba1","url":"/forums/InteractiveDesign"},{"name":"WPF/Silverlight","url":"/forums/Silverlight"},{"name":"Flex","url":"/forums/Flex"}]},{"name":"\\u786c\\u4ef6\\u4f7f\\u7528","url":"/forums/HardwareUse","children":[{"name":"\\u6570\\u7801\\u8bbe\\u5907","url":"/forums/Hardware_Digital"},{"name":"\\u7535\\u8111\\u6574\\u673a\\u53ca\\u914d\\u4ef6","url":"/forums/Hardware_Computer"},{"name":"\\u5916\\u8bbe\\u53ca\\u529e\\u516c\\u8bbe\\u5907","url":"/forums/Hardware_Peripheral"},{"name":"\\u88c5\\u673a\\u4e0e\\u5347\\u7ea7\\u53ca\\u5176\\u4ed6","url":"/forums/Hardware_DIY"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Hardware_NonTechnical"}]},{"name":"\\u4ea7\\u54c1/\\u5382\\u5bb6","url":"/forums/ADS","children":[{"name":"IBM \\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"\\u5fae\\u521b\\u8f6f\\u4ef6\\u5f00\\u53d1\\u7ba1\\u7406","url":"/forums/WeiChuang"},{"name":"\\u5176\\u4ed6","url":"/forums/ADSOther"}]}]},{"name":"\\u57f9\\u8bad\\u8ba4\\u8bc1","url":"/forums/Trainning","children":[{"name":"IT\\u57f9\\u8bad","url":"/forums/ITCertificate"}]},{"name":"\\u7ad9\\u52a1\\u4e13\\u533a","url":"/forums/Support","children":[{"name":"\\u793e\\u533a\\u516c\\u544a","url":"/forums/placard"},{"name":"\\u6d3b\\u52a8\\u4e13\\u533a","url":"/forums/Activity"},{"name":"\\u5ba2\\u670d\\u4e13\\u533a","url":"/forums/Service"},{"name":"\\u7248\\u4e3b\\u4e13\\u533a","url":"/forums/Moderator"},{"name":"\\u300a\\u7a0b\\u5e8f\\u5458\\u300b\\u6742\\u5fd7","url":"/forums/Programmer"}]}],\
    "isLogined": "false",\
    "isModerator": "false",\
    "favoriteForumUrls": [],\
    "lastForumNodes": [{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b ","url":"/forums/DevChallenge2016"},{"name":"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\\u8bba\\u575b","url":"/forums/DE"},{"name":"OpenCV","url":"/forums/OpenCV"},{"name":"FusionInsight HD","url":"/forums/fusioninsightdeveloper"},{"name":"HUAWEI Code Craft","url":"/forums/hwcodecraft"},{"name":"JetBrains\\u6280\\u672f\\u8bba\\u575b","url":"/forums/JetBrains"},{"name":"Enterprise Architect","url":"/forums/EA"}]\
  }'''\
menu = json.loads(menu)\
\
\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'周前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'个月前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'年前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
\
class Handler(BaseHandler):\
    crawl_config = {\
'headers':{\
#'proxy':'123.161.133.18:63574',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
},\
'proxy':'124.88.67.7:843',\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for each in menu['forumNodes']:\
            if each.has_key('children'):\
                for ea in each['children']:\
                    if ea.has_key('url'):\
                            url = ea['url']\
                            self.crawl('http://bbs.csdn.net'+url+'/closed',headers=self.crawl_config['headers'],proxy=self.crawl_config['proxy'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.title > a').items():\
            _dict = {}\
            _dict['title'] = each.text()\
            self.crawl(each.attr.href, save = _dict,headers=self.crawl_config['headers'],proxy=self.crawl_config['proxy'], callback=self.detail_page)\
        #翻页\
        for each in response.doc('.next').items():\
            self.crawl(each.attr.href, headers=self.crawl_config['headers'],proxy=self.crawl_config['proxy'],callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        #print len(response.doc('table.post.topic'))\
        content_list = []\
        for info in response.doc('.post').items():\
            if 'topic' in info.attr['class']:\
                continue\
            if u'被管理员删除' in info.find('.post_body').remove('fieldset').html().strip():\
                continue\
            _dic = {}\
            #print info.find('.answer_author').text()\
            _dic['name'] = info.find('.username > a').text()\
            _dic['date'] = info.find('.time').text().split()[-2]\
            _dic['content'] = info.find('.post_body').remove('fieldset').html().strip()\
            content_list.append((_dic))\
        \
        if not content_list:\
            return None\
        return {\
            "url": response.url,\
            "title": response.save['title'],\
            #"subject": response.save['subject'],\
            "subject": u'程序员',\
            "answers": content_list,\
            "source": u'csdn',\
            "question_detail": re.sub('<!--.*','',response.doc('.topic > .post_body').remove('*').html()).strip(),\
            "class": 47,\
            "data_weight": 0,\
        }\
$$$$$1472035810.0612
cxy_openkfjyk$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-19 16:20:13\
# Project: cxy_openkfjyk\
\
from pyspider.libs.base_handler import *\
import sys\
import traceback\
from pyquery import PyQuery\
reload(sys)\
sys.setdefaultencoding("utf-8")\
cxy_dict = {\
    u'软件开发':[u'软件设计'],\
    u'开发语言与工具':[u'编程语言'],\
    u'网站系统':[u'软件设计'],\
    u'企业应用':[u'软件设计'],\
    u'服务器软件':[u'软件设计'],\
    u'插件和扩展':[u'软件设计'],\
    u'数据库相关':[u'数据库技术'],\
    u'操作系统':[u'操作系统'],\
    u'软件开发管理':[u'软件设计'],\
    u'管理和监控':[u'软件设计'],\
    u'应用工具':[u'软件设计'],\
    u'游戏相关':[u'软件设计'],\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.open-open.com/lib/list/all', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('h4 > a').items():\
            _dict = {}\
            _dict['bread'] = cxy_dict[each.text()]\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
    \
    def list_page(self, response):\
        for each in response.doc('a > h3').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.text()\
            #print each.parent().outerHtml()\
            self.crawl(each.parent().attr.href, save = _dict, callback=self.detail_page)\
        #翻页    \
        for each in response.doc('.next').items():\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content = response.doc('article').html().strip()\
        if not content:\
            return\
        #print content\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread": response.save.get("bread"),\
            "content": content,\
            "source": u"open开发经验库",\
            "data_weight": 0,\
            "class": 33,\
            "subject": u"程序员",\
            "date": response.doc('.meta > .item').text().split()[0].strip(),\
\
        }$$$$$1471860369.2068
dead_link_check$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-14 10:30:47\
# Project: dead_link_check\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.genshuixue.com/bj/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
        }\
$$$$$1464142626.9024
dongman_missevan$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-18 15:50:19\
# Project: dongman_missevan\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as P\
import re\
import json\
import cPickle\
import time\
\
max_pageno = 5\
class Parser(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".newslist"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret:\
                a = P(url_node).find(".newstitle").find("a")\
                ret.append(P(a))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        page_node = P(response.doc(".pagelist"))\
        try:\
            next_page = page_node.find(".next")\
            if next_page:\
                current_pageno = page_node.find(".selected a").text()\
                next_pageno = re.findall(r"p=(\\d+)", next_page.find("a").attr.href)\
                if not next_pageno:\
                    return False\
                next_pageno = next_pageno[0]\
                    \
                if int(next_pageno) > int(current_pageno):\
                    # 限制只抓前20页\
                    #if int(next_pageno) > max_pageno:\
                    #    return False\
                    return P('''<a href="%s">next</a>''' % next_page.find("a").attr.href)\
                return False\
\
            else:\
                return False\
        except Exception as info:\
            return False\
    \
    @staticmethod\
    def parse_detail(response):\
        ret = {}\
        try:\
            content_node = response.doc("#articlebox")\
            if not content_node:\
                return False\
            title = P(content_node).find("#articletitle").text()\
            \
            detail = P(content_node).find("#articlecontent").html().strip()\
            \
            article_info = P(content_node).find("#articleinfo").text()\
            info = re.findall(u"来源:\\s+([^\\s]+?)\\s+时间:\\s+(\\d{4}-\\d{2}-\\d{2})", article_info)\
            source = False\
            date_string = time.strftime("%Y-%m-%d", time.localtime(time.time()))\
            if info:\
                info = info[0]\
                source = info[0]\
                date_string = info[1]\
\
            bread = response.save.get("bread")\
        except:\
            return False\
                \
        url = response.url\
        if not detail.strip():\
            return False\
        \
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
        return {"title": title,\
                "url": url,\
                "content": detail,\
                "bread": bread,\
                "source": source if source else u"news.missevan.com",\
                "data_weight": 0,\
                "class": 46,\
                "subject": u"动漫",\
                "date": date_string,\
                "image_list": image_list,\
                }\
\
class Processor(object):\
    @staticmethod\
    def list_processor(spider_handle, parser, response):\
        # 解析列表页\
        list_result = parser.parse_list(response)\
        if list_result:\
            for item in list_result:\
                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\
\
        # 是否有翻页\
        next_page = parser.parse_nextpage(response)\
        if next_page:\
            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\
        \
    @staticmethod\
    def detail_processor(spider_handle, parser, response):\
        return parser.parse_detail(response)\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    crawler_parser = {\
        "1": {\
            "processor": Processor,\
            "parser": Parser,\
        }\
    }\
    crawler_list = [\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=2",\
            "bread": ["动画"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=3",\
            "bread": ["音乐"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=4",\
            "bread": ["游戏"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=5",\
            "bread": ["声优"],\
        },\
\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=6",\
            "bread": ["小说"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=7",\
            "bread": ["漫画"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=8",\
            "bread": ["Cosplay"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=9",\
            "bread": ["杂志"],\
        },\
\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=10",\
            "bread": ["周边"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=11",\
            "bread": ["展会"],\
        },\
\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=12",\
            "bread": ["电影"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=13",\
            "bread": ["萌宅"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=14",\
            "bread": ["杂谈"],\
        },\
\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=15",\
            "bread": ["DVD/BD"],\
        },\
        {   \
            "key": "1",\
            "url": "http://news.missevan.com/news/index?ntype=16",\
            "bread": ["其他"],\
        },\
        \
    ]\
    \
    @every(minutes=1 * 60)\
    def on_start(self):\
        for crawler in self.crawler_list:\
            self.crawl(crawler["url"],\
                       save = {\
                           'bread': crawler.get("bread", []),\
                           '__parser_key__': crawler.get("key"),\
                           'base_url': crawler["url"],\
                       },\
                       callback = self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        crawler_parser_dict.get("processor").list_processor(self, crawler_parser_dict.get("parser"), response)\
\
    #@config(priority=2)\
    @config(age=1)\
    def detail_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        return crawler_parser_dict.get("processor").detail_processor(self, crawler_parser_dict.get("parser"), response)\
\
$$$$$1471330340.9388
dongman_qiongkong$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-11 17:15:33\
# Project: dongman\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://qingkong.net/anime/dmzx/', callback=self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        for each in response.doc('.nLink').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
        #for each in response.doc('.p_current > a').items():\
        #    self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        data = response.doc('.zxtitle td').text()\
        source = data.split(u'：')[1].split('[')[0].strip()\
        date = data.split('[')[-1].split(']')[0]\
        content = response.doc('#content > div > div').html()\
        if not content:\
            content = response.doc('#content').html()\
        if not content:\
            return None\
        return {\
            "url": response.url,\
            "title": response.doc('.hzxnr').text(),\
            'content': content,\
            "bread": [u'动漫资讯'],\
            "source": source if source else u"qingkong.net",\
            "data_weight": 0,\
            "class": 46,\
            "subject": u"动漫",\
            "date": date,\
        }\
$$$$$1471330343.5662
dota_guanwang_news$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-11 12:14:31\
# Project: dota_news\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as P\
import re\
import json\
import time\
\
def full_img_url(url):\
    if url.startswith("http"):\
        return url\
    \
    return "/".join(["http://www.dota2.com.cn", url.strip("/")])\
\
max_pageno = 3\
class Parser(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".hd_list"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find(".hd_li dl dt a"):\
                ret.append(P(url_node))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        next_page = P(response.doc(".hd_list .page"))\
        next_page = next_page.find("span").next()\
        if next_page:\
            current_page = re.findall(r"index(\\d+)\\.htm", response.url)\
            cpo = "1"\
            if current_page:\
                cpo = current_page[0]\
            npo = re.findall(r"index(\\d+)\\.htm", next_page.attr.href)\
            if npo:\
                npo = npo[0]\
            else:\
                return False\
            if int(cpo) < int(npo):\
                if int(npo) > max_pageno:\
                    return False\
                return P(next_page)\
            else:\
                return False\
        return False\
    \
    @staticmethod\
    def parse_detail(spider_handle, parser, response):\
        url = response.url\
        if not url.startswith("http://www.dota2.com.cn/"):\
            return False\
        detail_node = response.doc(".newsart")\
        title = P(detail_node).find(".art_title h1").text()\
        date_string = re.findall(r"\\d{4}-\\d{2}-\\d{2}", P(detail_node).find(".art_title h3").text())\
        if date_string:\
            date_string = date_string[0]\
        else:\
            time.strftime("%Y-%m-%d", time.localtime(time.time()))\
            \
        detail = P(detail_node).find(".font_style").html().strip()\
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
        if image_list:\
            image_list = [full_img_url(img) for img in image_list]\
            \
        return {\
                "url": url,\
                "title": title,\
                "content": detail,\
                "bread": response.save.get("bread"),\
                "image_list": image_list,\
                "source": "www.dota2.com.cn",\
                "class": 33,\
                "date": date_string,\
                "subject": u"刀塔",\
                "data_weight": 0,\
                }\
    \
class Parser2(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".newlist"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find(".ullist li .imgpic a"):\
                ret.append(P(url_node))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        next_page = P(response.doc(".newlist .page"))\
        next_page = next_page.find("span").next()\
        if next_page:\
            current_page = re.findall(r"index(\\d+)\\.htm", response.url)\
            cpo = "1"\
            if current_page:\
                cpo = current_page[0]\
            npo = re.findall(r"index(\\d+)\\.htm", next_page.attr.href)\
            if npo:\
                npo = npo[0]\
            else:\
                return False\
            if int(cpo) < int(npo):\
                if int(npo) > max_pageno:\
                    return False\
                return P(next_page)\
            else:\
                return False\
        return False\
    \
    @staticmethod\
    def parse_detail(spider_handle, parser, response):\
        url = response.url\
        if not url.startswith("http://www.dota2.com.cn/"):\
            return False\
        detail_node = response.doc(".newsart")\
        title = P(detail_node).find(".art_title h1").text()\
        date_string = re.findall(r"\\d{4}-\\d{2}-\\d{2}", P(detail_node).find(".art_title h3").text())\
        if date_string:\
            date_string = date_string[0]\
        else:\
            time.strftime("%Y-%m-%d", time.localtime(time.time()))\
            \
        detail = P(detail_node).find(".font_style").html().strip()\
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
        if image_list:\
            image_list = [full_img_url(img) for img in image_list]\
            \
        return {\
                "url": url,\
                "title": title,\
                "content": detail,\
                "bread": response.save.get("bread"),\
                "image_list": image_list,\
                "source": "www.dota2.com.cn",\
                "class": 33,\
                "date": date_string,\
                "subject": u"刀塔",\
                "data_weight": 0,\
                }\
\
class Processor(object):\
    @staticmethod\
    def list_processor(spider_handle, parser, response):\
        # 解析列表页\
        list_result = parser.parse_list(response)\
        if list_result:\
            for item in list_result:\
                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\
\
        # 是否有翻页\
        next_page = parser.parse_nextpage(response)\
        if next_page:\
            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\
        \
    @staticmethod\
    def detail_processor(spider_handle, parser, response):\
        return parser.parse_detail(spider_handle, parser, response)\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    crawler_parser = {\
        "1": {\
            "processor": Processor,\
            "parser": Parser,\
        },\
        "2": {\
            "processor": Processor,\
            "parser": Parser2,\
        }\
    }\
    crawler_list = [\
        {   \
            "key": "1",\
            "url": "http://www.dota2.com.cn/raiders/index.htm",\
            "bread": ["dota攻略"],\
        },\
        {   \
            "key": "2",\
            "url": "http://www.dota2.com.cn/news/index.htm",\
            "bread": ["dota新闻"],\
        },\
    ]\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        for crawler in self.crawler_list:\
            self.crawl(crawler["url"],\
                       save = {\
                           'bread': crawler.get("bread", []),\
                           '__parser_key__': crawler.get("key"),\
                           'base_url': crawler["url"],\
                       },\
                       callback = self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        crawler_parser_dict.get("processor").list_processor(self, crawler_parser_dict.get("parser"), response)\
\
    #@config(priority=2)\
    @config(age=1)\
    def detail_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        return crawler_parser_dict.get("processor").detail_processor(self, crawler_parser_dict.get("parser"), response)\
\
$$$$$1466562398.9171
dota_replays_news$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-11 12:14:31\
# Project: dota_news\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as P\
import re\
import json\
import time\
\
max_pageno = 3\
class Parser(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".newslist"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find(".newstitle a"):\
                ret.append(P(url_node))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        next_page = P(response.doc(".nweslist_Page .pagination"))\
        next_page = next_page.find(".act").next().find("a")\
        if next_page:\
            try:\
                npo = int(next_page.text().strip())\
                if npo > max_pageno:\
                    return False\
                return P(next_page)\
            except:\
                return False\
        return False\
    \
    @staticmethod\
    def parse_detail(spider_handle, parser, response):\
        url = response.url\
        if url.find("replays.net") == -1:\
            return False\
        \
        detail_node = response.doc(".News-content")\
        title = P(detail_node).find(".News-top h2").text()\
        \
        date_string = re.findall(ur"\\d{4}-\\d{2}-\\d{2}", P(detail_node).find(".News-address").text())\
        if date_string:\
            date_string = date_string[0]\
        else:\
            date_string = time.strftime("%Y-%m-%d", time.localtime(time.time()))\
            \
        detail = P(detail_node).find(".News-main-table").html().strip().replace("&#13;", "")\
        if len(detail) < 50:\
            return False\
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
            \
        return {\
                "url": url,\
                "title": title,\
                "content": detail,\
                "bread": response.save.get("bread"),\
                "image_list": image_list,\
                "source": "replays.net",\
                "class": 33,\
                "date": date_string,\
                "subject": u"刀塔",\
                "data_weight": 0,\
                }\
\
class Processor(object):\
    @staticmethod\
    def list_processor(spider_handle, parser, response):\
        # 解析列表页\
        list_result = parser.parse_list(response)\
        if list_result:\
            for item in list_result:\
                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\
\
        # 是否有翻页\
        next_page = parser.parse_nextpage(response)\
        if next_page:\
            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\
        \
    @staticmethod\
    def detail_processor(spider_handle, parser, response):\
        return parser.parse_detail(spider_handle, parser, response)\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    crawler_parser = {\
        "1": {\
            "processor": Processor,\
            "parser": Parser,\
        }\
    }\
    crawler_list = [\
        {   \
            "key": "1",\
            "url": "http://dota2.replays.net/news/list_3.html",\
            "bread": ["dota资讯"],\
        },\
    ]\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        for crawler in self.crawler_list:\
            self.crawl(crawler["url"],\
                       save = {\
                           'bread': crawler.get("bread", []),\
                           '__parser_key__': crawler.get("key"),\
                           'base_url': crawler["url"],\
                       },\
                       callback = self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        crawler_parser_dict.get("processor").list_processor(self, crawler_parser_dict.get("parser"), response)\
\
    #@config(priority=2)\
    @config(age=1)\
    def detail_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        return crawler_parser_dict.get("processor").detail_processor(self, crawler_parser_dict.get("parser"), response)\
\
$$$$$1466507148.1636
dota_tgbus_news$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-11 12:14:31\
# Project: dota_news\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as P\
import re\
import json\
import time\
\
max_pageno = 3\
class Parser(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".list"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find(".list_tit p a"):\
                db_id = re.findall(r"(\\d+)\\.shtml\$", P(url_node).attr.href)\
                if db_id:\
                    db_id = db_id[0]\
                    ret.append(P('''<a href="http://app.tgbus.com/saver/read.aspx?type=olgame&id=%s"></a>''' % db_id))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        next_page = P(response.doc(".page"))\
        next_page = next_page.find("font").next()\
        if next_page:\
            try:\
                npo = int(next_page.text().strip())\
                if npo > max_pageno:\
                    return False\
                return P(next_page)\
            except:\
                return False\
        return False\
    \
    @staticmethod\
    def parse_detail(spider_handle, parser, response):\
        url = response.url\
        if url.find("tgbus.com") == -1:\
            return False\
        \
        detail_node = response.doc(".text")\
        title = P(detail_node).find("h1").text()\
        \
        date_string = re.findall(ur"(\\d{4})年(\\d{1,2})月(\\d{1,2})日", P(detail_node).find("h2").text())\
        if date_string:\
            date_string = date_string[0]\
            year = date_string[0]\
            month = date_string[1]\
            day = date_string[2]\
            if len(month) == 1:\
                month = "0" + month\
            if len(day) == 1:\
                day = "0" + day    \
            date_string = "-".join([year, month, day])\
        else:\
            date_string = time.strftime("%Y-%m-%d", time.localtime(time.time()))\
            \
        detail = P(detail_node).find("#ct").html().strip().replace("&#13;", "")\
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
            \
        return {\
                "url": url,\
                "title": title,\
                "content": detail,\
                "bread": response.save.get("bread"),\
                "image_list": image_list,\
                "source": "tgbus.com",\
                "class": 33,\
                "date": date_string,\
                "subject": u"刀塔",\
                "data_weight": 0,\
                }\
    \
class Parser2(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".indexlist"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find("tr td a"):\
                db_id = re.findall(r"(\\d+)\\.shtml\$", P(url_node).attr.href)\
                if db_id:\
                    db_id = db_id[0]\
                    ret.append(P('''<a href="http://app.tgbus.com/saver/read.aspx?type=olgame&id=%s"></a>''' % db_id))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        next_page = P(response.doc(".showpage"))\
        next_page = next_page.find("font").next()\
        if next_page:\
            try:\
                npo = int(next_page.text().strip())\
                if npo > max_pageno:\
                    return False\
                return P(next_page)\
            except:\
                return False\
        return False\
    \
    @staticmethod\
    def parse_detail(spider_handle, parser, response):\
        url = response.url\
        if url.find("tgbus.com") == -1:\
            return False\
        \
        detail_node = response.doc(".text")\
        title = P(detail_node).find("h1").text()\
        \
        date_string = re.findall(ur"(\\d{4})年(\\d{1,2})月(\\d{1,2})日", P(detail_node).find("h2").text())\
        if date_string:\
            date_string = date_string[0]\
            year = date_string[0]\
            month = date_string[1]\
            day = date_string[2]\
            if len(month) == 1:\
                month = "0" + month\
            if len(day) == 1:\
                day = "0" + day    \
            date_string = "-".join([year, month, day])\
        else:\
            date_string = time.strftime("%Y-%m-%d", time.localtime(time.time()))\
            \
        detail = P(detail_node).find("#ct").html().strip().replace("&#13;", "")\
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
            \
        return {\
                "url": url,\
                "title": title,\
                "content": detail,\
                "bread": response.save.get("bread"),\
                "image_list": image_list,\
                "source": "tgbus.com",\
                "class": 33,\
                "date": date_string,\
                "subject": u"刀塔",\
                "data_weight": 0,\
                }\
    \
class Parser3(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".heroes"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find("a"):\
                ret.append(P(url_node))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        return False\
    \
    @staticmethod\
    def parse_detail(spider_handle, parser, response):\
        url = response.url\
        if url.find("tgbus.com") == -1:\
            return False\
       \
        title = response.doc("title").text().split("-")[0]\
        date_string = time.strftime("%Y-%m-%d", time.localtime(time.time()))\
            \
        detail_node = response.doc(".contentd")\
        detail_node.find(".rightContent").remove()\
        detail_node.find(".cl").remove()\
        detail_node.find(".overall").remove()\
        detail = detail_node.html().strip().replace("&#13;", "")\
        detail = re.sub(r"display\\s*:\\s*none", "", detail)\
        \
        \
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
        for img in image_list:\
            if img.startswith("http:"):\
                continue\
            detail = detail.replace(img, "/".join([url.strip("/"), img.strip("/")]))\
            \
        return {\
                "url": url,\
                "title": title,\
                "content": detail,\
                "bread": response.save.get("bread"),\
                "image_list": [],\
                "source": "tgbus.com",\
                "class": 33,\
                "date": date_string,\
                "subject": u"刀塔",\
                "data_weight": 0,\
                }\
\
class Processor(object):\
    @staticmethod\
    def list_processor(spider_handle, parser, response):\
        # 解析列表页\
        list_result = parser.parse_list(response)\
        if list_result:\
            for item in list_result:\
                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\
\
        # 是否有翻页\
        next_page = parser.parse_nextpage(response)\
        if next_page:\
            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\
        \
    @staticmethod\
    def detail_processor(spider_handle, parser, response):\
        return parser.parse_detail(spider_handle, parser, response)\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    crawler_parser = {\
        "1": {\
            "processor": Processor,\
            "parser": Parser,\
        },\
        "2": {\
            "processor": Processor,\
            "parser": Parser2,\
        },\
        "3": {\
            "processor": Processor,\
            "parser": Parser3,\
        }\
    }\
    crawler_list = [\
        {   \
            "key": "1",\
            "url": "http://dota2.tgbus.com/strategy/",\
            "bread": ["dota资讯"],\
        },\
        {   \
            "key": "2",\
            "url": "http://dota2.tgbus.com/news/",\
            "bread": ["dota资讯"],\
        },\
        {   \
            "key": "2",\
            "url": "http://dota2.tgbus.com/wenda/",\
            "bread": ["dota资讯"],\
        },\
        {   \
            "key": "3",\
            "url": "http://dota2.tgbus.com/heroes/",\
            "bread": ["dota英雄", "出装加点"],\
        },\
    ]\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        for crawler in self.crawler_list:\
            self.crawl(crawler["url"],\
                       save = {\
                           'bread': crawler.get("bread", []),\
                           '__parser_key__': crawler.get("key"),\
                           'base_url': crawler["url"],\
                       },\
                       callback = self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        crawler_parser_dict.get("processor").list_processor(self, crawler_parser_dict.get("parser"), response)\
\
    #@config(priority=2)\
    @config(age=1)\
    def detail_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        return crawler_parser_dict.get("processor").detail_processor(self, crawler_parser_dict.get("parser"), response)\
\
$$$$$1466507144.5948
eoffcn_gongwuyuan$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-29 10:07:32\
# Project: eoffcn_tiku\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.eoffcn.com/gwy/stixz/xingce/zhenti/',save = {'bread':[u'行测']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/xingce/moniti/',save = {'bread':[u'行测']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/xingce/lxt/',save = {'bread':[u'行测']}, callback=self.index_page)\
        \
        self.crawl('http://www.eoffcn.com/gwy/stixz/shenlun/zhenti/',save = {'bread':[u'申论']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/shenlun/moniti/',save = {'bread':[u'申论']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/shenlun/lxt/',save = {'bread':[u'申论']}, callback=self.index_page)\
        \
        \
        self.crawl('http://www.eoffcn.com/gwy/stixz/ms/zhenti/',save = {'bread':[u'面试']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/ms/moniti/',save = {'bread':[u'面试']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/ms/lxt/',save = {'bread':[u'面试']}, callback=self.index_page)\
        \
        \
        \
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.xm_gwy_l_mainbottom li').items():\
            _dict = {}\
            url = each.find('a').attr.href\
            _dict['title'] = each.find('a').text().replace('中公','').replace('网校','')\
            _dict['date'] = each.find('span').text()\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(url, save =_dict,callback=self.detail_page)\
        for each in response.doc('#pages > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save =_dict,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       res_dict['subject'] = u'公务员'\
       res_dict['class'] = 46\
       res_dict['data_weight'] = 0\
       res_dict['source'] = 'eoffcn.com'\
       res_dict['url'] = response.url\
       res_dict['tdk_title'] = u'跟谁学 '+response.doc('title').text().replace('中公','').replace('网校','')\
       res_dict['tdk_desc'] = response.doc('meta[name="description"]').attr.content.replace('中公','').replace('网校','')\
       res_dict['tdk_keywords'] = response.doc('meta[name="keywords"]').attr.content.replace('中公','').replace('网校','')\
       content_list = []\
       for each in response.doc('.main_l_cont > p').items():\
            if each and u'相关推荐' in each.text():\
                continue\
            if each and u'中公' in each.text():\
                continue\
            if each and u'责任编辑' in each.text():\
                break\
            if each:\
                content_list.append(each.remove('a').html())\
            pass\
       if not content_list:\
            return\
       res_dict['content'] = ''.join(['<p>%s</p>'%(s) for s in content_list if s])\
       return res_dict\
$$$$$1470815710.7088
eoffcn_gongwuyuan_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-29 10:07:32\
# Project: eoffcn_tiku\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.eoffcn.com/gwy/stixz/xingce/zhenti/',save = {'bread':[u'行测']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/xingce/moniti/',save = {'bread':[u'行测']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/xingce/lxt/',save = {'bread':[u'行测']}, callback=self.index_page)\
        \
        self.crawl('http://www.eoffcn.com/gwy/stixz/shenlun/zhenti/',save = {'bread':[u'申论']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/shenlun/moniti/',save = {'bread':[u'申论']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/shenlun/lxt/',save = {'bread':[u'申论']}, callback=self.index_page)\
        \
        \
        self.crawl('http://www.eoffcn.com/gwy/stixz/ms/zhenti/',save = {'bread':[u'面试']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/ms/moniti/',save = {'bread':[u'面试']}, callback=self.index_page)\
        self.crawl('http://www.eoffcn.com/gwy/stixz/ms/lxt/',save = {'bread':[u'面试']}, callback=self.index_page)\
        \
        \
        \
\
    @config(age=1)\
    def index_page(self, response):\
        for each in response.doc('.xm_gwy_l_mainbottom li').items():\
            _dict = {}\
            url = each.find('a').attr.href\
            _dict['title'] = each.find('a').text().replace('中公','').replace('网校','')\
            _dict['date'] = each.find('span').text()\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(url, save =_dict,callback=self.detail_page)\
        #for each in response.doc('#pages > a').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread')\
            #self.crawl(each.attr.href, save =_dict,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       res_dict['subject'] = u'公务员'\
       res_dict['class'] = 46\
       res_dict['data_weight'] = 0\
       res_dict['source'] = 'eoffcn.com'\
       res_dict['url'] = response.url\
       res_dict['tdk_title'] = u'跟谁学 '+response.doc('title').text().replace('中公','').replace('网校','')\
       res_dict['tdk_desc'] = response.doc('meta[name="description"]').attr.content.replace('中公','').replace('网校','')\
       res_dict['tdk_keywords'] = response.doc('meta[name="keywords"]').attr.content.replace('中公','').replace('网校','')\
       content_list = []\
       for each in response.doc('.main_l_cont > p').items():\
            if each and u'相关推荐' in each.text():\
                continue\
            if each and u'中公' in each.text():\
                continue\
            if each and u'责任编辑' in each.text():\
                break\
            if each:\
                content_list.append(each.remove('a').html())\
            pass\
       if not content_list:\
            return\
       res_dict['content'] = ''.join(['<p>%s</p>'%(s) for s in content_list if s])\
       return res_dict\
$$$$$1471571703.7546
eol_gaozhong$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-13 14:04:45\
# Project: eol_gaozhong\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import requests\
from pyquery import PyQuery as pq\
\
\
\
def getdistrict(s):\
    if u'地区：' in s.text():\
        return True\
    else:\
        return False\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.2',\
        'headers':{\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
\
        }\
    }\
\
    set_dict = {\
        u'学校类型':'school_type',\
        u'电话':'phone',\
        u'校址':'address',\
        u'网址':'site',\
\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for index in range(332):\
            self.crawl('http://haogaozhong.eol.cn/school_area.php?page=%s'%(index+1), callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('div.mar_t_30').items():\
            _dict = {}\
            arr =  each.find('.w_360 table td').eq(0).text().replace('&nbsp',' ').split()\
            _dict['province'] = arr[0]\
            _dict['city'] = arr[1].replace('市','')\
            if  _dict['province'] == u'北京' or  _dict['province'] == u'上海' or  _dict['province'] == u'天津' or  _dict['province'] == u'重庆':\
                _dict['city'] = arr[2]\
            url = each.find('.img_160 p a').attr.href\
            self.crawl(url,save = _dict, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       district =  filter(getdistrict,response.doc('.w_460 .font_14 td').items())[0].text()\
       res_dict['district'] = district.split('：')[-1].strip() if district!=u'地区：' else ''\
       school_type =  response.doc('div.w_460 table td').eq(0).text().split('：')\
       phone =  response.doc('div.w_460 table td').eq(2).text().split('：')\
       address = response.doc('div.w_460 table td').eq(4).text().split('：')\
       site = response.doc('div.w_460 table td').eq(5).text().split('：')\
       if school_type[0] in self.set_dict:\
           res_dict[self.set_dict[school_type[0]]] = school_type[1]\
       if phone[0] in self.set_dict:\
           res_dict[self.set_dict[phone[0]]] = phone[1]\
       if address[0] in self.set_dict:\
           res_dict[self.set_dict[address[0]]] = address[1]\
       if site[0] in self.set_dict:\
           res_dict[self.set_dict[site[0]]] = site[1]\
       res_dict['school_url'] = response.url \
       res_dict['school_name'] = response.doc('div.w_260 h2 a').text()\
       #res_dict['enrol_plan'] = response.doc('div.mar_t_20 > table.t_table').html()\
       res_dict['school_imgs'] = []\
       for each in response.doc('ul.slider li img').items():\
                     res_dict['school_imgs'].append(each.attr.src)\
       baseUrl = response.url.replace('index.html','')\
       resp = requests.get(baseUrl+'intro.html')\
       res_dict['school_summary'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.mar_t_20').eq(3).html()\
       resp = requests.get(baseUrl+'gaokao.html')                     \
       res_dict['gaokao_socre'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div#gaokao').html()\
       #res_dict['enrol_socre'] = pq(requests.get(baseUrl+'score.html').text).find('div#gaokao').html()\
       resp = requests.get(baseUrl+'plan.html')                    \
       res_dict['enrol_plan'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.mar_t_10').eq(1).html()\
       resp = requests.get(baseUrl+'teacher.html')  \
       res_dict['faculty'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.mar_t_20').eq(3).html()\
       resp = requests.get(baseUrl+'honor.html')  \
       res_dict['academic_achievement'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.mar_t_10').eq(1).html()    \
       resp = requests.get(baseUrl+'tese.html')  \
       res_dict['feature_course'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.mar_t_10').eq(1).html() \
       resp = requests.get(baseUrl+'xiaoyou.html')  \
       res_dict['xiaoyou'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.pad_l_20').html() \
       resp = requests.get(baseUrl+'chuguo.html')  \
       #print resp.encoding\
       res_dict['chuguo'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.mar_t_10').eq(1).html()\
       return res_dict\
$$$$$1469434545.1995
erge_sohu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-14 16:21:18\
# Project: guitar_china\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        _list = []\
        with open ('/apps/home/rd/xuzhihao/sohu_url2') as f:\
            for line in f:\
                _list.append(line.strip('\\n'))\
        for line in _list:\
            self.crawl(line, callback=self.detail_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('td td td td td a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
        '''\
        for each in response.doc('.normalfont > a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
        '''\
    @config(priority=2)\
    def detail_page(self, response):\
        #content = ''.join(['<p>%s</p>'%v.html() for v in response.doc('p').items()])\
        content = response.doc('#contentText').html()\
        title = response.doc('h1').text()\
        if u'儿歌' not in title and u'儿歌' not in content:\
            return None\
        \
        return {\
            "url": response.url,\
            "subject": u'儿歌',\
            "source": response.doc('.writer > a').text(),\
            "content": content,\
            "title": title,\
            "date": response.doc('[itemprop="datePublished"]').text()[:10],\
            "class": 46,\
            "bread": response.doc('.tag > a').text().split(),\
            "data_weight": 0,\
        }\
$$$$$1471341414.3742
gangqin_new_163_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-25 15:04:32\
# Project: gangqin_new_163_inc\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://edu.163.com/keywords/9/a/94a27434/1.html', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.titleBar > h3 > a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content = response.doc('.post_text').remove('.ep-source').html()\
        if not content:\
            content = response.doc('.end-text').remove('.ep-source').html()\
        if not content:\
            return None\
        return {\
            "url": response.url,\
            "title": response.doc('h1').text(),\
            "content": content.replace('\\n','').replace('\\t','').replace('\\r',''),\
            "subject": u'钢琴',\
            "source": '163.com',\
            "date": response.doc('.post_time_source').text()[:10],\
            "bread": [u'钢琴资讯',],\
            "class": 46,\
            "data_weight": 0,\
        }\
$$$$$1471329973.5625
gaokao_51test_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-07 11:22:37\
# Project: gmat_51test\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.51test.net/gaokao/st/', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.news-list-left-content li').items():\
            url = each.find('a').attr.href\
            date = each.find('span').text()\
            self.crawl(url, save={'date': date}, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        date = response.save['date']\
        '''\
        for each in response.doc('.show_content_next > a').items():\
            self.crawl(each.attr.href,save={'date': date}, callback=self.detail_page)\
        '''\
\
        content = response.doc('.show_content').remove('div').html().replace('\\r','').replace('\\n','').replace('\\t','').replace(u'【无忧考网 - GMAT研究生管理考试试题】','')\
        if not content:\
            return None\
        return {\
            "url": response.url,\
            "title": response.doc('h1').text(),\
            "date": date,\
            "subject": '高考',\
            "source": '51test',\
            "bread": [u'模拟真题',],\
            "class": 26,\
            "data_weight": 0,\
            "content": content, #''.join(['<p>%s</p>'%v for v in content_list]),\
        }\
$$$$$1470815072.2272
gaozhongtiku_weipan$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-04 19:31:20\
# Project: gaozhongtiku_weipan\
\
from pyspider.libs.base_handler import *\
\
#fw = open('/Users/bjhl/Documents/gaozhongtiku/tiku_urls.log', 'a')\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://vdisk.weibo.com/u/2234165192', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.sort_name_intro a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.vd_page_btn').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
        \
    @config(priority=2)\
    def detail_page(self, response):\
        for each in response.doc('script').items():\
            if 'download_list' in each.text():\
                url = each.text().split('"download_list":["')[1].split('"]')[0].replace('\\\\','')\
                #print url\
                #fw.write(url + '\\n')\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
            "download_url":url\
        }\
$$$$$1467942992.0229
gmat_tiandao_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-05 15:21:53\
# Project: gmat_tiandao_inc\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    page_dict = {\
        "advice": u'GMAT机经',\
        'read': u'GMAT阅读',\
        'syntax': u'GMAT语法',\
        'write': u'GMAT写作',\
        'math': u'GMAT数学',\
        'logic': u'GMAT逻辑',\
        'news': u'快讯动态',\
        'comments': u'冲刺宝典',\
        'experience': u'备考计划',\
    }\
    @every(minutes=12 * 60)\
    def on_start(self):\
        for k, v in self.page_dict.iteritems():\
            self.crawl('http://gmat.tiandaoedu.com/%s/'%k,save={'bread': v}, callback=self.index_page)\
        self.crawl('http://tiandaoedu.com/yyxx/syyd/', save={'bread': u'GMAT阅读'}, callback=self.index_page)\
        self.crawl('http://tiandaoedu.com/yyxx/xcxy/', save={'bread': u'GMAT词汇'}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.sty_two > .ptit > a').items():\
            self.crawl(each.attr.href, save={'bread': bread}, callback=self.detail_page)\
        for each in response.doc('.rf > .ptit > a').items():\
            self.crawl(each.attr.href, save={'bread': bread}, callback=self.detail_page)\
        #for each in response.doc('.pages > a').items():\
        #    self.crawl(each.attr.href, save={'bread': bread}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('.wzlist > .yh').text(),\
            "date": response.doc('.p_span > span').text().split()[2].split(u'：')[-1],\
            "brief": response.doc('.zhw_p').text(),\
            "subject": 'GMAT',\
            'source': 'tiandao',\
            'content': response.doc('.wzy_bot').html().replace('\\t','').replace('\\n','').replace('\\r','').strip(' ').replace(u'天道',''),\
            "bread": [response.save['bread'],],\
            "class": 30,\
            "data_weight": 0,\
        }\
$$$$$1471334123.6224
gmat_zhan_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-29 18:19:22\
# Project: gmat_xiaozhan_inc\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    type_dict = {\
        'yufa': u'GMAT语法',\
        'cihui': u'GMAT词汇', \
        'yuedu': 'GMAT阅读', \
        'luoji': u'GMAT逻辑',\
        'tuili': u'GMAT逻辑',\
        'zuowen': u'GMAT作文',\
        'shuxue': u'GMAT数学', \
        'jihua': u'备考计划', \
        'tifen/beikao': u'备考计划', \
        'gaofen': u'高分心得',\
        'fuxi': u'复习攻略',  \
\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for type, v in self.type_dict.iteritems():\
            self.crawl('http://gmat.zhan.com/%s/'%type, save={'bread': v}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/gmat/kaoqian/', save={'bread': u'冲刺宝典'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/gmat/fukao/', save={'bread': u'冲刺宝典'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/gmat/beikao/', save={'bread': u'备考计划'}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.experience-item').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['date'] = each.find('.index-middle-info-3 > .pull-left').text().split()[0]\
            #_save['tag'] = each.find('dl > .pull-left > a').text().split()\
            _save['brief'] = each.find('.index-middle-info-2').text()\
            _save['bread'] = bread\
            self.crawl(url, save=_save, callback=self.detail_page)\
        for each in response.doc('.things_list > .pull-right').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['date'] = each.find('.padding_right_10').text()\
            #_save['tag'] = each.find('dl > .pull-left > a').text().split()\
            _save['brief'] = each.find('.text').text()\
            _save['bread'] = bread\
            self.crawl(url, save=_save, callback=self.detail_page)\
        \
        #for each in response.doc('nav a').items():\
        #    self.crawl(each.attr.href, save={'bread': bread}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict["url"] = response.url\
        res_dict["title"] = response.doc('h1').text()\
        content_list = []\
        for each in  response.doc('.article-content > p').items():\
            content_list.append((each.html().replace('\\r','').replace('\\n','').replace('\\t','').replace(u'小站','')))\
        content_list = content_list[:-1]\
        if not content_list:\
            return None\
        res_dict['content'] = ''.join(['<p>%s</p>'% v for v in content_list])\
        res_dict['subject'] = u'GMAT'\
        res_dict['source'] = u'小站'\
        res_dict['tag'] = response.doc('.tag > a').text().split(' ')\
        bread = [res_dict['bread'], ]\
        bread.extend(response.doc('.head-crumbs-a-active').text().split(' '))\
        if res_dict['date'][:3] != '201':\
            res_dict['date'] = response.doc('.pull-left > span').text().split()[0].replace(u'年','-').replace(u'月','-').replace(u'日','')\
        res_dict['bread'] = list(set(bread))\
        res_dict['class'] = 30\
        res_dict['data_weight'] = 0\
        return res_dict$$$$$1471334127.3871
gre_tiandao_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-05 15:21:53\
# Project: gre_tiandao_inc\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    page_dict = {\
        "advice": u'GRE机经',\
        "glossary": u'GRE词汇',\
        'read': u'GRE阅读',\
        'completion': u'GRE填空',\
        'write': u'GRE作文',\
        'math': u'GRE数学',\
        'news': u'快讯动态',\
        'comments': u'冲刺宝典',\
        'experience': u'备考计划',\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k, v in self.page_dict.iteritems():\
            self.crawl('http://gre.tiandaoedu.com/%s/'%k,save={'bread': v}, callback=self.index_page)\
        self.crawl('http://tiandaoedu.com/yyxx/syyd/', save={'bread': u'GRE阅读'}, callback=self.index_page)\
    @config(age=1 * 1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.sty_two > .ptit > a').items():\
            self.crawl(each.attr.href, save={'bread': bread}, callback=self.detail_page)\
        for each in response.doc('.rf > .ptit > a').items():\
            self.crawl(each.attr.href, save={'bread': bread}, callback=self.detail_page)\
        #for each in response.doc('.pages > a').items():\
        #    self.crawl(each.attr.href, save={'bread': bread}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('.wzlist > .yh').text(),\
            "date": response.doc('.p_span > span').text().split()[2].split(u'：')[-1],\
            "brief": response.doc('.zhw_p').text(),\
            "subject": 'GRE',\
            'source': 'tiandao',\
            'content': response.doc('.wzy_bot').html().replace('\\t','').replace('\\n','').replace('\\r',''),\
            "bread": [response.save['bread'],],\
            "class": 29,\
            "data_weight": 0,\
        }\
$$$$$1471334156.2028
gre_xiaozhan_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-29 18:19:22\
# Project: gre_xiaozhan\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    type_dict = {\
        'zhenti': u'GRE机经', 'cihui': u'GRE词汇', 'yuedu': 'GRE阅读', 'tiankong': u'GRE填空', 'zuowen': u'GRE作文',\
        'shuxue': u'GRE数学', 'jihua': u'备考计划', 'tifen/beikao': u'备考计划', 'gaofen': u'高分心得',\
        'fuxi': u'复习攻略',  \
\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for type, v in self.type_dict.iteritems():\
            self.crawl('http://gre.zhan.com/%s/'%type, save={'bread': v}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/gre/kaoqian/', save={'bread': u'冲刺宝典'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/gre/fukao/', save={'bread': u'冲刺宝典'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/gre/beikao/', save={'bread': u'备考计划'}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.experience-item').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['date'] = each.find('.index-middle-info-3 > .pull-left').text().split()[0]\
            #_save['tag'] = each.find('dl > .pull-left > a').text().split()\
            _save['brief'] = each.find('.index-middle-info-2').text()\
            _save['bread'] = bread\
            self.crawl(url, save=_save, callback=self.detail_page)\
        for each in response.doc('.things_list > .pull-right').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['date'] = each.find('.padding_right_10').text()\
            #_save['tag'] = each.find('dl > .pull-left > a').text().split()\
            _save['brief'] = each.find('.text').text()\
            _save['bread'] = bread\
            self.crawl(url, save=_save, callback=self.detail_page)\
        '''\
        for each in response.doc('.col-sm-9').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['date'] = each.find('.move-top > .pull-left').text().split()[0]\
            #_save['tag'] = each.find('dl > .pull-left > a').text().split()\
            _save['brief'] = each.find('.index-middle-info-5').text()\
            _save['bread'] = bread\
            self.crawl(url, save=_save, callback=self.detail_page)\
        '''\
        #for each in response.doc('nav a').items():\
        #    self.crawl(each.attr.href, save={'bread': bread}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict["url"] = response.url\
        res_dict["title"] = response.doc('h1').text()\
        content_list = []\
        for each in  response.doc('.article-content > p').items():\
            content_list.append((each.html().replace('src="/uploadfile','src="http://gre.zhan.com/uploadfile').replace('\\t','').replace('\\r','').replace('\\n','')))\
        if not content_list:\
            return None\
        res_dict['content'] = ''.join(['<p>%s</p>'% v for v in content_list])\
        res_dict['subject'] = u'GRE'\
        res_dict['source'] = u'小站'\
        res_dict['tag'] = response.doc('.tag > a').text().split(' ')\
        bread = [res_dict['bread'], ]\
        bread.extend(response.doc('.head-crumbs-a-active').text().split(' '))\
        if res_dict['date'][:3] != '201':\
            res_dict['date'] = response.doc('.pull-left > span').text().split()[0].replace(u'年','-').replace(u'月','-').replace(u'日','')\
        res_dict['bread'] = list(set(bread))\
        res_dict['class'] = 29\
        res_dict['data_weight'] = 0\
        return res_dict$$$$$1471334159.2649
guitarchina_news$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-14 16:21:18\
# Project: guitar_china\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://news.guitarchina.com/sort/1.html', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('td td td td td a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
        '''\
        for each in response.doc('.normalfont > a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
        '''\
    @config(priority=2)\
    def detail_page(self, response):\
        #content = ''.join(['<p>%s</p>'%v.html() for v in response.doc('p').items()])\
        content = response.doc('.content').html()\
        if not content:\
            return None\
        return {\
            "url": response.url,\
            "subject": u'吉他',\
            "source": u'吉他中国',\
            "content": content,\
            "title": response.doc('.bigfont > b').text(),\
            "date": response.doc('.normalfont > font').text(),\
            "class": 16,\
            "data_weight": 0,\
        }\
$$$$$1471329638.5885
guojiasifa_zhengbao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-01 15:29:58\
# Project: guojiasifa_zhengbao\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
type_dict = {\
    u'民法': [u'职业资格', u'国家司法'],\
    u'刑法': [u'职业资格', u'国家司法'],\
    u'行政法': [u'职业资格', u'国家司法'],\
    u'民事诉讼法': [u'职业资格', u'国家司法'],\
    u'刑事诉讼法': [u'职业资格', u'国家司法'],\
    u'行政诉讼法': [u'职业资格', u'国家司法'],\
    u'商法': [u'职业资格', u'国家司法'],\
    u'经济法': [u'职业资格', u'国家司法'],\
    u'三国法': [u'职业资格', u'国家司法'],\
    u'宪法': [u'职业资格', u'国家司法'],\
    u'法理学': [u'职业资格', u'国家司法'],\
    u'法制史': [u'职业资格', u'国家司法'],\
    u'其他': [u'职业资格', u'国家司法'],\
}\
\
some_url =[\
    'http://www.chinaacc.com/chujizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/chujizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/',\
    'http://www.chinaacc.com/zaojia/zt/',\
    'http://www.chinaacc.com/zaojia/mnst/',    \
]\
\
special_name = [\
    '民法',\
    '刑法',\
    '行政法',\
]\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.chinalawedu.com/sifakaoshi/ziliao/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.cnav1 > a').items():\
            if '选课' in each.text():\
                continue\
            if each.text() in special_name:\
                _dict = {}\
                if each.text().replace(' ', '') in type_dict.keys():\
                    _dict['bread'] = type_dict[each.text().replace(' ', '')]\
                else:\
                    _dict['bread'] = ['职业资格', each.text().replace(' ', '')]\
                self.crawl(each.attr.href, save = _dict, callback=self.special_list_page)\
            else:\
                _dict = {}\
                if each.text().replace(' ', '') in type_dict.keys():\
                    _dict['bread'] = type_dict[each.text().replace(' ', '')]\
                else:\
                    _dict['bread'] = ['职业资格', each.text().replace(' ', '')]\
                self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
      \
    @config(age=10 * 24 * 60 * 60)\
    def special_list_page(self, response):\
        for each in response.doc('.w666 > .tr a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)          \
     \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.info-list li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').text()\
            if '报名' in _dict['title'] or '名师' in _dict['title'] or '汇总' in _dict['title']:\
                continue\
            _dict['date'] = each.text().replace(_dict['title'],'').replace('[','').replace(']','').replace('·','').replace(' ','')\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
        \
        #翻页 \
        for each in response.doc('.p1 > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('#fontzoom').items():\
            for each1 in each.find('p').items():\
                if u'全网首发' in each1.text():\
                    continue\
                if u'估分更准确' in each1.text():\
                    continue\
                if u'独家' in each1.text():\
                    continue\
                if u'点击' in each1.text():\
                    continue\
                if u'到建设工程教育网论坛' in each1.text():\
                    continue\
                if u'特色班' in each1.text():\
                    break\
                if u'近几年' in each1.text():\
                    break\
                if u'考友咨询' in each1.text():\
                    break\
                if u'更多' in each1.text() and u'资讯' in each1.text():\
                    break\
                if u'推荐信息' in each1.text() or u'更多推荐' in each1.text() or u'推荐阅读' in each1.text() or u'相关推荐' in each1.text() or u'精彩推荐' in each1.text():\
                    break\
                if u'免费在线测试' in each1.text():\
                    continue\
                if u'在线测试系统' in each1.text():\
                    continue\
                if u'转载请注明出处' in each1.text():\
                    continue\
                if u'欢迎考生' in each1.text():\
                    continue\
                if u'责任编辑' in each1.text():\
                    break\
                if u'相关链接' in each1.text() or u'精彩链接' in each1.text():\
                    break\
                if u'以上' in each1.text() and u'是法律教育网' in each1.text():\
                    break\
                else:\
                    list.append(each1.text().replace(' ',''))\
            \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            \
            return {\
                "url": response.url,\
                "title": response.save.get('title'),\
                "content": content.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace(u'法律教育网', '').replace('【 】', ''),\
                "subject": u"考证题库",\
                "bread": response.save.get('bread'),\
                "date": response.save.get('date'),\
                "tdk_keywords": str(response.doc('meta[http-equiv="keywords"]').eq(0).attr.content).replace(u'法律教育网', '').replace(\
                    'None', '') + str(response.doc('meta[name="keywords"]').eq(0).attr.content).replace(u'法律教育网', '').replace(\
                    'None', ''),\
                "tdk_desc": str(response.doc('meta[http-equiv="description"]').eq(0).attr.content).replace(u'法律教育网', u'').replace(\
                    '建设网校', '').replace('None', '') + str(\
                    response.doc('meta[name="description"]').eq(0).attr.content).replace(\
                    u'法律教育网', '').replace('建设网校', '').replace('None', ''),\
                "tdk_title": response.doc('head > title').eq(0).text().replace(u'法律教育网', '') + u' 跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"法律教育网",\
            }\
\
\
$$$$$1468461113.7398
gu_drumchina$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-24 14:10:09\
# Project: gu_drumchina\
\
from pyspider.libs.base_handler import *\
import re\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
        contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.2'\
    }\
    \
    page_dict = {\
\
        'http://news.drumchina.com/sort/1.html':[u'鼓资讯'],\
        'http://news.drumchina.com/sort/4.html':[u'鼓手志'],\
        'http://news.drumchina.com/sort/6.html':[u'鼓教室'],\
        'http://news.drumchina.com/sort/5.html':[u'鼓世界'],\
        'http://news.drumchina.com/sort/7.html':[u'鼓谱台'],\
        'http://news.drumchina.com/sort/9.html':[u'鼓服务']\
\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k,save = {'bread':v}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('td td td td td td').items():\
            if each.find('a') and each.find('font').text().split():\
                _dict = {}\
                _dict['title'] = each.find('a').text()\
                url = each.find('a').attr.href\
                _dict['date'] = each.find('font').text().split()[0]\
                _dict['bread'] = response.save.get('bread')\
                self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('td[align="right"] > .normalfont > a[href]').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict,callback=self.index_page)    \
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['content'] = response.doc('.content').remove('script').remove('embed').html()\
        if '</a>' in res_dict['content']:\
            res_dict['content'] = removeLink(res_dict['content'])\
        if res_dict['content'] == '<p/>':\
            return\
        res_dict['data_weight'] = 0\
        res_dict['class'] = 33\
        res_dict['subject'] = u'鼓'\
        res_dict['source'] = 'drumchina.com'\
        res_dict['url'] = response.url\
        return res_dict\
        \
$$$$$1472107079.3516
gu_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-24 14:10:09\
# Project: gu_drumchina\
\
from pyspider.libs.base_handler import *\
import re\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
        contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.2'\
    }\
    \
    page_dict = {\
\
        'http://news.drumchina.com/sort/1.html':[u'鼓资讯'],\
        'http://news.drumchina.com/sort/4.html':[u'鼓手志'],\
        'http://news.drumchina.com/sort/6.html':[u'鼓教室'],\
        'http://news.drumchina.com/sort/5.html':[u'鼓世界'],\
        'http://news.drumchina.com/sort/7.html':[u'鼓谱台'],\
        'http://news.drumchina.com/sort/9.html':[u'鼓服务']\
\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k,save = {'bread':v}, callback=self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        for each in response.doc('td td td td td td').items():\
            if each.find('a') and each.find('font').text().split():\
                _dict = {}\
                _dict['title'] = each.find('a').text()\
                url = each.find('a').attr.href\
                _dict['date'] = each.find('font').text().split()[0]\
                _dict['bread'] = response.save.get('bread')\
                self.crawl(url, save = _dict,callback=self.detail_page)\
       \
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['content'] = response.doc('.content').remove('script').remove('embed').html()\
        if res_dict['content'] == '<p/>':\
            return\
        res_dict['data_weight'] = 0\
        res_dict['class'] = 33\
        res_dict['subject'] = u'鼓'\
        res_dict['source'] = 'drumchina.com'\
        res_dict['url'] = response.url\
        return res_dict\
        \
$$$$$1472439095.1450
gwyjingyan_zhonggong$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-08 18:25:49\
# Project: gwyjingyan_zhonggong\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.offcn.com/gjgwy/ziliao/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.zg_lm_list > li').items():\
            _dict = {}\
            _dict['title'] = each.find('.zg_lm_2').text()\
            _dict['date'] = each.find('font').text()\
            self.crawl(each.find('.zg_lm_2').attr.href, save = _dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.a1').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):        \
        return {\
            "url": response.url,\
            'bread': [u'职场分享'],\
            'title': response.save.get('title'),\
            'date': response.save.get('date'),\
            "html": response.doc('.zg_show_word').html(),\
            'source': u'敲墙简历',\
            'class': 36,\
            'subject': u'经验',\
            'data_weight': 0,\
        }$$$$$1468413501.2042
hujiang_deyu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 14:18:29\
# Project: hujiang_deyu\
\
from pyspider.libs.base_handler import *\
import re\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
        contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'v0.2'\
    }\
\
    headers = {\
\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
\
    }\
    page_dict = {\
            'http://de.hujiang.com/new/rumen/':[u'德语基础'],\
            'http://de.hujiang.com/new/shiyong/':[u'实用德语'],\
            'http://de.hujiang.com/new/yule/':[u'德语文化'],\
            'http://de.hujiang.com/new/topic/627/':[u'德国留学'],\
            'http://de.hujiang.com/new/topic/1024/':[u'德语考试']\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('ul#article_list > li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h2 > a[title]').text().replace('沪江','').replace('网校','')\
            url =  each.find('h2 > a.a_article_title ').attr.href\
            _dict['date'] = each.find('p.article_list_moreinfo').find('span.green').text().split()[0]\
            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\
        for each in response.doc('div.page_list > a[href]').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       content_list = []\
       for each in response.doc('div.main_article > p').items():\
            info = each.remove('img').remove('embed').html().replace('沪江','').replace('网校','')\
            if u'小编推荐'  in info:\
                break\
            if u'沪江' in info or '转载' in info or '声明：' in info:\
                continue\
            if info and '</a>' in info:\
                content_list.append(removeLink(info))\
            elif info:\
                content_list.append(info)\
                \
       if not content_list:\
            return\
       res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
       if not res_dict['content'] and len(content_list)<=1:\
            return\
       if res_dict['content'] and not res_dict['content'].strip():\
            return\
       if len(res_dict['content'])<=10:\
            return\
       res_dict['class'] = 33\
       res_dict['subject'] = u'德语'\
       res_dict['source'] = u'沪江'\
       res_dict['data_weight'] = 0\
       res_dict['url'] = response.url\
       return res_dict\
$$$$$1472456349.3527
hujiang_deyu_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 14:18:29\
# Project: hujiang_deyu\
\
from pyspider.libs.base_handler import *\
import re\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
        contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    headers = {\
\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
\
    }\
    page_dict = {\
            'http://de.hujiang.com/new/rumen/':[u'德语基础'],\
            'http://de.hujiang.com/new/shiyong/':[u'实用德语'],\
            'http://de.hujiang.com/new/yule/':[u'德语文化'],\
            'http://de.hujiang.com/new/topic/627/':[u'德国留学'],\
            'http://de.hujiang.com/new/topic/1024/':[u'德语考试']\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        for each in response.doc('ul#article_list > li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h2 > a[title]').text().replace('沪江','').replace('网校','')\
            url =  each.find('h2 > a.a_article_title ').attr.href\
            _dict['date'] = each.find('p.article_list_moreinfo').find('span.green').text().split()[0]\
            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\
      \
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       content_list = []\
       for each in response.doc('div.main_article > p').items():\
            info = each.remove('img').remove('embed').html().replace('沪江','').replace('网校','')\
            if u'小编推荐'  in info:\
                break\
            if u'沪江' in info or '转载' in info or '声明：' in info:\
                continue\
            if info:\
                content_list.append(info)\
                \
       if not content_list:\
            return\
       res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
       if not res_dict['content'] and len(content_list)<=1:\
            return\
       if res_dict['content'] and not res_dict['content'].strip():\
            return\
       if len(res_dict['content'])<=10:\
            return\
       res_dict['class'] = 33\
       res_dict['subject'] = u'德语'\
       res_dict['source'] = u'沪江'\
       res_dict['data_weight'] = 0\
       res_dict['url'] = response.url\
       return res_dict\
$$$$$1472438792.6994
hujiang_riyu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 14:18:29\
# Project: hujiang_deyu\
\
from pyspider.libs.base_handler import *\
import re\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    headers = {\
\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
\
    }\
    page_dict = {\
            'http://jp.hjenglish.com/new/c4010/':[u'日语基础'],\
            'http://jp.hjenglish.com/new/c4040/':[u'实用日语'],\
            'http://jp.hjenglish.com/new/c4020/':[u'日语考试'],\
            'http://jp.hjenglish.com/new/c4070/':[u'日语文化'],\
            'http://jp.hjenglish.com/new/c4090/':[u'商务日语']\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('ul#article_list > li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h2 > a[title]').text().replace('沪江','').replace('网校','')\
            url =  each.find('h2 > a.a_article_title ').attr.href\
            _dict['date'] = each.find('p.article_list_moreinfo').find('span.green').text().split()[0]\
            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\
        for each in response.doc('div.page_list > a[href]').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       content_list = []\
       for each in response.doc('div.main_article > p').items():\
            info = each.remove('img').remove('embed').remove('script').html().replace('沪江','').replace('网校','')\
            if u'小编推荐'  in info:\
                break\
            if u'沪江' in info or '转载' in info or '声明：' in info:\
                continue\
            if info and '</a>' in info:\
                content_list.append(removeLink(info))\
            elif info:\
                content_list.append(info)\
                \
       if not content_list:\
            return\
       res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
       if not res_dict['content'] and len(content_list)<=1:\
            return\
       if res_dict['content'] and not res_dict['content'].strip():\
            return\
       if len(res_dict['content'])<=10:\
            return\
       res_dict['class'] = 33\
       res_dict['subject'] = u'德语'\
       res_dict['source'] = u'沪江'\
       res_dict['data_weight'] = 0\
       res_dict['url'] = response.url\
       return res_dict\
$$$$$1471927304.9607
hujiang_riyu_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 14:18:29\
# Project: hujiang_deyu\
\
from pyspider.libs.base_handler import *\
import re\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    headers = {\
\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
\
    }\
    page_dict = {\
            'http://jp.hjenglish.com/new/c4010/':[u'日语基础'],\
            'http://jp.hjenglish.com/new/c4040/':[u'实用日语'],\
            'http://jp.hjenglish.com/new/c4020/':[u'日语考试'],\
            'http://jp.hjenglish.com/new/c4070/':[u'日语文化'],\
            'http://jp.hjenglish.com/new/c4090/':[u'商务日语']\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        for each in response.doc('ul#article_list > li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h2 > a[title]').text().replace('沪江','').replace('网校','')\
            url =  each.find('h2 > a.a_article_title ').attr.href\
            _dict['date'] = each.find('p.article_list_moreinfo').find('span.green').text().split()[0]\
            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       content_list = []\
       for each in response.doc('div.main_article > p').items():\
            info = each.remove('img').remove('embed').remove('script').html().replace('沪江','').replace('网校','')\
            if u'小编推荐'  in info:\
                break\
            if u'沪江' in info or '转载' in info or '声明：' in info:\
                continue\
            if info:\
                content_list.append(info)\
                \
       if not content_list:\
            return\
       res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
       if not res_dict['content'] and len(content_list)<=1:\
            return\
       if res_dict['content'] and not res_dict['content'].strip():\
            return\
       if len(res_dict['content'])<=10:\
            return\
       res_dict['class'] = 33\
       res_dict['subject'] = u'德语'\
       res_dict['source'] = u'沪江'\
       res_dict['data_weight'] = 0\
       res_dict['url'] = response.url\
       return res_dict\
$$$$$1472438893.0176
iask_question$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-08 13:59:11\
# Project: iask_wenda\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
import traceback\
import MySQLdb\
class ConnectionUtil(object):\
\
    def __init__(self,connection):\
        self.connection = connection\
\
    def cursor(self):\
        if self.connection:\
            self.cursor  = self.connection.cursor()\
            return self.cursor\
        else:\
            return  None\
\
    def __enter__(self):\
        return self.cursor()\
\
    def __exit__(self, exc_type, exc_val, exc_tb):\
            #print 'ok'\
            if self.connection:\
                self.connection.commit()\
            if self.cursor:\
                self.cursor.close()\
            if self.connection:\
                self.connection.close()\
            if exc_type is not None:\
                #print 'errror'\
                print exc_val\
                #traceback.print_exc()\
                return True\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
    \
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1',\
        'headers':{\
        'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\
'Accept-Encoding':'gzip, deflate, sdch',\
'Accept-Language':'zh-CN,zh;q=0.8,en;q=0.6',\
'Cache-Control':'max-age=0',\
'Connection':'keep-alive',\
'Host':'iask.sina.com.cn',\
#Referer:http://iask.sina.com.cn/c/997-essence-1-new.html\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36',\
\
        }\
    }\
\
    @every(minutes=1 * 20)\
    def on_start(self):\
       sql = 'select * from iask where flag = 0 limit 5'\
       con = MySQLdb.connect(db='test',host='127.0.0.1',user='root',passwd='123',charset='utf8')\
       with ConnectionUtil(con) as cursor:\
           cursor.execute(sql)\
           result =  cursor.fetchall()\
       if result:\
           for each in result:\
               _dict = {}\
               _dict['id'] = each[1]\
               self.crawl(each[2],save = _dict,callback=self.index_page)\
\
    @config(priority=2)\
    def index_page(self, response):\
       res_dict = response.save\
       sql = 'update iask set flag = 1 where iask_id=%s'\
       con = MySQLdb.connect(db='test',host='127.0.0.1',user='root',passwd='123',charset='utf8')\
       with ConnectionUtil(con) as cursor:\
           cursor.execute(sql,(res_dict['id']))              \
       question_other = response.doc('span.ask-time ').text()\
       res_dict['create_time'] = get_date(question_other)\
       res_dict['bread'] = response.doc('div.dib span a').text().split()\
       res_dict['title'] = response.doc('h3.title-f22').text()\
       if u'title' not in res_dict or not res_dict['title']:\
                res_dict['title'] = response.doc('div.question_text > .pre_img').remove('div.link_layer').text()\
       res_dict['question_detail'] = response.doc('.question_text').remove('.link_layer').text()\
       answers_list = []\
       for each in response.doc('li.clearfix').items():\
            info = each.find('.answer_txt').text()\
            if info:\
                 _dict = {}\
                 _dict['content'] = info\
                 _dict['user_name'] = each.find('.user_wrap > a').text()\
                 question_time = each.find('.answer_t').text()\
                 _dict['create_time'] = get_date(question_time)\
                 answers_list.append(_dict)\
       for each in response.doc('.good_answer').items():\
            info = each.find('.answer_text').text()\
            if info:\
                 _dict = {}\
                 _dict['content'] = info\
                 _dict['user_name'] = each.find('.answer_tip > a').text()\
                 question_time = each.find('.answer_tip .time').text().replace('|','').strip()\
                 _dict['create_time'] = get_date(question_time)\
                 answers_list.append(_dict)\
       if not answers_list:\
                 return \
       res_dict['answers'] = answers_list\
       res_dict['url'] = response.url\
       res_dict['source'] = 'sina'\
       res_dict['subject'] = u'主站问答'\
       res_dict['class'] = 34\
       res_dict['data_weight'] = 0\
       res_dict['create_user'] = response.doc('.ask_autho span.user_wrap').children('a').text()               \
       return res_dict\
\
            \
    @config(priority=2)\
    def detail_page(self, response):\
       pass$$$$$1469524369.5301
iask_sina$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-08 13:59:11\
# Project: iask_sina\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
import datetime\
import MySQLdb\
\
class ConnectionUtil(object):\
\
    def __init__(self,connection):\
        self.connection = connection\
\
    def cursor(self):\
        if self.connection:\
            self.cursor  = self.connection.cursor()\
            return self.cursor\
        else:\
            return  None\
\
    def __enter__(self):\
        return self.cursor()\
\
    def __exit__(self, exc_type, exc_val, exc_tb):\
            #print 'ok'\
            if self.connection:\
                self.connection.commit()\
            if self.cursor:\
                self.cursor.close()\
            if self.connection:\
                self.connection.close()\
            if exc_type is not None:\
                #print 'errror'\
                print exc_val\
                #traceback.print_exc()\
                return True\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'天前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-1*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
    \
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'v0.2'\
        #'headers':self.headers\
    }\
    \
    headers = {\
     'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\
'Accept-Encoding':'gzip, deflate, sdch',\
'Accept-Language':'zh-CN,zh;q=0.8,en;q=0.6',\
'Cache-Control':'max-age=0',\
'Connection':'keep-alive',\
'Host':'iask.sina.com.cn',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36',\
\
    }\
\
    @every(minutes=1*30)\
    def on_start(self):\
       sql = 'select * from tb_query where sina_flag = 0 limit 5000'\
       con = MySQLdb.connect(db='querydb',host='127.0.0.1',user='root',passwd='123',charset='utf8')\
       with ConnectionUtil(con) as cursor:\
           cursor.execute(sql)\
           result =  cursor.fetchall()\
       for each in result:\
           _dict = {}\
           _dict['query'] = each[2]\
           _dict['id'] = each[0]\
           line = each[2]\
           for index in range(3):\
               self.crawl('http://iask.sina.com.cn/search?searchWord=%s&page=%s'%(line,index),save = _dict ,headers=self.headers,callback=self.index_page)\
                            \
                       \
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('.search_list .search_item').items():\
            _dict = {}\
            #_dict['category_id'] = response.save.get('category_id')\
            _dict['title'] = each.find('h2 a').text()\
            _dict['id'] = response.save.get('id')\
            url = each.find('h2 a').attr.href\
            question_other = each.find('.answer_det a').eq(0).text()\
            _dict['create_time'] = get_date(question_other)\
            self.crawl(url, save = _dict,headers=self.headers,callback=self.detail_page)\
        \
            \
    @config(priority=2)\
    def detail_page(self, response):\
       sql = 'update tb_query  set sina_flag = %s  where id = %s'\
       con = MySQLdb.connect(db='querydb',host='127.0.0.1',user='root',passwd='123',charset='utf8')\
       with ConnectionUtil(con) as cursor:\
           cursor.execute(sql,(1,int(response.save.get('id'))))\
       res_dict = response.save\
       question_other = response.doc('span.ask-time').text().strip()\
       res_dict['create_time'] = get_date(question_other)\
       res_dict['category_id'] = response.doc('div.dib span a').text().split()\
       res_dict['title'] = response.doc('h3.title-f22').text()\
       if u'title' not in res_dict or not res_dict['title']:\
                res_dict['title'] = response.doc('div.question_text > .pre_img').remove('div.link_layer').text()\
       res_dict['question_detail'] = response.doc('.question_text').remove('.link_layer').text()\
       answers_list = []\
                \
       for each in response.doc('li.clearfix').items():\
            info = each.find('.answer_txt').text()\
            if info:\
                 _dict = {}\
                 _dict['content'] = info\
                 _dict['user_name'] = each.find('.user_wrap > a').text()\
                 question_time = each.find('.answer_t').text().strip()\
                 _dict['create_time'] = get_date(question_time)\
                 answers_list.append(_dict)\
                    \
       for each in response.doc('.good_answer').items():\
            info = each.find('.answer_text').text()\
            if info:\
                 _dict = {}\
                 _dict['content'] = info\
                 _dict['user_name'] = each.find('.answer_tip > a').text()\
                 question_time = each.find('.answer_tip .time').text().replace('|','').strip()\
                 _dict['create_time'] = get_date(question_time)\
                 answers_list.append(_dict)\
                    \
       if not answers_list:\
                 return \
       res_dict['answers'] = answers_list\
       res_dict['url'] = response.url\
       res_dict['source'] = 'sina'\
       res_dict['subject'] = u'主站问答'\
       res_dict['class'] = 34\
       res_dict['data_weight'] = 0\
       res_dict['create_user'] = response.doc('.ask_autho span.user_wrap').children('a').text() \
       del res_dict['id']\
       return res_dict\
$$$$$1469670208.5139
inc_10_youku$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-03-30 16:44:48\
# Project: inc_10_youku\
\
from pyspider.libs.base_handler import *\
import time\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=5 * 60)\
    def on_start(self):\
        self.crawl('http://i.youku.com/i/UNDg1NzQyODA=/videos', callback=self.index_page)\
        self.crawl('http://i.youku.com/u/UMjg2MDY1OTYxMg==', callback=self.index_page)\
        self.crawl('http://www.soku.com/search_video/q_%E5%90%89%E4%BB%96_orderby_3_lengthtype_1_hd_7?site=14&_lg=10&limitdate=0', callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('.v').items():\
            cover = each.find('img').attr.src\
           # print cover\
            vlink_title = each.find('.v-link > a').attr.title\
            if vlink_title == u'该视频已被发布者加密':\
                continue\
            url = each.find('.v-meta-title > a').attr.href\
            title = each.find('.v-meta-title > a').text()\
            if title.find(u'吉他') == -1 and title.find(u'吉它') == -1:\
                pass\
            else:\
                self.crawl(url, save={'title': title, 'cover': cover},  callback=self.detail_page)\
\
   # @config(priority=2)\
    @config(age=1*1)\
    def detail_page(self, response):\
        try:\
            url = response.url\
        #http://v.youku.com/v_show/id_XMTUxNzMxODAyMA==.html?from=s1.8-1-1.2\
            id = url.split('id_')[1].split('==')[0]\
            video_url_element = 'http://player.youku.com/embed/%s' %(id)\
            video_url = []\
            video_url.append(video_url_element)\
            title = response.save['title']\
            cover = response.save['cover']\
           # vlink_title = response.save['vlink_title']\
           # title = response.doc('.base_info > .title').text()\
            subject = u'吉他'\
            source = 'youku.com'\
            publish_time = time.strftime('%Y-%m-%d',time.localtime(time.time()))\
        except:\
            return None\
        return {\
            "url": url,\
            "video_url":video_url,\
            "title": title,\
            "subject": subject,\
            "source": source,\
            "publish_time": publish_time,\
            "cover": cover,\
            "class": 10,\
            "data_weight": 0,\
        }\
$$$$$1471335003.5497
inc_16_guitarworld$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-03-30 14:13:13\
# Project: guitar_news_inc\
\
from pyspider.libs.base_handler import *\
import re\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.guitarworld.com.cn/news', callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('h3 > a').items():\
            url = each.attr.href\
            if re.match('http://www.guitarworld.com.cn/news/', url):\
                self.crawl(url, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        url = response.url\
        subject = u'吉他'\
        source = 'guitarworld.com.cn'\
        title = response.doc('.news-title').text()\
        content = response.doc('.news-content').html()\
        info_arr = []\
        for item in  response.doc('.news-link > span').items():\
            info_arr.append(item.text())\
        content = content.replace('news/data/attachment', 'data/attachment')\
        return {\
            "url": url,\
            "subject": subject,\
            "source": source,\
            "content": content,\
            "title": title,\
            "date": info_arr[1].split(' ')[0],\
            "class": 16,\
            "data_weight": 0,\
        }\
$$$$$1471329630.2764
inc_gangqin_youku_6$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-19 11:22:03\
# Project: inc_gangqin_youku_6\
\
\
from pyspider.libs.base_handler import *\
import time\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.soku.com/search_video/q_%E9%92%A2%E7%90%B4_limitdate_0?site=14&_lg=10&orderby=2', callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('.v').items():\
            cover = each.find('img').attr.src\
           # print cover\
            url = each.find('.v-meta-title > a').attr.href\
            title = each.find('.v-meta-title > a').text()\
            self.crawl(url, save={'title': title, 'cover': cover},  callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        try:\
            url = response.url\
            id = url.split('id_')[1].split('==')[0]\
            video_url_element = 'http://player.youku.com/embed/%s' %(id)\
            video_url = []\
            video_url.append(video_url_element)\
            title = response.save['title']\
            cover = response.save['cover']\
           # title = response.doc('.base_info > .title').text()\
            subject = u'钢琴'\
            source = 'youku.com'\
            publish_time = time.strftime('%Y-%m-%d',time.localtime(time.time()))\
        except:\
            return None\
        return {\
            "url": url,\
            "video_url":video_url,\
            "title": title,\
            "subject": subject,\
            "source": source,\
            "publish_time": publish_time,\
            "cover": cover,\
            "class": 6,\
            "data_weight": 0,\
        }\
$$$$$1471335046.7183
jianlijingyan_qqjl$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-08 18:06:20\
# Project: jianlijingyan_qqjl\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.qqjianli.com/blog/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('h2 > a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.nextpostslink').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        methods = []\
        content = []\
        for each in response.doc('.post_content p').items():\
            if each.text().strip() != '':\
                content.append(each.html())   \
        if len(content) == 0:\
            return \
        img = ''\
        title = ''\
        index = 0\
        if len(content[0].strip()) > 1 and content[0].strip()[1] != u'、' and '--' != content[0].strip()[0:2]:\
            title = content[0]\
            index = 1\
        _list = []  \
        step_title = ''\
        substeps = []\
        flag_first = True\
        flag_mothod = False\
        for each in content[index:]:\
            if each.strip() == '':\
                continue\
            if each.strip()[1] == u'、' or '--' == each.strip()[0:2]:\
                flag_mothod = True\
                if not flag_first:\
                    _list.append({\
                        'img': '',\
                        'title': step_title,\
                        'substeps': substeps,\
                    })\
                    #print each\
                    if '<br' in each:\
                        step_title = '<strong>'+each.split('<br />\\n')[0]+'</strong>'\
                        substeps = [each.split('<br />\\n')[1]]\
                    else:\
                        step_title = '<strong>'+each+'</strong>'\
                        substeps = []\
                if flag_first:\
                    if '<br' in each:\
                        step_title = '<strong>'+each.split('<br />\\n')[0]+'</strong>'\
                        substeps = [each.split('<br />\\n')[1]]\
                    else:\
                        step_title = '<strong>'+each+'</strong>'\
                        substeps = []\
                    flag_first = False\
            else:\
                if flag_mothod:\
                    substeps.append('<p>'+each+'</p>')\
                else:\
                    step_title = '<strong>'+each+'</strong>'\
            _list.append({\
                'img': '',\
                'title': step_title,\
                'substeps': substeps,\
            })\
        methods.append({'title': u'方法/步骤', 'steps': _list})\
                \
        return {\
            "url": response.url,\
            "title": response.doc('h2').text(),\
            "abstract": {\
                'title': '',\
                'steps': ['<strong>'+title+'</strong>'],\
                'img': '',\
            },\
            "methods": methods,\
            'bread': [u'职场分享'],\
            'date': response.doc('.date').text(),\
            'source': u'敲墙简历',\
            'class': 36,\
            'subject': u'经验',\
            'data_weight': 0,\
        }\
$$$$$1468230727.6999
jianli_jianlisky$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-07 11:01:22\
# Project: qiuzhi_kuaiji\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.jianli-sky.com/', callback=self.list_page)\
        '''\
        self.crawl('http://jianli.yjbys.com/jianlimoban/gerenjianlimoban/', callback=self.index_page)\
\
        self.crawl('http://jianli.yjbys.com/jianlimoban/yingwenjianlimoban/', callback=self.index_page)\
\
        self.crawl('http://jianli.yjbys.com/jianlimoban/qiuzhijianlimoban/', callback=self.index_page)\
        '''\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.pleft strong > a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.w1 > a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
        for each in response.doc('.dede_pages a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = ['<p>%s</p>'%v.text() for v in response.doc('p').items()][:-1]\
        if not content_list:\
            return None\
        content = ''.join(content_list)\
        \
        return {\
            "url": response.url,\
            "title": response.doc('h1').text(),\
            "content": content, \
            "data_weight": 0,\
            "subject": u'求职',\
            "bread": [u'简历',],\
            "class": 46,\
            "source": u'jianli-sky',\
        }\
$$$$$1468230735.7347
jingyan$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-25 16:12:34\
# Project: jingyan\
\
from pyspider.libs.base_handler import *\
import time\
\
class Handler(BaseHandler):\
\
    \
    page_dict = {\
\
        '1':u'电脑数码',\
        '2':u'美食烹饪',\
        '3':u'健康养生',\
        '4':u'时尚美容',\
        '5':u'情感家庭',\
        '6':u'游戏攻略',\
        '7':u'职场理财',\
        '8':u'生活技巧',\
        '9':u'体育运动',\
\
        \
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        '''\
        for line in open('/apps/home/worker/xuzhihao/jingyan/tags'):\
            tag = line.strip('\\n')\
            self.crawl('http://jingyan.baidu.com/tag?tagName='+tag, save={'tag': tag},  callback=self.index_page)\
        '''\
        for k,v in self.page_dict.items():\
            self.crawl('http://xinzhi.wenda.so.com/home/list?cid=%s'%(k),save={'tag': v,'cid':k}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        url = 'http://xinzhi.wenda.so.com/home/partList?type=1&cid=%s&pn=%s&t=%s'\
        for each in range(300):\
            _dict = {}\
            _dict['tag'] = response.save.get('tag')\
            _dict['cid'] = response.save.get('cid')\
            timestr =  '%f'%(time.time()*1000)\
            self.crawl(url%(response.save.get('cid'),each,timestr.split('.')[0]), save=_dict, callback=self.list_page)\
            \
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('h3 > a').items():\
            self.crawl(each.attr.href, save={'tag': response.save['tag']}, callback=self.detail_page)\
        \
\
    @config(priority=2)\
    def detail_page(self, response):\
        methods = []\
        for each in response.doc('.steps').items():\
            title = each.find('.title').text() #u'方法'\
            print title\
            _list = []\
            for step in each.find('li').items():\
                try:\
                    img = step.find('.pic > img').attr.src\
                except:\
                    img = ''\
                _list.append({\
                    "title": step.find('.text').html(),\
                    "img": img,\
                    #"title": step.text(),\
                    "substeps": [],\
                })\
\
            methods.append({"title": title, "steps": _list})\
        abstract = {'title': '',\
                    'steps': [response.doc('.brief > div').text(),],\
                    'img': ''\
                    }\
        prepare = {'title': response.doc('.tools .title').text(),\
                   'steps': [response.doc('.tools .content').text(), ],\
                   }\
        return {\
            "url": response.url,\
            "title": response.doc('.art-title').text(),\
            "methods": methods,\
            "abstract": abstract,\
            "prepare": prepare,\
            "date": response.doc('.art-time').text().replace(u'创建于', '')[:10],\
            "bread": [response.save['tag'],],\
            "source": "360",\
            "class": 36,\
            "subject": '经验',\
            "data_weight": 0,\
        }\
$$$$$1468374374.4084
jingyan2_gaosanwang$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-24 09:39:15\
# Project: jingyan2_gaosanwang\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
\
url_dict = {\
'http://www.gaosan.com/zhukao/1/': u'高考',\
'http://www.gaosan.com/zhuanyejiedu/1/': u'大学',\
}\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for url,bread in url_dict.iteritems():\
            self.crawl(url,save = {'bread':bread},  callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.showMoreNChildren > li').items():\
            if 'index.html' in each.find('a').attr.href:\
                continue\
            _dict = {}\
            _dict['bread'] = response.save['bread']\
            _dict['title'] = each.find('a > b').text().strip()\
            _dict['date'] = each.find('i').text().strip().split()[-1].replace(u'年','-').replace(u'月','-').replace(u'日','')\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('#AspNetPager1 > a').items():\
            self.crawl(each.attr.href, save = response.save, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        #print pattern.match(str).group(0)\
        #print response.doc('.content div.left.d > div').eq(-2).html()\
        if response.doc('.content div.left.d > div').eq(-2).find('.lcontent'):\
            con = response.doc('.content div.left.d > div').eq(-2).find('.lcontent')\
        else:\
            con = response.doc('.content div.left.d > div').eq(-2)\
        for each in con.children().items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if each.text() == 'None' or not each.html():\
                continue\
            if u'高三网小编推荐你' in each.text() or u'扫一扫' in each.text() or u'相关链接' in each.text():\
                break  \
            if u'点击查看' in each.text() or u'查看更多' in each.text() or u'相关链接' in each.text():\
                continue  \
            if pattern.match(each.text().strip()) or each.html().strip().startswith('<strong>') or '<h' in each.outerHtml():\
                flag_abstract = False\
                flag_method = True\
            if '<img' in each.html():\
                #print each.html()\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                    #print desc_img\
                img = each.find('img').attr.src\
                each.remove('img')\
            if not each.text():\
                continue\
            if flag_abstract:\
                if '<table' in each.outerHtml():\
                    steps.append(each.outerHtml().replace(u'高三网',''))\
                else:\
                    steps.append(each.text().replace(u'高三网',''))\
            else:\
                if  pattern.match(each.text().strip()) or each.html().strip().startswith('<strong>') or '<h' in each.outerHtml():\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n','').replace(u'高三网',''))    \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': steps,\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [response.save.get('bread'),],\
            "source": u"高三网",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1472024595.5304
jingyan_dida$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-11 16:02:50\
# Project: jingyan_dida\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
import sys,json\
reload(sys)\
sys.setdefaultencoding('utf-8')\
urls_list = [\
'http://studyabroad.tigtag.com/application/',   \
'http://studyabroad.tigtag.com/schoolguide/',                                    \
'http://studyabroad.tigtag.com/rank/',                                         \
'http://studyabroad.tigtag.com/schoolfile/',    \
'http://studyabroad.tigtag.com/major/',   \
'http://studyabroad.tigtag.com/scholarship/',  \
'http://studyabroad.tigtag.com/writing/',    \
'http://studyabroad.tigtag.com/visa/',                                          \
'http://studyabroad.tigtag.com/experience/',\
]\
\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    header = {\
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for url in urls_list:\
            self.crawl(url,headers = self.header, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.col-list li').items():\
            _dict = {}\
            _dict['title'] = each.find('a').text().strip()\
            _dict['date'] = each.find('span').text().strip()\
            self.crawl(each.find('a').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.page-control > a').items():\
            self.crawl(each.attr.href,headers = self.header, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        pattern2 = re.compile(ur'[0123456789]{1,2}[：].*')\
        pattern3 = re.compile(r'[0123456789]{2}.*')\
        pattern4 = re.compile(r'<strong>')\
        if response.doc('#articon'):\
            #print 'ok'\
            for each in response.doc('#articon > *').items():\
                #print each\
                #print dir(each)\
                #print each.outerHtml()\
                #print each.html()\
                if not each.text().strip().replace(u'滴答网',''):\
                    continue\
                \
                if u'专注免费' in each.text() or u'版权声明' in each.text() or u'上一页' in each.text():\
                    break \
                #print i,each.html()\
                if '<img' in each.html():\
                    if desc_img == '':\
                        desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
                if each.text() == 'None' or each.text().strip() == '':\
                    continue\
                #if each.find('strong'):\
                 #   print each.find('strong').outerHtml()\
                if '【' in each.text() or 'strong' in each.outerHtml() or each.text().strip()[-1] == u'：':\
                    flag_abstract = False\
                    flag_method = True\
                if u'讯' in each.text():\
                    flag_abstract = True\
                if flag_abstract:\
                    steps.append(each.text().replace(u'滴答网',''))\
\
                else:\
                    if '【' in each.text() or 'strong' in each.outerHtml() or each.text().strip()[-1] == u'：':\
                        #print each.strip()[1]\
                        #print each.html()\
                        if not flag_first:\
                            _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                            })\
                            img = ''\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            substeps = []\
                        if flag_first:\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            flag_first = False    \
\
                    else:\
                        substeps.append(each.text().replace('\\n','').replace(u'滴答网','').replace(u'留学益网','')) \
                        \
                        \
        elif response.doc('.articon > span') and response.doc('.articon > p').size() < 5 and response.doc('.articon > div').size() < 5:\
            for each in response.doc('.articon > *').items():\
                #print dir(each)\
                #print each.outerHtml()\
                #print each.html()\
                if not each.html():\
                    continue\
                if u'专注免费' in each.text() or u'版权声明' in each.text() or u'上一页' in each.text():\
                    break \
\
                if '<img' in each.html():\
                    if desc_img == '':\
                        desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
                if each.text() == 'None' or each.text().strip() == '':\
                    continue\
                #if each.find('strong'):\
                 #   print each.find('strong').outerHtml()\
                if pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                    flag_abstract = False\
                    flag_method = True\
\
                if flag_abstract:\
                    steps.append(each.text().replace(u'滴答网',''))\
\
                else:\
                    if pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                        #print each.strip()[1]\
                        #print each.html()\
                        con_list = each.html().split('<br style="padding: 0px; margin: 0px; border: none;"/>&#13;')\
                        for ea in con_list:\
                            #print ea\
                            if not ea.strip():\
                                continue\
                            if '<strong' in ea:\
                                if not flag_first:\
                                    _list.append({\
                                        'img': img,\
                                        'title': step_title,\
                                        'substeps': substeps,\
                                    })\
                                    img = ''\
                                    step_title = '<strong>'+pq(ea).text().replace(u'留学益网','')+'</strong>'\
                                    substeps = []\
                                if flag_first:\
                                    step_title = '<strong>'+pq(ea).text().replace(u'留学益网','')+'</strong>'\
                                    flag_first = False\
                            else:\
                                substeps.append(ea.replace('\\n','').replace(u'留学益网',''))    \
\
                    else:\
                        substeps.append(each.text().replace('\\n','').replace(u'留学益网','')) \
                        \
        elif response.doc('.articon > p').size() < 5 and response.doc('.articon > div').size() < 5:\
            #print response.doc('.articon').html()\
            con_list = response.doc('.articon').html().split('<br/>&#13;')\
            i = 0\
            #print len(con_list)\
            for each in con_list:\
                if not each.strip():\
                    continue\
                #print each\
                each = pq(each)\
                #print dir(each)\
                #print each.outerHtml()\
                #print each.html()\
                if not each.html():\
                    continue\
                \
                if u'专注免费' in each.text() or u'版权声明' in each.text() or u'上一页' in each.text():\
                    break \
                i += 1\
                #print i,each.html()\
                if '<img' in each.html():\
                    if desc_img == '':\
                        desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
                if each.text() == 'None' or each.text().strip() == '':\
                    continue\
                #if each.find('strong'):\
                 #   print each.find('strong').outerHtml()\
                if '【' in each.text() or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.outerHtml()) or each.text().strip()[-1] == u'：':\
                    flag_abstract = False\
                    flag_method = True\
                if u'讯' in each.text():\
                    flag_abstract = True\
                if flag_abstract:\
                    steps.append(each.text().replace(u'滴答网',''))\
\
                else:\
                    if '【' in each.text() or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.outerHtml()) or each.text().strip()[-1] == u'：':\
                        #print each.strip()[1]\
                        #print each.html()\
                        if not flag_first:\
                            _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                            })\
                            img = ''\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            substeps = []\
                        if flag_first:\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            flag_first = False    \
\
                    else:\
                        substeps.append(each.text().replace('\\n','').replace(u'滴答网','').replace(u'留学益网','')) \
         \
        else:\
            #print 'ok'\
            for each in response.doc('.articon > *').items():\
                #print each\
                #print dir(each)\
                #print each.outerHtml()\
                #print each.html()\
                if not each.text().strip().replace(u'滴答网',''):\
                    continue\
                \
                if u'专注免费' in each.text() or u'版权声明' in each.text() or u'上一页' in each.text():\
                    break \
                #print i,each.html()\
                if '<img' in each.html():\
                    if desc_img == '':\
                        desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
                if each.text() == 'None' or each.text().strip() == '':\
                    continue\
                #if each.find('strong'):\
                 #   print each.find('strong').outerHtml()\
                if '【' in each.text() or 'strong' in each.outerHtml() or each.text().strip()[-1] == u'：':\
                    flag_abstract = False\
                    flag_method = True\
                if u'讯' in each.text():\
                    flag_abstract = True\
                if flag_abstract:\
                    steps.append(each.text().replace(u'滴答网',''))\
\
                else:\
                    if '【' in each.text() or 'strong' in each.outerHtml() or each.text().strip()[-1] == u'：':\
                        #print each.strip()[1]\
                        #print each.html()\
                        if not flag_first:\
                            _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                            })\
                            img = ''\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            substeps = []\
                        if flag_first:\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            flag_first = False    \
\
                    else:\
                        substeps.append(each.text().replace('\\n','').replace(u'滴答网','').replace(u'留学益网','')) \
            \
                        \
         \
                        \
        \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': [''.join(steps)],\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        if len(methods[0]['steps']) == 0:\
           return \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [u'留学',],\
            "source": u"滴答",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1470979530.7900
jingyan_dida2$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-12 13:25:00\
# Project: jingyan_dida2\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
import sys,json\
reload(sys)\
sys.setdefaultencoding('utf-8')\
urls_list = [\
'http://studyabroad.tigtag.com/application/',   \
'http://studyabroad.tigtag.com/schoolguide/',                                    \
'http://studyabroad.tigtag.com/rank/',                                         \
'http://studyabroad.tigtag.com/schoolfile/',    \
'http://studyabroad.tigtag.com/major/',   \
'http://studyabroad.tigtag.com/scholarship/',  \
'http://studyabroad.tigtag.com/writing/',    \
'http://studyabroad.tigtag.com/visa/',                                          \
'http://studyabroad.tigtag.com/experience/',\
]\
\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    header = {\
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for url in urls_list:\
            self.crawl(url,headers = self.header, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.col-list li').items():\
            _dict = {}\
            _dict['title'] = each.find('a').text().strip()\
            _dict['date'] = each.find('span').text().strip()\
            self.crawl(each.find('a').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.page-control > a').items():\
            self.crawl(each.attr.href,headers = self.header, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        pattern2 = re.compile(ur'[0123456789]{1,2}[：].*')\
        pattern3 = re.compile(r'[0123456789]{2}.*')\
        pattern4 = re.compile(r'<strong>')\
        if response.doc('#articon'):\
            #print 'ok'\
            for each in response.doc('#articon > *').items():\
                #print each\
                #print dir(each)\
                #print each.outerHtml()\
                #print each.html()\
                if not each.text().strip().replace(u'滴答网',''):\
                    continue\
                \
                if u'专注免费' in each.text() or u'版权声明' in each.text() or u'上一页' in each.text():\
                    break \
                #print i,each.html()\
                if '<img' in each.html():\
                    if desc_img == '':\
                        desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
                if each.text() == 'None' or each.text().strip() == '':\
                    continue\
                #if each.find('strong'):\
                 #   print each.find('strong').outerHtml()\
                if '【' in each.text() or 'strong' in each.outerHtml() or each.text().strip()[-1] == u'：':\
                    flag_abstract = False\
                    flag_method = True\
                if u'讯' in each.text():\
                    flag_abstract = True\
                if flag_abstract:\
                    steps.append(each.text().replace(u'滴答网',''))\
\
                else:\
                    if '【' in each.text() or 'strong' in each.outerHtml() or each.text().strip()[-1] == u'：':\
                        #print each.strip()[1]\
                        #print each.html()\
                        if not flag_first:\
                            _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                            })\
                            img = ''\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            substeps = []\
                        if flag_first:\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            flag_first = False    \
\
                    else:\
                        substeps.append(each.text().replace('\\n','').replace(u'滴答网','').replace(u'留学益网','')) \
                        \
                        \
        elif response.doc('.articon > span') and response.doc('.articon > p').size() < 5 and response.doc('.articon > div').size() < 5:\
            for each in response.doc('.articon > *').items():\
                #print dir(each)\
                #print each.outerHtml()\
                #print each.html()\
                if not each.html():\
                    continue\
                if u'专注免费' in each.text() or u'版权声明' in each.text() or u'上一页' in each.text():\
                    break \
\
                if '<img' in each.html():\
                    if desc_img == '':\
                        desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
                if each.text() == 'None' or each.text().strip() == '':\
                    continue\
                #if each.find('strong'):\
                 #   print each.find('strong').outerHtml()\
                if pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                    flag_abstract = False\
                    flag_method = True\
\
                if flag_abstract:\
                    steps.append(each.text().replace(u'滴答网',''))\
\
                else:\
                    if pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                        #print each.strip()[1]\
                        #print each.html()\
                        con_list = each.html().split('<br style="padding: 0px; margin: 0px; border: none;"/>&#13;')\
                        for ea in con_list:\
                            #print ea\
                            if not ea.strip():\
                                continue\
                            if '<strong' in ea:\
                                if not flag_first:\
                                    _list.append({\
                                        'img': img,\
                                        'title': step_title,\
                                        'substeps': substeps,\
                                    })\
                                    img = ''\
                                    step_title = '<strong>'+pq(ea).text().replace(u'留学益网','')+'</strong>'\
                                    substeps = []\
                                if flag_first:\
                                    step_title = '<strong>'+pq(ea).text().replace(u'留学益网','')+'</strong>'\
                                    flag_first = False\
                            else:\
                                substeps.append(ea.replace('\\n','').replace(u'留学益网',''))    \
\
                    else:\
                        substeps.append(each.text().replace('\\n','').replace(u'留学益网','')) \
                        \
        elif response.doc('.articon > p').size() < 5 and response.doc('.articon > div').size() < 5:\
            #print response.doc('.articon').html()\
            con_list = response.doc('.articon').html().split('<br/>&#13;')\
            i = 0\
            #print len(con_list)\
            for each in con_list:\
                if not each.strip():\
                    continue\
                #print each\
                each = pq(each)\
                #print dir(each)\
                #print each.outerHtml()\
                #print each.html()\
                if not each.html():\
                    continue\
                \
                if u'专注免费' in each.text() or u'版权声明' in each.text() or u'上一页' in each.text():\
                    break \
                i += 1\
                #print i,each.html()\
                if '<img' in each.html():\
                    if desc_img == '':\
                        desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
                if each.text() == 'None' or each.text().strip() == '':\
                    continue\
                #if each.find('strong'):\
                 #   print each.find('strong').outerHtml()\
                if '【' in each.text() or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.outerHtml()) or each.text().strip()[-1] == u'：':\
                    flag_abstract = False\
                    flag_method = True\
                if u'讯' in each.text():\
                    flag_abstract = True\
                if flag_abstract:\
                    steps.append(each.text().replace(u'滴答网',''))\
\
                else:\
                    if '【' in each.text() or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.outerHtml()) or each.text().strip()[-1] == u'：':\
                        #print each.strip()[1]\
                        #print each.html()\
                        if not flag_first:\
                            _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                            })\
                            img = ''\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            substeps = []\
                        if flag_first:\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            flag_first = False    \
\
                    else:\
                        substeps.append(each.text().replace('\\n','').replace(u'滴答网','').replace(u'留学益网','')) \
         \
        else:\
            #print 'ok'\
            for each in response.doc('.articon > *').items():\
                #print each\
                #print dir(each)\
                #print each.outerHtml()\
                #print each.html()\
                if not each.text().strip().replace(u'滴答网',''):\
                    continue\
                \
                if u'专注免费' in each.text() or u'版权声明' in each.text() or u'上一页' in each.text():\
                    break \
                #print i,each.html()\
                if '<img' in each.html():\
                    if desc_img == '':\
                        desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
                if each.text() == 'None' or each.text().strip() == '':\
                    continue\
                #if each.find('strong'):\
                 #   print each.find('strong').outerHtml()\
                if '【' in each.text() or 'strong' in each.outerHtml() or each.text().strip()[-1] == u'：':\
                    flag_abstract = False\
                    flag_method = True\
                if u'讯' in each.text():\
                    flag_abstract = True\
                if flag_abstract:\
                    steps.append(each.text().replace(u'滴答网',''))\
\
                else:\
                    if '【' in each.text() or 'strong' in each.outerHtml() or each.text().strip()[-1] == u'：':\
                        #print each.strip()[1]\
                        #print each.html()\
                        if not flag_first:\
                            _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                            })\
                            img = ''\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            substeps = []\
                        if flag_first:\
                            step_title = '<strong>'+each.text().replace(u'滴答网','').replace(u'留学益网','')+'</strong>'\
                            flag_first = False    \
\
                    else:\
                        substeps.append(each.text().replace('\\n','').replace(u'滴答网','').replace(u'留学益网','')) \
            \
                        \
         \
                        \
        \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': [''.join(steps)],\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        if len(methods[0]['steps']) == 0:\
           return \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [u'留学',],\
            "source": u"滴答",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1470996047.8872
jingyan_gaosanwang$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 21:50:33\
# Project: jingyan_gaosanwang\
\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
\
url_dict = {\
'http://www.gaosan.com/zhukao/1/': u'高考',\
'http://www.gaosan.com/zhuanyejiedu/1/': u'大学',\
}\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for url,bread in url_dict.iteritems():\
            self.crawl(url,save = {'bread':bread},  callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.showMoreNChildren > li').items():\
            if 'index.html' in each.find('a').attr.href:\
                continue\
            _dict = {}\
            _dict['bread'] = response.save['bread']\
            _dict['title'] = each.find('a > b').text().strip()\
            _dict['date'] = each.find('i').text().strip().split()[-1].replace(u'年','-').replace(u'月','-').replace(u'日','')\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('#AspNetPager1 > a').items():\
            self.crawl(each.attr.href, save = response.save, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        #print pattern.match(str).group(0)\
        #print response.doc('.content div.left.d > div').eq(-2).html()\
        for each in response.doc('.content div.left.d > div').eq(-2).children().items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if each.text() == 'None' or not each.html():\
                continue\
            if u'高三网小编推荐你' in each.text() or u'扫一扫' in each.text() or u'相关链接' in each.text():\
                break  \
            if u'点击查看' in each.text() or u'查看更多' in each.text() or u'相关链接' in each.text():\
                continue  \
            if pattern.match(each.text().strip()) or '<strong>' in each.html() or '<h' in each.outerHtml():\
                flag_abstract = False\
                flag_method = True\
            if '<img' in each.html():\
                #print each.html()\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                    #print desc_img\
                img = each.find('img').attr.src\
                each.remove('img')\
            if not each.text():\
                continue\
            if flag_abstract:\
                if '<table' in each.outerHtml():\
                    steps.append(each.outerHtml().replace(u'高三网',''))\
                else:\
                    steps.append(each.text().replace(u'高三网',''))\
            else:\
                if  pattern.match(each.text().strip()) or '<strong>' in each.html() or '<h' in each.outerHtml():\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n','').replace(u'高三网',''))    \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': steps,\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [response.save.get('bread'),],\
            "source": u"高三网",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1472002804.2373
jingyan_sohu_gaokao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-20 18:05:47\
# Project: jingyan_sohu_gaokao\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    header = {\
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://learning.sohu.com/tag/0313/000000313.shtml',headers = self.header, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.published').items():\
            _dict = {}\
            _dict['title'] = each.find('.content-title > a').text().strip()\
            _dict['date'] = each.find('.time').text().strip().split(u'日')[0].replace(u'年','-').replace(u'月','-')\
            self.crawl(each.find('.content-title > a').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\
        #翻页\
        for i in range(263,362,1):\
            self.crawl('http://learning.sohu.com/tag/0313/000000313_'+str(i)+'.shtml',headers = self.header, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        pattern2 = re.compile(ur'[0123456789]{1,2}[：].*')\
        pattern3 = re.compile(r'[0123456789]{2}.*')\
        pattern4 = re.compile(r'<strong>')\
        for each in response.doc('#contentText > p').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if not each.html():\
                continue\
            if u'编辑推荐' in each.text() or u'聚铭师教育' in each.text() or u'相关链接' in each.text():\
                break \
            \
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                img = each.find('img').attr.src\
                each.remove('img')\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            #if each.find('strong'):\
             #   print each.find('strong').outerHtml()\
            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                flag_abstract = False\
                flag_method = True\
            \
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n',''))    \
        for each in response.doc('.text > p').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if not each.html():\
                continue\
            if u'编辑推荐' in each.text() or u'聚铭师教育' in each.text() or u'相关链接' in each.text():\
                break  \
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                img = each.find('img').attr.src\
                each.remove('img')\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            #if each.find('strong'):\
             #   print each.find('strong').outerHtml()\
            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or '<em>' in each.html() or each.text().strip()[-1] == u'：':\
                flag_abstract = False\
                flag_method = True\
            \
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or '<em>' in each.html() or each.text().strip()[-1] == u'：':\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n','')) \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': steps,\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        if len(methods[0]['steps']) == 0:\
           return \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [u'高考',],\
            "source": u"搜狐",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1471592197.8046
jingyan_sohu_gaokao_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-19 15:36:33\
# Project: jingyan_sohu_gaokao_inc\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    header = {\
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://learning.sohu.com/tag/0313/000000313.shtml',headers = self.header, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.published').items():\
            _dict = {}\
            _dict['title'] = each.find('.content-title > a').text().strip()\
            _dict['date'] = each.find('.time').text().strip().split(u'日')[0].replace(u'年','-').replace(u'月','-')\
            self.crawl(each.find('.content-title > a').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\
        #翻页\
        #for i in range(263,362,1):\
            #self.crawl('http://learning.sohu.com/tag/0313/000000313_'+str(i)+'.shtml',headers = self.header, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        pattern2 = re.compile(ur'[0123456789]{1,2}[：].*')\
        pattern3 = re.compile(r'[0123456789]{2}.*')\
        pattern4 = re.compile(r'<strong>')\
        for each in response.doc('#contentText > p').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if not each.html():\
                continue\
            if u'编辑推荐' in each.text() or u'聚铭师教育' in each.text() or u'相关链接' in each.text():\
                break \
            \
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                img = each.find('img').attr.src\
                each.remove('img')\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            #if each.find('strong'):\
             #   print each.find('strong').outerHtml()\
            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                flag_abstract = False\
                flag_method = True\
            \
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n',''))    \
        for each in response.doc('.text > p').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if not each.html():\
                continue\
            if u'编辑推荐' in each.text() or u'聚铭师教育' in each.text() or u'相关链接' in each.text():\
                break  \
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                img = each.find('img').attr.src\
                each.remove('img')\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            #if each.find('strong'):\
             #   print each.find('strong').outerHtml()\
            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or '<em>' in each.html() or each.text().strip()[-1] == u'：':\
                flag_abstract = False\
                flag_method = True\
            \
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or '<em>' in each.html() or each.text().strip()[-1] == u'：':\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n','')) \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': steps,\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        if len(methods[0]['steps']) == 0:\
           return \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [u'高考',],\
            "source": u"搜狐",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1471592283.3611
jingyan_sohu_liuxue$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-20 18:13:10\
# Project: jingyan_sohu_liuxue\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    header = {\
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://learning.sohu.com/tag/0073/000000073.shtml',save = {'tag':1}, headers = self.header, callback=self.index_page)\
        self.crawl('http://learning.sohu.com/tag/0249/000000249.shtml',save = {'tag':2},headers = self.header, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.published').items():\
            _dict = {}\
            _dict['title'] = each.find('.content-title > a').text().strip()\
            _dict['date'] = each.find('.time').text().strip().split(u'日')[0].replace(u'年','-').replace(u'月','-')\
            self.crawl(each.find('.content-title > a').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\
        #翻页\
        if response.save.get('tag') == 1:\
            for i in range(356,455,1):\
                self.crawl('http://learning.sohu.com/tag/0073/000000073_'+str(i)+'.shtml',save = response.save, headers = self.header, callback=self.index_page)\
        if response.save.get('tag') == 2:\
            for i in range(44,143,1):\
                self.crawl('http://learning.sohu.com/tag/0249/000000249_'+str(i)+'.shtml',save = response.save, headers = self.header, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        pattern2 = re.compile(ur'[0123456789]{1,2}[：].*')\
        pattern3 = re.compile(r'[0123456789]{2}.*')\
        pattern4 = re.compile(r'<strong>')\
        for each in response.doc('#contentText > p').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if not each.html():\
                continue\
            if u'编辑推荐' in each.text() or u'聚铭师教育' in each.text() or u'相关链接' in each.text():\
                break \
            \
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                img = each.find('img').attr.src\
                each.remove('img')\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            #if each.find('strong'):\
             #   print each.find('strong').outerHtml()\
            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                flag_abstract = False\
                flag_method = True\
            \
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n',''))    \
        for each in response.doc('.text > p').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if not each.html():\
                continue\
            if u'编辑推荐' in each.text() or u'聚铭师教育' in each.text() or u'相关链接' in each.text():\
                break  \
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                img = each.find('img').attr.src\
                each.remove('img')\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            #if each.find('strong'):\
             #   print each.find('strong').outerHtml()\
            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or '<em>' in each.html() or each.text().strip()[-1] == u'：':\
                flag_abstract = False\
                flag_method = True\
            \
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or '<em>' in each.html() or each.text().strip()[-1] == u'：':\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n','')) \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': steps,\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        if len(methods[0]['steps']) == 0:\
           return \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [u'留学',],\
            "source": u"新东方",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1469098533.8882
jingyan_sohu_liuxue_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-19 15:38:51\
# Project: jingyan_sohu_liuxue_inc\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    header = {\
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://learning.sohu.com/tag/0073/000000073.shtml',save = {'tag':1}, headers = self.header, callback=self.index_page)\
        self.crawl('http://learning.sohu.com/tag/0249/000000249.shtml',save = {'tag':2},headers = self.header, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.published').items():\
            _dict = {}\
            _dict['title'] = each.find('.content-title > a').text().strip()\
            _dict['date'] = each.find('.time').text().strip().split(u'日')[0].replace(u'年','-').replace(u'月','-')\
            self.crawl(each.find('.content-title > a').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\
        #翻页\
        #if response.save.get('tag') == 1:\
         #   for i in range(356,455,1):\
                #self.crawl('http://learning.sohu.com/tag/0073/000000073_'+str(i)+'.shtml',save = response.save, headers = self.header, callback=self.index_page)\
 #       if response.save.get('tag') == 2:\
  #          for i in range(44,143,1):\
                #self.crawl('http://learning.sohu.com/tag/0249/000000249_'+str(i)+'.shtml',save = response.save, headers = self.header, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        pattern2 = re.compile(ur'[0123456789]{1,2}[：].*')\
        pattern3 = re.compile(r'[0123456789]{2}.*')\
        pattern4 = re.compile(r'<strong>')\
        for each in response.doc('#contentText > p').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if not each.html():\
                continue\
            if u'编辑推荐' in each.text() or u'聚铭师教育' in each.text() or u'相关链接' in each.text():\
                break \
            \
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                img = each.find('img').attr.src\
                each.remove('img')\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            #if each.find('strong'):\
             #   print each.find('strong').outerHtml()\
            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                flag_abstract = False\
                flag_method = True\
            \
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or each.text().strip()[-1] == u'：':\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n',''))    \
        for each in response.doc('.text > p').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if not each.html():\
                continue\
            if u'编辑推荐' in each.text() or u'聚铭师教育' in each.text() or u'相关链接' in each.text():\
                break  \
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                img = each.find('img').attr.src\
                each.remove('img')\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            #if each.find('strong'):\
             #   print each.find('strong').outerHtml()\
            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or '<em>' in each.html() or each.text().strip()[-1] == u'：':\
                flag_abstract = False\
                flag_method = True\
            \
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find('strong').outerHtml().strip() if each.find('strong') else each.text()) or '<em>' in each.html() or each.text().strip()[-1] == u'：':\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n','')) \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': steps,\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        if len(methods[0]['steps']) == 0:\
           return \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [u'留学',],\
            "source": u"搜狐",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1471592434.2140
jingyan_wikiHow$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-27 10:30:13\
# Project: wikiHow_jingyan\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://zh.wikihow.com/Special:Sitemap', callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('#catentry a').items():\
            self.crawl(each.attr.href, save={'tag': each.text()}, callback=self.index_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('td a').items():\
            \
            self.crawl(each.attr.href, save={'tag': response.save['tag']}, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        steps = []\
        for each in response.doc('.steps').items():\
            method = each.find('h3').text()\
            if not method:\
                method = each.find('h2').text()\
\
            _list = []\
            for step in each.find('ol > li').items():\
                try:\
                    img = step.find('a').html().split('data-src="')[-1].split('"')[0]\
                except:\
                    img = ''\
                step_title = step.find('.whb').text()\
                step_list = []\
                for _step in step.find('ul > li').items():\
                    step_list.append((_step.text()))\
                _list.append({\
                    "img": img,\
                    "title": step_title,\
                    "substeps": step_list,\
                })\
            steps.append({"data": _list, "title": method})\
        summary = {'title': response.doc(u'.小提示 h2').text(),\
                   'steps': [v.text() for v in response.doc(u'.小提示 li').items()]\
                   }\
        abstract = {'title': '',\
                    'img': '',\
                    'steps': [v.text() for v in response.doc('#intro > p').items()][-1]\
                    }\
        return {\
            "url": response.url,\
            "title": response.doc('[itemprop="name"] > a').text(),\
            "methods": steps,\
            "abstract": abstract,\
            "summary": summary,,\
            "date": '',\
            "bread": [response.save['tag'],],\
            "source": "wikiHow",\
            "class": 36,\
            "subject": '经验',\
            "data_weight": 0,\
        }\
$$$$$1467344591.6652
jingyan_xdf$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-20 14:23:32\
# Project: jingyan_xdf\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://xiaoxue.xdf.cn/list_1220_1.html', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.txt_lists01 > li').items():\
            _dict = {}\
            _dict['title'] = each.find('a').text().strip()\
            _dict['date'] = each.find('.time').text().strip()\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.teacherNum > a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        #print pattern.match(str).group(0)\
        for each in response.doc('.air_con > *').items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if each.text() == 'None' or each.text().strip() == '':\
                continue\
            if u'编辑推荐' in each.text() or u'扫一扫' in each.text() or u'相关链接' in each.text():\
                break    \
            if pattern.match(each.text().strip()) or '<strong>' in each.html() or '<h' in each.outerHtml():\
                flag_abstract = False\
                flag_method = True\
            if '<img' in each.html():\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                    img = each.find('img').attr.src\
                    each.remove('img')\
            if flag_abstract:\
                steps.append(each.text())\
\
            else:\
                if  pattern.match(each.text().strip()) or '<strong>' in each.html() or '<h' in each.outerHtml():\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n',''))    \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': steps,\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.save.get('date'),\
            "bread": [u'小学',],\
            "source": u"新东方",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1468998052.0216
jingyan_xuexila$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-24 10:00:50\
# Project: jingyan_xuexila\
\
from pyspider.libs.base_handler import *\
import random\
from pyquery import PyQuery as pq\
import re\
\
url_list = [\
'http://www.xuexila.com/shenghuo/',\
'http://www.xuexila.com/naoli/',\
'http://www.xuexila.com/zhishi/',\
]\
\
url2_list = [\
'http://www.xuexila.com/diannao/',\
]\
url3_list = [\
'http://www.xuexila.com/tiyu/',\
]\
bread_dict = {\
    u'生活小常识':[u'生活技巧'],\
}\
class Handler(BaseHandler):\
    crawl_config = {\
    'itag':'2',\
    'headers':{\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for url in url_list:\
            bread = [u'生活技巧',]\
            self.crawl(url,headers = self.crawl_config['headers'], save = {'bread':bread}, callback=self.index_page)\
        for url in url2_list:\
            bread = [u'电脑数码',]\
            self.crawl(url,headers = self.crawl_config['headers'], save = {'bread':bread}, callback=self.index_page)\
        for url in url3_list:\
            bread = [u'体育运动',]\
            self.crawl(url,headers = self.crawl_config['headers'], save = {'bread':bread}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.box > div').items():\
            if u'最强大脑' in each.find('b > a').text() or u'一站到底' in each.find('b > a').text():\
                continue\
            self.crawl(each.find('.i_more > a').attr.href, save = response.save,  headers = self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.r_list a').items():\
            _dict = {}\
            _dict['title'] = each.text().strip()\
            _dict['bread'] = response.save['bread']\
            self.crawl(each.attr.href, save = _dict,headers = self.crawl_config['headers'], callback=self.detail_page)\
            \
        for each in response.doc('.box > div').items():\
            self.crawl(each.find('.i_more > a').attr.href, save = response.save,  headers = self.crawl_config['headers'], callback=self.list1_page)\
        \
        #翻页\
        for each in response.doc('div > li > a').items():\
            self.crawl(each.attr.href, save = response.save, headers = self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list1_page(self, response):\
        for each in response.doc('.r_list a').items():\
            _dict = {}\
            _dict['title'] = each.text().strip()\
            _dict['bread'] = response.save['bread']\
            self.crawl(each.attr.href, save = _dict,headers = self.crawl_config['headers'], callback=self.detail_page)\
\
        #翻页\
        for each in response.doc('div > li > a').items():\
            self.crawl(each.attr.href, save = response.save, headers = self.crawl_config['headers'], callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        abstract = {}\
        #abstract['steps'] = []\
        #abstract['img'] = ''\
        #abstract['title'] = ''\
        methods = []\
        #_dict = {}\
        #_dict['steps'] = []\
        #_dict['title'] = u'方法/步骤'\
        flag_abstract = True\
        flag_method = False\
        flag_first = True\
        steps = []\
        _list = []\
        img = ''\
        desc_img = ''\
        step_title = ''\
        substeps = []\
        pattern = re.compile(ur'[一二三四五六七八九十][、].*')\
        #print pattern.match(str).group(0)\
        #print response.doc('.content div.left.d > div').eq(-2).html()\
        \
        for each in response.doc('#contentText').children().items():\
            #print dir(each)\
            #print each.outerHtml()\
            #print each.html()\
            if u'高三网小编推荐你' in each.text() or '<hr />'==each.outerHtml().strip() or u'猜你喜欢' in each.text() or 'BAIDU_CLB' in each.text() or u'点击进入' in each.text() or u'相关文章：' in each.text() or u'的人还看了：' in each.text():\
                break \
            if each.text() == 'None' or not each.html():\
                continue\
             \
            if u'点击查看' in each.text() or u'查看更多' in each.text() or u'相关链接' in each.text():\
                continue  \
            if pattern.match(each.text().strip()) or each.html().strip().startswith('<strong>') or each.text().strip().startswith(u'【') or '<h' in each.outerHtml():\
                flag_abstract = False\
                flag_method = True\
            if '<img' in each.html():\
                #print each.html()\
                if desc_img == '':\
                    desc_img = each.find('img').attr.src\
                    #print desc_img\
                img = each.find('img').attr.src\
                each.remove('img')\
            if not each.text():\
                continue\
            if flag_abstract:\
                if '<table' in each.outerHtml():\
                    steps.append(each.outerHtml().replace(u'高三网',''))\
                else:\
                    steps.append(each.text().replace(u'高三网',''))\
            else:\
                if  pattern.match(each.text().strip()) or each.html().strip().startswith('<strong>') or each.text().strip().startswith(u'【') or '<h' in each.outerHtml():\
                    #print each.strip()[1]\
                    if not flag_first:\
                        _list.append({\
                            'img': img,\
                            'title': step_title,\
                            'substeps': substeps,\
                        })\
                        img = ''\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        substeps = []\
                    if flag_first:\
                        step_title = '<strong>'+each.text()+'</strong>'\
                        flag_first = False\
                else:\
                    substeps.append(each.text().replace('\\n','').replace(u'高三网',''))    \
        _list.append({\
            'img': img,\
            'title': step_title,\
            'substeps': substeps,\
        })\
        if desc_img == '':\
            desc_img = 'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg'%(random.randrange(1, 20))\
        if flag_method:\
            methods.append({'title': u'方法/步骤', 'steps': _list})\
            abstract = {\
                'title': '',\
                'steps': steps,\
                'img': desc_img,\
            }\
        else:\
            _list1 = []\
            for v in steps[1:]:\
                _list1.append({\
                    'img': '',\
                    'title': v,\
                    'substeps': '',\
                })\
            if len(steps) == 1:\
                _list1.append({\
                    'img': '',\
                    'title': steps[0],\
                    'substeps': '',\
                })\
            methods.append({'title': u'方法/步骤', 'steps': _list1})\
            if len(steps) == 0:\
                steps = ['']\
            abstract = {\
                'title': '',\
                'steps': [steps[0]],\
                'img': desc_img,\
            }\
        #_dict1 = {}\
        #_dict1['substeps'] = []\
        #_dict1['img'] = ''\
        \
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "methods": methods,\
            "abstract": abstract,  \
            "date": response.doc('.read_people_time').text().split(u'：')[-1],\
            "bread": response.save.get('bread'),\
            "source": u"学习啦",\
            "class": 36,\
            "subject": u'经验',\
            "data_weight": 0,\
        }$$$$$1472546236.1874
jinrong_chinaacc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-18 17:20:52\
# Project: jinrong_chinaacc\
\
from pyspider.libs.base_handler import *\
import re\
\
urls = {\
    'http://www.chinaacc.com/acca/hyxw/': ['ACCA'],\
    'http://www.chinaacc.com/zhongjijingjishi/zhengcetiaojian/': [u'经济师'],\
}\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.2',\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for url,bread in urls.iteritems():\
            self.crawl(url, save = {'bread': bread}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('li > .fl').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').text()\
            if u'中华会计' in each.find('a').text():\
                continue\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
        for each in response.doc('.nr li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').text()\
            if u'中华会计' in each.find('a').text():\
                continue\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
        #翻页\
        #for each in response.doc('divpagestrcjzc a').items():\
        #    self.crawl(each.attr.href, save = response.save, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        date = ''\
        source = u'中华会计网校'\
        if response.doc('.mark'):\
            date = response.doc('.mark').text().strip().split()[0]\
            source = response.doc('.mark').text().strip().split(u'来源：')[1].split()[0]\
        \
        cover = '' \
        pattern = re.compile(r'src=".*?"')\
        _list = []\
        for each in response.doc('#fontzoom').children().items():\
            if not each.html():\
                continue\
            #print each.html()\
            if '<script>' in each.html() or u'中华会计网校' in each.text():\
                continue\
            if u'我要纠错' in each.text() or u'责任编辑' in each.text() or u'编辑推荐' in each.text() or u'点击阅读' in each.text():\
                break\
            if '<img' in each.html():\
                if cover == '':\
                    cover = pattern.search(each.html()).group(0).replace('src="','').replace('"','')\
                _list.append('<p>'+ each.html()+'</p>')\
            elif each.text().strip() != '':\
                _list.append('<p>'+each.text()+'</p>')\
        \
        content = ''.join([v for v in _list if v])\
        #print content\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread": response.save.get("bread"),\
            "cover": cover,\
            "content": content,\
            "source": source,\
            "data_weight": 0,\
            "class": 33,\
            "subject": u"金融",\
            "date": date,\
\
        }$$$$$1471334568.9845
jinrong_xinlang$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-18 15:32:42\
# Project: jinrong_xinlang\
\
from pyspider.libs.base_handler import *\
import re\
\
urls = {\
    'http://roll.finance.sina.com.cn/finance/zq1/index_': [u'证券从业'],\
    'http://roll.finance.sina.com.cn/finance/yh/index_': [u'银行从业'],\
    'http://roll.finance.sina.com.cn/finance/jj4/index_': [u'基金从业'],\
}\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.2',\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for url,bread in urls.iteritems():\
            for i in range(1, 2):\
                self.crawl(url + str(i) +'.shtml', save = {'bread': bread}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.list_009 > li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('a').text()\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        date = ''\
        source = u'新浪财经'\
        if response.doc('#pub_date'):\
            date = response.doc('#pub_date').text().strip().split()[0].split(u'日')[0].replace(u'年','-').replace(u'月','-')\
            source = response.doc('#media_name').text().strip()\
        elif response.doc('.time-source'):\
            date = response.doc('.time-source').text().strip().split()[0].split(u'日')[0].replace(u'年','-').replace(u'月','-')\
            source = response.doc('.time-source').text().strip().split()[1]\
        elif response.doc('.articalTitle > .SG_txtc'):\
            date = response.doc('.articalTitle > .SG_txtc').text().strip().split()[0].replace('(','')\
        cover = '' \
        pattern = re.compile(r'src=".*?"')\
        _list = []\
        flag_first = False\
        for each in response.doc('.article_16').children().items():\
            if not each.html():\
                continue\
            #print each.html()\
            flag_first = True\
            if '<script>' in each.html() or u'新浪财经' in each.text() or 'finance_app_zqtg' in each.html():\
                continue\
            if u'新浪声明' in each.text() or u'新浪财经股吧' in each.text():\
                break\
            if '<img' in each.html():\
                if cover == '':\
                    cover = pattern.search(each.html()).group(0).replace('src="','').replace('"','')\
                _list.append('<p>'+ each.html()+'</p>')\
            elif each.text().strip() != '':\
                _list.append('<p>'+each.text()+'</p>')\
        if not flag_first:\
            for each in response.doc('#artibody').children().items():\
                if not each.html():\
                    continue\
                flag_first = True\
                #print each.html()\
                if '<script>' in each.html() or u'新浪财经' in each.text() or 'finance_app_zqtg' in each.html():\
                    continue\
                if u'新浪声明' in each.text() or u'新浪财经股吧' in each.text():\
                    break\
                if '<img' in each.html():\
                    if cover == '':\
                        cover = pattern.search(each.html()).group(0).replace('src="','').replace('"','')\
                    _list.append('<p>'+ each.html()+'</p>')\
                elif each.text().strip() != '':\
                    _list.append('<p>'+each.text()+'</p>')\
        if not flag_first:\
            for each in response.doc('.newfont_family').children().items():\
                if not each.html():\
                    continue\
                #print each.html()\
                if '<script>' in each.html() or u'新浪财经' in each.text() or 'finance_app_zqtg' in each.html():\
                    continue\
                if u'新浪声明' in each.text() or u'新浪财经股吧' in each.text():\
                    break\
                if '<img' in each.html():\
                    if cover == '':\
                        cover = pattern.search(each.html()).group(0).replace('src="','').replace('"','')\
                    _list.append('<p>'+ each.html()+'</p>')\
                elif each.text().strip() != '':\
                    _list.append('<p>'+each.text()+'</p>')\
        content = ''.join([v for v in _list if v])\
        #print content\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread": response.save.get("bread"),\
            "cover": cover,\
            "content": content,\
            "source": source,\
            "data_weight": 0,\
            "class": 33,\
            "subject": u"金融",\
            "date": date,\
\
        }$$$$$1471334586.9698
jita_10_youku_base$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-19 14:57:23\
# Project: jita_10_youku_base\
\
from pyspider.libs.base_handler import *\
import time\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.soku.com/search_video/q_%E5%90%89%E4%BB%96_limitdate_0?site=14&_lg=10&orderby=3', callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('.sk_pager a').items():\
            url = each.attr.href\
            self.crawl(url, callback=self.index_page)\
        for each in response.doc('.v').items():\
            cover = each.find('img').attr.src\
           # print cover\
            url = each.find('.v-meta-title > a').attr.href\
            title = each.find('.v-meta-title > a').text()\
            self.crawl(url, save={'title': title, 'cover': cover},  callback=self.detail_page)\
\
   # @config(priority=2)\
    @config(age=1*1)\
    def detail_page(self, response):\
        try:\
            url = response.url\
        #http://v.youku.com/v_show/id_XMTUxNzMxODAyMA==.html?from=s1.8-1-1.2\
            id = url.split('id_')[1].split('==')[0]\
            video_url_element = 'http://player.youku.com/embed/%s' %(id)\
            video_url = []\
            video_url.append(video_url_element)\
            title = response.save['title']\
            cover = response.save['cover']\
           # title = response.doc('.base_info > .title').text()\
            subject = u'吉他'\
            source = 'youku.com'\
            publish_time = time.strftime('%Y-%m-%d',time.localtime(time.time()))\
        except:\
            return None\
        if len(video_url) == 0:\
            return None\
        return {\
            "url": url,\
            "video_url":video_url,\
            "title": title,\
            "subject": subject,\
            "source": source,\
            "publish_time": publish_time,\
            "cover": cover,\
            "class": 10,\
            "data_weight":0,\
        }\
$$$$$1468374380.6862
jzgc_zhengbao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-30 18:17:48\
# Project: jzgc_zhengbao\
\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
type_dict = {\
            'chuji':[u'财会经济',u'初级会计师'],\
            'zhongji':[u'财会经济',u'中级会计师'],\
            'shiwushi':[u'财会经济',u'注册税务师'],\
            'zhukuai':[u'财会经济',u'注册会计师'],\
            'gaoji':[u'财会经济',u'高级会计师'],\
            'congye':[u'财会经济',u'会计从业'],\
            u'注册会计师':[u'财会经济',u'注册会计师'],\
            u'经济师':[u'财会经济',u'经济师'],\
            u'初级会计职称':[u'财会经济',u'初级会计师'],\
            u'中级会计职称':[u'财会经济',u'中级会计师'],\
            u'税务师':[u'财会经济',u'注册税务师'],\
            u'统计师':[u'财会经济',u'统计师'],\
            u'审计师':[u'财会经济',u'审计师'],\
            u'高级经济师':[u'财会经济',u'经济师'],\
            u'高级会计':[u'财会经济',u'高级会计师'],\
            u'理财规划师':[u'财会经济',u'理财规划师'],\
\
\
            u'英语四级':[u'外语考试',u'英语四六级'],\
            u'英语六级':[u'外语考试',u'英语四六级'],\
            u'雅思':[u'外语考试',u'雅思'],\
            u'托福':[u'外语考试',u'托福'],\
            u'职称英语':[u'外语考试',u'职称英语'],\
            u'商务英语':[u'外语考试',u'商务英语'],\
            u'公共英语':[u'外语考试',u'公共英语'],\
            u'日语':[u'外语考试',u'日语'],\
            u'GRE考试':[u'外语考试',u'GRE考试'],\
            u'专四专八':[u'外语考试',u'专四专八'],\
            u'口译笔译':[u'外语考试',u'口译笔译'],\
\
            u'一级建造师':[u'建筑工程',u'一级建造师'],\
            u'二级建造师':[u'建筑工程',u'二级建造师'],\
            u'咨询工程师':[u'建筑工程',u'咨询工程师'],\
            u'造价工程师':[u'建筑工程','造价工程师'],\
            u'结构工程师':[u'建筑工程','结构工程师'],\
            u'物业管理':[u'建筑工程',u'物业管理师'],\
            u'城市规划':[u'建筑工程',u'城市规划师'],\
            u'给排水工程':[u'建筑工程',u'给排水工程'],\
            u'电气工程':[u'建筑工程',u'电气工程师'],\
            u'公路监理师':[u'建筑工程',u'公路监理师'],\
            u'消防工程师':[u'建筑工程',u'消防工程师'],\
            u'消防':[u'建筑工程',u'消防工程师'],\
\
            u'物流师':[u'职业资格',u'物流师'],\
            u'人力资源':[u'职业资格',u'人力资源'],\
            u'心理咨询师':[u'职业资格',u'心理咨询师'],\
            u'公共营养师':[u'职业资格',u'公共营养师'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'证券从业资格':[u'职业资格',u'证券经纪人'],\
            u'电子商务师':[u'职业资格',u'电子商务'],\
            u'期货从业':[u'职业资格',u'期货从业'],\
            u'教师资格':[u'职业资格',u'教师资格'],\
            u'管理咨询师':[u'职业资格',u'管理咨询师'],\
            u'导游证':[u'职业资格',u'导游证'],\
            \
            u'英语':[u'学历教育',u'考研'],\
            u'数学':[u'学历教育',u'考研'],\
            u'政治':[u'学历教育',u'考研'],\
            u'专业课':[u'学历教育',u'考研'],\
\
            u'执业医师':[u'医学卫生',u'执业医师'],\
            u'执业药师':[u'医学卫生',u'执业药师'],\
            u'临床医师':[u'医学卫生',u'临床执业'],\
            u'中医医师':[u'医学卫生',u'中医执业'],\
            u'中西医医师':[u'医学卫生',u'中西医执业'],\
            u'中医助理':[u'医学卫生',u'中医助理'],\
            u'中西医助理':[u'医学卫生',u'中西医助理'],\
            u'主治':[u'医学卫生',u'主治'],\
            u'检验':[u'医学卫生',u'检验'],\
            u'执业护士资格':[u'医学卫生',u'执业护士'],\
\
\
            u'成人高考':[u'学历教育',u'成人高考'],\
            u'自学考试':[u'学历教育',u'自考'],\
            u'MBA考试':[u'学历教育',u'MBA'],\
            u'法律硕士':[u'学历教育',u'法律硕士'],\
            u'专升本':[u'学历教育',u'专升本'],\
            #u'':[u'学历教育',u'工程硕士'],\
            u'MPA考试':[u'学历教育',u'公共硕士'],\
            #u'':[u'学历教育',u'考研'],\
}\
\
\
add_url =[\
    'zhenti/',\
    'moniti/',\
    'ziliao/',\
    'jingyan/',\
]\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.jianshe99.com/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.category > a').items():\
            if each.text() == '其他' or each.text() == '消防':\
                continue\
            _dict = {}\
            if each.text() in type_dict.keys():\
                _dict['bread'] = type_dict[each.text()]\
            else:\
                _dict['bread'] = ['建筑工程',each.text()]\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for val in add_url:\
            self.crawl(response.url + val, save = response.save, callback=self.list_page1)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page1(self, response):      \
        for each in response.doc('.mleft li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('.fl > a').text()\
            if '报名' in _dict['title'] or '名师' in _dict['title']:\
                continue\
            _dict['date'] = each.find('.fr').text().replace('[','').replace(']','')\
            self.crawl(each.find('.fl > a').attr.href, save = _dict, callback=self.detail_page)\
        \
        #翻页 \
        for each in response.doc('divpagestr2016 a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
        \
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('#fontzoom').items():\
            for each1 in each.find('p').items():\
                if u'全网首发' in each1.text():\
                    continue\
                if u'估分更准确' in each1.text():\
                    continue\
                if u'独家' in each1.text():\
                    continue\
                if u'点击查看' in each1.text():\
                    continue\
                if u'点击参与' in each1.text():\
                    continue\
                if u'到建设工程教育网论坛' in each1.text():\
                    continue\
                if u'特色班' in each1.text():\
                    break\
                if u'近几年' in each1.text():\
                    break\
                if u'考友咨询' in each1.text():\
                    break\
                if u'更多' in each1.text() and u'资讯' in each1.text():\
                    break\
                if u'推荐信息' in each1.text() or u'更多推荐' in each1.text():\
                    break\
                if u'免费在线测试' in each1.text():\
                    continue\
                if u'在线测试系统' in each1.text():\
                    continue\
                if u'转载请注明出处' in each1.text():\
                    continue\
                if u'责任编辑' in each1.text():\
                    break\
                else:\
                    list.append(each1.remove('a').html())\
            for each1 in each.find('div').items():\
                if u'点击查看' in each1.text():\
                    continue\
                if u'点击参与' in each1.text():\
                    continue\
                if u'到建设工程教育网论坛' in each1.text():\
                    continue\
                if u'特色班' in each1.text():\
                    break\
                if u'近几年' in each1.text():\
                    break\
                if u'考友咨询' in each1.text():\
                    break\
                if u'更多' in each1.text() and u'资讯' in each1.text():\
                    break\
                if u'推荐信息' in each1.text() or u'更多推荐' in each1.text():\
                    break\
                if u'免费在线测试' in each1.text():\
                    continue\
                if u'在线测试系统' in each1.text():\
                    continue\
                if u'转载请注明出处' in each1.text():\
                    continue\
                if u'责任编辑' in each1.text():\
                    break\
                else:\
                    list.append(each1.remove('a').html())\
            \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            return {\
                "url": response.url,\
                "title": response.save.get('title'),\
                "content": content.replace('\\r','').replace('\\n','').replace('\\t','').replace(u'建设工程教育网','').replace('【 】',''),\
                "subject": u"考证题库",\
                "bread": response.save.get('bread'), \
                "date": response.save.get('date'),\
                "tdk_keywords": str(response.doc('meta[http-equiv="keywords"]').eq(0).attr.content).replace(u'建设工程教育网','').replace('None','') + str(response.doc('meta[name="keywords"]').eq(0).attr.content).replace(u'建设工程教育网','').replace('None',''),\
                "tdk_desc": str(response.doc('meta[http-equiv="description"]').eq(0).attr.content).replace(u'建设工程教育网',u'').replace('建设网校', '').replace('None','') + str(response.doc('meta[name="description"]').eq(0).attr.content).replace(u'建设工程教育网','').replace('建设网校', '').replace('None',''),\
                "tdk_title":response.doc('head > title').eq(0).text().replace(u'建设工程教育网','') + u' 跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"建设工程教育网",\
            }\
$$$$$1467943284.4006
kaoyan_chinakaoyan_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-11 19:41:56\
# Project: kaoyan_chinakaoyan\
\
from pyspider.libs.base_handler import *\
from random import randrange\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        page_list = {'11': [u'政策新闻'],\
                     '12': [u'报考指南'],\
                     '14': [u'招生信息'],\
                     '13': [u'择选院校'],\
                     '91': [u'专业介绍'],\
                     '22': [u'考研分数线'],\
                     '24': [u'调剂指南'],\
                     '26': [u'考研英语'],\
                     '25': [u'考研政治'],\
                     '27': [u'考研数学'],\
                     '32': [u'计算机',u'专业课'],\
                     '33': [u'教育学',u'专业课'],\
                     '34': [u'心理学',u'专业课'],\
                     '35': [u'历史学',u'专业课'],\
                     '47': [u'其他',u'专业课'],\
                     '65': [u'大纲解析'],\
                     '64': [u'大纲解析'],\
                     '63': [u'大纲解析'],\
                     '62': [u'大纲解析'],\
                     '61': [u'大纲解析'],\
                     '49': [u'成绩查询'],\
                     '50': [u'研招动态'],\
                     '9': [u'研招动态'],\
                     '31': [u'研招动态'],\
                     '10': [u'研招动态'],\
                     }\
        for k,v in page_list.iteritems():\
            self.crawl('http://www.chinakaoyan.com/info/list/ClassID/%s.shtml'%k, save={'bread': v}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.uc_ulbox a').items():\
            self.crawl(each.attr.href, save={'bread': bread}, callback=self.detail_page)\
\
        #for each in response.doc('.dajax > a').items():\
        #    self.crawl(each.attr.href,  save={'bread': bread}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        for each in response.doc('.arcont > .cont').items():\
            content = each.html().replace('\\t','').replace('\\r','').replace('\\n','').strip()\
        if not content:\
            return None\
        dates = response.doc('.time').text().split()\
        for d in dates:\
            if d[:3] == '201':\
                break\
        #bread = set(response.doc('.change_con a').text().split()[1:-1])\
        #bread.add(response.save['bread'])\
        try:\
            source = response.doc('.time').remove('a').text().split()[0].split(u'：')[-1]\
        except:\
            source = u'中国考研网'\
        return {\
            "url": response.url,\
            "title": response.doc('h1').text(),\
            "date": d,\
            "subject": u'考研',\
            "source": source,\
            "content": content,\
            "bread": response.save['bread'],\
            "cover": 'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg'%(randrange(20)),\
            "class": 46,\
            "data_weight": 0,\
        }\
$$$$$1471332804.5242
kaoyan_chsi$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-03-31 17:01:19\
# Project: gaokao_chsi_com\
\
from pyspider.libs.base_handler import *\
import pyquery\
from random import randrange\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    url_content = {\
        'kyzx/kydt/': u'研招动态',\
        'kyzx/jyxd/': u'备考经验',\
        'kyzx/politics/': u'考研政治',\
        'kyzx/en/': u'考研英语',\
        'kyzx/math/': u'考研数学',\
        'kyzx/zyk/': u'专业课',\
        'kyzx/yxzc/': u'政策新闻',\
        'kyzx/zcdh/': u'政策新闻',\
    }\
        \
    @every(minutes=1 * 60)\
    def on_start(self):\
        url_prefix = 'http://yz.chsi.com.cn/'\
        for key in self.url_content:\
            url = url_prefix + key\
            self.crawl(url, save={'key': key}, callback=self.index_page)\
\
    @config(age=24 * 60)\
    def index_page(self, response):\
        key = response.save['key']\
        for each in response.doc('.news_list li').items():\
            url = each.find('a').attr.href\
            date = each.find('.spanTime').text()\
            self.crawl(url, save = {'key': key, 'date': date}, callback=self.detail_page)\
        '''\
        for each in response.doc('form a').items():\
            \
            self.crawl(each.attr.href, save = {'key': key}, callback=self.index_page)\
        '''\
        \
\
    @config(priority=2)\
    def detail_page(self, response):\
        key = response.save['key']\
        url = response.url\
        title = response.doc('title').text()\
        dt = response.save['date']\
        '''\
        content = ''\
        content_list = []\
        for item in response.doc('.left > * > p'):\
            content_list.append(pyquery.PyQuery(item).html())\
        content = ''.join(["<p>%s</p>"%v for v in content_list])\
        '''\
        content = response.doc('#article_dnull').html()\
        if not content:\
            return None\
        bread = self.url_content[key]\
        subject = u'考研'\
        try:\
            source = response.doc('.sate_info span').text().split(u'来源：')[-1]\
        except:\
            source = u'研究生招生信息网'\
        return {\
            "url": url,\
            "title": title,\
            "date": dt,\
            "content": content,\
            "bread": [bread,],\
            "subject": subject,\
            "source": source,\
            "class": 46,\
            "data_weight": 0,\
            "cover": 'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg'%(randrange(20)),\
        }\
$$$$$1471332811.1229
kaoyan_cnky_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-11 14:19:24\
# Project: kaoyan_cnky_inc\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    headers = {}\
    \
    page_dict = {\
\
        'http://www.cnky.net/fuxi/zhengzhi/index.shtml':[u'公共课',u'政治'],\
        'http://www.cnky.net/fuxi/yingyu/index.shtml':[u'公共课',u'英语'],\
        'http://www.cnky.net/fuxi/shuxue/index.shtml':[u'公共课',u'数学'],\
        'http://www.cnky.net/fuxi/kaoyanzhuanyeke/index.shtml':[u'专业课',u'其他'],\
        'http://www.cnky.net/fuxi/zhuanyeke/index.shtml':[u'备考准备',u'备考经验'],\
        'http://www.cnky.net/fuxi/mingshi/index.shtml':[u'备考准备',u'备考经验']\
    }\
    \
    @every(minutes=1*60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('div#listcontent li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('h2 > a').text()\
            _dict['date'] = each.find('span').text().replace('年','-').replace('月','-').replace('日','')\
            url = each.find('h2 > a').attr.href\
            self.crawl(url, save = _dict ,callback=self.detail_page)\
       \
            \
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save\
       content_list = []\
       for each in response.doc('div.g-pct p').items():\
            info = each.html()\
            if info:\
                content_list.append(info)\
       if not content_list:\
            return\
       res_dict['content'] = ''.join('<p>%s</p>'%k for k in content_list if k and k.strip())\
       res_dict['source'] = 'cnky.net'\
       res_dict['data_weight'] = 0\
       res_dict['subject'] = u'考研'\
       res_dict['url'] = response.url\
       res_dict['class'] = 46\
       return res_dict\
$$$$$1471332836.1628
kaoyan_eol_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-11 19:54:16\
# Project: kaoyan_kaoyan\
\
from pyspider.libs.base_handler import *\
from random import randrange\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        _dict = {\
            'zhinan': u'备考经验',           \
            'zhengzhi': u'考研政治',\
            'yingyu': u'考研英语',\
            'shuxue': u'考研数学',\
            'zhuan_ye_ke': u'专业课',\
        }\
        #for i in range(2):\
        for k, v in _dict.iteritems():\
            self.crawl('http://kaoyan.eol.cn/fu_xi/%s/'%(k),  save={'bread': v}, callback=self.index_page)\
        self.crawl('http://kaoyan.eol.cn/nnews/', save={'bread': u'研招动态'}, callback=self.index_page)\
        self.crawl('http://kaoyan.eol.cn/bao_kao/zheng_ce_bian_hua/', save={'bread': u'政策新闻'}, callback=self.index_page)\
        self.crawl('http://kaoyan.eol.cn/bao_kao/re_men/', save={'bread': u'高校动态'}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.page_left li').items():\
            url = each.find('a').attr.href\
            date = each.find('span').text()\
            self.crawl(url,save={'bread': bread, 'date': date}, callback=self.detail_page)\
        #for each in response.doc('.tPage > a').items():\
        #    self.crawl(each.attr.href,save={'bread': bread}, callback=self.detail_page)\
\
    def strip(self, _str):\
        if not _str:\
            return _str\
        return _str.replace('\\t','').replace('\\n','').replace('\\r','').strip(' ').replace(u'考研帮','').replace(u'帮学堂刷视频','').replace(u'帮学堂','')\
    \
    @config(priority=2)\
    def detail_page(self, response):\
        bread = response.save['bread']\
        date = response.save['date']\
        if not date:\
            date = response.doc('.articleInfo').text()[:10]\
        content = self.strip(response.doc('.TRS_Editor').remove('img').remove('a').html())\
        if not content:\
            return None\
        try:\
            source = response.doc('.page_time').text().split()[1].strip()\
        except:\
            source = 'eol'\
        return {\
            "url": response.url,\
            "title": response.doc('.page_title').text(),\
            "date": date,\
            "bread": [bread,],\
            "content": content,\
            "subject": u'考研',\
            "source": source,\
            "cover": 'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg'%(randrange(20)),\
            "class": 46,\
            "data_weight": 0,\
        }\
$$$$$1471332814.6296
kaoyan_kaoshidian$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-31 09:48:14\
# Project: kaoyan_kaoshidian\
\
from pyspider.libs.base_handler import *\
from random import randrange\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    dic = {\
        'zixun/yxxx/': u'院校资讯',\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k, v in self.dic.iteritems():\
            self.crawl('http://bbs.kaoshidian.com/%s'%k, save={'bread': v}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.newsList li').items():\
            bread = response.save['bread']\
            url = each.find('a').attr.href.replace('zixun/yxxx/','')\
            img = each.find('img').attr.src.replace('zixun/yxxx/','')\
            if img.startswith('data'):\
                img = 'http://bbs.kaoshidian.com/' + img\
            self.crawl(url, save={'bread': bread, 'cover': img}, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        sources = response.doc('.article-resource').text().split()\
        try:\
            source = sources[0].split(u'：')[-1]\
        except:\
            source = u'考试点'\
        try:\
            date = sources[1].split(u'：')[-1].split('-')\
            date = '%s-%.2d-%.2d'%(date[0], int(date[1]), int(date[2]))\
        except:\
            date = '2016-04-01'\
        content_list = []\
        for each in response.doc('.articleCon p').items():\
            if each.text().startswith(u'【考试点编辑'):\
                break\
            try:\
                tmp = each.remove('a').html().replace(u'考试点', '')\
                content_list.append((tmp))\
            except:\
                continue\
        if not content_list:\
            return None\
        try:\
            cover = response.save['cover']\
        except:\
            cover = 'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg'%(randrange(1, 20))\
        return {\
            "url": response.url,\
            "title": response.doc('.articleTitle').text(),\
            "source": source,\
            "date": date,\
            "subject": u'考研',\
            "class": 46,\
            "data_weight": 0,\
            "bread": [response.save['bread'],],\
            "content": ''.join(['<p>%s</p>'%v for v in content_list]),\
            "cover": cover,\
        }\
$$$$$1471332838.6387
kaoyan_kaoyan_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-11 19:54:16\
# Project: kaoyan_kaoyan\
\
from pyspider.libs.base_handler import *\
from random import randrange\
    \
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        _dict = {\
            'baokao/zhinan': u'报考指南',\
            'baokao/jingyan': u'报考经验',\
            'xinwen/zhengce/': u'政策新闻',\
            'zhaosheng': u'招生信息',\
            'baokao/zexiao/': u'择选院校',\
            'beikao/jingyan/': u'备考经验',\
            'fushi/jingyan/': u'复试攻略',\
            'yingyu/zhenti/': u'考研英语',\
            'zhengzhi/zhenti/': u'考研政治',\
            'zhengzhi/dagang/': u'考研政治',\
            'zhengzhi/jingyan/': u'考研政治',\
            'yingyu/dagang/': u'考研英语',\
            'yingyu/jingyan/': u'考研英语',\
            'shuxue/jingyan/': u'考研数学',\
            'shuxue/dagang/': u'考研数学',\
            'shuxue/zhenti/': u'考研数学',\
            'zhuanyeke/zhenti/': u'专业课',\
            'zhuanyeke/jingyan/': u'专业课',\
            'zhuanyeke/dagang/': u'专业课',\
        }\
        for k, v in _dict.iteritems():\
            self.crawl('http://www.kaoyan.com/%s/'%k, save={'bread': v}, callback=self.index_page)\
        self.crawl('http://tiaoji.kaoyan.com/xinxi/', save={'bread': u'调剂指南'}, callback=self.index_page)\
        self.crawl('http://mba.kaoyan.com/beikao/', save={'bread': u'MBA'}, callback=self.index_page)\
        self.crawl('http://mba.kaoyan.com/zixun/', save={'bread': u'MBA'}, callback=self.index_page)\
        self.crawl('http://mba.kaoyan.com/baokao/', save={'bread': u'MBA'}, callback=self.index_page)\
        self.crawl('http://mpacc.kaoyan.com/tiaoji/', save={'bread': u'MPAcc'}, callback=self.index_page)\
        self.crawl('http://mpacc.kaoyan.com/baokao/', save={'bread': u'MPAcc'}, callback=self.index_page)\
        self.crawl('http://mpacc.kaoyan.com/beikao/', save={'bread': u'MPAcc'}, callback=self.index_page)\
        self.crawl('http://mpacc.kaoyan.com/fushi/', save={'bread': u'MPAcc'}, callback=self.index_page)\
        self.crawl('http://www.kaoyan.com/jianzhang/', save={'bread': u'招生简章'}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.areaZslist > li').items():\
            url = each.find('a').attr.href\
            date = each.find('.fr').text()\
            self.crawl(url,save={'bread': bread, 'date': date}, callback=self.detail_page)\
        #for each in response.doc('.tPage > a').items():\
        #    self.crawl(each.attr.href,save={'bread': bread}, callback=self.index_page)\
\
    def strip(self, _str):\
        if not _str:\
            return _str\
        return _str.replace('\\t','').replace('\\n','').replace('\\r','').strip(' ').replace(u'考研帮','').replace(u'帮学堂刷视频','').replace(u'帮学堂','')\
\
    @config(priority=2)\
    def detail_page(self, response):\
        bread = response.save['bread']\
        date = response.save['date']\
        if not date:\
            date = response.doc('.articleInfo').text()[:10]\
        content = self.strip(response.doc('.articleCon').html())\
        if not content:\
            return None\
        title = response.doc('.articleTitle').text()\
        if u'汇总' in title:\
            return None\
        try:\
            source = response.doc('.ml30').text().split()[0].strip()\
        except:\
            source = u'考研帮'\
        if source == u'本站原创':\
            source = u'考研帮'\
        return {\
            "url": response.url,\
            "title": title,\
            "date": date,\
            "bread": [bread,],\
            "content": content,\
            "subject": u'考研',\
            "source": source,\
            "cover": 'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg'%(randrange(20)),\
            "class": 46,\
            "data_weight": 0,\
        }\
$$$$$1471332818.5245
kaoyan_kuakao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-11 19:54:16\
# Project: kaoyan_kaoyan\
\
from pyspider.libs.base_handler import *\
from random import randrange\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        _dict = {\
            'politics': u'考研政治',\
            'english': u'考研英语',\
            'maths': u'考研数学',\
            'zyk': u'专业课',\
        }\
        #for i in range(2):\
        for k, v in _dict.iteritems():\
            self.crawl('http://www.kuakao.com/%s/'%(k),  save={'bread': v}, callback=self.index_page)\
        \
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.examBar li').items():\
            url = each.find('a').attr.href\
            date = each.find('span').text()\
            self.crawl(url,save={'bread': bread, 'date': date}, callback=self.detail_page)\
        #for each in response.doc('.tPage > a').items():\
        #    self.crawl(each.attr.href,save={'bread': bread}, callback=self.detail_page)\
\
    def strip(self, _str):\
        if not _str:\
            return _str\
        return _str.replace('\\t','').replace('\\n','').replace('\\r','').strip(' ').replace(u'考研帮','').replace(u'帮学堂刷视频','').replace(u'帮学堂','')\
    \
    @config(priority=2)\
    def detail_page(self, response):\
        bread = response.save['bread']\
        date = response.save['date']\
        if not date:\
            date = response.doc('.articleInfo').text()[:10]\
        content_list = []\
        for each in response.doc('.artTxt > p').items():\
            content_list.append((each.text()))\
             \
        content_list = content_list[2:-4]\
        if not content_list:\
            return None\
        try:\
            source = response.doc('.green').text().split(u'：')[-1].strip()\
        except:\
            source = u'跨考网'\
        return {\
            "url": response.url,\
            "title": response.doc('.artTit').text(),\
            "date": date,\
            "bread": [bread,],\
            "content": ''.join(['<p>%s</p>'%v for v in content_list]),\
            "subject": u'考研',\
            "source": source,\
            "cover": 'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg'%(randrange(20)),\
            "class": 46,\
            "data_weight": 0,\
        }\
$$$$$1471332822.5563
kaoyan_yuloo$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-24 16:11:21\
# Project: kaoyan_yuloo\
\
from pyspider.libs.base_handler import *\
from random import randrange\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    dic = {\
        'political/zhidao/': u'考研政治',\
        'political/jyfd/': u'考研政治',\
        #'political/zhenti/': u'政治',\
        'english/zhidao/': u'考研英语',\
        'english/jyfd/': u'考研英语',\
        #'english/zhenti/': u'英语',\
        'math/zhidao/': u'考研数学',\
        'math/jyfd/': u'考研数学',\
        #'math/zhenti/': u'数学',\
        'zyss/': u'专业课',\
        'dagang/zhuanyeke/': u'专业课',\
        'kyjy': u'备考经验',\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k, v in self.dic.iteritems():\
            self.crawl('http://www.yuloo.com/kaoyan/%s'%k, save={'key': v}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.left .clearfix li').items():\
            key = response.save['key']\
            url = each.find('a').attr.href\
            date = each.find('span').text()\
            if date[:4] >= '2015':\
                self.crawl(url, save={'key': key, 'date': date}, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        bread = response.save['key']\
        content = response.doc('.jiathis_streak').html().replace('\\n','')\
        if not content:\
            return None\
        source = response.doc('.top_h2 > p').text().split(u'发布时间')[0].split(':')[-1].lstrip().rstrip()\
        return {\
            "url": response.url,\
            "title": response.doc('h1').text(),\
            "date": response.save['date'],\
            "bread": [bread,],\
            "source": source if source else u'育路考研网',\
            "subject": u'考研',\
            "class": 46,\
            "data_weight": 0,\
            "content": content,\
            'cover': 'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg'%(randrange(20)),\
        }\
$$$$$1471332826.2625
kaozhengtikt_qnr$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-28 09:43:57\
# Project: kaozhengtiku_qingnianrenwang\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
dalei = [\
    u'建筑工程',\
    u'财会经济',\
    u'医学卫生',\
    u'外语考试',\
    u'职业资格',\
    u'学历教育',\
    u'计算机考试'\
]\
xiaolei = {\
            u'一级建造师':u'一级建造师',\
            u'二级建造师':u'二级建造师',\
            u'监理工程师':u'监理工程师',\
            u'咨询工程师':u'咨询工程师',\
            u'造价工程师':u'造价工程师',\
            u'结构工程师':u'结构工程师',\
            u'电气工程师':u'电气工程师',\
            u'物业管理师':u'物业管理师',\
            u'经济师':u'经济师',\
            u'设计师':u'设计师',\
            u'初级会计':u'初级会计师',\
            u'中级会计师':u'中级会计师',\
            u'注册会计师':u'注册会计师',\
            u'统计师':u'统计师',\
            u'审计师':u'审计师',\
            u'注册税务师':u'注册税务师',\
            u'执业药师':u'执业药师',\
            u'执业药师':u'执业药师',\
            u'执业护士':u'执业护士',\
            u'临床执业':u'临床执业',\
            u'中西医执业':u'中西医执业',\
            u'中医执业':u'中医执业',\
            u'主治':u'主治',\
            u'检验':u'检验',\
            u'英语四级':u'英语四六级',\
            u'英语六级':u'英语四六级',\
            u'雅思':u'雅思',\
            u'托福':u'托福',\
            u'GRE考试':u'GRE考试',\
            u'职称英语':u'职称英语',\
            u'公共英语':u'公共英语',\
            u'商务英语':u'商务英语',\
            u'日语':u'日语',\
            u'人力资源':u'人力资源',\
            u'心理咨询师':u'心理咨询师',\
            u'物流师':u'物流师',\
            u'公共营养师':u'公共营养师',\
            u'秘书资格':u'秘书资格',\
            u'证券经纪人':u'证券经纪人',\
            u'电子商务':u'电子商务',\
            u'国家司法':u'国家司法',\
            u'成人高考':u'成人高考',\
            u'自考':u'自考',\
            u'MBA':u'MBA',\
            u'法律硕士':u'法律硕士',\
            u'会计硕士':u'会计硕士',\
            u'工程硕士':u'工程硕士',\
            u'MPA':u'公共硕士',\
            u'考研':u'考研',\
            u'计算机等级':u'计算机等级',\
            u'软件水平':u'软件水平',\
            u'微软认证':u'微软认证',\
            u'Cisco认证':u'Cisco认证',\
            u'Oracle认证':u'Oracle认证',\
            u'职称计算机':u'职称计算机',\
            u'Java认证':u'Java认证',\
            u'华为认证':u'华为认证',\
    }\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.qnr.cn/zhenti/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('table').items():\
            for each1 in each.find('a').items():\
                _dict = {}\
                _dict['bread'] = [each.find('td').eq(0).text().split('(')[0].strip(),'']\
                _dict['bread'][1] = each1.text().strip()\
                self.crawl(each1.attr.href, save = _dict, callback=self.list_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        _dict = {}\
        for each in response.doc('.Right_last_lst').items():\
            _dict['bread'] = response.save.get('bread')\
            _dict['date'] = each.find('.R_date').text().replace('/','-')\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
            \
        for each in response.doc('.P_Con').items():\
            _dict['bread'] = response.save.get('bread')\
            _dict['date'] = each.find('.time').text().replace('/','-')\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []\
        for each in response.doc('.mar10 > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
            \
        for each in response.doc('#manadona > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
        if len(list) == 0 and response.doc('#manadona'):     \
            list.append(response.doc('#manadona').remove('a').remove('p').html())\
            \
        for each in response.doc('#xx20 > div > p').items():\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
                       \
            \
        for each in response.doc('#xx23 > p').items():\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.remove('a').html())\
            \
        for each in response.doc('#xx27 > p').items():\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.remove('a').html())\
\
        for each in response.doc('.hao > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
            \
        for each in response.doc('.mini > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
        if len(list) == 0 and response.doc('.mini'):     \
            list.append(response.doc('.mini').remove('a').remove('p').html())\
                       \
        \
        for each in response.doc('#shtdxlnews_4 > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
         \
        for each in response.doc('#tb42 > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
            \
        for each in response.doc('#gtsadfas > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
            \
        for each in response.doc('.nali').items():\
            list.append(response.doc('.nali').remove('a').remove('p').html())\
            \
        content = ''.join('<p>%s<p/>' % s for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            bread = []\
            for val in dalei:\
                if val[0:2] in response.save.get('bread')[0]:\
                    bread.append(val)\
                    for k, v in xiaolei.iteritems():\
                        if k in response.save.get('bread')[1]:\
                            bread.append(v)\
            if len(bread) < 2:\
                bread = response.save.get('bread')\
            return {\
                "url": response.url,\
                "title": response.doc('#tb41 > span').text(),\
                "content": content.replace('\\r','').replace('\\n','').replace('\\t','').replace('青年人网讯','').replace('青年人',''),\
                "subject": u"考证题库",\
                "bread": bread, \
                "date": response.save.get('date'),\
                "tdk_description":response.doc('meta').eq(1).attr.content.replace('青年人','').replace('-',' '),\
                "tdk_keywords":response.doc('meta').eq(0).attr.content.replace('青年人','').replace('-',' '),\
                "tdk_title":response.doc('title').eq(0).text().replace('青年人','').replace('-',' '),\
                "class": 33,\
                "data_weight": 0,\
                "source": u"青年人网",\
\
            }\
$$$$$1468461101.9733
keyword_monitor$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-05 10:17:49\
# Project: baidu_keyword_2\
\
from pyspider.libs.base_handler import *\
from urllib import quote, unquote\
import redis\
import urlparse\
import json\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
class Handler(BaseHandler):\
\
    crawl_config = {\
        'itag':'0.1',\
        "headers": {\
        'Host': 'www.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1',\
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.84 Safari/537.36'\
        }\
    }\
\
    def __init__(self):\
        self.r = redis.StrictRedis(host='localhost', port=6379, db=13,charset='utf－8')\
        \
        self.words = {}\
        for line in open('/apps/home/worker/xuzhihao/keyword2'):\
            (word, info) = line.strip('\\n').split('\\t')\
            data = json.loads(info)\
            #self.words[word] = {k: {'title': v, 'find': 0} for k, v in data.iteritems()}\
            self.words[u'%s'%word] = {}\
            for k, v in data.iteritems():\
                self.words[u'%s'%word][k] = {'title': v, 'find': 0}\
        '''\
        self.words = {u'成都MPA培训':\
                          {'http://www.genshuixue.com/bj/st--878_1116.html':\
                               {'title': u'【成都MPA培训|成都MPA辅导机构|成都MPA培训班费用】-跟谁学成都站',\
                                'find': 0\
                                }\
                          },\
                    }\
        '''\
\
    @every(minutes=24*60)\
    def on_start(self):\
        for word, word_info in self.words.iteritems():\
            self.crawl('https://www.baidu.com/s?wd=%s&rsv_spt=1&rsv_iqid=0xffd5385a0000af37&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=5&rsv_sug1=3&rsv_sug7=100'%(word), save={"word": word, "word_info": word_info,},  callback=self.index_page)\
\
    def judge(self, data):\
        for k, v in data.iteritems():\
            if v['find'] == 0:\
                return False\
        return True\
\
    @config(priority=2)\
    def index_page(self, response):\
        try:\
            word = response.save['word']\
            word_info = response.save['word_info']\
        except Exception, e:\
            return\
\
        for each in response.doc('#page a').items():\
            if self.r.get(word):\
                #print 'redis in'\
                return\
            if 'pn' in each.attr.href:\
                _parse = urlparse.urlparse(each.attr.href)\
                keys = urlparse.parse_qs(_parse.query)\
                try:\
                    pn = int(keys['pn'][0])\
                except Exception, e:\
                    pn = -1\
                r_key = '%s_%d'%(word, pn)\
                #print r_key, self.r.get(r_key)\
                if not self.r.get(r_key):\
                    self.r.set(r_key, 1)\
                    self.r.expire(r_key, 10*60*60)\
                    self.crawl(each.attr.href, save={"pn": pn, "word": word, "word_info": word_info}, callback=self.index_page)\
\
        for index, each in enumerate(response.doc('.result').items()):\
            target_url = each.find('.f13 a').text()\
            #print target_url\
            res ={\
                    "pn": None,\
                    "index": index,\
                    "word": word,\
                    "word_info": None,\
                    "target_title": each.text(),\
                    "url":None\
                }\
\
            if 'genshuixue' in target_url:\
                try:\
                    pn = response.save['pn']\
                except Exception, e:\
                    pn = 0\
                bd_title = each.find('.t').text().replace(' ','')\
                if bd_title[-3:] == '...':\
                    bd_title = bd_title[:-3]\
                for k, v in word_info.iteritems():\
                    if u'%s'%bd_title in v['title']:\
                        #print bd_title\
                        res['pn'] = pn\
                        res['word_info'] = {k: v}\
                        #print self.words\
                        self.words[word][k]['find'] = 1\
                        res['url'] = k\
                        if self.judge(self.words[word]):\
                            #print word\
                            self.r.set(word, 1)\
                            self.r.expire(word, 60)\
                        return res\
$$$$$1467701059.6946
keyword_monitor_accurate$$$$$# -*- encoding: utf-8 -*-\
# Created on 2016-07-04 15:44:10\
# Project: keyword_monitor_accurate\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import urlparse\
import time\
import json\
import MySQLdb\
\
class ConnectionUtil(object):\
\
    def __init__(self,connection):\
        self.connection = connection\
\
    def cursor(self):\
        if self.connection:\
            self.cursor  = self.connection.cursor()\
            return self.cursor\
        else:\
            return  None\
\
    def __enter__(self):\
        return self.cursor()\
\
    def __exit__(self, exc_type, exc_val, exc_tb):\
            #print 'ok'\
            if self.connection:\
                self.connection.commit()\
            if self.cursor:\
                self.cursor.close()\
            if self.connection:\
                self.connection.close()\
            if exc_type is not None:\
                #print 'errror'\
                print exc_val\
                #traceback.print_exc()\
                return True\
\
def qs(url):\
    query = urlparse.urlparse(url).query\
    return dict([(k,v[0]) for k,v in urlparse.parse_qs(query).items()])\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.1',\
        "headers": {\
        'Host': 'www.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1',\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
\
    index_dict = {}\
    \
    @every(minutes=3 * 24 * 60)\
    def on_start(self):\
        with open('/apps/home/rd/hexing/data/keyword','r') as f:\
            for line in f:\
                if line :\
                    line = line.strip()\
                    arr = line.split('\\t')\
                    for index in range(10):\
                        self.crawl('https://www.baidu.com/s?wd=%s&pn=%s&rsv_spt=1&rsv_iqid=0xa0eaa7930001b982&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=1&rsv_sug2=0&inputT=995&rsv_sug4=995'%(arr[0],index*10),save = {'query':arr[0],'info':json.loads(arr[1])},priority = 100 - index * 10,callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
      if response.save.get('query') not in self.index_dict:\
            #time.sleep(1)\
            for index , each in enumerate(response.doc('.result').items()):\
                _dict = {}\
                _dict['title'] = each.find('h3 a').text()\
                _dict['query'] = response.save.get('query')\
                _dict['url'] = each.find('.f13 a').text()\
                _dict['info'] = response.save.get('info')\
                page_dict = qs(response.url)\
                if 'pn' not in page_dict:\
                    pn = 0\
                else:\
                    pn = page_dict['pn']\
                for k,v in response.save.get('info').items():\
                    if each.find('h3 a').text().replace('...','').replace(' ','') in v:\
\
                        rank = index + 1 + int(pn) \
                        self.index_dict[response.save.get('query')] = rank\
                        _dict['rank'] = rank\
                        _dict['accurate_url'] =  k\
                        return _dict\
               \
\
    @config(priority=2)\
    def detail_page(self, response):\
        #res_dict = response.save\
        #res_dict['index'] = self.index_dict[response.save.get('query')]\
        #return res_dict\
        pass$$$$$1469408683.4581
keyword_monitor_fuzzy$$$$$# -*- encoding: utf-8 -*-\
# Created on 2016-07-04 15:44:10\
# Project: keyword_monitor_m\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import urlparse\
import time\
import json\
from urllib import quote \
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'v0.1',\
        "headers": {\
        'Host': 'www.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1',\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
\
    index_dict = {}\
    \
    @every(minutes=3*24*60)\
    def on_start(self):\
       with open('/apps/home/rd/hexing/data/keyword','r') as f:\
            for line in f:\
                if line :\
                    line = line.strip()\
                    arr = line.split('\\t')\
                    for index in range(10):\
                        self.crawl('https://www.baidu.com/s?wd=%s&pn=%s&rsv_spt=1&rsv_iqid=0xa0eaa7930001b982&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=1&rsv_sug2=0&inputT=995&rsv_sug4=995'%(arr[0],index*10),save = {'query':arr[0],'info':json.loads(arr[1])},priority = 100 - index * 10,callback=self.index_page)\
                      \
\
    @config(age=1*1)\
    def index_page(self, response):\
      if response.save.get('query') not in self.index_dict:\
            for index , each in enumerate(response.doc('.result').items()):\
                _dict = {}\
                _dict['title'] = each.find('h3 a').text()\
                _dict['query'] = response.save.get('query')\
                _dict['url'] = each.find('.f13 a').text()\
                page_dict = qs(response.url)\
                if 'pn' not in page_dict:\
                    pn = 0\
                else:\
                    pn = page_dict['pn']\
                if 'genshuixue' in _dict['url']:\
                    self.index_dict[response.save.get('query')] = index + 1 + int(pn)      \
                    _dict['rank'] = index + 1 +int(pn)                   \
                    return _dict\
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
        pass$$$$$1472548779.4720
kuaiji_dongaokuaiji$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-30 14:19:18\
# Project: kuaiji_dongaokuaiji\
\
from pyspider.libs.base_handler import *\
\
\
key_url =[\
    #'http://chuji.dongao.com/zchjsdh/kstk/mryl/',\
    'http://chuji.dongao.com/zchjsdh/kstk/lnzt/',\
    'http://chuji.dongao.com/zchjsdh/kstk/tblx/',\
    'http://chuji.dongao.com/zchjsdh/kstk/mnst/',\
    'http://zhongji.dongao.com/ghzcdh/kstk/lnzt/',\
    #'http://zhongji.dongao.com/ghzcdh/kstk/mryl/',\
    'http://zhongji.dongao.com/ghzcdh/kstk/tblx/',\
    'http://shuiwushi.dongao.com/taxdh/kstk/lnzt/',\
    'http://shuiwushi.dongao.com/taxdh/kstk/mryl/',\
    'http://shuiwushi.dongao.com/taxdh/kstk/tblx/',\
    'http://zhukuai.dongao.com/zchjsdh/kstk/lnzt/',\
    #'http://zhukuai.dongao.com/zchjsdh/kstk/mryl/',\
    'http://zhukuai.dongao.com/zchjsdh/kstk/tblx/',\
    'http://gaoji.dongao.com/ghzcdh/kstk/lnzt/',\
    #'http://gaoji.dongao.com/ghzcdh/kstk/mryl/',\
    'http://congye.dongao.com/qg/kstk/lnzt/',\
    #'http://congye.dongao.com/qg/kstk/mryl/',\
    'http://congye.dongao.com/qg/kstk/mnks/',\
]\
\
type_dict = {\
            'chuji':[u'财会经济',u'初级会计师'],\
            'zhongji':[u'财会经济',u'中级会计师'],\
            'shiwushi':[u'财会经济',u'注册税务师'],\
            'zhukuai':[u'财会经济',u'注册会计师'],\
            'gaoji':[u'财会经济',u'高级会计师'],\
            'congye':[u'财会经济',u'会计从业'],\
            u'注册会计师':[u'财会经济',u'注册会计师'],\
            u'经济师':[u'财会经济',u'经济师'],\
            u'初级会计职称':[u'财会经济',u'初级会计师'],\
            u'中级会计职称':[u'财会经济',u'中级会计师'],\
            u'税务师':[u'财会经济',u'注册税务师'],\
            u'统计师':[u'财会经济',u'统计师'],\
            u'审计师':[u'财会经济',u'审计师'],\
            u'高级经济师':[u'财会经济',u'经济师'],\
            u'高级会计':[u'财会经济',u'高级会计师'],\
            u'理财规划师':[u'财会经济',u'理财规划师'],\
\
\
            u'英语四级':[u'外语考试',u'英语四六级'],\
            u'英语六级':[u'外语考试',u'英语四六级'],\
            u'雅思':[u'外语考试',u'雅思'],\
            u'托福':[u'外语考试',u'托福'],\
            u'职称英语':[u'外语考试',u'职称英语'],\
            u'商务英语':[u'外语考试',u'商务英语'],\
            u'公共英语':[u'外语考试',u'公共英语'],\
            u'日语':[u'外语考试',u'日语'],\
            u'GRE考试':[u'外语考试',u'GRE考试'],\
            u'专四专八':[u'外语考试',u'专四专八'],\
            u'口译笔译':[u'外语考试',u'口译笔译'],\
\
            u'一级建造师':[u'建筑工程',u'一级建造师'],\
            u'二级建造师':[u'建筑工程',u'二级建造师'],\
            u'咨询工程师':[u'建筑工程',u'咨询工程师'],\
            u'造价工程师':[u'建筑工程','造价工程师'],\
            u'结构工程师':[u'建筑工程','结构工程师'],\
            u'物业管理':[u'建筑工程',u'物业管理师'],\
            u'城市规划':[u'建筑工程',u'城市规划师'],\
            u'给排水工程':[u'建筑工程',u'给排水工程'],\
            u'电气工程':[u'建筑工程',u'电气工程师'],\
            u'公路监理师':[u'建筑工程',u'公路监理师'],\
            u'消防工程师':[u'建筑工程',u'消防工程师'],\
\
            u'物流师':[u'职业资格',u'物流师'],\
            u'人力资源':[u'职业资格',u'人力资源'],\
            u'心理咨询师':[u'职业资格',u'心理咨询师'],\
            u'公共营养师':[u'职业资格',u'公共营养师'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'证券从业资格':[u'职业资格',u'证券经纪人'],\
            u'电子商务师':[u'职业资格',u'电子商务'],\
            u'期货从业':[u'职业资格',u'期货从业'],\
            u'教师资格':[u'职业资格',u'教师资格'],\
            u'管理咨询师':[u'职业资格',u'管理咨询师'],\
            u'导游证':[u'职业资格',u'导游证'],\
            \
            u'英语':[u'学历教育',u'考研'],\
            u'数学':[u'学历教育',u'考研'],\
            u'政治':[u'学历教育',u'考研'],\
            u'专业课':[u'学历教育',u'考研'],\
\
            u'执业医师':[u'医学卫生',u'执业医师'],\
            u'执业药师':[u'医学卫生',u'执业药师'],\
            u'临床医师':[u'医学卫生',u'临床执业'],\
            u'中医医师':[u'医学卫生',u'中医执业'],\
            u'中西医医师':[u'医学卫生',u'中西医执业'],\
            u'中医助理':[u'医学卫生',u'中医助理'],\
            u'中西医助理':[u'医学卫生',u'中西医助理'],\
            u'主治':[u'医学卫生',u'主治'],\
            u'检验':[u'医学卫生',u'检验'],\
            u'执业护士资格':[u'医学卫生',u'执业护士'],\
\
\
            u'成人高考':[u'学历教育',u'成人高考'],\
            u'自学考试':[u'学历教育',u'自考'],\
            u'MBA考试':[u'学历教育',u'MBA'],\
            u'法律硕士':[u'学历教育',u'法律硕士'],\
            u'专升本':[u'学历教育',u'专升本'],\
            #u'':[u'学历教育',u'工程硕士'],\
            u'MPA考试':[u'学历教育',u'公共硕士'],\
            #u'':[u'学历教育',u'考研'],\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag': '0.1',\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for href in key_url:\
            _dict = {}\
            if href.split('//')[1].split('.')[0] in type_dict:\
                _dict['bread'] = type_dict[href.split('//')[1].split('.')[0]]\
                self.crawl(href, save = _dict, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.column_list li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['date'] = each.find('span').text().replace('/', '-')\
            _dict['title'] = each.find('a').text()\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
        for each in response.doc('.page_number a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict, callback=self.index_page)\
        for each in response.doc('.showpage a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('.pabt-30').items():\
            for each1 in each.find('p').items():\
                if u'相关链接' in each1.text():\
                    break\
                if u'相关推荐' in each1.text():\
                    continue\
                if each1.find('b'):\
                    break\
                else:\
                    list.append(each1.remove('a').html())  \
                    \
        for each in response.doc('.article').items():\
            for each1 in each.find('.content > p').items():\
                if u'相关链接' in each1.text():\
                    break\
                if u'相关推荐' in each1.text():\
                    continue\
                if each1.find('b'):\
                    break\
                else:\
                    list.append(each1.remove('a').html())   \
     \
                    \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            return {\
                "url": response.url,\
                "title": response.save.get('title'),\
                "content": content.replace('\\r','').replace('\\n','').replace('\\t','').replace(u'东奥会计在线',u'跟谁学').replace('东奥',''),\
                "subject": u"考证题库",\
                "bread": response.save.get('bread'), \
                "date": response.save.get('date'),\
                "tdk_keywords": str(response.doc('meta[http-equiv="keywords"]').eq(0).attr.content).replace(u'东奥会计在线','').replace('None','') + str(response.doc('meta[name="keywords"]').eq(0).attr.content).replace(u'东奥会计在线','').replace('None',''),\
                "tdk_desc": str(response.doc('meta[http-equiv="description"]').eq(0).attr.content).replace(u'东奥会计在线',u'跟谁学').replace('None','') + str(response.doc('meta[name="description"]').eq(0).attr.content).replace(u'东奥会计在线',u'跟谁学').replace('None',''),\
                "tdk_title":response.doc('head > title').eq(0).text().replace(u'东奥会计在线','') + u'跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"东奥会计在线",\
\
            }\
$$$$$1471334630.9780
kuaiji_dongaokuaiji_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-16 16:06:51\
# Project: kuaiji_dongaokuaiji_inc\
\
from pyspider.libs.base_handler import *\
\
\
key_url =[\
    #'http://chuji.dongao.com/zchjsdh/kstk/mryl/',\
    'http://chuji.dongao.com/zchjsdh/kstk/lnzt/',\
    'http://chuji.dongao.com/zchjsdh/kstk/tblx/',\
    'http://chuji.dongao.com/zchjsdh/kstk/mnst/',\
    'http://zhongji.dongao.com/ghzcdh/kstk/lnzt/',\
    #'http://zhongji.dongao.com/ghzcdh/kstk/mryl/',\
    'http://zhongji.dongao.com/ghzcdh/kstk/tblx/',\
    'http://shuiwushi.dongao.com/taxdh/kstk/lnzt/',\
    'http://shuiwushi.dongao.com/taxdh/kstk/mryl/',\
    'http://shuiwushi.dongao.com/taxdh/kstk/tblx/',\
    'http://zhukuai.dongao.com/zchjsdh/kstk/lnzt/',\
    #'http://zhukuai.dongao.com/zchjsdh/kstk/mryl/',\
    'http://zhukuai.dongao.com/zchjsdh/kstk/tblx/',\
    'http://gaoji.dongao.com/ghzcdh/kstk/lnzt/',\
    #'http://gaoji.dongao.com/ghzcdh/kstk/mryl/',\
    'http://congye.dongao.com/qg/kstk/lnzt/',\
    #'http://congye.dongao.com/qg/kstk/mryl/',\
    'http://congye.dongao.com/qg/kstk/mnks/',\
]\
\
type_dict = {\
            'chuji':[u'财会经济',u'初级会计师'],\
            'zhongji':[u'财会经济',u'中级会计师'],\
            'shiwushi':[u'财会经济',u'注册税务师'],\
            'zhukuai':[u'财会经济',u'注册会计师'],\
            'gaoji':[u'财会经济',u'高级会计师'],\
            'congye':[u'财会经济',u'会计从业'],\
            u'注册会计师':[u'财会经济',u'注册会计师'],\
            u'经济师':[u'财会经济',u'经济师'],\
            u'初级会计职称':[u'财会经济',u'初级会计师'],\
            u'中级会计职称':[u'财会经济',u'中级会计师'],\
            u'税务师':[u'财会经济',u'注册税务师'],\
            u'统计师':[u'财会经济',u'统计师'],\
            u'审计师':[u'财会经济',u'审计师'],\
            u'高级经济师':[u'财会经济',u'经济师'],\
            u'高级会计':[u'财会经济',u'高级会计师'],\
            u'理财规划师':[u'财会经济',u'理财规划师'],\
\
\
            u'英语四级':[u'外语考试',u'英语四六级'],\
            u'英语六级':[u'外语考试',u'英语四六级'],\
            u'雅思':[u'外语考试',u'雅思'],\
            u'托福':[u'外语考试',u'托福'],\
            u'职称英语':[u'外语考试',u'职称英语'],\
            u'商务英语':[u'外语考试',u'商务英语'],\
            u'公共英语':[u'外语考试',u'公共英语'],\
            u'日语':[u'外语考试',u'日语'],\
            u'GRE考试':[u'外语考试',u'GRE考试'],\
            u'专四专八':[u'外语考试',u'专四专八'],\
            u'口译笔译':[u'外语考试',u'口译笔译'],\
\
            u'一级建造师':[u'建筑工程',u'一级建造师'],\
            u'二级建造师':[u'建筑工程',u'二级建造师'],\
            u'咨询工程师':[u'建筑工程',u'咨询工程师'],\
            u'造价工程师':[u'建筑工程','造价工程师'],\
            u'结构工程师':[u'建筑工程','结构工程师'],\
            u'物业管理':[u'建筑工程',u'物业管理师'],\
            u'城市规划':[u'建筑工程',u'城市规划师'],\
            u'给排水工程':[u'建筑工程',u'给排水工程'],\
            u'电气工程':[u'建筑工程',u'电气工程师'],\
            u'公路监理师':[u'建筑工程',u'公路监理师'],\
            u'消防工程师':[u'建筑工程',u'消防工程师'],\
\
            u'物流师':[u'职业资格',u'物流师'],\
            u'人力资源':[u'职业资格',u'人力资源'],\
            u'心理咨询师':[u'职业资格',u'心理咨询师'],\
            u'公共营养师':[u'职业资格',u'公共营养师'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'证券从业资格':[u'职业资格',u'证券经纪人'],\
            u'电子商务师':[u'职业资格',u'电子商务'],\
            u'期货从业':[u'职业资格',u'期货从业'],\
            u'教师资格':[u'职业资格',u'教师资格'],\
            u'管理咨询师':[u'职业资格',u'管理咨询师'],\
            u'导游证':[u'职业资格',u'导游证'],\
            \
            u'英语':[u'学历教育',u'考研'],\
            u'数学':[u'学历教育',u'考研'],\
            u'政治':[u'学历教育',u'考研'],\
            u'专业课':[u'学历教育',u'考研'],\
\
            u'执业医师':[u'医学卫生',u'执业医师'],\
            u'执业药师':[u'医学卫生',u'执业药师'],\
            u'临床医师':[u'医学卫生',u'临床执业'],\
            u'中医医师':[u'医学卫生',u'中医执业'],\
            u'中西医医师':[u'医学卫生',u'中西医执业'],\
            u'中医助理':[u'医学卫生',u'中医助理'],\
            u'中西医助理':[u'医学卫生',u'中西医助理'],\
            u'主治':[u'医学卫生',u'主治'],\
            u'检验':[u'医学卫生',u'检验'],\
            u'执业护士资格':[u'医学卫生',u'执业护士'],\
\
\
            u'成人高考':[u'学历教育',u'成人高考'],\
            u'自学考试':[u'学历教育',u'自考'],\
            u'MBA考试':[u'学历教育',u'MBA'],\
            u'法律硕士':[u'学历教育',u'法律硕士'],\
            u'专升本':[u'学历教育',u'专升本'],\
            #u'':[u'学历教育',u'工程硕士'],\
            u'MPA考试':[u'学历教育',u'公共硕士'],\
            #u'':[u'学历教育',u'考研'],\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag': '0.1',\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for href in key_url:\
            _dict = {}\
            if href.split('//')[1].split('.')[0] in type_dict:\
                _dict['bread'] = type_dict[href.split('//')[1].split('.')[0]]\
                self.crawl(href, save = _dict, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.column_list li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['date'] = each.find('span').text().replace('/', '-')\
            _dict['title'] = each.find('a').text()\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
        #for each in response.doc('.page_number a').items():\
         #   _dict = {}\
          #  _dict['bread'] = response.save.get('bread')\
           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\
        #for each in response.doc('.showpage a').items():\
         #   _dict = {}\
          #  _dict['bread'] = response.save.get('bread')\
           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('.pabt-30').items():\
            for each1 in each.find('p').items():\
                if u'相关链接' in each1.text():\
                    break\
                if u'相关推荐' in each1.text():\
                    continue\
                if each1.find('b'):\
                    break\
                else:\
                    list.append(each1.remove('a').html())  \
                    \
        for each in response.doc('.article').items():\
            for each1 in each.find('.content > p').items():\
                if u'相关链接' in each1.text():\
                    break\
                if u'相关推荐' in each1.text():\
                    continue\
                if each1.find('b'):\
                    break\
                else:\
                    list.append(each1.remove('a').html())   \
     \
                    \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            return {\
                "url": response.url,\
                "title": response.save.get('title'),\
                "content": content.replace('\\r','').replace('\\n','').replace('\\t','').replace(u'东奥会计在线',u'跟谁学').replace('东奥',''),\
                "subject": u"考证题库",\
                "bread": response.save.get('bread'), \
                "date": response.save.get('date'),\
                "tdk_keywords": str(response.doc('meta[http-equiv="keywords"]').eq(0).attr.content).replace(u'东奥会计在线','').replace('None','') + str(response.doc('meta[name="keywords"]').eq(0).attr.content).replace(u'东奥会计在线','').replace('None',''),\
                "tdk_desc": str(response.doc('meta[http-equiv="description"]').eq(0).attr.content).replace(u'东奥会计在线',u'跟谁学').replace('None','') + str(response.doc('meta[name="description"]').eq(0).attr.content).replace(u'东奥会计在线',u'跟谁学').replace('None',''),\
                "tdk_title":response.doc('head > title').eq(0).text().replace(u'东奥会计在线','') + u'跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"东奥会计在线",\
\
            }\
$$$$$1472546254.4227
kuaiji_zhengbao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-30 18:18:41\
# Project: kuaiji_zhengbao\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
type_dict = {\
            'chuji':[u'财会经济',u'初级会计师'],\
            'chujizhicheng':[u'财会经济',u'初级会计师'],\
            'zhongji':[u'财会经济',u'中级会计师'],\
            'zhongjizhicheng':[u'财会经济',u'中级会计师'],\
            'shiwushi':[u'财会经济',u'注册税务师'],\
            'zhukuai':[u'财会经济',u'注册会计师'],\
            'gaoji':[u'财会经济',u'高级会计师'],\
            'congye':[u'财会经济',u'会计从业'],\
            u'注册会计师':[u'财会经济',u'注册会计师'],\
            u'经济师':[u'财会经济',u'经济师'],\
            u'初级会计职称':[u'财会经济',u'初级会计师'],\
            u'中级会计职称':[u'财会经济',u'中级会计师'],\
            u'税务师':[u'财会经济',u'注册税务师'],\
            u'统计师':[u'财会经济',u'统计师'],\
            u'审计师':[u'财会经济',u'审计师'],\
            u'高级经济师':[u'财会经济',u'经济师'],\
            u'经济师':[u'财会经济',u'经济师'],\
            u'高级会计':[u'财会经济',u'高级会计师'],\
            u'理财规划师':[u'财会经济',u'理财规划师'],\
\
\
            u'英语四级':[u'外语考试',u'英语四六级'],\
            u'英语六级':[u'外语考试',u'英语四六级'],\
            u'雅思':[u'外语考试',u'雅思'],\
            u'托福':[u'外语考试',u'托福'],\
            u'职称英语':[u'外语考试',u'职称英语'],\
            u'商务英语':[u'外语考试',u'商务英语'],\
            u'公共英语':[u'外语考试',u'公共英语'],\
            u'日语':[u'外语考试',u'日语'],\
            u'GRE考试':[u'外语考试',u'GRE考试'],\
            u'专四专八':[u'外语考试',u'专四专八'],\
            u'口译笔译':[u'外语考试',u'口译笔译'],\
\
            'zaojia':[u'建筑工程','造价工程师'],\
            u'一级建造师':[u'建筑工程',u'一级建造师'],\
            u'二级建造师':[u'建筑工程',u'二级建造师'],\
            u'咨询工程师':[u'建筑工程',u'咨询工程师'],\
            u'造价工程师':[u'建筑工程','造价工程师'],\
            u'结构工程师':[u'建筑工程','结构工程师'],\
            u'物业管理':[u'建筑工程',u'物业管理师'],\
            u'城市规划':[u'建筑工程',u'城市规划师'],\
            u'给排水工程':[u'建筑工程',u'给排水工程'],\
            u'电气工程':[u'建筑工程',u'电气工程师'],\
            u'公路监理师':[u'建筑工程',u'公路监理师'],\
            u'消防工程师':[u'建筑工程',u'消防工程师'],\
            u'消防':[u'建筑工程',u'消防工程师'],\
\
            u'物流师':[u'职业资格',u'物流师'],\
            u'人力资源':[u'职业资格',u'人力资源'],\
            u'心理咨询师':[u'职业资格',u'心理咨询师'],\
            u'公共营养师':[u'职业资格',u'公共营养师'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'证券从业资格':[u'职业资格',u'证券经纪人'],\
            u'电子商务师':[u'职业资格',u'电子商务'],\
            u'期货从业':[u'职业资格',u'期货从业'],\
            u'教师资格':[u'职业资格',u'教师资格'],\
            u'管理咨询师':[u'职业资格',u'管理咨询师'],\
            u'导游证':[u'职业资格',u'导游证'],\
            \
            u'英语':[u'学历教育',u'考研'],\
            u'数学':[u'学历教育',u'考研'],\
            u'政治':[u'学历教育',u'考研'],\
            u'专业课':[u'学历教育',u'考研'],\
\
            u'执业医师':[u'医学卫生',u'执业医师'],\
            u'执业药师':[u'医学卫生',u'执业药师'],\
            u'临床医师':[u'医学卫生',u'临床执业'],\
            u'中医医师':[u'医学卫生',u'中医执业'],\
            u'中西医医师':[u'医学卫生',u'中西医执业'],\
            u'中医助理':[u'医学卫生',u'中医助理'],\
            u'中西医助理':[u'医学卫生',u'中西医助理'],\
            u'主治':[u'医学卫生',u'主治'],\
            u'检验':[u'医学卫生',u'检验'],\
            u'执业护士资格':[u'医学卫生',u'执业护士'],\
\
\
            u'成人高考':[u'学历教育',u'成人高考'],\
            u'自学考试':[u'学历教育',u'自考'],\
            u'MBA考试':[u'学历教育',u'MBA'],\
            u'法律硕士':[u'学历教育',u'法律硕士'],\
            u'专升本':[u'学历教育',u'专升本'],\
            #u'':[u'学历教育',u'工程硕士'],\
            u'MPA考试':[u'学历教育',u'公共硕士'],\
            #u'':[u'学历教育',u'考研'],\
}\
\
\
some_url =[\
    'http://www.chinaacc.com/chujizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/chujizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/',\
    'http://www.chinaacc.com/zaojia/zt/',\
    'http://www.chinaacc.com/zaojia/mnst/',    \
]\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for url in some_url:\
            _dict = {}\
            _dict['bread'] = type_dict[url.split('/')[3]]\
            self.crawl(url, save = _dict, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.xinxi li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('.fl > a').text()\
            if '报名' in _dict['title'] or '名师' in _dict['title'] or '汇总' in _dict['title']:\
                continue\
            _dict['date'] = each.find('.fr').text().replace('[','').replace(']','')\
            self.crawl(each.find('.fl > a').attr.href, save = _dict, callback=self.detail_page)\
        \
        #翻页 \
        for each in response.doc('divpagestrcjzc a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('#fontzoom').items():\
            for each1 in each.find('p').items():\
                if u'全网首发' in each1.text():\
                    continue\
                if u'估分更准确' in each1.text():\
                    continue\
                if u'独家' in each1.text():\
                    continue\
                if u'点击查看' in each1.text():\
                    continue\
                if u'点击参与' in each1.text():\
                    continue\
                if u'到建设工程教育网论坛' in each1.text():\
                    continue\
                if u'特色班' in each1.text():\
                    break\
                if u'近几年' in each1.text():\
                    break\
                if u'考友咨询' in each1.text():\
                    break\
                if u'更多' in each1.text() and u'资讯' in each1.text():\
                    break\
                if u'推荐信息' in each1.text() or u'更多推荐' in each1.text() or u'推荐阅读' in each1.text():\
                    break\
                if u'免费在线测试' in each1.text():\
                    continue\
                if u'在线测试系统' in each1.text():\
                    continue\
                if u'转载请注明出处' in each1.text():\
                    continue\
                if u'责任编辑' in each1.text():\
                    break\
                if u'相关链接' in each1.text():\
                    break\
                if u'以上' in each1.text() and u'是中华会计网' in each1.text():\
                    break\
                else:\
                    list.append(each1.remove('a').html())\
            \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            \
            return {\
                "url": response.url,\
                "title": response.save.get('title'),\
                "content": content.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace(u'中华会计网校', '').replace('【 】', ''),\
                "subject": u"考证题库",\
                "bread": response.save.get('bread'),\
                "date": response.save.get('date'),\
                "tdk_keywords": str(response.doc('meta[http-equiv="keywords"]').eq(0).attr.content).replace(u'中华会计网校', '').replace(\
                    'None', '') + str(response.doc('meta[name="keywords"]').eq(0).attr.content).replace(u'中华会计网校', '').replace(\
                    'None', ''),\
                "tdk_desc": str(response.doc('meta[http-equiv="description"]').eq(0).attr.content).replace(u'中华会计网校', u'').replace(\
                    '建设网校', '').replace('None', '').replace('建设工程教育网', '') + str(response.doc('meta[name="description"]').eq(0).attr.content).replace(\
                    u'中华会计网校', '').replace('建设网校', '').replace('None', '').replace('建设工程教育网', ''),\
                "tdk_title": response.doc('head > title').eq(0).text().replace(u'中华会计网校', '') + u' 跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"中华会计网校",\
            }\
\
$$$$$1471334642.4074
kuaiji_zhengbao_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-16 16:11:21\
# Project: kuaiji_zhengbao_inc\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
type_dict = {\
            'chuji':[u'财会经济',u'初级会计师'],\
            'chujizhicheng':[u'财会经济',u'初级会计师'],\
            'zhongji':[u'财会经济',u'中级会计师'],\
            'zhongjizhicheng':[u'财会经济',u'中级会计师'],\
            'shiwushi':[u'财会经济',u'注册税务师'],\
            'zhukuai':[u'财会经济',u'注册会计师'],\
            'gaoji':[u'财会经济',u'高级会计师'],\
            'congye':[u'财会经济',u'会计从业'],\
            u'注册会计师':[u'财会经济',u'注册会计师'],\
            u'经济师':[u'财会经济',u'经济师'],\
            u'初级会计职称':[u'财会经济',u'初级会计师'],\
            u'中级会计职称':[u'财会经济',u'中级会计师'],\
            u'税务师':[u'财会经济',u'注册税务师'],\
            u'统计师':[u'财会经济',u'统计师'],\
            u'审计师':[u'财会经济',u'审计师'],\
            u'高级经济师':[u'财会经济',u'经济师'],\
            u'经济师':[u'财会经济',u'经济师'],\
            u'高级会计':[u'财会经济',u'高级会计师'],\
            u'理财规划师':[u'财会经济',u'理财规划师'],\
\
\
            u'英语四级':[u'外语考试',u'英语四六级'],\
            u'英语六级':[u'外语考试',u'英语四六级'],\
            u'雅思':[u'外语考试',u'雅思'],\
            u'托福':[u'外语考试',u'托福'],\
            u'职称英语':[u'外语考试',u'职称英语'],\
            u'商务英语':[u'外语考试',u'商务英语'],\
            u'公共英语':[u'外语考试',u'公共英语'],\
            u'日语':[u'外语考试',u'日语'],\
            u'GRE考试':[u'外语考试',u'GRE考试'],\
            u'专四专八':[u'外语考试',u'专四专八'],\
            u'口译笔译':[u'外语考试',u'口译笔译'],\
\
            'zaojia':[u'建筑工程','造价工程师'],\
            u'一级建造师':[u'建筑工程',u'一级建造师'],\
            u'二级建造师':[u'建筑工程',u'二级建造师'],\
            u'咨询工程师':[u'建筑工程',u'咨询工程师'],\
            u'造价工程师':[u'建筑工程','造价工程师'],\
            u'结构工程师':[u'建筑工程','结构工程师'],\
            u'物业管理':[u'建筑工程',u'物业管理师'],\
            u'城市规划':[u'建筑工程',u'城市规划师'],\
            u'给排水工程':[u'建筑工程',u'给排水工程'],\
            u'电气工程':[u'建筑工程',u'电气工程师'],\
            u'公路监理师':[u'建筑工程',u'公路监理师'],\
            u'消防工程师':[u'建筑工程',u'消防工程师'],\
            u'消防':[u'建筑工程',u'消防工程师'],\
\
            u'物流师':[u'职业资格',u'物流师'],\
            u'人力资源':[u'职业资格',u'人力资源'],\
            u'心理咨询师':[u'职业资格',u'心理咨询师'],\
            u'公共营养师':[u'职业资格',u'公共营养师'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'秘书资格':[u'职业资格',u'秘书资格'],\
            u'证券从业资格':[u'职业资格',u'证券经纪人'],\
            u'电子商务师':[u'职业资格',u'电子商务'],\
            u'期货从业':[u'职业资格',u'期货从业'],\
            u'教师资格':[u'职业资格',u'教师资格'],\
            u'管理咨询师':[u'职业资格',u'管理咨询师'],\
            u'导游证':[u'职业资格',u'导游证'],\
            \
            u'英语':[u'学历教育',u'考研'],\
            u'数学':[u'学历教育',u'考研'],\
            u'政治':[u'学历教育',u'考研'],\
            u'专业课':[u'学历教育',u'考研'],\
\
            u'执业医师':[u'医学卫生',u'执业医师'],\
            u'执业药师':[u'医学卫生',u'执业药师'],\
            u'临床医师':[u'医学卫生',u'临床执业'],\
            u'中医医师':[u'医学卫生',u'中医执业'],\
            u'中西医医师':[u'医学卫生',u'中西医执业'],\
            u'中医助理':[u'医学卫生',u'中医助理'],\
            u'中西医助理':[u'医学卫生',u'中西医助理'],\
            u'主治':[u'医学卫生',u'主治'],\
            u'检验':[u'医学卫生',u'检验'],\
            u'执业护士资格':[u'医学卫生',u'执业护士'],\
\
\
            u'成人高考':[u'学历教育',u'成人高考'],\
            u'自学考试':[u'学历教育',u'自考'],\
            u'MBA考试':[u'学历教育',u'MBA'],\
            u'法律硕士':[u'学历教育',u'法律硕士'],\
            u'专升本':[u'学历教育',u'专升本'],\
            #u'':[u'学历教育',u'工程硕士'],\
            u'MPA考试':[u'学历教育',u'公共硕士'],\
            #u'':[u'学历教育',u'考研'],\
}\
\
\
some_url =[\
    'http://www.chinaacc.com/chujizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/chujizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/',\
    'http://www.chinaacc.com/zaojia/zt/',\
    'http://www.chinaacc.com/zaojia/mnst/',    \
]\
\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for url in some_url:\
            _dict = {}\
            _dict['bread'] = type_dict[url.split('/')[3]]\
            self.crawl(url, save = _dict, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.xinxi li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('.fl > a').text()\
            if '报名' in _dict['title'] or '名师' in _dict['title'] or '汇总' in _dict['title']:\
                continue\
            _dict['date'] = each.find('.fr').text().replace('[','').replace(']','')\
            self.crawl(each.find('.fl > a').attr.href, save = _dict, callback=self.detail_page)\
        \
        #翻页 \
        #for each in response.doc('divpagestrcjzc a').items():\
         #   _dict = {}\
          #  _dict['bread'] = response.save.get('bread')\
           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('#fontzoom').items():\
            for each1 in each.find('p').items():\
                if u'全网首发' in each1.text():\
                    continue\
                if u'估分更准确' in each1.text():\
                    continue\
                if u'独家' in each1.text():\
                    continue\
                if u'点击查看' in each1.text():\
                    continue\
                if u'点击参与' in each1.text():\
                    continue\
                if u'到建设工程教育网论坛' in each1.text():\
                    continue\
                if u'特色班' in each1.text():\
                    break\
                if u'近几年' in each1.text():\
                    break\
                if u'考友咨询' in each1.text():\
                    break\
                if u'更多' in each1.text() and u'资讯' in each1.text():\
                    break\
                if u'推荐信息' in each1.text() or u'更多推荐' in each1.text() or u'推荐阅读' in each1.text():\
                    break\
                if u'免费在线测试' in each1.text():\
                    continue\
                if u'在线测试系统' in each1.text():\
                    continue\
                if u'转载请注明出处' in each1.text():\
                    continue\
                if u'责任编辑' in each1.text():\
                    break\
                if u'相关链接' in each1.text():\
                    break\
                if u'以上' in each1.text() and u'是中华会计网' in each1.text():\
                    break\
                else:\
                    list.append(each1.remove('a').html())\
            \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            \
            return {\
                "url": response.url,\
                "title": response.save.get('title'),\
                "content": content.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace(u'中华会计网校', '').replace('【 】', ''),\
                "subject": u"考证题库",\
                "bread": response.save.get('bread'),\
                "date": response.save.get('date'),\
                "tdk_keywords": str(response.doc('meta[http-equiv="keywords"]').eq(0).attr.content).replace(u'中华会计网校', '').replace(\
                    'None', '') + str(response.doc('meta[name="keywords"]').eq(0).attr.content).replace(u'中华会计网校', '').replace(\
                    'None', ''),\
                "tdk_desc": str(response.doc('meta[http-equiv="description"]').eq(0).attr.content).replace(u'中华会计网校', u'').replace(\
                    '建设网校', '').replace('None', '').replace('建设工程教育网', '') + str(response.doc('meta[name="description"]').eq(0).attr.content).replace(\
                    u'中华会计网校', '').replace('建设网校', '').replace('None', '').replace('建设工程教育网', ''),\
                "tdk_title": response.doc('head > title').eq(0).text().replace(u'中华会计网校', '') + u' 跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"中华会计网校",\
            }\
\
$$$$$1472546258.5260
kztk_haoxue$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-29 16:24:42\
# Project: kztk_haoxue\
\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
dalei = [\
    u'建筑工程',\
    u'财会经济',\
    u'医学卫生',\
    u'外语考试',\
    u'职业资格',\
    u'学历教育',\
    u'计算机考试'\
]\
xiaolei = {\
    u'一级建造师': u'一级建造师',\
    u'二级建造师': u'二级建造师',\
    u'监理工程师': u'监理工程师',\
    u'咨询工程师': u'咨询工程师',\
    u'造价工程师': u'造价工程师',\
    u'结构工程师': u'结构工程师',\
    u'电气工程师': u'电气工程师',\
    u'物业管理师': u'物业管理师',\
    u'经济师': u'经济师',\
    u'设计师': u'设计师',\
    u'初级会计职称': u'初级会计师',\
    u'会计从业证': u'初级会计师',\
    u'中级会计师': u'中级会计师',\
    u'注册会计师': u'注册会计师',\
    u'统计师': u'统计师',\
    u'审计师': u'审计师',\
    u'注册税务师': u'注册税务师',\
    u'执业药师': u'执业药师',\
    u'护士资格': u'执业护士',\
    u'临床执业医师': u'临床执业',\
    u'中西医执业医师': u'中西医执业',\
    u'中医执业医师': u'中医执业',\
    u'主治': u'主治',\
    u'检验': u'检验',\
    u'英语四级': u'英语四六级',\
    u'英语六级': u'英语四六级',\
    u'雅思': u'雅思',\
    u'托福': u'托福',\
    u'GRE考试': u'GRE考试',\
    u'职称英语': u'职称英语',\
    u'公共英语': u'公共英语',\
    u'商务英语': u'商务英语',\
    u'日语': u'日语',\
    u'人力资源': u'人力资源',\
    u'心理咨询师': u'心理咨询师',\
    u'物流师': u'物流师',\
    u'公共营养师': u'公共营养师',\
    u'秘书资格': u'秘书资格',\
    u'证券经纪人': u'证券经纪人',\
    u'电子商务': u'电子商务',\
    u'国家司法': u'国家司法',\
    u'成人高考': u'成人高考',\
    u'自考': u'自考',\
    u'MBA': u'MBA',\
    u'法律硕士': u'法律硕士',\
    u'会计硕士': u'会计硕士',\
    u'工程硕士': u'工程硕士',\
    u'MPA': u'公共硕士',\
    u'考研': u'考研',\
    u'计算机等级': u'计算机等级',\
    u'软件水平': u'软件水平',\
    u'微软认证': u'微软认证',\
    u'Cisco认证': u'Cisco认证',\
    u'Oracle认证': u'Oracle认证',\
    u'职称计算机': u'职称计算机',\
    u'Java认证': u'Java认证',\
    u'华为认证': u'华为认证',\
}\
\
add_url =[\
    'lnzt/',\
    'mnst/',\
    'mryl/',\
    'kddq/',\
    'zjlx/'\
]\
\
da_xiao_lei = {\
    u'一级建造师':u'建筑工程',\
    u'二级建造师':u'建筑工程',\
    u'消防工程师':u'建筑工程',\
    u'护士资格':u'医学卫生',\
    u'初级护师':u'医学卫生',\
    u'主管护师':u'医学卫生',\
    u'临床执业医师':u'医学卫生',\
    u'临床助理医师':u'医学卫生',\
    u'中医执业医师':u'医学卫生',\
    u'中医助理医师':u'医学卫生',\
    u'中西医执业医师':u'医学卫生',\
    u'中西医助理医师':u'医学卫生',\
    u'执业药师':u'医学卫生',\
    u'会计从业证':u'财会经济',\
    u'初级会计职称':u'财会经济',\
    u'银行从业':u'财会经济',\
    u'证券从业':u'财会经济',\
    u'基金从业':u'财会经济',\
\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.5haoxue.net/sitemap/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.tit a').items():\
            _dict = {}\
            _dict['bread'] = ['', '']\
            _dict['bread'][1] = each.text()\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for val in add_url:\
            self.crawl(response.url + val, save = response.save, callback=self.list_page1)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page1(self, response):      \
        for each in response.doc('.m-post-col li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['date'] = each.find('span').text()\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('.f-clear > .f-f14').items():\
            for each1 in each.find('p').items():\
                if u'小编推荐' in each1.text():\
                    continue\
                if u'编辑推荐' in each1.text():\
                    continue\
                if u'点击查看' in each1.text():\
                    continue\
                if u'直接点击' in each1.text():\
                    continue\
                if u'我们特别开通了' in each1.text():\
                    continue\
                if u'希望对您有所帮助' in each1.text():\
                    continue\
                if u'编辑第一时间为您搜集整理了' in each1.text():\
                    continue\
                if each1.find('a') and each1.find('img'):\
                    continue\
                else:\
                    list.append(each1.remove('a').html())\
            for each2 in each.find('center').items():\
                list.append(each2.html())\
            \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            bread = []\
            bread.append(da_xiao_lei[response.save.get('bread')[1]])\
            if response.save.get('bread')[1] in xiaolei.keys():\
                bread.append(xiaolei[response.save.get('bread')[1]])\
            if len(bread) < 2:\
                bread.append(response.save.get('bread')[1])\
            return {\
                "url": response.url,\
                "title": response.doc('.g-news h1').text().strip(),\
                "content": content.replace('\\r','').replace('\\n','').replace('\\t','').replace(u'好学教育',u'跟谁学'),\
                "subject": u"考证题库",\
                "bread": bread, \
                "date": response.save.get('date'),\
                "tdk_keywords": response.doc('meta[name="keywords"]').eq(0).attr.content.replace(u'好学教育',''),\
                "tdk_desc": response.doc('meta[name="description"]').eq(0).attr.content.replace(u'好学教育',u'跟谁学'),\
                "tdk_title":response.doc('head > title').eq(0).text().replace(u'好学教育','') + u'跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"好学教育",\
\
            }\
\
$$$$$1469249846.0428
kztk_haoxuejiaoyu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-29 15:42:04\
# Project: kztk_haoxuejiaoyu\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
dalei = [\
    u'建筑工程',\
    u'财会经济',\
    u'医学卫生',\
    u'外语考试',\
    u'职业资格',\
    u'学历教育',\
    u'计算机考试'\
]\
xiaolei = {\
    u'一级建造师': u'一级建造师',\
    u'二级建造师': u'二级建造师',\
    u'监理工程师': u'监理工程师',\
    u'咨询工程师': u'咨询工程师',\
    u'造价工程师': u'造价工程师',\
    u'结构工程师': u'结构工程师',\
    u'电气工程师': u'电气工程师',\
    u'物业管理师': u'物业管理师',\
    u'经济师': u'经济师',\
    u'设计师': u'设计师',\
    u'初级会计职称': u'初级会计师',\
    u'会计从业证': u'初级会计师',\
    u'中级会计师': u'中级会计师',\
    u'注册会计师': u'注册会计师',\
    u'统计师': u'统计师',\
    u'审计师': u'审计师',\
    u'注册税务师': u'注册税务师',\
    u'执业药师': u'执业药师',\
    u'护士资格': u'执业护士',\
    u'临床执业医师': u'临床执业',\
    u'中西医执业医师': u'中西医执业',\
    u'中医执业医师': u'中医执业',\
    u'主治': u'主治',\
    u'检验': u'检验',\
    u'英语四级': u'英语四六级',\
    u'英语六级': u'英语四六级',\
    u'雅思': u'雅思',\
    u'托福': u'托福',\
    u'GRE考试': u'GRE考试',\
    u'职称英语': u'职称英语',\
    u'公共英语': u'公共英语',\
    u'商务英语': u'商务英语',\
    u'日语': u'日语',\
    u'人力资源': u'人力资源',\
    u'心理咨询师': u'心理咨询师',\
    u'物流师': u'物流师',\
    u'公共营养师': u'公共营养师',\
    u'秘书资格': u'秘书资格',\
    u'证券经纪人': u'证券经纪人',\
    u'电子商务': u'电子商务',\
    u'国家司法': u'国家司法',\
    u'成人高考': u'成人高考',\
    u'自考': u'自考',\
    u'MBA': u'MBA',\
    u'法律硕士': u'法律硕士',\
    u'会计硕士': u'会计硕士',\
    u'工程硕士': u'工程硕士',\
    u'MPA': u'公共硕士',\
    u'考研': u'考研',\
    u'计算机等级': u'计算机等级',\
    u'软件水平': u'软件水平',\
    u'微软认证': u'微软认证',\
    u'Cisco认证': u'Cisco认证',\
    u'Oracle认证': u'Oracle认证',\
    u'职称计算机': u'职称计算机',\
    u'Java认证': u'Java认证',\
    u'华为认证': u'华为认证',\
}\
\
add_url =[\
    'lnzt/',\
    'mnst/',\
    'mryl/',\
    'kddq/',\
    'zjlx/'\
]\
\
da_xiao_lei = {\
    u'一级建造师':u'建筑工程',\
    u'二级建造师':u'建筑工程',\
    u'消防工程师':u'建筑工程',\
    u'护士资格':u'医学卫生',\
    u'初级护师':u'医学卫生',\
    u'主管护师':u'医学卫生',\
    u'临床执业医师':u'医学卫生',\
    u'临床助理医师':u'医学卫生',\
    u'中医执业医师':u'医学卫生',\
    u'中医助理医师':u'医学卫生',\
    u'中西医执业医师':u'医学卫生',\
    u'中西医助理医师':u'医学卫生',\
    u'执业医师':u'医学卫生',\
    u'会计从业证':u'财会经济',\
    u'初级会计职称':u'财会经济',\
    u'银行从业':u'财会经济',\
    u'证券从业':u'财会经济',\
    u'基金从业':u'财会经济',\
\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.5haoxue.net/sitemap/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.tit a').items():\
            _dict = {}\
            _dict['bread'] = ['', '']\
            _dict['bread'][1] = each.text()\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for val in add_url:\
            self.crawl(response.url + val, save = response.save, callback=self.list_page1)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page1(self, response):      \
        for each in response.doc('.m-post-col li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['date'] = each.find('span').text()\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('.f-clear > .f-f14').items():\
            for each1 in each.find('p').items():\
                if u'小编推荐' in each1.text():\
                    continue\
                if u'编辑推荐' in each1.text():\
                    continue\
                if u'点击查看' in each1.text():\
                    continue\
                if u'直接点击' in each1.text():\
                    continue\
                if u'我们特别开通了' in each1.text():\
                    continue\
                if u'希望对您有所帮助' in each1.text():\
                    continue\
                if u'编辑第一时间为您搜集整理了' in each1.text():\
                    continue\
                if each1.find('a') and each1.find('img'):\
                    continue\
                else:\
                    list.append(each1.remove('a').html())\
            for each2 in each.find('center').items():\
                list.append(each2.html())\
            \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            bread = []\
            bread.append(da_xiao_lei[response.save.get('bread')[1]])\
            if response.save.get('bread')[1] in xiaolei.keys():\
                bread.append(xiaolei[response.save.get('bread')[1]])\
            if len(bread) < 2:\
                bread = response.append(response.save.get('bread')[1])\
            return {\
                "url": response.url,\
                "title": response.doc('.g-news h1').text().strip(),\
                "content": content.replace('\\r','').replace('\\n','').replace('\\t','').replace(u'好学教育',u'跟谁学'),\
                "subject": u"考证题库",\
                "bread": bread, \
                "date": response.save.get('date'),\
                "tdk_keywords": response.doc('meta[name="keywords"]').eq(0).attr.content.replace(u'好学教育',''),\
                "tdk_desc": response.doc('meta[name="description"]').eq(0).attr.content.replace(u'好学教育',u'跟谁学'),\
                "tdk_title":response.doc('head > title').eq(0).text().replace(u'好学教育','') + u'跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"好学教育",\
\
            }\
$$$$$1467186282.2766
kztk_qnr_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-19 15:42:14\
# Project: kztk_qnr_inc\
\
#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-28 09:43:57\
# Project: kaozhengtiku_qingnianrenwang\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
dalei = [\
    u'建筑工程',\
    u'财会经济',\
    u'医学卫生',\
    u'外语考试',\
    u'职业资格',\
    u'学历教育',\
    u'计算机考试'\
]\
xiaolei = {\
            u'一级建造师':u'一级建造师',\
            u'二级建造师':u'二级建造师',\
            u'监理工程师':u'监理工程师',\
            u'咨询工程师':u'咨询工程师',\
            u'造价工程师':u'造价工程师',\
            u'结构工程师':u'结构工程师',\
            u'电气工程师':u'电气工程师',\
            u'物业管理师':u'物业管理师',\
            u'经济师':u'经济师',\
            u'设计师':u'设计师',\
            u'初级会计':u'初级会计师',\
            u'中级会计师':u'中级会计师',\
            u'注册会计师':u'注册会计师',\
            u'统计师':u'统计师',\
            u'审计师':u'审计师',\
            u'注册税务师':u'注册税务师',\
            u'执业药师':u'执业药师',\
            u'执业药师':u'执业药师',\
            u'执业护士':u'执业护士',\
            u'临床执业':u'临床执业',\
            u'中西医执业':u'中西医执业',\
            u'中医执业':u'中医执业',\
            u'主治':u'主治',\
            u'检验':u'检验',\
            u'英语四级':u'英语四六级',\
            u'英语六级':u'英语四六级',\
            u'雅思':u'雅思',\
            u'托福':u'托福',\
            u'GRE考试':u'GRE考试',\
            u'职称英语':u'职称英语',\
            u'公共英语':u'公共英语',\
            u'商务英语':u'商务英语',\
            u'日语':u'日语',\
            u'人力资源':u'人力资源',\
            u'心理咨询师':u'心理咨询师',\
            u'物流师':u'物流师',\
            u'公共营养师':u'公共营养师',\
            u'秘书资格':u'秘书资格',\
            u'证券经纪人':u'证券经纪人',\
            u'电子商务':u'电子商务',\
            u'国家司法':u'国家司法',\
            u'成人高考':u'成人高考',\
            u'自考':u'自考',\
            u'MBA':u'MBA',\
            u'法律硕士':u'法律硕士',\
            u'会计硕士':u'会计硕士',\
            u'工程硕士':u'工程硕士',\
            u'MPA':u'公共硕士',\
            u'考研':u'考研',\
            u'计算机等级':u'计算机等级',\
            u'软件水平':u'软件水平',\
            u'微软认证':u'微软认证',\
            u'Cisco认证':u'Cisco认证',\
            u'Oracle认证':u'Oracle认证',\
            u'职称计算机':u'职称计算机',\
            u'Java认证':u'Java认证',\
            u'华为认证':u'华为认证',\
    }\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.qnr.cn/zhenti/', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('table').items():\
            for each1 in each.find('a').items():\
                _dict = {}\
                _dict['bread'] = [each.find('td').eq(0).text().split('(')[0].strip(),'']\
                _dict['bread'][1] = each1.text().strip()\
                self.crawl(each1.attr.href, save = _dict, callback=self.list_page)\
            \
    @config(age=1 * 1)\
    def list_page(self, response):\
        _dict = {}\
        for each in response.doc('.Right_last_lst').items():\
            _dict['bread'] = response.save.get('bread')\
            _dict['date'] = each.find('.R_date').text().replace('/','-')\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
            \
        for each in response.doc('.P_Con').items():\
            _dict['bread'] = response.save.get('bread')\
            _dict['date'] = each.find('.time').text().replace('/','-')\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []\
        for each in response.doc('.mar10 > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
            \
        for each in response.doc('#manadona > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
        if len(list) == 0 and response.doc('#manadona'):     \
            list.append(response.doc('#manadona').remove('a').remove('p').html())\
            \
        for each in response.doc('#xx20 > div > p').items():\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
                       \
            \
        for each in response.doc('#xx23 > p').items():\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.remove('a').html())\
            \
        for each in response.doc('#xx27 > p').items():\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.remove('a').html())\
\
        for each in response.doc('.hao > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
            \
        for each in response.doc('.mini > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
        if len(list) == 0 and response.doc('.mini'):     \
            list.append(response.doc('.mini').remove('a').remove('p').html())\
                       \
        \
        for each in response.doc('#shtdxlnews_4 > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
         \
        for each in response.doc('#tb42 > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
            \
        for each in response.doc('#gtsadfas > p').items():\
            if each.find('a'):\
                continue\
            if each.attr.align == 'left':\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            elif each.attr.align:\
                if each.find('img') and each.text().strip() == '':\
                    pass\
                else:\
                    continue\
            list.append(each.html())\
            \
        for each in response.doc('.nali').items():\
            list.append(response.doc('.nali').remove('a').remove('p').html())\
            \
        content = ''.join('<p>%s<p/>' % s for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            bread = []\
            for val in dalei:\
                if val[0:2] in response.save.get('bread')[0]:\
                    bread.append(val)\
                    for k, v in xiaolei.iteritems():\
                        if k in response.save.get('bread')[1]:\
                            bread.append(v)\
            if len(bread) < 2:\
                bread = response.save.get('bread')\
            return {\
                "url": response.url,\
                "title": response.doc('#tb41 > span').text(),\
                "content": content.replace('\\r','').replace('\\n','').replace('\\t','').replace('青年人网讯','').replace('青年人',''),\
                "subject": u"考证题库",\
                "bread": bread, \
                "date": response.save.get('date'),\
                "tdk_description":response.doc('meta').eq(1).attr.content.replace('青年人','').replace('-',' '),\
                "tdk_keywords":response.doc('meta').eq(0).attr.content.replace('青年人','').replace('-',' '),\
                "tdk_title":response.doc('title').eq(0).text().replace('青年人','').replace('-',' '),\
                "class": 33,\
                "data_weight": 0,\
                "source": u"青年人网",\
\
            }\
$$$$$1472546284.3822
parenting$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-17 17:03:44\
# Project: parenting\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as pq\
import urllib\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for index in xrange(87):\
            self.crawl('http://www.parenting.com.tw/subcategory/10-健康與營養/?page=%s'%(index+1), callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('div.articleList ul li').items():\
            _dict = {}\
            _dict['title'] = each.find('.txtWrap h1 a').text()\
            url = each.find('.txtWrap h1 a').attr.href\
            _dict['cover'] = each.find('.coverPic').attr.style or ''\
            _dict['cover'] = 'http:' + _dict['cover'].split('url(')[-1].replace(');','')\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        \
            \
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        content_list = []\
        \
        for each in response.doc('div.articleContent > p').items():\
            info = each.html()\
            if info and u'延伸閱讀' in info :\
                break\
            if info and u'請勿轉載' in info :\
                continue\
            elif info:\
                content_list.append(info)\
           \
                \
        count = 0\
        for each in response.doc('ul.pagination li').items():\
            page = each.text()\
            if page and page.isdigit():\
                count = count + 1\
                \
        baseUrl = response.url\
        if count > 1:\
            for index in range(1,count):\
               try:\
                 next_info = pq(urllib.urlopen(baseUrl+'?page='+str(index+1)).read().decode('utf-8'))\
                 for each in next_info.find('div.articleContent > p').items():\
                    info = each.html()\
                    if info and u'延伸閱讀' in info:\
                        break\
                    if info and u'請勿轉載' in info :\
                        continue\
                    elif info:\
                        content_list.append(info)\
               except:\
                    continue                  \
                    \
        if not content_list:\
            return\
        res_dict['content'] = ''.join(['<p class="newClass">%s</p>'%s for s in content_list if s and s.strip()])\
        res_dict['date'] = response.doc('div.info').text().split()[0]\
        res_dict['subject'] = u'亲子'\
        res_dict['class'] = 33\
        res_dict['source'] = '跟谁学'\
        res_dict['url'] = response.url\
        res_dict['bread'] = [u'营养健康']\
        res_dict['data_weight'] = 0\
        return res_dict$$$$$1471581483.4126
proxy_monitor$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-03-19 17:37:58\
# Project: proxy_monitor\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        url = 'http://10.252.40.85:5000/tasks?project=gaokao_school_point_v2'\
        self.crawl(url, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
        }\
$$$$$1459164249.8550
qa_360$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-05 10:49:43\
# Project: qa_360\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        '''\
        with open('/apps/home/worker/100zhan.txt') as f:\
            for line in f:\
                subject = line.strip('\\n')\
                self.crawl('http://wenda.so.com/search/?q=' + subject + '&filt=20', save={'subject': subject}, callback=self.index_page)\
        '''\
        word = u'考证'\
        self.crawl('http://wenda.so.com/search/?q=' + word + '&filt=20', save={'subject': word}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        subject = response.save['subject']\
        for each in response.doc('.qa-i-hd a').items():\
            title = each.text()\
            self.crawl(each.attr.href,save={'title': title, 'subject': subject}, callback=self.detail_page)\
        for each in response.doc('.pagination a').items():\
            self.crawl(each.attr.href, save={'subject': subject}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for info in response.doc('.mod-resolved-ans > .bd').items():\
            _dic = {}\
            _dic['name'] = info.find('.text > div a').text()\
            _dic['date'] = info.find('.text > span').text().split()[-1].replace('.','-')\
            _dic['content'] = info.find('.resolved-cnt').html()\
            _dic['avatar'] = info.find('.info img').attr.src\
            content_list.append((_dic))\
        if not content_list:\
            return None\
        return {\
            "url": response.url,\
            "title": response.save['title'],\
            #"subject": response.save['subject'],\
            "subject": u'考证题库',\
            "answers": content_list,\
            "source": '360',\
            "question_detail": response.doc('.q-cnt').text(),\
            "class": 47,\
            "data_weight": 0,\
        }\
$$$$$1471335493.6510
qa_sougou$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-02 14:39:04\
# Project: sogou_wenwen\
\
from pyspider.libs.base_handler import *\
import re\
import pyquery\
import json\
\
class Handler(BaseHandler):\
   \
    crawl_config = {\
\
#'proxy': '111.56.13.152:80',\
            "headers":{\
'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\
'Accept-Encoding':'gzip, deflate, sdch',\
'Accept-Language':'zh-CN,zh;q=0.8,en;q=0.6',\
'Cache-Control':'max-age=0',\
'Connection':'keep-alive',\
'Cookie':'ww_search_tips=nulln; ww_sTitle=%u6570%u5B66%u77E5%u8BC6%u624B%u5DE5%u5236%u4F5C*GMAT*%u4E2D%u8003*%u6258%u798F; ww_filter=1; CXID=E52AD3C93ACC10138699410B1397001C; IPLOC=CN4201; SUID=E105133A5EC90D0A0000000056FE42CD; ssuid=2208855100; ww_orig_ref="http%3A%2F%2Fwenwen.sogou.com%2Fs%2F%3Fw%3D%25E6%2595%25B0%25E5%25AD%25A6%25E7%259F%25A5%25E8%25AF%2586%25E6%2589%258B%25E5%25B7%25A5%25E5%2588%25B6%25E4%25BD%259C%26st%3D4"; ld=blllllllll2glh9nlllllVtRwyclllllnPGVwZllll9lllllxylll5@@@@@@@@@@; MAIN_SESSIONID=n111i0ehc1fgbna386m03gin89ti.n11',\
'Host':'wenwen.sogou.com',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.76 Mobile Safari/537.36',\
    }\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        '''\
        with open('/apps/home/worker/100zhan.txt') as f:\
            for line in f:\
                self.crawl('http://wenwen.sogou.com/s/?w=' + line.strip('\\n') + '&search=%E6%90%9C%E7%8B%97%E6%90%9C%E7%B4%A2&st=4&ch=11', save={'subject': line.strip('\\n')}, callback=self.index_page)\
        '''\
        self.crawl('http://wenwen.sogou.com/s/?w=%E6%89%98%E7%A6%8F&st=4&ch=11', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        subject = '' #response.save.get('subject','')\
        for each in response.doc('a[href^="http://wenwen.sogou.com/z/"]').items():\
            title = each.text()\
            self.crawl(each.attr.href, save={'title': title, 'subject': subject}, callback=self.detail_page)\
        for each in response.doc('a[href^="http://wenwen.sogou.com/s/"]').items():\
            if '&pg=' in each.attr.href:\
                self.crawl(each.attr.href, save={'subject': subject}, callback=self.index_page)\
            \
        \
    @config(priority=2)\
    def detail_page(self, response):\
        title = response.save['title'] #response.doc('#questionTitle').text()        \
        \
        content_list = []\
        for info in response.doc('.satisfaction-answer').items():\
            _dic = {}\
            _dic['name'] = info.find('.question-info > div').text().split(' ')[0]\
            _dic['date'] = info.find('.question-info span').text()\
            _dic['content'] = info.find('.answer-con').html()\
            _dic['avatar'] = info.find('.user-pic img').attr.src\
            content_list.append((_dic))\
        for info in response.doc('.default-answer').items():\
            _dic = {}\
            _dic['name'] = info.find('.info-wrap').text().split(' ')[0]\
            _dic['date'] = info.find('.time').text()\
            _dic['content'] = info.find('.answer-con').html()\
            _dic['avatar'] = info.find('.user-pic img').attr.src\
            content_list.append((_dic))\
        try:\
            subject = response.save['subject']\
        except:\
            subject = None\
\
        if not content_list:\
            return\
        return {\
            "url": response.url,\
            "title": title,\
            "subject": subject,\
            "answers": content_list,\
            "question_detail": response.doc('.question-con').text(),\
        }$$$$$1460186026.7081
qiuzhi_jianliwang$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-07 11:01:22\
# Project: qiuzhi_kuaiji\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://jianli.yjbys.com/jianlimoban/dianzijianlimoban/', callback=self.index_page)\
\
        self.crawl('http://jianli.yjbys.com/jianlimoban/gerenjianlimoban/', callback=self.index_page)\
\
        self.crawl('http://jianli.yjbys.com/jianlimoban/yingwenjianlimoban/', callback=self.index_page)\
\
        self.crawl('http://jianli.yjbys.com/jianlimoban/qiuzhijianlimoban/', callback=self.index_page)\
\
\
    @config(age=1* 60)\
    def index_page(self, response):\
        for each in response.doc('dt > a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
        '''\
        for each in response.doc('.pagelist a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
        '''\
        \
\
    @config(priority=2)\
    def detail_page(self, response):\
        try:\
            table = '<table align="center">' +  response.doc('table').html() + '</table>'\
        except:\
            table = ''\
        #intro = response.doc('.content > p').text().split()[0]\
        return {\
            "url": response.url,\
            "title": response.doc('.title').text(),\
            "content": response.doc('.content').remove('script').remove('div').remove('a').html().replace('jianli.yjbys.com', '').replace('yjbys', '').replace('\\n',''), #intro + table, \
            "data_weight": 0,\
            "subject": u'求职',\
            "bread": [u'简历',],\
            "class": 46,\
            "source": u'简历网',\
        }\
$$$$$1471329820.5476
qiuzhi_shixisheng$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-16 10:50:19\
# Project: qiuzhi_shixisheng\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.yingjiesheng.com/commend-parttime-1.html', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.bg_0').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['title'] = each.find('a').text()\
            _save['date'] = each.find('.date').text()\
            #print _save['title'], _save['date']\
            self.crawl(url,save=_save, callback=self.detail_page)\
\
        for each in response.doc('.bg_1').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['title'] = each.find('a').text()\
            _save['date'] = each.find('.date').text()\
            self.crawl(url, save=_save, callback=self.detail_page)\
        '''\
        for each in response.doc('.page > a').items():\
            url = each.attr.href\
            self.crawl(url, callback=self.index_page)\
        '''\
    \
    @config(priority=2)\
    def detail_page(self, response):\
        res = response.save\
        res["url"] = response.url\
        res["content"] = response.doc('.jobIntro').html()\
        res["subject"] = u'求职'\
        res["source"] = u'yingjiesheng'\
        res['bread'] = [u'实习生',]\
        if not res['content']: \
            res['content'] = response.doc('.job_list').html()\
        res['class'] = 46 \
        res['data_weight'] = 0\
        return res\
$$$$$1471329837.9661
qiuzhi_shixisheng_shixisheng$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-11 19:09:16\
# Project: qiuzhi_shixi_shixisheng\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    dic = {\
    "opj_p0wwm34iuy3y": [\
        {\
            "name": "教育",\
            "child": [\
                {\
                    "uuid": "opj_ydckmvemhhj9",\
                    "name": "教务"\
                },\
                {\
                    "uuid": "opj_i2atsrq7meb2",\
                    "name": "教师"\
                },\
                {\
                    "uuid": "opj_gsfxm6khk1mw",\
                    "name": "幼教"\
                },\
                {\
                    "uuid": "opj_anu5gkjal5cq",\
                    "name": "培训"\
                },\
                {\
                    "uuid": "opj_ovpebzlsbjbs",\
                    "name": "课程"\
                }\
            ]\
        },\
        {\
            "name": "咨询",\
            "child": [\
                {\
                    "uuid": "opj_v7lu487vtqcj",\
                    "name": "咨询/顾问"\
                }\
            ]\
        }\
    ],\
    "opj_4mun5l8wqssz": [\
        {\
            "name": "软件",\
            "child": [\
                {\
                    "uuid": "opj_twuuznrzmbdu",\
                    "name": "IOS"\
                },\
                {\
                    "uuid": "opj_frm2gpvkcpua",\
                    "name": "数据库"\
                },\
                {\
                    "uuid": "opj_worxhycw0gym",\
                    "name": "C#/.NET"\
                },\
                {\
                    "uuid": "opj_fwnc2trdffiw",\
                    "name": "Hadoop"\
                },\
                {\
                    "uuid": "opj_edurcbhia2n3",\
                    "name": "Android"\
                },\
                {\
                    "uuid": "opj_qc9afdwzavw9",\
                    "name": "算法"\
                },\
                {\
                    "uuid": "opj_dzsdsfwm0ntt",\
                    "name": "IT运维"\
                },\
                {\
                    "uuid": "opj_bgq6sdxqx1i8",\
                    "name": "Python"\
                },\
                {\
                    "uuid": "opj_rjdqlspswhli",\
                    "name": "云计算/大数据"\
                },\
                {\
                    "uuid": "opj_lsvexg8ehsjx",\
                    "name": "Node.js"\
                },\
                {\
                    "uuid": "opj_wbbl3ezi6ecv",\
                    "name": "数据挖掘"\
                },\
                {\
                    "uuid": "opj_bkbverpen0w3",\
                    "name": "PHP"\
                },\
                {\
                    "uuid": "opj_qpgp7xcerkex",\
                    "name": "Ruby/Perl"\
                },\
                {\
                    "uuid": "opj_ykka3ldcn21u",\
                    "name": "测试"\
                },\
                {\
                    "uuid": "opj_dxnurenognxo",\
                    "name": "Java"\
                },\
                {\
                    "uuid": "opj_jlm8stx4zz26",\
                    "name": "C/C++"\
                },\
                {\
                    "uuid": "opj_snjlgjshvfwn",\
                    "name": "前端"\
                }\
            ]\
        },\
        {\
            "name": "运营",\
            "child": [\
                {\
                    "uuid": "opj_ielyo96oix78",\
                    "name": "新媒体"\
                },\
                {\
                    "uuid": "opj_okd5xorkjaks",\
                    "name": "内容运营"\
                },\
                {\
                    "uuid": "opj_uu6f6lasdshg",\
                    "name": "编辑"\
                },\
                {\
                    "uuid": "opj_ysojgsemw0fe",\
                    "name": "SEO"\
                },\
                {\
                    "uuid": "opj_cmp8uy9f6v2m",\
                    "name": "产品运营"\
                }\
            ]\
        },\
        {\
            "name": "硬件",\
            "child": [\
                {\
                    "uuid": "opj_rnbmq9wmvb54",\
                    "name": "嵌入式"\
                },\
                {\
                    "uuid": "opj_an2cblxq5b7w",\
                    "name": "集成电路"\
                }\
            ]\
        },\
        {\
            "name": "设计",\
            "child": [\
                {\
                    "uuid": "opj_mrxvzqkechw2",\
                    "name": "Flash"\
                },\
                {\
                    "uuid": "opj_0ic5vg9cusry",\
                    "name": "UI/UE"\
                },\
                {\
                    "uuid": "opj_uvbdobspakmu",\
                    "name": "特效"\
                },\
                {\
                    "uuid": "opj_5upcwefmaqhj",\
                    "name": "网页/美工"\
                },\
                {\
                    "uuid": "opj_uvhgcoyjkeyl",\
                    "name": "2D/3D"\
                }\
            ]\
        },\
        {\
            "name": "通信",\
            "child": [\
                {\
                    "uuid": "opj_ppqp36lwiw1i",\
                    "name": "物联网"\
                },\
                {\
                    "uuid": "opj_gy6kzykxrpiq",\
                    "name": "射频"\
                },\
                {\
                    "uuid": "opj_unqfld9abm0b",\
                    "name": "通信"\
                }\
            ]\
        },\
        {\
            "name": "产品",\
            "child": [\
                {\
                    "uuid": "opj_zbmvshsqlpco",\
                    "name": "用户研究"\
                },\
                {\
                    "uuid": "opj_zs7hmkfz698q",\
                    "name": "产品助理"\
                }\
            ]\
        }\
    ],\
    "opj_32vfxjgfgkad": [\
        {\
            "name": "金融",\
            "child": [\
                {\
                    "uuid": "opj_tp0s0i4mg3jc",\
                    "name": "基金"\
                },\
                {\
                    "uuid": "opj_nrvscepwebo6",\
                    "name": "证券"\
                },\
                {\
                    "uuid": "opj_scgzknrfc9o8",\
                    "name": "风控"\
                },\
                {\
                    "uuid": "opj_pr9lpdarivfz",\
                    "name": "金融"\
                }\
            ]\
        },\
        {\
            "name": "投资",\
            "child": [\
                {\
                    "uuid": "opj_e7omu0xnhmi1",\
                    "name": "分析师"\
                },\
                {\
                    "uuid": "opj_swxgid8t2ylp",\
                    "name": "投资"\
                }\
            ]\
        },\
        {\
            "name": "法务",\
            "child": [\
                {\
                    "uuid": "opj_g3ufnzwf3ic7",\
                    "name": "合规"\
                },\
                {\
                    "uuid": "opj_5pv0h0gmmks4",\
                    "name": "律师"\
                },\
                {\
                    "uuid": "opj_d00vwoe9xk2f",\
                    "name": "法务"\
                }\
            ]\
        },\
        {\
            "name": "银行",\
            "child": [\
                {\
                    "uuid": "opj_yiffuqvioqk4",\
                    "name": "客户经理"\
                },\
                {\
                    "uuid": "opj_v8dzgkcdej8i",\
                    "name": "部门经理"\
                },\
                {\
                    "uuid": "opj_dfhiwt0i19fe",\
                    "name": "贷款"\
                },\
                {\
                    "uuid": "opj_a9ayetvpyzdw",\
                    "name": "大堂经理"\
                }\
            ]\
        },\
        {\
            "name": "保险",\
            "child": [\
                {\
                    "uuid": "opj_p2ifplcmars6",\
                    "name": "业务"\
                },\
                {\
                    "uuid": "opj_qww5tuazg0zs",\
                    "name": "保单"\
                }\
            ]\
        },\
        {\
            "name": "财会",\
            "child": [\
                {\
                    "uuid": "opj_a3hiwtqufpe2",\
                    "name": "审计"\
                },\
                {\
                    "uuid": "opj_hxlreh5cirnh",\
                    "name": "税务"\
                },\
                {\
                    "uuid": "opj_ydfam0kaxoa7",\
                    "name": "财务"\
                },\
                {\
                    "uuid": "opj_b38dn1wq26td",\
                    "name": "会计/出纳"\
                }\
            ]\
        }\
    ],\
    "opj_v7ygyfozpgy2": [\
        {\
            "name": "商务",\
            "child": [\
                {\
                    "uuid": "opj_jlsgwadhibpr",\
                    "name": "商务"\
                },\
                {\
                    "uuid": "opj_afjnklufvsfh",\
                    "name": "招投标"\
                }\
            ]\
        },\
        {\
            "name": "销售",\
            "child": [\
                {\
                    "uuid": "opj_qwrmbcrtzaud",\
                    "name": "推广"\
                },\
                {\
                    "uuid": "opj_dkhdshammijx",\
                    "name": "销售"\
                }\
            ]\
        },\
        {\
            "name": "公关",\
            "child": [\
                {\
                    "uuid": "opj_uuhho9meehxo",\
                    "name": "媒介"\
                },\
                {\
                    "uuid": "opj_fli2uh3axvam",\
                    "name": "公关"\
                }\
            ]\
        },\
        {\
            "name": "客服",\
            "child": [\
                {\
                    "uuid": "opj_eiy2odktulsm",\
                    "name": "客户服务"\
                },\
                {\
                    "uuid": "opj_jcvrwy2p6up9",\
                    "name": "销售支持"\
                }\
            ]\
        },\
        {\
            "name": "市场",\
            "child": [\
                {\
                    "uuid": "opj_xvmkicckmgan",\
                    "name": "渠道"\
                },\
                {\
                    "uuid": "opj_xtkcoq6vmcdr",\
                    "name": "分析/调研"\
                },\
                {\
                    "uuid": "opj_jf4xyp7jigdd",\
                    "name": "策划"\
                },\
                {\
                    "uuid": "opj_icis7su6fxju",\
                    "name": "品牌"\
                },\
                {\
                    "uuid": "opj_egxi3f7yog0h",\
                    "name": "市场"\
                }\
            ]\
        }\
    ],\
    "opj_g68s2gm09jdv": [\
        {\
            "name": "人力资源",\
            "child": [\
                {\
                    "uuid": "opj_acrvakvqyzik",\
                    "name": "人事/HR"\
                },\
                {\
                    "uuid": "opj_qtc4so3cydmo",\
                    "name": "招聘"\
                },\
                {\
                    "uuid": "opj_awun8u4ogl3m",\
                    "name": "企业文化"\
                }\
            ]\
        },\
        {\
            "name": "猎头",\
            "child": [\
                {\
                    "uuid": "opj_ytjvc22kmoob",\
                    "name": "猎头"\
                }\
            ]\
        },\
        {\
            "name": "行政",\
            "child": [\
                {\
                    "uuid": "opj_y7p3orvwtr2v",\
                    "name": "行政"\
                },\
                {\
                    "uuid": "opj_m1mxy4ydijfc",\
                    "name": "前台"\
                },\
                {\
                    "uuid": "opj_89efrspgjjcn",\
                    "name": "助理"\
                }\
            ]\
        }\
    ],\
    "opj_cn9j3euwuvtl": [\
        {\
            "name": "外语",\
            "child": [\
                {\
                    "uuid": "opj_gs8yoyp0nefd",\
                    "name": "英语"\
                },\
                {\
                    "uuid": "opj_0hdaz9wznwv2",\
                    "name": "日语"\
                },\
                {\
                    "uuid": "opj_ys0eyzn5kkbt",\
                    "name": "翻译"\
                }\
            ]\
        },\
        {\
            "name": "外贸",\
            "child": [\
                {\
                    "uuid": "opj_b0ktbhlgvfnb",\
                    "name": "报关员"\
                },\
                {\
                    "uuid": "opj_z2vez1xgsf0x",\
                    "name": "外贸专员"\
                }\
            ]\
        }\
    ],\
    "opj_jxvy9fuotcvj": [\
        {\
            "name": "广告",\
            "child": [\
                {\
                    "uuid": "opj_l7utfoe88ylr",\
                    "name": "创意"\
                },\
                {\
                    "uuid": "opj_gojfv3twwx5r",\
                    "name": "策划"\
                },\
                {\
                    "uuid": "opj_x3tgzqznj1dj",\
                    "name": "AE"\
                }\
            ]\
        },\
        {\
            "name": "编辑",\
            "child": [\
                {\
                    "uuid": "opj_mspx4safdbwp",\
                    "name": "编辑/采编"\
                },\
                {\
                    "uuid": "opj_2672dgvnktgm",\
                    "name": "校对/排版"\
                }\
            ]\
        },\
        {\
            "name": "设计",\
            "child": [\
                {\
                    "uuid": "opj_fxel0nldueqw",\
                    "name": "美术设计"\
                },\
                {\
                    "uuid": "opj_zrgyhehcendi",\
                    "name": "工业设计"\
                },\
                {\
                    "uuid": "opj_izptkxcdrwdb",\
                    "name": "平面设计"\
                },\
                {\
                    "uuid": "opj_2xaf5axvltwq",\
                    "name": "视觉设计"\
                }\
            ]\
        },\
        {\
            "name": "媒体",\
            "child": [\
                {\
                    "uuid": "opj_mgnrscpzykac",\
                    "name": "记者"\
                },\
                {\
                    "uuid": "opj_rc5eetcarzu5",\
                    "name": "主持/播音"\
                },\
                {\
                    "uuid": "opj_vu4ra0k6bmiz",\
                    "name": "编导"\
                }\
            ]\
        },\
        {\
            "name": "艺术",\
            "child": [\
                {\
                    "uuid": "opj_bd9fqvuufgg6",\
                    "name": "演艺"\
                },\
                {\
                    "uuid": "opj_xziotarow0eq",\
                    "name": "摄影"\
                }\
            ]\
        }\
    ],\
    "opj_e4tvvgcavs2j": [\
        {\
            "name": "体育快消",\
            "child": [\
                {\
                    "uuid": "opj_ilurdnsopa6g",\
                    "name": "快消"\
                },\
                {\
                    "uuid": "opj_z8dhzwpigjlk",\
                    "name": "体育"\
                }\
            ]\
        },\
        {\
            "name": "机械制造",\
            "child": [\
                {\
                    "uuid": "opj_vbauw4zsmsnn",\
                    "name": "质量"\
                },\
                {\
                    "uuid": "opj_oheplpt1dlow",\
                    "name": "机械设计"\
                },\
                {\
                    "uuid": "opj_urjflgxfv4dd",\
                    "name": "生产"\
                },\
                {\
                    "uuid": "opj_hf93dmd6ombh",\
                    "name": "安全"\
                },\
                {\
                    "uuid": "opj_327srkkqritm",\
                    "name": "设备"\
                },\
                {\
                    "uuid": "opj_un7mpmwbhwhs",\
                    "name": "自动化"\
                }\
            ]\
        },\
        {\
            "name": "物流采购",\
            "child": [\
                {\
                    "uuid": "opj_bcwafqijkvfc",\
                    "name": "采购"\
                },\
                {\
                    "uuid": "opj_bqhq5es6a7mk",\
                    "name": "供应链"\
                },\
                {\
                    "uuid": "opj_4zrljtoab9qg",\
                    "name": "物流"\
                }\
            ]\
        },\
        {\
            "name": "建筑房产",\
            "child": [\
                {\
                    "uuid": "opj_fvmfqadehva7",\
                    "name": "城规/市政"\
                },\
                {\
                    "uuid": "opj_itkittci96ka",\
                    "name": "工程造价"\
                },\
                {\
                    "uuid": "opj_kxy7uvay18ez",\
                    "name": "建筑"\
                },\
                {\
                    "uuid": "opj_waxyjpf3jy45",\
                    "name": "土木"\
                },\
                {\
                    "uuid": "opj_prf03bdanvzr",\
                    "name": "园林"\
                },\
                {\
                    "uuid": "opj_svfrapw9izjb",\
                    "name": "地产开发/策划"\
                },\
                {\
                    "uuid": "opj_0ajhcvn62omh",\
                    "name": "房产销售"\
                },\
                {\
                    "uuid": "opj_4bu0spgnss1c",\
                    "name": "给排水"\
                },\
                {\
                    "uuid": "opj_kj9bj8nck8n2",\
                    "name": "物业管理"\
                }\
            ]\
        },\
        {\
            "name": "生物医疗",\
            "child": [\
                {\
                    "uuid": "opj_a6wuiaia7zz7",\
                    "name": "医生"\
                },\
                {\
                    "uuid": "opj_fntj9r5yanty",\
                    "name": "医药"\
                },\
                {\
                    "uuid": "opj_cvbkzomriwym",\
                    "name": "生物"\
                },\
                {\
                    "uuid": "opj_jqoi0qrflscd",\
                    "name": "护理"\
                }\
            ]\
        },\
        {\
            "name": "能源环保",\
            "child": [\
                {\
                    "uuid": "opj_odmfoqvehqd0",\
                    "name": "矿产"\
                },\
                {\
                    "uuid": "opj_y5xs9hzcw8rw",\
                    "name": "能源"\
                },\
                {\
                    "uuid": "opj_ctqjqjhzzftu",\
                    "name": "环保"\
                }\
            ]\
        },\
        {\
            "name": "食品材料",\
            "child": [\
                {\
                    "uuid": "opj_grb2ruhukb8i",\
                    "name": "材料"\
                },\
                {\
                    "uuid": "opj_8mj8jkeniaev",\
                    "name": "食品"\
                }\
            ]\
        },\
        {\
            "name": "NGO公益",\
            "child": [\
                {\
                    "uuid": "opj_ijdiapsz9edf",\
                    "name": "志愿者"\
                }\
            ]\
        }\
    ],\
    "opj_texrs8przurn": [\
        {\
            "name": "电子",\
            "child": [\
                {\
                    "uuid": "opj_mpcknopmcbsn",\
                    "name": "光电"\
                },\
                {\
                    "uuid": "opj_gdsqvqa3xrjf",\
                    "name": "半导体/芯片"\
                },\
                {\
                    "uuid": "opj_4ydypxlridqr",\
                    "name": "电子工程"\
                }\
            ]\
        },\
        {\
            "name": "电气",\
            "child": [\
                {\
                    "uuid": "opj_3zamopr7jxkd",\
                    "name": "电气设计"\
                },\
                {\
                    "uuid": "opj_ufpnyxgzxeyj",\
                    "name": "电气工程"\
                }\
            ]\
        }\
    ]\
}\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k in self.dic:\
            for childs in self.dic[k]:\
                for child in childs['child']:\
                    name = child['name']\
                    for i in range(1, 5):\
                        self.crawl('http://www.shixiseng.com/interns?k=' + name + '&p=%d'%i, callback=self.index_page)\
\
    @config(age=1 * 60)\
    def index_page(self, response):\
        for each in response.doc('.job_inf_inf').items():\
            title = '【%s】%s招聘%s实习生'%(each.find('.addr_box > span').text(), each.find('.company_name').text() , each.find('a > h3').text())\
            self.crawl(each.find('a').attr.href, save={'title': title}, callback=self.detail_page)\
    \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.parent_job > li').items():\
            self.crawl(each.attr.href, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        title = response.save['title']\
        return {\
            "url": response.url,\
            "title": title,\
            "content": response.doc('.dec_content').html(),\
            "date": response.doc('.update_time').text()[:10],\
            "data_weight": 0,\
            "subject": u'求职',\
            "bread": [u'实习生',],\
            "class": 46,\
            "source": u'shixisheng',\
        }\
$$$$$1471329830.4341
qiuzhi_xuanjianghui_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-15 17:25:49\
# Project: qiuzhi_yingjiesheng\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://my.yingjiesheng.com/xuanjianghui.html', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.bg0').items():\
            tr = [info for info in  each.find('td').items()]\
            _dict = {}\
            _dict['city'] =  tr[0].text()\
            _dict['date'] = tr[1].text()\
            _dict['hour'] = tr[2].find('img').attr.src\
            _dict['company'] = tr[3].text()\
            url = tr[3].find('a').attr.href\
            _dict['school'] = tr[4].text()\
            _dict['address'] = tr[5].text()\
            self.crawl(url, save=_dict, callback=self.detail_page)\
        for each in response.doc('.bg1').items():\
            tr = [info for info in  each.find('td').items()]\
            _dict = {}\
            _dict['city'] =  tr[0].text()\
            _dict['date'] = tr[1].text()\
            _dict['hour'] = tr[2].find('img').attr.src\
            _dict['company'] = tr[3].text()\
            url = tr[3].find('a').attr.href\
            _dict['school'] = tr[4].text()\
            _dict['address'] = tr[5].text()\
            self.crawl(url, save=_dict, callback=self.detail_page)\
        #for each in response.doc('.page > a').items():\
        #    self.crawl(each.attr.href, callback=self.index_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res = response.save\
        res["url"] = response.url\
        res["title"] = response.doc('.xjh_infos_h1').text()\
        res["content"] = response.doc('.colspan2').html()\
        res['bread'] = [u'宣讲会']\
        res['subject'] = u'求职'\
        res['source'] = u'yingjiesheng'\
        return res\
$$$$$1471329841.7512
qiuzhi_zhaopin$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-16 10:50:19\
# Project: qiuzhi_shixi\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.yingjiesheng.com/commend-fulltime-1.html', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.bg_0').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['title'] = each.find('a').text()\
            _save['date'] = each.find('.date').text()\
            #print _save['title'], _save['date']\
            self.crawl(url,save=_save, callback=self.detail_page)\
\
        for each in response.doc('.bg_1').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['title'] = each.find('a').text()\
            _save['date'] = each.find('.date').text()\
            self.crawl(url, save=_save, callback=self.detail_page)\
        '''\
        for each in response.doc('.page > a').items():\
            url = each.attr.href\
            self.crawl(url, callback=self.index_page)\
        '''\
    @config(priority=2)\
    def detail_page(self, response):\
        res = response.save\
        res["url"] = response.url\
        res["content"] = response.doc('.jobIntro').html()\
        res["subject"] = u'求职'\
        res["source"] = u'yingjiesheng'\
        res['bread'] = [u'招聘信息',]\
        if not res['content']: \
            res['content'] = response.doc('.job_list').html()\
        res['class'] = 46 \
        res['data_weight'] = 0\
        return res\
$$$$$1471329846.4821
qq_wenda$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-09 15:09:15\
# Project: qq_wenda\
\
from pyspider.libs.base_handler import *\
import time\
import datetime\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
\
\
def get_Date(d):\
    if not d or u'分钟' in d or u'小时' in d:\
        return time.strftime('%Y-%m-%d',time.localtime())\
    elif u'1天前' in d:\
         now_time = datetime.datetime.now()\
         yes_time = now_time + datetime.timedelta(days=-1)\
         return yes_time.strftime('%Y-%m-%d')\
    elif u'2天前' in d:\
         now_time = datetime.datetime.now()\
         yes_time = now_time + datetime.timedelta(days=-2)\
         return yes_time.strftime('%Y-%m-%d')\
    else:\
        return d\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.2',\
        'headers':{\
            'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://wenwen.qq.com/cate/?cid=0&tp=6&pg=', callback=self.list_page)\
\
    \
    @config(age=24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('ul.category-list li a').items():\
            _dict = {}\
            _dict['bread'] = each.text()\
            #_dict['bread'].append(each.text())\
            self.crawl(each.attr.href, save = _dict,callback=self.type_page)\
            \
    \
    @config(age=24 * 60 * 60)\
    def type_page(self, response):\
        for each in response.doc('ul.category-list li a').items():\
            _dict = {}\
            _dict['bread'] = [ response.save.get('bread') , each.text() ]\
            \
            #_dict['bread'].append(each.text())\
            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\
        \
    @config(age=24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('div.lib-list li').items():\
            url = each.find('.qa-title a').eq(0).attr.href\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('.qa-title a').eq(0).text()\
            #create_time = each.find('div.qa-info .t').text()\
            #_dict['create_time'] = get_Date(create_time)\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('.pagination a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            #_dict['title'] = each.find('.qa-title a').eq(0).text()\
            #create_time = each.find('div.qa-info .t').text()\
            #_dict['create_time'] = get_Date(create_time)\
            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\
            \
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['question_detail'] = response.doc('h3#questionTitle').html()\
        res_dict['create_user'] = response.doc('.question-head a').eq(0).text()\
        res_dict['url'] = response.url\
        res_dict['source'] = '搜狗问问'\
        res_dict['subject'] = u'主站问答'\
        res_dict['class'] = 34\
        res_dict['data_weight'] = 0\
        res_dict['create_time'] = response.doc('.question-head .time').text().split()[0]\
        answers_list = []\
        for each in response.doc('.satisfaction-answer').items():\
                _dict = {}\
                #_dict['img'] = each.find('.user-conent img').eq(0).attr.src\
                _dict['user_name'] = each.find('.user-conent .question-info').text().split()[0]\
                _dict['create_time'] = each.find('.user-conent .question-info').text().split()[-1]\
                _dict['content'] = each.find('.answer-con').html()\
                answers_list.append(_dict)\
        for each in response.doc('.answer-wrap .default-answer').items():\
                _dict = {}\
                #_dict['img'] = each.find('.user-conent img').eq(0).attr.src\
                _dict['user_name'] = each.find('.user-conent .question-info').text().split()[0]\
                _dict['create_time'] = each.find('.user-conent .question-info').text().split()[-1]\
                _dict['content'] = each.find('.answer-con').html()\
                answers_list.append(_dict)\
        if not answers_list:\
            return\
        res_dict['answers'] = answers_list\
        return res_dict\
$$$$$1469236598.0451
question_360$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-05 10:49:43\
# Project: qa_360\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    'proxy': '111.13.136.46:80',\
 "headers":{\
         \
'Cache-Control': 'no-cache',\
'Connection': 'keep-alive',\
'Host': 'wenda.so.com',\
'Pragma': 'no-cache',\
'Upgrade-Insecure-Requests': '1',\
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        #with open('/apps/home/worker/xuzhihao/school_list') as f:\
        #    for line in f:\
        #        word = line.strip('\\t')\
        word = u'高考'\
        self.crawl('http://wenda.so.com/search/?q=' + word + '&filt=20', headers= self.crawl_config['headers'], save={'school': word}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        school = response.save['school']\
        for each in response.doc('.qa-i-hd a').items():\
            title = each.text()\
            self.crawl(each.attr.href,save={'title': title, 'school': response.save['school']}, callback=self.detail_page)\
        for each in response.doc('.pagination a').items():\
            self.crawl(each.attr.href, save={'school': school}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for info in response.doc('.mod-resolved-ans > .bd').items():\
            _dic = {}\
            _dic['name'] = info.find('.text > div a').text()\
            _dic['date'] = info.find('.text > span').text().split()[-1].replace('.','-')\
            _dic['content'] = info.find('.resolved-cnt').html()\
            _dic['avatar'] = info.find('.info img').attr.src\
            content_list.append((_dic))\
        return {\
            "url": response.url,\
            "title": response.doc('.js-ask-title').text(),\
            "school": response.save['school'],\
            "answers": content_list,\
            "question_detail": response.doc('.q-cnt').text(),\
        }\
$$$$$1469000269.4831
question_baidu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-26 19:40:36\
# Project: question_baidu\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'headers': {\
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36',\
            'Host':'zhidao.baidu.com',\
            'Referer': 'http://zhidao.baidu.com/question/295410048.html?fr=iks&word=%C8%FD%D1%C7%B3%C7%CA%D0%D6%B0%D2%B5%D1%A7%D4%BA&ie=gbk'\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for line in open('/apps/home/worker/xuzhihao/school_list'):\
            data = line.strip('\\n')\
            self.crawl('http://zhidao.baidu.com/search?ct=17&pn=0&tn=ikaslist&rn=10&word=' + data + '&fr=wwwt', save={'ques': data}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        ques = response.save['ques']\
        for each in response.doc('.dl > .line > a').items():\
            url = each.attr.href\
            if url.startswith('http://zhidao.baidu.com/'):\
                self.crawl(each.attr.href,save={'ques': ques}, fetch_type='js', callback=self.detail_page)\
               \
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for info in response.doc('.answer').items(): \
            _dic = {}\
            _dic['name'] = info.find('.question-info > div').text().split(' ')[0]\
            _dic['date'] = info.find('.question-info span').text()\
            _dic['content'] = info.find('.content').html()\
            _dic['avatar'] = info.find('.user-pic img').attr.src\
            content_list.append((_dic))\
        return {\
            "url": response.url,\
            "title": response.doc('.ask-title').text(),\
            "content": content_list,\
            "ques": response.save['ques'],\
        }\
$$$$$1462354837.4214
question_baidu_2$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-26 19:40:36\
# Project: question_baidu\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'headers': {\
            'Cookie': 'ipInfoCity=undefined; BIDUPSID=E86D6D1C5EB675F2CB9CF17431C6D0F8; PSTM=1456297492; IK_CID_84=1; IK_CID_85=1; IK_CID_74=12; IK_CID_83=6; BDUSS=mhPREFSNVN5QmJhWWlTQU05OHlrTFJ1RXNncU8wTjJ4ZzlDYXhFV3JafklRRXBYQVFBQUFBJCQAAAAAAAAAAAEAAACa-JFPsNm80rulwaq4-sut0acAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMizIlfIsyJXa; plus_cv=0::m:1-nav:6290bd16-hotword:40d8db89; BAIDUID=12455DCFF0A2EEB24A1668FB3AC77E9E:FG=1; IK_CID_1031=1; IK_CID_78=1; IK_CID_82=2; IK_CID_80=3; BDRCVFR[HE-BYYE3Dh3]=I67x6TjHwwYf0; pgv_pvi=2805833728; pgv_si=s8603317248; SFSSID=ts1e5cc3g3392p34livqhqn532; MCITY=-%3A; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; H_PS_PSSID=18880_18286_1453_7477_19671_18280_19805_19899_19559_19808_19843_19902_19860_15142_11478_19910; IK_USERVIEW=1; Hm_lvt_6859ce5aaf00fb00387e6434e4fcc925=1461937476,1461937578,1461938068,1462354380; Hm_lpvt_6859ce5aaf00fb00387e6434e4fcc925=1462354429; IK_12455DCFF0A2EEB24A1668FB3AC77E9E=8; IK_CID_1=5; Hm_lvt_16bc67e4f6394c05d03992ea0a0e9123=1461671023,1462354505; Hm_lpvt_16bc67e4f6394c05d03992ea0a0e9123=1462354547',\
        'Host':'zhidao.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1',\
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36',\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        #for line in open('/apps/home/worker/xuzhihao/school_list'):\
        #    data = line.strip('\\n')\
        data = '高考'\
        self.crawl('http://zhidao.baidu.com/search?ct=17&pn=0&tn=ikaslist&rn=10&word=' + data + '&fr=wwwt', save={'school': data}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.dl > .line > a').items():\
            self.crawl(each.attr.href, save={'school': response.save['school']}, callback=self.detail_page)\
        for each in response.doc('.list-inner div > a').items():\
            self.crawl(each.attr.href, save={'school': response.save['school']}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        _dic = {}\
        _dic['content'] = response.doc('.quality-content-detail').html()\
        content_list.append(_dic)\
        for info in response.doc('.answer').items(): \
            _dic = {}\
            _dic['name'] = info.find('.question-info > div').text().split(' ')[0]\
            _dic['date'] = info.find('.pos-time').text()\
\
            content = info.find('.best-text').html()\
            if not content:\
                content = info.find('.answer-text').html()\
            if not content:\
                content = info.html()\
            _dic['content'] = content\
            _dic['avatar'] = info.find('.user-pic img').attr.src\
            content_list.append((_dic))\
        return {\
            "url": response.url,\
            "title": response.doc('.ask-title').text(),\
            "content": content_list,\
            "school": response.save['school'],\
        }$$$$$1464604257.9534
question_sogou$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-02 14:39:04\
# Project: sogou_wenwen\
\
from pyspider.libs.base_handler import *\
import re\
import pyquery\
import json\
\
class Handler(BaseHandler):\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        for line in open('/apps/home/worker/xuzhihao/question'):\
            word = line.split()[0]\
            subject = line.split()[-1]\
            self.crawl('http://wenwen.sogou.com/s/?w=' + word + '&st=4&ch=11', save={'ques': line.split()}, callback=self.index_page)\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        ques = response.save['ques']\
        for each in response.doc('a[href^="http://wenwen.sogou.com/z/"]').items():\
            title = each.text()\
            self.crawl(each.attr.href, save={'title': title, 'ques': ques}, callback=self.detail_page)\
            break\
        '''\
        for each in response.doc('a[href^="http://wenwen.sogou.com/s/"]').items():\
            if '&pg=' in each.attr.href:\
                self.crawl(each.attr.href, save={'bread': bread},  callback=self.index_page)\
        '''\
            \
        \
    @config(priority=2)\
    def detail_page(self, response):\
        title = response.save['title'] #response.doc('#questionTitle').text()        \
        \
        content_list = []\
        for info in response.doc('.satisfaction-answer').items():\
            _dic = {}\
            _dic['name'] = info.find('.question-info > div').text().split(' ')[0]\
            _dic['date'] = info.find('.question-info span').text()\
            _dic['content'] = info.find('.answer-con').html()\
            _dic['avatar'] = info.find('.user-pic img').attr.src\
            content_list.append((_dic))\
        for info in response.doc('.default-answer').items():\
            _dic = {}\
            _dic['name'] = info.find('.info-wrap').text().split(' ')[0]\
            _dic['date'] = info.find('.time').text()\
            _dic['content'] = info.find('.answer-con').html()\
            _dic['avatar'] = info.find('.user-pic img').attr.src\
            content_list.append((_dic))\
        for info in response.doc('.resolved > div').items():\
            _dic = {}\
            _dic['name'] = info.find('.info-wrap').text().split(' ')[0]\
            _dic['date'] = info.find('.time').text()\
            _dic['content'] = info.find('.answer-con').html()\
            _dic['avatar'] = info.find('.user-pic img').attr.src\
            content_list.append((_dic)) \
        try:\
            subject = response.save['bread']\
        except:\
            subject = None\
\
        if not content_list:\
            return\
        return {\
            "url": response.url,\
            "title": title,\
            "subject": subject,\
            "answers": content_list,\
            "ques": response.save['ques'],\
            "question_detail": response.doc('.question-con').text(),\
        }$$$$$1462411543.9880
question_wenda_gaokao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-05 14:55:40\
# Project: question_wenda_gaokao\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://gaokao.chsi.com.cn/zxdy/zxs--method-listDefault,forumid-6267733,year-2007,start-0.dhtml', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
\
        for each in response.doc('.ulPage a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
        _list = []    \
        for each in response.doc('.question_cnt_tr > td > div').items():\
            answer = each.remove('strong').find('.question_a').html()\
            question = each.remove('div').text()\
            if '<a' in answer:\
                self.crawl(each.find('a').attr.href, callback=self.detail_page)\
            else:\
                _list.append({\
                    "url": response.url,\
                    "question": question,\
                    "answer": answer,\
                })\
        return _list\
          \
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
            "answer": ''.join(['<p>%s</p>'%each.text() for each in response.doc('p').items()]),\
            "question": response.doc('.question_cnt_tr > td > div').remove('div').text(),\
        }\
$$$$$1463388055.4471
sat_zhan_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-29 18:19:22\
# Project: sat_xiaozhan\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    type_dict = {\
        'yufa': u'SAT语法',\
        'yuedu': u'SAT阅读',\
        'xiezuo': u'SAT写作',\
        'shuxue': u'SAT数学',\
        'zonghe': u'SAT其他',\
        'jihua': u'备考计划',\
        'tifen/beikao': u'备考计划',\
        'gaofen': u'高分心得',\
        'fuxi': u'复习攻略',\
\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for type, v in self.type_dict.iteritems():\
            self.crawl('http://sat.zhan.com/%s/'%type, save={'bread': v}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/sat/kaoqian/', save={'bread': u'冲刺宝典'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/sat/kaohou/', save={'bread': u'包括指南'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/sat/fukao/', save={'bread': u'冲刺宝典'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/sat/beikao/', save={'bread': u'备考计划'}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.experience-item').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['date'] = each.find('.index-middle-info-3 > .pull-left').text().split()[0]\
            #_save['tag'] = each.find('dl > .pull-left > a').text().split()\
            _save['brief'] = each.find('.index-middle-info-2').text()\
            _save['bread'] = bread\
            self.crawl(url, save=_save, callback=self.detail_page)\
        for each in response.doc('.things_list > .pull-right').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['date'] = each.find('.padding_right_10').text()\
            #_save['tag'] = each.find('dl > .pull-left > a').text().split()\
            _save['brief'] = each.find('.text').text()\
            _save['bread'] = bread\
            self.crawl(url, save=_save, callback=self.detail_page)\
\
        #for each in response.doc('nav a').items():\
        #    self.crawl(each.attr.href, save={'bread': bread}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict["url"] = response.url\
        res_dict["title"] = response.doc('h1').text()\
        content_list = []\
        for each in  response.doc('.article-content > p').items():\
            content_list.append((each.html().replace('\\r','').replace('\\n','').replace('\\t','').replace(u'小站','')))\
        content_list = content_list[:-1]\
        if not content_list:\
            return None\
        res_dict['content'] = ''.join(['<p>%s</p>'% v for v in content_list])\
        res_dict['subject'] = u'SAT'\
        res_dict['source'] = u'小站'\
        res_dict['tag'] = response.doc('.tag > a').text().split(' ')\
        bread = [res_dict['bread'], ]\
        bread.extend(response.doc('.head-crumbs-a-active').text().split(' '))\
        if res_dict['date'][:3] != '201':\
            res_dict['date'] = response.doc('.pull-left > span').text().split()[0].replace(u'年','-').replace(u'月','-').replace(u'日','')\
        res_dict['bread'] = list(set(bread))\
        res_dict['class'] = 31\
        res_dict['data_weight'] = 0\
        return res_dict$$$$$1471334978.0441
sdgwy_zhonggong$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-08 18:32:43\
# Project: sdgwy_zhonggong\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.offcn.com/sdgwy/ziliao/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.zg_lm_list > li').items():\
            _dict = {}\
            _dict['title'] = each.find('.zg_lm_2').text()\
            _dict['date'] = each.find('font').text()\
            self.crawl(each.find('.zg_lm_2').attr.href, save = _dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.a1').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):        \
        return {\
            "url": response.url,\
            'bread': [u'公务员'],\
            'title': response.save.get('title'),\
            'date': response.save.get('date'),\
            "html": response.doc('.zg_show_word').html(),\
            'source': u'中公教育',\
            'class': 36,\
            'subject': u'经验',\
            'data_weight': 0,\
        }$$$$$1467975018.5144
SegementFault_wenda$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-05 10:05:14\
# Project: SegementFault_wenda\
\
from pyspider.libs.base_handler import *\
import datetime\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1',\
        'headers':{\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('https://segmentfault.com/questions?page=1', callback=self.index_page)\
\
    def parse_time(self, _str):\
        if u'小时' in _str or u'分钟' in _str:\
            return datetime.datetime.now().strftime('%Y-%m-%d')\
        if u'天前' in _str:\
            try:\
                gap = int(_str.split(u'天前')[0].strip(' '))\
            except:\
                gap = 0\
            return (datetime.datetime.now() - datetime.timedelta(days=gap)).strftime('%Y-%m-%d')\
        try:\
            if u'年' not in _str:\
                year = '2016'\
            else:\
                year = _str.split(u'年')[0].strip(' ')\
            \
            month = int(_str.split(u'年')[1].split(u'月')[0].strip(' '))\
            day = int(_str.split(u'月')[1].split(u'日')[0].strip(' '))\
            return '%s-%.2d-%.2d'%(year, month, day)\
        except Exception, e:\
            print e\
            return ''\
        \
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.stream-list__item').items():\
            _dict = {}\
            _dict['title'] = each.find('.summary .title').text()\
            url = each.find('.summary .title').find('a').attr.href\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('.pagination a').items():\
            self.crawl(each.attr.href,callback=self.index_page)\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['question_detail'] = response.doc('.question').text()\
        content_list = []\
        for each in response.doc('.widget-answers article').items():\
             info = each.find('.answer').html().replace('\\n', '')\
             if info:\
                    _dict = {}\
                    _dict['content'] = info\
                    _dict['img'] = each.find('.mr10 img').attr.src\
                    _dict['user_name'] = each.find('.answer__info--author-name').text()\
                    #print each.find('.list-inline').text()\
                    _dict['create_time'] = self.parse_time(each.find('.list-inline').text())\
                    content_list.append(_dict)\
        if not content_list:\
            return\
        res_dict['create_user'] = response.doc('.question__author a').text().split()[0]\
\
        res_dict['create_time'] = self.parse_time(response.doc('.question__author').remove('a').text())\
        res_dict['url'] = response.url\
        res_dict['answers'] = content_list\
        res_dict['data_weight'] = 0\
        res_dict['source'] = 'segmentfault'\
        res_dict['subject'] = u'主站问答'\
        res_dict['class'] = 34\
        return res_dict\
$$$$$1468027035.4104
sheying_fsbus$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-11 12:14:31\
# Project: sheying_fsbus\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as P\
import re\
import json\
import cPickle\
import time\
\
max_pageno = 5\
class Parser(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".post_box"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find(".post_img").find("a"):\
                ret.append(P(url_node))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        page_node = P(response.doc("#page .wp-pagenavi"))\
        next_page = page_node.find(".current").next()\
        if not next_page:\
            return False\
        pageno = re.findall(r"/page/(\\d+)/", next_page.attr.href)\
        if not pageno:\
            return False\
        try:\
            pageno = int(pageno[0])\
            if pageno > max_pageno:\
                return False\
        except:\
            return False\
        \
        if next_page:\
            return P(next_page)\
        else:\
            return False\
        \
        return False\
    \
    @staticmethod\
    def parse_detail(response):\
        ret = {}\
        detail_node = P(response.doc(".post-content"))\
        if not detail_node:\
            return ret\
        title = detail_node.find("h1").text()\
        detail_node = P(detail_node.find("h1").next().html())\
        detail_node.find("center").remove()\
        \
        detail = detail_node.html()\
        \
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
        \
        return {"title": title,\
                "url": response.url,\
                "content": detail,\
                "bread": response.save.get("bread"),\
                "source": u"www.fsbus.com",\
                "data_weight": 0,\
                "class": 33,\
                "subject": u"摄影",\
                "date": time.strftime("%Y-%m-%d", time.localtime(time.time())),\
                "image_list": image_list,\
                }\
\
class Processor(object):\
    @staticmethod\
    def list_processor(spider_handle, parser, response):\
        # 解析列表页\
        list_result = parser.parse_list(response)\
        if list_result:\
            for item in list_result:\
                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\
\
        # 是否有翻页\
        next_page = parser.parse_nextpage(response)\
        if next_page:\
            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\
            \
    @staticmethod\
    def detail_processor(spider_handle, parser, response):\
        return parser.parse_detail(response)\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    crawler_parser = {\
        "1": {\
            "processor": Processor,\
            "parser": Parser,\
        }\
    }\
    crawler_list = [\
        {   \
            "key": "1",\
            "url": "http://www.fsbus.com/sheyingjiqiao/",\
            "bread": ["摄影技巧"]\
        },\
        {   \
            "key": "1",\
            "url": "http://www.fsbus.com/sheyingjiaocheng/",\
            "bread": ["摄影教程"]\
        },\
        {   \
            "key": "1",\
            "url": "http://www.fsbus.com/renxiangsheying/",\
            "bread": ["人像摄影"]\
        },\
        {   \
            "key": "1",\
            "url": "http://www.fsbus.com/hunshasheying/",\
            "bread": ["婚纱摄影"]\
        },\
        {   \
            "key": "1",\
            "url": "http://www.fsbus.com/ertongsheying/",\
            "bread": ["儿童摄影"]\
        },\
        {   \
            "key": "1",\
            "url": "http://www.fsbus.com/sheyingqicai/",\
            "bread": ["器材新闻"]\
        },\
        {   \
            "key": "1",\
            "url": "http://www.fsbus.com/sheyinghouqi/",\
            "bread": ["后期处理"]\
        },\
        {   \
            "key": "1",\
            "url": "http://www.fsbus.com/danfanrumen/",\
            "bread": ["单反入门"]\
        },\
    ]\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        for crawler in self.crawler_list:\
            self.crawl(crawler["url"],\
                       save = {\
                           'bread': crawler.get("bread", []),\
                           '__parser_key__': crawler.get("key"),\
                       },\
                       callback = self.index_page)\
            \
    @config(age=1)\
    def index_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        crawler_parser_dict.get("processor").list_processor(self, crawler_parser_dict.get("parser"), response)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        return crawler_parser_dict.get("processor").detail_processor(self, crawler_parser_dict.get("parser"), response)\
\
$$$$$1468846932.2299
sheying_heiguangwang$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-11 12:14:31\
# Project: sheying_fsbus\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as P\
import re\
import json\
import cPickle\
import time\
\
max_pageno = 10\
class Parser(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".item"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find(".img").find("a"):\
                ret.append(P(url_node))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        page_node = P(response.doc(".pager"))\
        try:\
            next_page = page_node.find(".a1")\
            if next_page:\
                current_pageno = page_node.find("span").text()\
                next_pageno = P(next_page[-1]).attr.href.strip(")").split(",")[-1]\
                if int(next_pageno) > int(current_pageno):\
                    # 限制只抓前20页\
                    if int(next_pageno) > max_pageno:\
                        return False\
                    return P('''<a href="%s&p=%s">next</a>''' % (response.save.get("base_url"), next_pageno))\
                return False\
\
            else:\
                return False\
        except Exception as info:\
            return False\
    \
    @staticmethod\
    def parse_detail(spider_handle, parser, response):\
        if not re.search(r"heiguang.com", response.url):\
            return False\
        ret = {}\
        if response.url.find("http://bbs.heiguang.com/") != -1:\
            title = response.doc("#thread_subject").text()\
            detail = response.doc(".pcb ").html()\
            date_string = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time()))\
            bread = response.save.get("bread")\
        else:\
            title = response.doc(".artTitle").text()\
            detail_node = response.doc(".artContent .article")\
            detail_node.find("hr").next().remove()\
            detail = detail_node.html()\
            \
            date_string = response.doc(".artInfo").find("span").text().strip()\
            if not re.match(r"\\d{4}-\\d{2}-\\d{2}\$", date_string):\
                date_string = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time()))\
\
            bread = response.save.get("bread")\
            if response.url.find("photography/syjc/") != -1:\
                bread = [u"摄影教程"]\
                \
        url = response.url\
        if not detail.strip():\
            return False\
        \
        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
        return {"title": title,\
                "url": url,\
                "content": detail,\
                "bread": bread,\
                "source": u"www.heiguang.com",\
                "data_weight": 0,\
                "class": 33,\
                "subject": u"摄影",\
                "date": date_string,\
                "image_list": image_list,\
                }\
\
class Processor(object):\
    @staticmethod\
    def list_processor(spider_handle, parser, response):\
        # 解析列表页\
        list_result = parser.parse_list(response)\
        if list_result:\
            for item in list_result:\
                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\
\
        # 是否有翻页\
        next_page = parser.parse_nextpage(response)\
        if next_page:\
            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\
        \
    @staticmethod\
    def detail_processor(spider_handle, parser, response):\
        return parser.parse_detail(spider_handle, parser, response)\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    crawler_parser = {\
        "1": {\
            "processor": Processor,\
            "parser": Parser,\
        }\
    }\
    crawler_list = [\
        {   \
            "key": "1",\
            "url": "http://www.heiguang.com/api.php?op=news_jiekou&a=get_list&cat=32,36,145",\
            "bread": ["视觉盛宴"],\
        },\
        {   \
            "key": "1",\
            "url": "http://www.heiguang.com/api.php?op=news_jiekou&a=get_list&cat=22,24",\
            "bread": ["摄影资讯"],\
        },\
    ]\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        for crawler in self.crawler_list:\
            self.crawl(crawler["url"],\
                       save = {\
                           'bread': crawler.get("bread", []),\
                           '__parser_key__': crawler.get("key"),\
                           'base_url': crawler["url"],\
                       },\
                       callback = self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        crawler_parser_dict.get("processor").list_processor(self, crawler_parser_dict.get("parser"), response)\
\
    #@config(priority=2)\
    @config(age=1)\
    def detail_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        return crawler_parser_dict.get("processor").detail_processor(self, crawler_parser_dict.get("parser"), response)\
\
$$$$$1468413484.6050
sheying_xingshe$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-09 19:50:08\
# Project: sheying_xingshe\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as P\
import re\
import json\
\
max_pageno = 40\
\
class HrefType(object):\
    FENGNIAO_TRAVEL_XINGSHE = 1\
    FENGNIAO_ACADEMY = 2\
    FENGNIAO_IMAGE = 3\
    \
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    crawler_list = [\
        # 旅游摄影\
        {\
            "url": "http://travel.fengniao.com/list_1345.html",\
            "bread": ["行摄资讯",],\
            "type": HrefType.FENGNIAO_TRAVEL_XINGSHE,\
        },\
        {\
            "url": "http://travel.fengniao.com/list_1463.html",\
            "bread": ["国内旅游",],\
            "type": HrefType.FENGNIAO_TRAVEL_XINGSHE,\
        },\
        {\
            "url": "http://travel.fengniao.com/list_1464.html",\
            "bread": ["国外旅游",],\
            "type": HrefType.FENGNIAO_TRAVEL_XINGSHE,\
        },\
        {\
            "url": "http://travel.fengniao.com/list_1467.html",\
            "bread": ["美食摄影",],\
            "type": HrefType.FENGNIAO_TRAVEL_XINGSHE,\
        },\
        {\
            "url": "http://travel.fengniao.com/list_1466.html",\
            "bread": ["行摄装备",],\
            "type": HrefType.FENGNIAO_TRAVEL_XINGSHE,\
        },\
        {\
            "url": "http://travel.fengniao.com/list_1607.html",\
            "bread": ["行摄视觉",],\
            "type": HrefType.FENGNIAO_TRAVEL_XINGSHE,\
        },\
        {\
            "url": "http://travel.fengniao.com/list_1343.html",\
            "bread": ["行摄攻略",],\
            "type": HrefType.FENGNIAO_TRAVEL_XINGSHE,\
        },\
\
\
        # 摄影学院\
        {\
            "url": "http://academy.fengniao.com/list_968.html",\
            "bread": ["摄影技巧",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://academy.fengniao.com/list_969.html",\
            "bread": ["后期处理",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://academy.fengniao.com/list_967.html",\
            "bread": ["单反摄影技巧",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://academy.fengniao.com/list_970.html",\
            "bread": ["单反入门",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://academy.fengniao.com/list_1335.html",\
            "bread": ["摄影书籍",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://academy.fengniao.com/list_1501.html",\
            "bread": ["摄影入门教程",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
\
        # 大师作品\
        {\
            "url": "http://image.fengniao.com/list_1422.html",\
            "bread": ["大师作品资讯",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://image.fengniao.com/list_1586.html",\
            "bread": ["图说天下",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://image.fengniao.com/list_1425.html",\
            "bread": ["影史",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://image.fengniao.com/list_1421.html",\
            "bread": ["书屋电影",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://image.fengniao.com/list_1587.html",\
            "bread": ["视觉盛宴",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://image.fengniao.com/list_1556.html",\
            "bread": ["访谈",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
\
        {\
            "url": "http://auto.fengniao.com/list_1642.html",\
            "bread": ["行摄自驾",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://auto.fengniao.com/list_1643.html",\
            "bread": ["行摄自驾",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://auto.fengniao.com/list_1644.html",\
            "bread": ["行摄自驾图赏",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://auto.fengniao.com/list_1646.html",\
            "bread": ["汽车文化",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
\
        {\
            "url": "http://qicai.fengniao.com/list_1441.html",\
            "bread": ["摄影器材故事",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
\
        {\
            "url": "http://qicai.fengniao.com/list_1584.html",\
            "bread": ["摄影器材美图",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
\
        {\
            "url": "http://qicai.fengniao.com/list_1437.html",\
            "bread": ["评测试用",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
        {\
            "url": "http://qicai.fengniao.com/list_1436.html",\
            "bread": ["器材新闻",],\
            "type": HrefType.FENGNIAO_ACADEMY,\
        },\
\
    ]    \
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for crawler in self.crawler_list:\
            self.crawl(crawler["url"],\
                       save = {'bread': crawler.get("bread", []), 'href_type': crawler.get("type", HrefType.FENGNIAO_TRAVEL_XINGSHE)},\
                       callback=self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        bread = response.save.get("bread")\
        href_type = response.save.get("href_type", HrefType.FENGNIAO_TRAVEL_XINGSHE)\
        if href_type in (HrefType.FENGNIAO_TRAVEL_XINGSHE,\
                         HrefType.FENGNIAO_ACADEMY,):\
            try:\
                list_news = P(response.doc(".list_news"))\
                if not list_news:\
                    list_news = P(response.doc(".main .section .gallery"))\
                    \
                for url_node in list_news.find("a"):\
                    _dict = {"bread": bread}\
                    url = P(url_node).attr.href\
                    if re.match(r"http://[^/]+?fengniao.com/(?:slide/)?\\d+/\\d+(?:_\\d{1,3})?.html", url):\
                        self.crawl(url, save = response.save, callback=self.detail_page)\
                        \
                page_num = P(response.doc(".page_num"))\
                if not page_num:\
                    return False\
                next_page = page_num.find(".next")\
                if not next_page:\
                    return False\
                pageno = re.findall(r"list_\\d+_(\\d+).html", next_page.attr.href)\
                if not pageno:\
                    return False\
                pageno = int(pageno[0])\
                if pageno > max_pageno:\
                    return False\
                if next_page:\
                    self.crawl(next_page.attr.href, save = response.save, callback=self.index_page)\
                    \
            except Exception as info:\
                print info\
                return False\
            \
    #@config(priority=2)\
    @config(age=1)\
    def detail_page(self, response):\
        href_type = response.save.get("href_type", HrefType.FENGNIAO_TRAVEL_XINGSHE)\
        if href_type in (HrefType.FENGNIAO_TRAVEL_XINGSHE,\
                         HrefType.FENGNIAO_ACADEMY):\
            view_more = response.doc(".mod-btn2")\
            if view_more:\
                self.crawl(view_more.attr.href, save = response.save, callback=self.detail_page)\
            else:\
                content = P(response.doc(".cont"))\
                if content:\
                    try:\
                        title = content.find(".h2").text()\
                        if not title:\
                            return False\
                        detail = content.find(".txt-wrap").html().strip()\
                        if not detail:\
                            return detail\
                        date_node = content.find("span.txt")\
                        date_string = None\
                        if date_node:\
                            date_list = re.findall(r"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", date_node.text())\
                            if len(date_list):\
                                date_string = date_list[0]\
                            else:\
                                return False\
                        else:\
                            return False\
                        detail = re.sub(r"<a[^>]+?/>", "", detail)\
                        detail = re.sub(r'''class="[^"]+?"''', "", detail)\
                        image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
                        save = response.save\
                        ret = {\
                            "bread": save.get("bread"),\
                            "url": response.url,\
                            "content": detail,\
                            "source": u"travel.fengniao.com",\
                            "data_weight": 0,\
                            "class": 33,\
                            "subject": u"摄影",\
                            "date": date_string[:10],\
                            "title": title,\
                            "image_list": image_list,\
                            #"cover": image_list[0] if len(image_list) else ""\
                        }\
                        return ret\
                    except Exception as info:\
                        return False\
                else:\
                    content = P(response.doc(".main .section"))\
                    if content:\
                        try:\
                            title = content.find("h1").text()\
                            if not title:\
                                return False\
                            detail = content.find(".article").html().strip()\
                            if not detail:\
                                return detail\
                            date_node = content.find("#pubtime_baidu")\
                            date_string = None\
                            if date_node:\
                                date_list = re.findall(r"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", date_node.text())\
                                if len(date_list):\
                                    date_string = date_list[0][:10]\
                                else:\
                                    return False\
                            else:\
                                return False\
                            detail = re.sub(r"<a[^>]+?/>", "", detail)\
                            detail = re.sub(r'''class="[^"]+?"''', "", detail)\
                            image_list = re.findall(r'''<img[^>]+?src="([^"]+?)"[^>]+?>''', detail, re.S)\
                            save = response.save\
                            ret = {\
                                "bread": save.get("bread"),\
                                "url": response.url,\
                                "content": detail,\
                                "source": u"travel.fengniao.com",\
                                "data_weight": 0,\
                                "class": 33,\
                                "subject": u"摄影",\
                                "date": date_string[:10],\
                                "title": title,\
                                "image_list": image_list,\
                                #"cover": image_list[0] if len(image_list) else ""\
                            }\
                            return ret\
                        except Exception as info:\
                            return False\
                    else:\
                        pigInfoJson = re.findall(r'''var picInfosJson = '([^=]+?)\\'''', response.doc.html())\
                        if not len(pigInfoJson):\
                            return False\
                        pigInfoJson = json.JSONDecoder().decode(pigInfoJson[0])\
                        image_list = []\
                        image_html = u"<div><br />"\
                        for item in pigInfoJson:\
                            if "pic_url_1920_b" in item:\
                                image_list.append(item["pic_url_1920_b"])\
                                image_html += u'''<img src="%s" /><br />''' % item["pic_url_1920_b"]\
                        image_html += u"</div>"\
                                \
                        detail = P(response.doc(".description .temporary"))\
                        detail_string = image_html\
                        date_string = None\
                        for elem in detail.find("span"):\
                            if P(elem).attr("style") == "display:none;":\
                                html = P(elem).html()\
                                detail_string = html.strip() + detail_string\
                                date_string = re.findall(r"\\d{4}-\\d{2}-\\d{2}", html)\
                                if len(date_string):\
                                    date_string = date_string[0]\
                                else:\
                                    return False\
                                break\
                        title = P(response.doc("h1.title"))\
                        if title:\
                            title = re.sub(r"<span[^>]+?>.*?</span>", "", title.html())\
                        else:\
                            return False\
                            \
                        ret = {\
                                "bread": response.save.get("bread"),\
                                "url": response.url,\
                                "content": detail_string.strip(),\
                                "source": u"travel.fengniao.com",\
                                "data_weight": 0,\
                                "class": 33,\
                                "subject": u"摄影",\
                                "date": date_string[:10],\
                                "title": title,\
                                "image_list": image_list\
                            }\
                        return ret\
                        \
                        \
        else:\
            # TODO\
            return False\
$$$$$1468846929.2305
shici2_liuxue86$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 16:05:48\
# Project: shici2_liuxue86\
\
from pyspider.libs.base_handler import *\
import re,json\
\
shangxi_dict = {\
    u'译文及注释':'translation_note',\
    u'鉴赏':'parse_appreciation',\
    u'艺术特色':'art_appreciation',\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
'headers':{\
'Host':'tool.liuxue86.com',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
}\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://tool.liuxue86.com/shici/',headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.top20 .main_c2 > ul').eq(0).find('a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.shici_wai li > a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.detail_page)\
        \
        #翻页\
        for each in response.doc('.pagination_Q a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
        \
    \
    @config(priority=2)\
    def detail_page(self, response):\
        tags = ''\
        shici_type = '' \
        if u'类型' in response.doc('.main_content_shiren_left_div > span').eq(-1).text():\
            shici_type = response.doc('.main_content_shiren_left_div > span').eq(-1).text().split(':')[-1].strip()\
        if u'诗词的标签' in response.doc('.main_conter_right_1 > .top20 > div').eq(0).find('p').text():\
            tags = response.doc('.main_conter_right_1 > .top20 > div').eq(0).remove('p').text().replace(' ',',')\
        _dict = {\
            'translation_note':'',\
            'parse_appreciation':'',\
            'art_appreciation':'',\
        }\
        for each in response.doc('.content_shiren_wenben div.wenben_div').items():\
            if each.text() not in shangxi_dict.keys():\
                continue\
            each1 = each.next().next()\
            con = ''\
            #print each.html()\
            while each1.html() and '<h2' not in each1.html():\
                con += each1.html().strip()\
                each1 = each1.next()\
            _dict[shangxi_dict[each.text()]] = con\
            \
        res = {\
            \
            "url": response.url,\
            "author_url":response.doc('span > a').eq(1).attr.href,\
            "name":response.doc('h1 > span').text().strip(),\
            "type":shici_type,\
            "tags":tags,    \
            "author":response.doc('.main_content_shiren_left_div > span > a').eq(1).text().strip(),\
            "dynasty":response.doc('.main_content_shiren_left_div > span > a').eq(0).text().strip(),\
            "content":response.doc('.main_content_shiren_left_div2').remove('span').html().strip(),\
            "translation_note":re.sub(r'<a[^>]+>','',_dict['translation_note']).replace('</a>',''),\
            "parse_appreciation":re.sub(r'<a[^>]+>','',_dict['parse_appreciation']).replace('</a>',''),\
            "art_appreciation":re.sub(r'<a[^>]+>','',_dict['art_appreciation']).replace('</a>',''),\
        }\
        return res\
$$$$$1472002466.7032
shici_liuxue86$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 15:48:07\
# Project: shici_liuxue86\
\
from pyspider.libs.base_handler import *\
import re,json\
\
shangxi_dict = {\
    u'译文及注释':'translation_note',\
    u'鉴赏':'parse_appreciation',\
    u'艺术特色':'art_appreciation',\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
'headers':{\
'Host':'tool.liuxue86.com',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
}\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://tool.liuxue86.com/shici/',headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.top20 .main_c2 > ul').eq(0).find('a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.shici_wai li > a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.detail_page)\
        \
        #翻页\
        for each in response.doc('.pagination_Q a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
        \
    \
    @config(priority=2)\
    def detail_page(self, response):\
        tags = ''\
        shici_type = '' \
        if u'类型' in response.doc('.main_content_shiren_left_div > span').eq(-1).text():\
            shici_type = response.doc('.main_content_shiren_left_div > span').eq(-1).text().split(':')[-1].strip()\
        if u'诗词的标签' in response.doc('.main_conter_right_1 > .top20 > div').eq(0).find('p').text():\
            tags = response.doc('.main_conter_right_1 > .top20 > div').eq(0).remove('p').text().replace(' ',',')\
        _dict = {\
            'translation_note':'',\
            'parse_appreciation':'',\
            'art_appreciation':'',\
        }\
        for each in response.doc('.content_shiren_wenben div.wenben_div').items():\
            if each.text() not in shangxi_dict.keys():\
                continue\
            each1 = each.next().next()\
            con = ''\
            #print each.html()\
            while each1.html() and '<h2' not in each1.html():\
                con += each1.html().strip()\
                each1 = each1.next()\
            _dict[shangxi_dict[each.text()]] = con\
            \
        res = {\
            \
            "url": response.url,\
            "author_url":response.doc('span > a').eq(1).attr.href,\
            "name":response.doc('h1 > span').text().strip(),\
            "type":shici_type,\
            "tags":tags,    \
            "author":response.doc('.main_content_shiren_left_div > span > a').eq(1).text().strip(),\
            "dynasty":response.doc('.main_content_shiren_left_div > span > a').eq(0).text().strip(),\
            "content":response.doc('.main_content_shiren_left_div2 > p').html().strip(),\
            "translation_note":re.sub(r'<a[^>]+>','',_dict['translation_note']).replace('</a>',''),\
            "parse_appreciation":re.sub(r'<a[^>]+>','',_dict['parse_appreciation']).replace('</a>',''),\
            "art_appreciation":re.sub(r'<a[^>]+>','',_dict['art_appreciation']).replace('</a>',''),\
        }\
        return res\
$$$$$1471853117.6105
shiren2_liuxue86$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 09:43:29\
# Project: shiren2_liuxue86\
\
\
from pyspider.libs.base_handler import *\
import re,json\
from pyquery import PyQuery as pq \
\
shangxi_dict = {\
    u'家庭成员':'family',\
    u'主要成就':'achievement',\
    u'文学成就':'achievement',\
    u'为官生涯':'achievement',\
    u'为政举措':'achievement',\
    u'生平':'life_story',\
    u'评价':'evaluation',\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
'headers':{\
'Host':'tool.liuxue86.com',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
}\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://tool.liuxue86.com/shici/',headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.main_c2').eq(0).find('a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.clearfix.top20 li > a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.detail_page)\
        \
        #翻页\
        for each in response.doc('.pagination_Q a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
        \
    \
    @config(priority=2)\
    def detail_page(self, response):\
        _dict = {\
            'life_story':'',\
            'achievement':'',\
            'family':'',\
            'evaluation':'',\
        }\
        \
        \
        \
        if pq(url='http://baike.baidu.com/search/word?word='+response.doc('h1 > span').text().strip(),encoding='utf8'):\
            baike = pq(url='http://baike.baidu.com/search/word?word='+response.doc('h1 > span').text().strip(),encoding='utf8')\
            for each in baike.find('.para-title').remove('span').items():\
                #print each.find('h2').text()\
                if u'评价' in each.find('h2').text():\
                    each1 = each.next()\
                    con = ''\
                    #print each.html()\
                    while each1.text() and 'anchor-list' not in each1.outerHtml():\
                        con += each1.remove('img').remove('.description').html().strip()\
                        each1 = each1.next()\
                        _dict[shangxi_dict[u'评价']] += '<p>'+re.sub('<a[^>]+>','',con).replace('</a>','')+'</p>'\
        \
        \
        \
        \
        author_introduction = ''\
        if u'来自百科' in response.doc('.main_content_shiren_left > p').eq(1).text():\
            author_introduction = response.doc('.main_content_shiren_left > p').eq(1).text().split(u'来自百科：')[1].replace(u'无','').strip()\
        \
        \
        \
        \
        for each in response.doc('.content_shiren_wenben h2').items():\
            if each.text() not in shangxi_dict.keys():\
                continue\
            if each.next().text():\
                each1 = each.next()\
            else:\
                each1 = each.next().next()\
            con = ''\
            #print each.html()\
            while each1.html() and '<h2' not in each1.outerHtml():\
                con += each1.html().strip()\
                each1 = each1.next()\
            _dict[shangxi_dict[each.text()]] += '<p>'+re.sub('<a[^>]+>','',con).replace('</a>','')+'</p>'\
            \
            \
            \
            \
            \
        res = {\
            "url": response.url,\
            "author":response.doc('h1 > span').text().strip(),\
            "author_introduction":re.sub(r'<a[^>]+>','',author_introduction,re.S).replace('</a>',''),\
            "dynasty":response.doc('span > a').text().strip(),\
            "dynasty_introduction":pq(url=response.doc('span > a').attr.href,encoding='utf8').find('.main_conter_Art').html(),\
            "desc":response.doc('.main_content_shiren_left_div > span').eq(1).text().split(u'描述：')[1].strip().replace(u'无',''),\
            "abstract":response.doc('.main_content_shiren_left > p').eq(0).text().split(u'简介：')[1].strip().replace(u'无',''),\
            "life_story":_dict['life_story'],\
            "achievement":_dict['achievement'],\
            "family":_dict['family'],\
            "evaluation":_dict['evaluation'],\
        }\
        #return json.dumps(res)\
        return res\
$$$$$1471943751.0776
shiren_img$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 17:45:30\
# Project: shiren_img\
\
from pyspider.libs.base_handler import *\
import re,json\
from pyquery import PyQuery as pq \
\
shangxi_dict = {\
    u'家庭成员':'family',\
    u'主要成就':'achievement',\
    u'文学成就':'achievement',\
    #u'书法成就':'achievement',\
    u'生平':'life_story',\
    u'人物评价':'evaluation',\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
'headers':{\
'Host':'tool.liuxue86.com',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
}\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://tool.liuxue86.com/shici/',headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.main_c2').eq(0).find('a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.clearfix.top20 li > a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.detail_page)\
        \
        #翻页\
        for each in response.doc('.pagination_Q a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
        \
    \
    @config(priority=2)\
    def detail_page(self, response):\
        author_img = ''\
        if 'shiren.jpg' not in response.doc('.main_content_shiren_right > img').attr['src']:\
                author_img = response.doc('.main_content_shiren_right > img').attr['src']\
        return {\
            "url": response.url,\
            "name": response.doc('h1 > span').text().strip(),\
            "author_img":author_img\
        }$$$$$1471917124.1104
shiren_liuxue86$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 15:52:22\
# Project: shiren_liuxue86\
\
\
from pyspider.libs.base_handler import *\
import re,json\
from pyquery import PyQuery as pq \
\
shangxi_dict = {\
    u'家庭成员':'family',\
    u'主要成就':'achievement',\
    u'文学成就':'achievement',\
    u'为官生涯':'achievement',\
    u'生平':'life_story',\
    u'人物评价':'evaluation',\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
'headers':{\
'Host':'tool.liuxue86.com',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'\
}\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://tool.liuxue86.com/shici/',headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.main_c2').eq(0).find('a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.clearfix.top20 li > a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.detail_page)\
        \
        #翻页\
        for each in response.doc('.pagination_Q a').items(): \
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
        \
    \
    @config(priority=2)\
    def detail_page(self, response):\
        _dict = {\
            'life_story':'',\
            'achievement':'',\
            'family':'',\
            'evaluation':'',\
        }\
        \
        \
        \
        if pq(url='http://baike.baidu.com/search/word?word='+response.doc('h1 > span').text().strip(),encoding='utf8'):\
            baike = pq(url='http://baike.baidu.com/search/word?word='+response.doc('h1 > span').text().strip(),encoding='utf8')\
            for each in baike.find('.para-title').remove('span').items():\
                #print each.find('h2').text()\
                if u'人物评价' == each.find('h2').text():\
                    each1 = each.next()\
                    con = ''\
                    #print each.html()\
                    while each1.text() and 'anchor-list' not in each1.outerHtml():\
                        con += each1.remove('img').remove('.description').html().strip()\
                        each1 = each1.next()\
                        _dict[shangxi_dict[each.find('h2').remove('span').text()]] = re.sub('<a[^>]+>','',con).replace('</a>','')\
        \
        \
        \
        \
        author_introduction = ''\
        if u'来自百科' in response.doc('.main_content_shiren_left > p').eq(1).text():\
            author_introduction = response.doc('.main_content_shiren_left > p').eq(1).text().split(u'来自百科：')[1].replace(u'无','').strip()\
        \
        \
        \
        \
        for each in response.doc('.content_shiren_wenben h2').items():\
            if each.text() not in shangxi_dict.keys():\
                continue\
            if each.next().text():\
                each1 = each.next()\
            else:\
                each1 = each.next().next()\
            con = ''\
            #print each.html()\
            while each1.html() and '<h2' not in each1.outerHtml():\
                con += each1.html().strip()\
                each1 = each1.next()\
            _dict[shangxi_dict[each.text()]] = re.sub('<a[^>]+>','',con).replace('</a>','')\
            \
            \
            \
            \
            \
        res = {\
            "url": response.url,\
            "author":response.doc('h1 > span').text().strip(),\
            "author_introduction":re.sub(r'<a[^>]+>','',author_introduction,re.S).replace('</a>',''),\
            "dynasty":response.doc('span > a').text().strip(),\
            "dynasty_introduction":pq(url=response.doc('span > a').attr.href,encoding='utf8').find('.main_conter_Art').html(),\
            "desc":response.doc('.main_content_shiren_left_div > span').eq(1).text().split(u'描述：')[1].strip().replace(u'无',''),\
            "abstract":response.doc('.main_content_shiren_left > p').eq(0).text().split(u'简介：')[1].strip().replace(u'无',''),\
            "life_story":_dict['life_story'],\
            "achievement":_dict['achievement'],\
            "family":_dict['family'],\
            "evaluation":_dict['evaluation'],\
        }\
        #return json.dumps(res)\
        return res\
$$$$$1471917126.9192
shufa_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-19 10:22:48\
# Project: shufa_wzxx\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1'\
    }\
    \
    \
    page_list = {\
\
        'http://www.wzxx.org/shufalilun/shufashi/':[u'书法历史'],\
        'http://www.wzxx.org/shufalilun/gudaishujia/':[u'书法杂谈'],\
        'http://www.wzxx.org/shufalilun/lilunzhishi/':[u'书法知识'],\
        'http://www.wzxx.org/shufalilun/jibenjifa/':[u'基本技法'],\
        'http://www.wzxx.org/shufalilun/zhuanke/':[u'篆刻知识'],\
        'http://www.wzxx.org/shufalilun/yingbi/':[u'硬笔书法'],\
        'http://www.wzxx.org/shufalilun/diandi/':[u'书海点滴']\
\
    }\
    \
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_list.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        for each in response.doc('div.vlist li').items():\
            _dict = {}\
            _dict['title'] = each.find('a').text()\
            _dict['date'] = each.find('span').text()\
            _dict['bread'] = response.save.get('bread')\
            url = each.find('a').attr.href\
            self.crawl(url, save = _dict,callback=self.detail_page)\
      \
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['content'] = response.doc('div#text').remove('ins').remove('script').html()\
        res_dict['url'] = response.url\
        res_dict['subject'] = u'书法'\
        res_dict['class'] = 33\
        res_dict['source'] = 'wzxx.org'\
        res_dict['data_weight'] = 0\
        #res_dict['bread'] = [u'文章资讯']\
        return res_dict\
$$$$$1472546336.4362
shufa_wzxx$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-19 10:22:48\
# Project: shufa_wzxx\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1'\
    }\
    \
    \
    page_list = {\
\
        'http://www.wzxx.org/shufalilun/shufashi/':[u'书法历史'],\
        'http://www.wzxx.org/shufalilun/gudaishujia/':[u'书法杂谈'],\
        'http://www.wzxx.org/shufalilun/lilunzhishi/':[u'书法知识'],\
        'http://www.wzxx.org/shufalilun/jibenjifa/':[u'基本技法'],\
        'http://www.wzxx.org/shufalilun/zhuanke/':[u'篆刻知识'],\
        'http://www.wzxx.org/shufalilun/yingbi/':[u'硬笔书法'],\
        'http://www.wzxx.org/shufalilun/diandi/':[u'书海点滴']\
\
    }\
    \
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_list.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('div.vlist li').items():\
            _dict = {}\
            _dict['title'] = each.find('a').text()\
            _dict['date'] = each.find('span').text()\
            _dict['bread'] = response.save.get('bread')\
            url = each.find('a').attr.href\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('div.listpage a[href]').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href,save = _dict,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['content'] = response.doc('div#text').remove('ins').remove('script').html()\
        res_dict['url'] = response.url\
        res_dict['subject'] = u'书法'\
        res_dict['class'] = 33\
        res_dict['source'] = 'wzxx.org'\
        res_dict['data_weight'] = 0\
        #res_dict['bread'] = [u'文章资讯']\
        return res_dict\
$$$$$1472106881.7063
sina_wenda_2$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-28 18:23:33\
# Project: sina_wenda\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
import datetime\
import MySQLdb\
\
class ConnectionUtil(object):\
\
    def __init__(self,connection):\
        self.connection = connection\
\
    def cursor(self):\
        if self.connection:\
            self.cursor  = self.connection.cursor()\
            return self.cursor\
        else:\
            return  None\
\
    def __enter__(self):\
        return self.cursor()\
\
    def __exit__(self, exc_type, exc_val, exc_tb):\
            #print 'ok'\
            if self.connection:\
                self.connection.commit()\
            if self.cursor:\
                self.cursor.close()\
            if self.connection:\
                self.connection.close()\
            if exc_type is not None:\
                #print 'errror'\
                print exc_val\
                #traceback.print_exc()\
                return True\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'天前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-1*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
    \
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'v0.2'\
        #'headers':self.headers\
    }\
    \
    headers = {\
     'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\
'Accept-Encoding':'gzip, deflate, sdch',\
'Accept-Language':'zh-CN,zh;q=0.8,en;q=0.6',\
'Cache-Control':'max-age=0',\
'Connection':'keep-alive',\
'Host':'iask.sina.com.cn',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36',\
\
    }\
\
    @every(minutes=1*30)\
    def on_start(self):\
       sql = 'select * from tb_query where sina_flag = 0 limit 5000'\
       con = MySQLdb.connect(db='querydb',host='127.0.0.1',user='root',passwd='root',charset='utf8')\
       with ConnectionUtil(con) as cursor:\
           cursor.execute(sql)\
           result =  cursor.fetchall()\
       for each in result:\
           _dict = {}\
           _dict['query'] = each[2]\
           _dict['id'] = each[0]\
           line = each[2]\
           for index in range(3):\
               self.crawl('http://iask.sina.com.cn/search?searchWord=%s&page=%s'%(line,index),save = _dict ,headers=self.headers,callback=self.index_page)\
                            \
                       \
    @config(age=10*24*60*60)\
    def index_page(self, response):\
        for each in response.doc('.search_list .search_item').items():\
            _dict = {}\
            #_dict['category_id'] = response.save.get('category_id')\
            _dict['title'] = each.find('h2 a').text()\
            _dict['id'] = response.save.get('id')\
            url = each.find('h2 a').attr.href\
            question_other = each.find('.answer_det a').eq(0).text()\
            _dict['create_time'] = get_date(question_other)\
            self.crawl(url, save = _dict,headers=self.headers,callback=self.detail_page)\
        \
            \
    @config(priority=2)\
    def detail_page(self, response):\
       sql = 'update tb_query  set sina_flag = %s  where id = %s'\
       con = MySQLdb.connect(db='querydb',host='127.0.0.1',user='root',passwd='root',charset='utf8')\
       with ConnectionUtil(con) as cursor:\
           cursor.execute(sql,(1,int(response.save.get('id'))))\
       res_dict = response.save\
       question_other = response.doc('span.ask-time').text().strip()\
       res_dict['create_time'] = get_date(question_other)\
       res_dict['category_id'] = response.doc('div.dib span a').text().split()\
       res_dict['title'] = response.doc('h3.title-f22').text()\
       if u'title' not in res_dict or not res_dict['title']:\
                res_dict['title'] = response.doc('div.question_text > .pre_img').remove('div.link_layer').text()\
       res_dict['question_detail'] = response.doc('.question_text').remove('.link_layer').text()\
       answers_list = []\
                \
       for each in response.doc('li.clearfix').items():\
            info = each.find('.answer_txt').text()\
            if info:\
                 _dict = {}\
                 _dict['content'] = info\
                 _dict['user_name'] = each.find('.user_wrap > a').text()\
                 question_time = each.find('.answer_t').text().strip()\
                 _dict['create_time'] = get_date(question_time)\
                 answers_list.append(_dict)\
                    \
       for each in response.doc('.good_answer').items():\
            info = each.find('.answer_text').text()\
            if info:\
                 _dict = {}\
                 _dict['content'] = info\
                 _dict['user_name'] = each.find('.answer_tip > a').text()\
                 question_time = each.find('.answer_tip .time').text().replace('|','').strip()\
                 _dict['create_time'] = get_date(question_time)\
                 answers_list.append(_dict)\
                    \
       if not answers_list:\
                 return \
       res_dict['answers'] = answers_list\
       res_dict['url'] = response.url\
       res_dict['source'] = 'sina'\
       res_dict['subject'] = u'主站问答'\
       res_dict['class'] = 34\
       res_dict['data_weight'] = 0\
       res_dict['create_user'] = response.doc('.ask_autho span.user_wrap').children('a').text() \
       del res_dict['id']\
       return res_dict\
$$$$$1470020082.1280
sj33_cn$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-16 17:10:20\
# Project: sj33_cn\
\
from pyspider.libs.base_handler import *\
import re\
def getDate(date):\
    if date.startswith('201'):\
        return date\
    else:\
        return '2016-' + date\
\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    contents = ''\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
        content = content.replace('</a>','')\
    contents += content\
    return  contents\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.sj33.cn/news/sjxw/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('ul.list li').items():\
            _dict = {}\
            _dict['title'] =  each.find('a').text()\
            _dict['date'] = getDate(each.find('span').text().replace('[','').replace(']',''))\
            url =  each.find('a').attr.href\
            self.crawl(url, save =  _dict,callback=self.detail_page)\
        for each in response.doc('div.showpage a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        content_list = []\
        for each in response.doc('div.artcon > p').items():\
            info = each.html()\
            if info:\
                content_list.append(removeLink(info))\
        if not content_list:\
            return\
        res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
        res_dict['subject'] = '设计'\
        res_dict['class'] = 46\
        res_dict['source'] = '33sj.cn'\
        res_dict['data_weight'] = 0\
        res_dict['url'] = response.url\
        res_dict['bread'] = ['文章资讯']\
        return res_dict\
$$$$$1471588165.0484
taoerge$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-17 14:34:58\
# Project: taoerge\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'v0.5'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.taoerge.com/ertongequ/geci/' ,callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):       \
        for each in response.doc('div.txt ul > li').items():\
            _dict = {}\
            _dict['title'] = each.find('h2 b').text()\
            url = each.find('h2 b a').attr.href\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('ul.pagelist a').items():\
            self.crawl(each.attr.href,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['content'] = response.doc('div.content').remove('p').html().strip()\
        res_dict['date'] =  response.doc('div.info').eq(0).text().split()[1]\
        res_dict['source'] = 'taoerge.com'\
        res_dict['class'] = 46\
        res_dict['subject'] = '儿歌'\
        res_dict['data_weight'] = 0\
        res_dict['url'] = response.url\
        res_dict['bread'] = [ u'文章资讯']\
        if 'content' in res_dict and res_dict['content']:\
            return res_dict\
        else:\
            return \
$$$$$1472106047.4393
test_js$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-04 18:45:04\
# Project: test_js\
\
from pyspider.libs.base_handler import *\
import sys\
import MySQLdb\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import urllib\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    headers = {\
\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://121.42.178.220:8080/',fetch_type='js',save = {'school_name':u'石林'}, callback=self.index_page)\
        \
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        print response.doc('pre.best-text').html()\
        #url =  response.doc('div.summary-pic img').attr.src\
        #if url:\
            #urllib.urlretrieve(url, filename='/Users/bjhl/Documents/imgs/'+response.save.get('school_name')+'.jpg')\
        if not response.doc('.main-content'):\
            return None\
        img_list = []\
        for each in response.doc('div.para img').items():\
            img_list.append(each.attr.src)\
        _dict = {}\
        if not img_list:\
            return None\
        _dict['photoes'] = img_list\
        _dict['school_name'] = response.save.get('school_name')\
        return _dict\
        '''_dict = {}\
        for each in response.doc('div[label-module="para-title"]').items():\
            key =  each.children('h2').remove('span').text()\
            content_list = list()\
            nx = each.next()        \
            while nx:\
                if nx.attr['label-module'] == 'para-title' or nx.attr['id'] =='open-tag':\
                    break\
                if nx.attr['class'] == 'para':  \
                    content_list.append('<p>'+nx.remove('sup').text()+'</p>')            \
                nx = nx.next()\
            if content_list and key:\
                  _dict[key] = ''.join(content_list)\
        for each in response.doc('dt.basicInfo-item').items():\
            key = ''.join(v for v in each.text().split())\
            nx = each.next()\
            if 'basicInfo-item' in nx.attr['class'] and key:\
                _dict[key] = nx.text()\
        if not _dict:\
            return\
        return _dict'''\
\
            \
            \
            \
            \
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
        }\
$$$$$1470385949.4281
tiku_chazidian_k12$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-09 10:06:34\
# Project: tiku_chazidian_k12\
\
from pyspider.libs.base_handler import *\
from random import randrange\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://tiku.chazidian.com/gz/?book=&grade=&page=1', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.list_sj_xx > a').items():\
            for name in [u'语文', u'数学', u'英语', u'物理', u'化学', u'生物', u'政治', u'历史', u'地理']:\
                if name in each.text():\
                    subject = u'高中' + name\
                    break\
            self.crawl(each.attr.href, save={'subject': subject}, callback=self.list_page)\
        for each in response.doc('.page_link').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('p > a').items():\
            self.crawl(each.attr.href, save=response.save, callback=self.detail_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        content = [v.html() for v in response.doc('.select_c > dl').items()]\
        _dic = {\
        u'高中生物': 'P_548362',\
        u'高中化学': 'P_548361',\
        u'高中物理': 'P_548360',\
        u'高中地理': 'P_548359',\
        u'高中历史': 'P_548358',\
        u'高中政治': 'P_548357',\
        u'高中英语': 'P_548356',\
        u'高中数学': 'P_548355',\
        u'高中语文': 'P_548354',\
        }\
        subject = response.save['subject']\
        return {\
            "url": response.url,\
            "title": content[0],\
            "answer": content[1].replace('\\r','').replace('\\n',''),\
            "analyse": content[2].replace('\\r','').replace('\\n',''),\
            "degree": randrange(1, 5),\
            "source": "chazidian",\
            "data_weight": 0,\
            "subject": subject,\
            "class": "35",\
            "category_id": [_dic[subject], ],\
        }\
$$$$$1468227079.5816
tiku_cooco$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-09 10:06:34\
# Project: tiku_chazidian_k12\
\
from pyspider.libs.base_handler import *\
from random import randrange\
import urllib2\
\
class Handler(BaseHandler):\
    crawl_config = {\
\
         \
    }\
    \
    header_base = {\
'Cookie':'bdshare_firstime=1468894646778; AJSTAT_ok_pages=5; AJSTAT_ok_times=2; Hm_lvt_c8ad39b9579b4816dc8f6f805c190308=1468291884,1468894647; Hm_lpvt_c8ad39b9579b4816dc8f6f805c190308=1468895300',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36',\
'X-Prototype-Version':'1.5.0_rc0'\
\
    }\
    tag_dic = {\
        u'人物传记类': 'p_565',\
        u'作家作品': 'p_1501',\
        u'修改应用文': 'p_662',\
        u'修辞格': 'p_32',\
        u'先秦诸子百家': 'p_1541',\
        u'公文类': 'p_1456',\
        u'其他': 'p_204',\
        u'历史事件类': 'p_1551',\
        u'历史记载类(二十四史、地方志、野史等)': 'p_2954',\
        u'古代散文、兵法农医、天文术数、艺术杂学等': 'p_1517',\
        u'句子衔接': 'p_157',\
        u'命题作文': 'p_717',\
        u'图文转换': 'p_23',\
        u'外国文学': 'p_1504',\
        u'字形': 'p_19',\
        u'字音': 'p_31',\
        u'实用类文本阅读': 'p_1160',\
        u'小作文': 'p_1472',\
        u'小说': 'p_1502',\
        u'成语(熟语)': 'p_1457',\
        u'散文类': 'p_658',\
        u'曲': 'p_1518',\
        u'材料作文': 'p_710',\
        u'标点符号': 'p_33',\
        u'楚辞汉赋骈文(歌行、乐府)': 'p_1983',\
        u'现代文学类文本阅读': 'p_1161',\
        u'病句辨析': 'p_5',\
        u'论述类文本阅读': 'p_653',\
        u'词': 'p_1091',\
        u'词语': 'p_30',\
        u'诗': 'p_22',\
        u'话题作文': 'p_714',\
        u'语法': 'p_100',\
        u'语言的简明连贯得体': 'p_4',\
        u'近代文学': 'p_1515',\
        u'选用、仿用、变换句式': 'p_15',\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://gzyw.cooco.net.cn/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        _list = []\
        for each in response.doc('.lftTxt > a').items():\
            tag = each.text()\
            self.crawl(each.attr.href, save={'tag': tag}, callback=self.list_page)\
        \
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        tag = response.save['tag']\
        max_page = 1\
        print response.url\
        for each in response.doc('.page-numbers').items():\
            if each.text().isdigit() and int(each.text())>max_page:\
                max_page = int(each.text())\
        url = response.url\
        if url[-1] == '/':\
            url = url[:-1]\
        for i in range(1, max_page):\
            _url = '/'.join(url.split('/')[:-1]) + '/' + str(i)\
            print _url\
            _dict = {'X-Requested-With':'XMLHttpRequest','Referer':response.url,'Origin':'http://gzyw.cooco.net.cn'}\
            self.header_base.update(_dict)\
            self.crawl(_url, save=response.save, headers=self.header_base,callback=self.list_page2)\
    \
    @config(age=10 * 24 * 60 * 60)\
    def list_page2(self, response):       \
        for each in response.doc('span > a').items():\
            self.crawl(each.attr.href, save=response.save, callback=self.detail_page)\
        for each in response.doc('.bottom  a').items():\
            self.crawl(each.attr.href, save=response.save, callback=self.detail_page)\
        \
        \
    @config(age=10 * 24 * 60 * 60)\
    def detail_page(self, response):\
        tag = response.save['tag']\
\
        title = ''.join(['<p>%s</p>'%v.html() for v in response.doc('.txt p').items()])\
        subject = u'高中语文'\
            \
        answer = ''.join(['<p>%s</p>'%v.text() for v in response.doc('.daan p').items() if 'co' not in v.text() and 'oo' not in v.text()] )\
        dic = {\
                "url": response.url,\
                "title": title,\
                "answer": answer,\
                "analyse": '',\
                "degree": randrange(1, 5),\
                "source": "chazidian",\
                "data_weight": 0,\
                "subject": subject,\
                "class": "35",\
                "category_id": [self.tag_dic.get(tag, 'p_548354'), ],\
        }\
            \
        return dic\
$$$$$1468897194.8573
tiku_gzywtk$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-09 15:53:57\
# Project: tiku_gzywtk\
\
from pyspider.libs.base_handler import *\
from random import randrange\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    dic = {\
        'zy': 'p_31',\
        'bd': 'p_33',\
        'cy': 'p_1091',\
        'bj': 'p_5',\
        'wyw': 'p_18117',\
        'sg': 'p_18337',\
        'mj': 'p_478',\
        'yy': 'p_1163',\
        'xdw': 'p_482',\
        'zw': 'p_709',\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k, v in self.dic.iteritems():\
            self.crawl('http://www.gzywtk.com/kaodian/%s-list.aspx'%k, save={'cid': v}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('#typelist > li').items():\
            dic = {\
            'title': each.find('.limid').html(),\
            'category_id': [response.save['cid'],],\
            }\
            self.crawl(each.find('a').attr.href, save=dic, callback=self.detail_page)\
        for each in response.doc('.datapager > a').items():\
            self.crawl(each.attr.href, save={'cid': response.save['cid']}, callback=self.index_page)\
\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res = response.save\
        res["url"] = response.url\
        res['answer'] = response.doc('.content').eq(1).html()\
        res['degree'] = randrange(1, 5)\
        res["source"] = "gzywtk"\
        res["data_weight"] = 0\
        res["subject"] = u'高中语文'\
        res["class"] = 35\
        return res$$$$$1468227076.8284
toefl_taisha_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-29 16:14:58\
# Project: toefl_taisha\
\
\
from pyspider.libs.base_handler import *\
import pyquery\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    type_dict = {\
        'news': u'快讯动态', 'guidance': u'复习攻略', 'experience': u'复习攻略', 'download': u'快讯动态', 'guid': u'高分心得', 'machine': u'托福机经',\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k, v in self.type_dict.iteritems(): \
            self.crawl('http://www.taisha.org/toefl/%s/'%k, save={'bread': v}, callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.html_content dd').items():\
            _dict = {}\
            url = each.find('.title > a').attr.href\
            #_dict['url'] = url\
            _dict['bread'] = each.find('.bot > a').text().split(' ')\
            _dict['bread'].append((bread))\
            _dict['brief'] = self.replace(each.find('p').text().replace(u'[详情]', ''))\
            self.crawl(url, save=_dict, callback=self.detail_page)\
        #for each in response.doc('span > a').items():\
        #    self.crawl(each.attr.href, save={'bread': bread}, callback=self.index_page)\
    def replace(self, _str):\
        img1 = 'http://www.taisha.org/uploadfile/2015/1215/20151215035909345.jpg'\
        if _str:\
            return _str.replace(u'太傻留学', u'').replace(u'太傻', '').replace(img1, '')\
        return _str\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        try:\
            date = response.doc('.txt_title span').text()[-19:]\
        except:\
            date = ''\
        #content = response.doc('.txt_content').html()\
        content_info = response.doc('.txt_content > p')\
        content_list = []\
        for info in content_info:\
            answer = pyquery.PyQuery(info)\
            items = answer.html()\
            if '20151215035909345.jpg' not in items:\
                content_list.append((self.replace(items)))\
        content_list = content_list[:-7]\
        if not content_list:\
            return None\
        title = response.doc('.txt_title > h2').text()\
        if u'回忆版汇总' in title:\
            return None\
        res_dict['title'] = title\
        bread = res_dict['bread']\
        if u'托福机经' in response.save['bread']:\
            if u'汇总' in title:\
                return None\
            if u'听力' in title:\
                bread.append((u'听力机经'))\
            if u'阅读' in title:\
                bread.append((u'阅读机经'))\
            if u'口语' in title:\
                bread.append((u'口语机经'))\
            if u'写作' in title:\
                bread.append((u'写作机经'))\
        res_dict['bread'] = list(set(bread)) \
        res_dict['url'] = response.url\
        res_dict['date'] = date[:10]\
        res_dict['source'] = u'太傻留学'\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list])\
        res_dict['subject'] = u'托福'\
        res_dict['tag'] = response.doc('#nav_location a').text().split(' ')[1:]\
        res_dict['class'] = 27\
        res_dict['data_weight'] = 0\
        return res_dict$$$$$1471329875.8495
toefl_xdf_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-26 14:19:35\
# Project: toefl_xdf_inc\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://toefl.xdf.cn/list_9069_1.html', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.txt_lists01 > li').items():\
            url = each.find('a').attr.href\
            date = each.find('.time').text()\
            if u'汇总' not in each.find('a').text():\
                self.crawl(url, save={'date': date}, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for each in response.doc('.air_con > p').items():\
            if u'编辑推荐' in each.text():\
                break\
            content_list.append((each.html().replace('\\n','').replace('\\r','').replace('\\t','')))\
        content_list = content_list[1:]\
        if not content_list:\
            return None\
        bread = [u'托福机经',]\
        title = response.doc('.title1').text().replace(u'新东方名师：','')\
        if u'听力' in title:\
            bread.append((u'听力机经'))\
        if u'阅读' in title:\
            bread.append((u'阅读机经'))\
        if u'口语' in title:\
            bread.append((u'口语机经'))\
        if u'写作' in title:\
            bread.append((u'写作机经'))\
        return {\
            "url": response.url,\
            "title": title,\
            "date": response.save['date'],\
            "subject": u'托福',\
            "source": 'xdf',\
            "class": 27,\
            "data_weight":0,\
            "bread": bread,\
            "content": ''.join(['<p>%s</p>'%v for v in content_list]),\
        }\
$$$$$1471329882.3647
toefl_xiaozhan_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-29 18:19:22\
# Project: toefl_xiaozhan\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    type_dict = {\
        'fuxi': u'复习攻略', 'tpo': u'托福TPO', 'tingli': u'托福听力', 'kouyu': u'托福口语', \
        'yuedu': '托福阅读', 'zonghe': u'冲刺宝典', 'jihua': u'备考计划', 'tifen/beikao': u'备考计划',\
        'xiezuo': u'托福写作', 'tfziliao': u'托福综合', 'gaofen': u'高分心得', \
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for type, v in self.type_dict.iteritems():\
            self.crawl('http://toefl.zhan.com/%s/'%type, save={'bread': v}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/toefl/kaoqian/', save={'bread': u'冲刺宝典'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/toefl/fukao/', save={'bread': u'冲刺宝典'}, callback=self.index_page)\
        self.crawl('http://zt.zhan.com/toefl/beikao/', save={'bread': u'备考计划'}, callback=self.index_page)\
\
    @config(age=1 * 60)\
    def index_page(self, response):\
        bread = response.save['bread']\
        for each in response.doc('.things_list > .pull-right').items():\
            url = each.find('a').attr.href\
            _save = {}\
            _save['date'] = each.find('.padding_right_10').text()\
            #_save['tag'] = each.find('dl > .pull-left > a').text().split()\
            _save['brief'] = each.find('.text').text()\
            _save['bread'] = bread\
            self.crawl(url, save=_save, callback=self.detail_page)\
        #for each in response.doc('nav a').items():\
        #    self.crawl(each.attr.href, save={'bread': bread}, callback=self.index_page)\
\
    def resplace(self, _str):\
        if _str:\
            return _str.replace(u'小站','').replace(u'小站教育','')\
        return _str\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict["url"] = response.url\
        brief = response.save['brief']\
        res_dict['brief'] = self.resplace(brief)\
        res_dict["title"] = self.resplace(response.doc('h1').text())\
        content_list = []\
        for each in  response.doc('.article-content > p').items():\
            content_list.append((each.html().replace('\\r','').replace('\\t','').replace('\\n','')))\
        if not content_list:\
            return None\
        res_dict['content'] = self.resplace(''.join(['<p>%s</p>'% v for v in content_list]))\
        res_dict['subject'] = u'托福'\
        res_dict['source'] = u'小站'\
        res_dict['tag'] = response.doc('.tag > a').text().split(' ')\
        bread = [res_dict['bread'], ]\
        bread.extend(response.doc('.head-crumbs-a-active').text().split(' '))\
        if res_dict['date'][:3] != '201':\
            res_dict['date'] = response.doc('.pull-left > span').text().split()[0].replace(u'年','-').replace(u'月','-').replace(u'日','')\
        res_dict['bread'] = list(set(bread))\
        res_dict['class'] = 27\
        res_dict['data_weight'] = 0\
        return res_dict$$$$$1471329885.7682
tuijian_haici$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-14 15:22:58\
# Project: tuijian_haici\
\
from pyspider.libs.base_handler import *\
\
import sys\
import MySQLdb\
import traceback\
reload(sys)\
sys.setdefaultencoding("utf-8")\
\
conn = MySQLdb.connect(host = "127.0.0.1", user = "root", passwd = "123", db = "cidiandb", charset = "utf8")\
cursor = conn.cursor()\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.2',\
    }\
\
    @every(minutes=1 * 30)\
    def on_start(self):\
        sql = 'select id, word from tb_word where tuijian_flag = 0 limit 10000'\
        try:\
            cursor.execute(sql)\
            for (ci_id, word,) in cursor.fetchall():\
                #word = 'good'\
                self.crawl('http://dict.cn/'+word, save = {'ci': word, 'id': ci_id}, callback=self.detail_page)\
        except:\
            traceback.print_exc()\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        word = response.save.get('ci').strip()\
        ci_id = response.save.get('id')\
        sql = "update tb_word set tuijian_flag= 1 where id= %s" % (ci_id)\
        try:           \
            cursor.execute(sql)\
            conn.commit()\
        except Exception, e:\
            print e\
            \
        recommend = {}\
        \
        for each in response.doc('.rel > h3').items():\
            if u'缩略词' in each.text():\
                #for t in each.next().children().items():\
                 #   print t.html()\
                content = ''.join(['<p>'+v.text()+'</p>' for v in each.next().find('li').items()])\
                recommend[each.text().strip()] = content\
            elif u'临近单词' in each.text(): \
                recommend[each.text().strip()] = each.next().remove('.sound').text().replace('\\t','').replace('\\n','').replace('\\r','')\
            \
        return {\
            "url": response.url,\
            "title": response.doc('title').text().replace(u'海词',u'跟谁学'),\
            "word": response.save.get('ci'),\
            "recommend": recommend,\
        }$$$$$1469538533.6167
weibo_sina$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-17 18:59:47\
# Project: weibo_sina\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://weibo.com/p/1005053532679345', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
        }\
$$$$$1466161197.7664
weixin_sougou$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-11 12:14:31\
# Project: sheying_fsbus\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as P\
import re\
import json\
import cPickle\
import time\
\
detail_headers = {\
      "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",\
      "Upgrade-Insecure-Requests": "1",\
      "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"\
    }\
\
index_headers = {\
      "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",\
      "Cookie": "CXID=61CC362F8DB5AF26DB926FB349837DF7; SUV=00FA5F9E246E3FDC56C2B5484910E349; ssuid=2375315090; ad=zkllllllll2QZVNXq73CWONoohxQZrqfJpc@Tlllll9lllllVqxlw@@@@@@@@@@@; SUID=DC3F6E24516C860A56C0576700097B44; ABTEST=4|1466480404|v1; weixinIndexVisited=1; SNUID=4BA8F8B39693A2570288F98E972AD230; JSESSIONID=aaafIPdrNUcCPb_Tbeovv; ppinf=5|1466497966|1467707566|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZToyNzolRTUlQjAlOEYlRTUlQUUlODclRTUlQUUlOTl8Y3J0OjEwOjE0NjY0OTc5NjZ8cmVmbmljazoyNzolRTUlQjAlOEYlRTUlQUUlODclRTUlQUUlOTl8dXNlcmlkOjQ0OkM3NDBBQUFCMkM3RERCM0Y0Njg0NzFFRTUzMTYzQUJGQHFxLnNvaHUuY29tfA; pprdig=PMcHhapmHPZtX7WFM_2PIYC2cnXBsOAT_EIIh_UC9Z4kAqswRfsbRA3Mp2aFG1t9adYaCkjRQWP-dKMiNAxGJkHS9J6XJqRKV6s9h2iRMEFxkLGI05EGVIfa-bEO4KjhNrA3xzjoIDGqSwfmZizeN97d0f1wWxmM5nEzg-0nm5w; PHPSESSID=l3fu330sueq4uogqnavs6s5ot3; SUIR=4BA8F8B39693A2570288F98E972AD230; sct=14; IPLOC=CN3300; ppmdig=1466500977000000d7f8ecd6a79fbd98c17e6d636b5074b4",\
      "Host": "weixin.sogou.com",\
      "Upgrade-Insecure-Requests": "1",\
      "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"\
}\
\
max_pageno = 20\
class Parser(object):\
    @staticmethod\
    def parse_list(response):\
        list_ret = P(response.doc(".results"))\
        ret = []\
        if list_ret:\
            for url_node in list_ret.find(".txt-box h4").find("a"):\
                ret.append(P(url_node))\
        return ret\
\
    @staticmethod\
    def parse_nextpage(response):\
        page_node = P(response.doc("#pagebar_container"))\
        try:\
            next_page = page_node.find("span").next()\
            if next_page:\
                current_pageno = page_node.find("span").text()\
                next_pageno = next_page.text()\
                if int(next_pageno) > int(current_pageno):\
                    # 限制只抓前20页\
                    if int(next_pageno) > max_pageno:\
                        return False\
                    return P('''<a href="%s&page=%s">next</a>''' % (response.save.get("base_url"), next_pageno))\
                return False\
\
            else:\
                return False\
        except Exception as info:\
            return False\
    \
    @staticmethod\
    def parse_detail(spider_handle, parser, response):\
        ret = {}\
        detail_node = response.doc("#img-content")\
        title = P(detail_node).find(".rich_media_title").text()\
        ret["title"] = title\
        ret["date"] = P(detail_node).find("#post-date").text()\
        ret["nickname"] = P(detail_node).find("a.rich_media_meta_nickname").text()\
        ret["author"] = P(detail_node).find("#post-date").next().text()\
        ret["content"] = P(detail_node).find("#js_content").html().strip()\
        ret["readnum"] = P(detail_node).find("#sg_readNum3").text()\
        ret["likenum"] = P(detail_node).find("#sg_likeNum3").text()\
        ret["isorg"] = P(detail_node).find("#copyright_logo").text()\
        return ret\
\
class Processor(object):\
    @staticmethod\
    def list_processor(spider_handle, parser, response):\
        # 解析列表页\
        list_result = parser.parse_list(response)\
        if list_result:\
            for item in list_result:\
                spider_handle.crawl(item.attr.href, fetch_type = "js", save = response.save, headers = detail_headers, callback = spider_handle.detail_page)\
\
        # 是否有翻页\
        next_page = parser.parse_nextpage(response)\
        if next_page:\
            spider_handle.crawl(next_page.attr.href, save = response.save, headers = index_headers, callback = spider_handle.index_page)\
        \
    @staticmethod\
    def detail_processor(spider_handle, parser, response):\
        return parser.parse_detail(spider_handle, parser, response)\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    crawler_parser = {\
        "1": {\
            "processor": Processor,\
            "parser": Parser,\
        }\
    }\
    crawler_list = [\
        {   \
            "key": "1",\
            "url": "http://weixin.sogou.com/weixin?type=2&query=%E9%9B%85%E6%80%9D",\
            "bread": ["微信搜狗"],\
        },\
    ]\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        for crawler in self.crawler_list:\
            self.crawl(crawler["url"],\
                       save = {\
                           'bread': crawler.get("bread", []),\
                           '__parser_key__': crawler.get("key"),\
                           'base_url': crawler["url"],\
                       },\
                       headers = index_headers,\
                       callback = self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        crawler_parser_dict.get("processor").list_processor(self, crawler_parser_dict.get("parser"), response)\
\
    #@config(priority=2)\
    @config(age=1)\
    def detail_page(self, response):\
        crawler_parser_dict = self.crawler_parser.get(response.save.get("__parser_key__"))\
        return crawler_parser_dict.get("processor").detail_processor(self, crawler_parser_dict.get("parser"), response)\
\
\
$$$$$1466571067.6703
wenda2_ntce$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-26 17:43:42\
# Project: wenda2_ntce\
\
from pyspider.libs.base_handler import *\
import time,datetime\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'天之前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u'天之前', '')))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
    \
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
        "headers": {\
        'Host':'www.ntce.com',\
        'Referer':'http://www.ntce.com/ask',\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.ntce.com/ask/', headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.searchcontent a').items():\
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        if response.doc('.list13 li'):\
            for each in response.doc('.list13 li').items():\
                if 'ivl' in each.outerHtml():\
                    continue\
                _dict = {}\
                _dict['category_id'] = each.find('.role').text()\
                _dict['title'] = each.find('.con > a').text()\
                _dict['create_user'] = ''\
                self.crawl(each.find('.con > a').attr.href, headers=self.crawl_config['headers'], save=_dict, callback=self.detail_page)\
            #翻页\
            for each in response.doc('.pages a').items():\
                self.crawl(each.attr.href, headers=self.crawl_config['headers'],  save=response.save, callback=self.list_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['create_time'] = response.doc('.p_w > .time').text().split()[0]\
        res_dict['create_user'] = response.doc('.con > div').eq(0).text().split(u'：')[1]\
        res_dict['url'] = response.url\
        res_dict['source'] = u'教师资格网'\
        res_dict['subject'] = u'主站问答'\
        res_dict['class'] = 34\
        res_dict['question_detail'] = response.doc('.hist > div').text() if response.doc('.hist > div') else ''\
\
        answers_list = []\
\
        for each in response.doc('.r_box > .con').items():\
            if  each.find('.ss > .time'):\
                create_time = each.find('.ss > .time').text().split()[0]\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            if not each.find('.details > span'):\
                continue\
            #print each.html()\
            answers_list.append({\
                "content":  each.find('.details > span').html().strip(),\
                "create_time": create_time,\
                "user_name": response.doc('.r_box > .ttl > div').text().replace(u'回复',''),\
                })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        return res_dict\
$$$$$1469582510.2383
wenda2_xdf$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-26 10:58:07\
# Project: wenda2_xdf\
\
\
from pyspider.libs.base_handler import *\
import time,datetime\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'天之前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u'天之前', '')))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
    \
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
        "headers": {\
        'Host': 'ask.koolearn.com ',\
        'Referer':'http://ask.koolearn.com/new/list/1/1/0',\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://ask.koolearn.com/', headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.whole-nav .popup-title > a').items():\
            self.crawl(each.attr.href, headers=self.crawl_config['headers'], save={'category_id':[each.text()]}, callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.j-mouse-ev').items():\
            _dict = {}\
            _dict['category_id'] = response.save.get('category_id')\
            _dict['title'] = each.find('.list-item-name > a').text()\
            _dict['create_time'] = get_date(each.find('.item-time').text())\
            _dict['create_user'] = each.find('.item-user-name').text()\
            self.crawl(each.find('.list-item-name > a').attr.href, headers=self.crawl_config['headers'], save=_dict, callback=self.detail_page)\
        #翻页\
        for each in response.doc('.next').items():\
            self.crawl(each.attr.href, headers=self.crawl_config['headers'],  save=response.save, callback=self.list_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = u'新东方'\
        res_dict['subject'] = u'主站问答'\
        res_dict['class'] = 34\
        res_dict['question_detail'] = response.doc('.q-content').text() if response.doc('.q-content') else ''\
        \
        answers_list = []\
        \
        for each in response.doc('.answer-info-box').items():\
            if  each.find('.answer-time'):\
                create_time = get_date(each.find('.answer-time').text())\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            if not each.find('.teacher-answer-content > p'):\
                continue\
            answers_list.append({\
                "content":  each.find('.teacher-answer-content > p').html().strip(),\
                "create_time": create_time,\
                "user_name": each.find('.answer-teach-info > a').text(),\
            })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        return res_dict\
$$$$$1469524016.3383
wenda3_baidu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-23 14:09:42\
# Project: wenda3_baidu\
from pyspider.libs.base_handler import *\
import MySQLdb\
import traceback\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
from urllib import quote\
\
conn = MySQLdb.connect(host = "127.0.0.1", user = "root", passwd = "123", db = "querydb", charset = "utf8")\
cursor = conn.cursor()\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
\
        "headers": {\
        'Host': 'zhidao.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1', \
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
    \
\
    @every(minutes=1 * 30)\
    def on_start(self):\
        sql = 'select id, query from tb_query where flag = 0 limit 300'\
        try:\
            cursor.execute(sql)\
            for (query_id, query,) in cursor.fetchall():\
                #print type(query) \
                #query = query.encode("UTF-8")\
                #print type(query)\
                #print query\
                #query = unicode(query,'utf-8')\
                #print type(query)\
                #print query\
                #word = 'good'\
                \
                for i in range(0,51,10):\
                    \
                    self.crawl('http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s'%(i,query),save = {'query':query, 'id':query_id}, proxy="122.96.59.106:80", headers=self.crawl_config['headers'], callback=self.index_page)\
        except Exception,e:\
            print e\
            \
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.dl').items():\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.line > a').text()\
            url = each.find('.line > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'],callback=self.detail_page)\
        for each in response.doc('.mb-20 > dl').items():\
            if u'百度百科' in each.text():\
                #print 'ok'\
                continue\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.mb-8 > a').text()\
            \
            url = each.find('.mb-8 > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'], callback=self.detail_page)\
        #翻页\
        #for each in response.doc('.pager-next').items():\
         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
        query_id = response.save.get('id')\
        sql = "update tb_query set flag=1 where id=%s" % (query_id)\
        try:           \
            cursor.execute(sql)\
            #print query_id\
            #print 'ok'\
            conn.commit()\
        except Exception, e:\
            print e\
        #if response.doc('.word-replace'):\
        #    return\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = 'baidu'\
        res_dict['subject'] = u'主站问答'\
        res_dict['category_id'] = []\
        res_dict['class'] = 34\
        #print response.doc('.q-content').html()\
        res_dict['question_detail'] = response.doc('.q-content').text() if not response.doc('.q-content .word-replace') else ''\
        \
        answers_list = []\
        if response.doc('.content > .mb-10'):\
            if not response.doc('.content > .mb-10 .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('div.mb-15 > .pos-time'):\
                    create_time = response.doc('div.mb-15 > .pos-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.content > .mb-10').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('div.mt-10').text(),\
                })\
        if response.doc('.quality-content > .quality-content-detail'):\
            if not response.doc('.quality-content > .quality-content-detail .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if response.doc('.quality-info > .reply-time'):\
                    create_time = response.doc('.quality-info > .reply-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.quality-content > .quality-content-detail').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('.q-name > a').text(),\
                })\
        if response.doc('.ec-answer'):\
            #print u'企业回答'\
            if not response.doc('.ec-answer .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('.ec-time'):\
                    create_time = response.doc('.ec-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.ec-answer').html().strip(),\
                    "create_time": create_time,\
                    "user_name": '',\
                })\
        if response.doc('.best-related dd > span'):\
            if not response.doc('.best-related dd > span .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                res_dict['question_detail'] = response.doc('.qb-content').html()\
                if  response.doc('.best-related i > span'):\
                    create_time = response.doc('.best-related i').eq(-1).text().split()[0]\
                    user_name = response.doc('.best-related i').eq(0).text()\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                    user_name = ''\
                answers_list.append({\
                    "content":  response.doc('.best-related dd > span').html().strip(),\
                    "create_time": create_time,\
                    "user_name": user_name,\
                })\
        for each in response.doc('div.answer-text').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            if  each.parent().prev().find('.pos-time'):\
                create_time = each.parent().prev().find('.pos-time').text().split()[0]\
                if not each.parent().prev().find('.pos-time').next():\
                    user_name = each.parent().prev().remove('.pos-time').text()\
                else:\
                    user_name = each.parent().prev().find('.pos-time').next().text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
                user_name = ''\
            answers_list.append({\
                "content":  each.html().strip(),\
                "create_time": create_time,\
                "user_name": user_name,\
            })\
        for each in response.doc('dl.other-answer > .answer').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            res_dict['question_detail'] = response.doc('.qb-content').html()\
            if  each.find('.ext-info > i'):\
                create_time = each.find('.ext-info > i').eq(-1).text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            answers_list.append({\
                "content":  each.children().eq(0).html().strip(),\
                "create_time": create_time,\
                "user_name": each.find('.ext-info > i').eq(0).text(),\
            })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        res_dict['create_user'] = response.doc('.ask-info > span').eq(1).text()\
        return res_dict\
$$$$$1470017554.1515
wenda4_baidu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-01 10:09:00\
# Project: wenda4_baidu\
\
from pyspider.libs.base_handler import *\
import MySQLdb\
import traceback\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
from urllib import quote\
\
conn = MySQLdb.connect(host = "127.0.0.1", user = "root", passwd = "123", db = "querydb", charset = "utf8")\
cursor = conn.cursor()\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
\
        "headers": {\
        'Host': 'zhidao.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1', \
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
    \
\
    @every(minutes=1 * 30)\
    def on_start(self):\
        sql = 'select id, query from tb_query where flag = 0 limit 3000'\
        try:\
            cursor.execute(sql)\
            for (query_id, query,) in cursor.fetchall():\
                #print type(query) \
                #query = query.encode("UTF-8")\
                #print type(query)\
                #print query\
                #query = unicode(query,'utf-8')\
                #print type(query)\
                #print query\
                #word = 'good'\
                query = query.replace(' ','%20')\
\
                for i in range(0,51,10):\
                    \
                    self.crawl('http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s'%(i,query),save = {'query':query, 'id':query_id}, proxy="122.96.59.106:80", headers=self.crawl_config['headers'], callback=self.index_page)\
        except Exception,e:\
            print e\
            \
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.dl').items():\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.line > a').text()\
            url = each.find('.line > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'],callback=self.detail_page)\
        for each in response.doc('.mb-20 > dl').items():\
            if u'百度百科' in each.text():\
                #print 'ok'\
                continue\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.mb-8 > a').text()\
            \
            url = each.find('.mb-8 > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'], callback=self.detail_page)\
        #翻页\
        #for each in response.doc('.pager-next').items():\
         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
        query_id = response.save.get('id')\
        sql = "update tb_query set flag=1 where id=%s" % (query_id)\
        try:           \
            cursor.execute(sql)\
            #print query_id\
            #print 'ok'\
            conn.commit()\
        except Exception, e:\
            print e\
        #if response.doc('.word-replace'):\
        #    return\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = 'baidu'\
        res_dict['subject'] = u'主站问答'\
        res_dict['category_id'] = []\
        res_dict['class'] = 34\
        #print response.doc('.q-content').html()\
        res_dict['question_detail'] = response.doc('.q-content').text() if not response.doc('.q-content .word-replace') else ''\
        \
        answers_list = []\
        if response.doc('.content > .mb-10'):\
            if not response.doc('.content > .mb-10 .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('div.mb-15 > .pos-time'):\
                    create_time = response.doc('div.mb-15 > .pos-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.content > .mb-10').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('div.mt-10').text(),\
                })\
        if response.doc('.quality-content > .quality-content-detail'):\
            if not response.doc('.quality-content > .quality-content-detail .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if response.doc('.quality-info > .reply-time'):\
                    create_time = response.doc('.quality-info > .reply-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.quality-content > .quality-content-detail').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('.q-name > a').text(),\
                })\
        if response.doc('.ec-answer'):\
            #print u'企业回答'\
            if not response.doc('.ec-answer .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('.ec-time'):\
                    create_time = response.doc('.ec-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.ec-answer').html().strip(),\
                    "create_time": create_time,\
                    "user_name": '',\
                })\
        if response.doc('.best-related dd > span'):\
            if not response.doc('.best-related dd > span .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                res_dict['question_detail'] = response.doc('.qb-content').html()\
                if  response.doc('.best-related i > span'):\
                    create_time = response.doc('.best-related i').eq(-1).text().split()[0]\
                    user_name = response.doc('.best-related i').eq(0).text()\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                    user_name = ''\
                answers_list.append({\
                    "content":  response.doc('.best-related dd > span').html().strip(),\
                    "create_time": create_time,\
                    "user_name": user_name,\
                })\
        for each in response.doc('div.answer-text').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            if  each.parent().prev().find('.pos-time'):\
                create_time = each.parent().prev().find('.pos-time').text().split()[0]\
                if not each.parent().prev().find('.pos-time').next():\
                    user_name = each.parent().prev().remove('.pos-time').text()\
                else:\
                    user_name = each.parent().prev().find('.pos-time').next().text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
                user_name = ''\
            answers_list.append({\
                "content":  each.html().strip(),\
                "create_time": create_time,\
                "user_name": user_name,\
            })\
        for each in response.doc('dl.other-answer > .answer').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            res_dict['question_detail'] = response.doc('.qb-content').html()\
            if  each.find('.ext-info > i'):\
                create_time = each.find('.ext-info > i').eq(-1).text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            answers_list.append({\
                "content":  each.children().eq(0).html().strip(),\
                "create_time": create_time,\
                "user_name": each.find('.ext-info > i').eq(0).text(),\
            })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        res_dict['create_user'] = response.doc('.ask-info > span').eq(1).text()\
        return res_dict$$$$$1470476039.9230
wenda5_baidu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-06 17:33:45\
# Project: wenda5_baidu\
\
from pyspider.libs.base_handler import *\
import MySQLdb\
import traceback\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
from urllib import quote\
\
conn = MySQLdb.connect(host = "127.0.0.1", user = "root", passwd = "123", db = "querydb", charset = "utf8")\
cursor = conn.cursor()\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
\
        "headers": {\
        'Host': 'zhidao.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1', \
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
    \
\
    @every(minutes=1 * 30)\
    def on_start(self):\
        sql = 'select id, query from tb_query where flag = 0 limit 3000'\
        try:\
            cursor.execute(sql)\
            for (query_id, query,) in cursor.fetchall():\
                #print type(query) \
                #query = query.encode("UTF-8")\
                #print type(query)\
                #print query\
                #query = unicode(query,'utf-8')\
                #print type(query)\
                #print query\
                #word = 'good'\
                query = query.replace(' ','%20')\
\
                for i in range(0,51,10):\
                    \
                    self.crawl('http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s'%(i,query),save = {'query':query, 'id':query_id}, proxy="122.96.59.106:80", headers=self.crawl_config['headers'], callback=self.index_page)\
        except Exception,e:\
            print e\
            \
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.dl').items():\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.line > a').text()\
            url = each.find('.line > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'],callback=self.detail_page)\
        for each in response.doc('.mb-20 > dl').items():\
            if u'百度百科' in each.text():\
                #print 'ok'\
                continue\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.mb-8 > a').text()\
            \
            url = each.find('.mb-8 > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'], callback=self.detail_page)\
        #翻页\
        #for each in response.doc('.pager-next').items():\
         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
        query_id = response.save.get('id')\
        sql = "update tb_query set flag=1 where id=%s" % (query_id)\
        try:           \
            cursor.execute(sql)\
            #print query_id\
            #print 'ok'\
            conn.commit()\
        except Exception, e:\
            print e\
        #if response.doc('.word-replace'):\
        #    return\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = 'baidu'\
        res_dict['subject'] = u'主站问答'\
        res_dict['category_id'] = []\
        res_dict['class'] = 34\
        #print response.doc('.q-content').html()\
        res_dict['question_detail'] = response.doc('.q-content').text() if not response.doc('.q-content .word-replace') else ''\
        \
        answers_list = []\
        if response.doc('.content > .mb-10'):\
            if not response.doc('.content > .mb-10 .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('div.mb-15 > .pos-time'):\
                    create_time = response.doc('div.mb-15 > .pos-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.content > .mb-10').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('div.mt-10').text(),\
                })\
        if response.doc('.quality-content > .quality-content-detail'):\
            if not response.doc('.quality-content > .quality-content-detail .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if response.doc('.quality-info > .reply-time'):\
                    create_time = response.doc('.quality-info > .reply-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.quality-content > .quality-content-detail').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('.q-name > a').text(),\
                })\
        if response.doc('.ec-answer'):\
            #print u'企业回答'\
            if not response.doc('.ec-answer .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('.ec-time'):\
                    create_time = response.doc('.ec-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.ec-answer').html().strip(),\
                    "create_time": create_time,\
                    "user_name": '',\
                })\
        if response.doc('.best-related dd > span'):\
            if not response.doc('.best-related dd > span .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                res_dict['question_detail'] = response.doc('.qb-content').html()\
                if  response.doc('.best-related i > span'):\
                    create_time = response.doc('.best-related i').eq(-1).text().split()[0]\
                    user_name = response.doc('.best-related i').eq(0).text()\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                    user_name = ''\
                answers_list.append({\
                    "content":  response.doc('.best-related dd > span').html().strip(),\
                    "create_time": create_time,\
                    "user_name": user_name,\
                })\
        for each in response.doc('div.answer-text').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            if  each.parent().prev().find('.pos-time'):\
                create_time = each.parent().prev().find('.pos-time').text().split()[0]\
                if not each.parent().prev().find('.pos-time').next():\
                    user_name = each.parent().prev().remove('.pos-time').text()\
                else:\
                    user_name = each.parent().prev().find('.pos-time').next().text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
                user_name = ''\
            answers_list.append({\
                "content":  each.html().strip(),\
                "create_time": create_time,\
                "user_name": user_name,\
            })\
        for each in response.doc('dl.other-answer > .answer').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            res_dict['question_detail'] = response.doc('.qb-content').html()\
            if  each.find('.ext-info > i'):\
                create_time = each.find('.ext-info > i').eq(-1).text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            answers_list.append({\
                "content":  each.children().eq(0).html().strip(),\
                "create_time": create_time,\
                "user_name": each.find('.ext-info > i').eq(0).text(),\
            })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        res_dict['create_user'] = response.doc('.ask-info > span').eq(1).text()\
        return res_dict$$$$$1471225514.1555
wenda6_baidu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-11 09:57:18\
# Project: wenda6_baidu\
\
from pyspider.libs.base_handler import *\
import MySQLdb\
import traceback\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
from urllib import quote\
\
conn = MySQLdb.connect(host = "127.0.0.1", user = "root", passwd = "123", db = "querydb", charset = "utf8")\
cursor = conn.cursor()\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
\
        "headers": {\
        'Host': 'zhidao.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1', \
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
    \
\
    @every(minutes=1 * 30)\
    def on_start(self):\
        sql = 'select id, query from tb_query where flag = 0 limit 3000'\
        try:\
            cursor.execute(sql)\
            for (query_id, query,) in cursor.fetchall():\
                #print type(query) \
                #query = query.encode("UTF-8")\
                #print type(query)\
                #print query\
                #query = unicode(query,'utf-8')\
                #print type(query)\
                #print query\
                #word = 'good'\
                query = query.replace(' ','%20')\
\
                for i in range(0,51,10):\
                    \
                    self.crawl('http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s'%(i,query),save = {'query':query, 'id':query_id}, proxy="122.96.59.106:80", headers=self.crawl_config['headers'], callback=self.index_page)\
        except Exception,e:\
            print e\
            \
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.dl').items():\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.line > a').text()\
            url = each.find('.line > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'],callback=self.detail_page)\
        for each in response.doc('.mb-20 > dl').items():\
            if u'百度百科' in each.text():\
                #print 'ok'\
                continue\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.mb-8 > a').text()\
            \
            url = each.find('.mb-8 > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'], callback=self.detail_page)\
        #翻页\
        #for each in response.doc('.pager-next').items():\
         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
        query_id = response.save.get('id')\
        sql = "update tb_query set flag=1 where id=%s" % (query_id)\
        try:           \
            cursor.execute(sql)\
            #print query_id\
            #print 'ok'\
            conn.commit()\
        except Exception, e:\
            print e\
        #if response.doc('.word-replace'):\
        #    return\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = 'baidu'\
        res_dict['subject'] = u'主站问答'\
        res_dict['category_id'] = []\
        res_dict['class'] = 34\
        #print response.doc('.q-content').html()\
        res_dict['question_detail'] = response.doc('.q-content').text() if not response.doc('.q-content .word-replace') else ''\
        \
        answers_list = []\
        if response.doc('.content > .mb-10'):\
            if not response.doc('.content > .mb-10 .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('div.mb-15 > .pos-time'):\
                    create_time = response.doc('div.mb-15 > .pos-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.content > .mb-10').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('div.mt-10').text(),\
                })\
        if response.doc('.quality-content > .quality-content-detail'):\
            if not response.doc('.quality-content > .quality-content-detail .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if response.doc('.quality-info > .reply-time'):\
                    create_time = response.doc('.quality-info > .reply-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.quality-content > .quality-content-detail').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('.q-name > a').text(),\
                })\
        if response.doc('.ec-answer'):\
            #print u'企业回答'\
            if not response.doc('.ec-answer .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('.ec-time'):\
                    create_time = response.doc('.ec-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.ec-answer').html().strip(),\
                    "create_time": create_time,\
                    "user_name": '',\
                })\
        if response.doc('.best-related dd > span'):\
            if not response.doc('.best-related dd > span .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                res_dict['question_detail'] = response.doc('.qb-content').html()\
                if  response.doc('.best-related i > span'):\
                    create_time = response.doc('.best-related i').eq(-1).text().split()[0]\
                    user_name = response.doc('.best-related i').eq(0).text()\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                    user_name = ''\
                answers_list.append({\
                    "content":  response.doc('.best-related dd > span').html().strip(),\
                    "create_time": create_time,\
                    "user_name": user_name,\
                })\
        for each in response.doc('div.answer-text').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            if  each.parent().prev().find('.pos-time'):\
                create_time = each.parent().prev().find('.pos-time').text().split()[0]\
                if not each.parent().prev().find('.pos-time').next():\
                    user_name = each.parent().prev().remove('.pos-time').text()\
                else:\
                    user_name = each.parent().prev().find('.pos-time').next().text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
                user_name = ''\
            answers_list.append({\
                "content":  each.html().strip(),\
                "create_time": create_time,\
                "user_name": user_name,\
            })\
        for each in response.doc('dl.other-answer > .answer').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            res_dict['question_detail'] = response.doc('.qb-content').html()\
            if  each.find('.ext-info > i'):\
                create_time = each.find('.ext-info > i').eq(-1).text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            answers_list.append({\
                "content":  each.children().eq(0).html().strip(),\
                "create_time": create_time,\
                "user_name": each.find('.ext-info > i').eq(0).text(),\
            })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        res_dict['create_user'] = response.doc('.ask-info > span').eq(1).text()\
        return res_dict$$$$$1471225518.0182
wenda7_baidu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-13 12:59:36\
# Project: wenda7_baidu\
\
#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-11 09:57:18\
# Project: wenda6_baidu\
\
from pyspider.libs.base_handler import *\
import MySQLdb\
import traceback\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
from urllib import quote\
\
conn = MySQLdb.connect(host = "127.0.0.1", user = "root", passwd = "123", db = "querydb", charset = "utf8")\
cursor = conn.cursor()\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
\
        "headers": {\
        'Host': 'zhidao.baidu.com',\
        'Pragma': 'no-cache',\
        'Upgrade-Insecure-Requests': '1', \
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
        }\
    }\
    \
\
    @every(minutes=1 * 30)\
    def on_start(self):\
        sql = 'select id, query from tb_query where flag = 0 limit 3000'\
        try:\
            cursor.execute(sql)\
            for (query_id, query,) in cursor.fetchall():\
                #print type(query) \
                #query = query.encode("UTF-8")\
                #print type(query)\
                #print query\
                #query = unicode(query,'utf-8')\
                #print type(query)\
                #print query\
                #word = 'good'\
                query = query.replace(' ','%20')\
\
                for i in range(0,51,10):\
                    \
                    self.crawl('http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s'%(i,query),save = {'query':query, 'id':query_id}, proxy="122.96.59.106:80", headers=self.crawl_config['headers'], callback=self.index_page)\
        except Exception,e:\
            print e\
            \
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.dl').items():\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.line > a').text()\
            url = each.find('.line > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'],callback=self.detail_page)\
        for each in response.doc('.mb-20 > dl').items():\
            if u'百度百科' in each.text():\
                #print 'ok'\
                continue\
            _dict = {}\
            _dict['id'] = response.save.get('id') \
            _dict['query'] = response.save.get('query') \
            _dict['title'] = each.find('.mb-8 > a').text()\
            \
            url = each.find('.mb-8 > a').attr.href\
            _dict['create_time'] = each.find('.mr-8').text().split()[0]\
            self.crawl(url, save = _dict,headers=self.crawl_config['headers'], callback=self.detail_page)\
        #翻页\
        #for each in response.doc('.pager-next').items():\
         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
        query_id = response.save.get('id')\
        sql = "update tb_query set flag=1 where id=%s" % (query_id)\
        try:           \
            cursor.execute(sql)\
            #print query_id\
            #print 'ok'\
            conn.commit()\
        except Exception, e:\
            print e\
        #if response.doc('.word-replace'):\
        #    return\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = 'baidu'\
        res_dict['subject'] = u'主站问答'\
        res_dict['category_id'] = []\
        res_dict['class'] = 34\
        #print response.doc('.q-content').html()\
        res_dict['question_detail'] = response.doc('.q-content').text() if not response.doc('.q-content .word-replace') else ''\
        \
        answers_list = []\
        if response.doc('.content > .mb-10'):\
            if not response.doc('.content > .mb-10 .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('div.mb-15 > .pos-time'):\
                    create_time = response.doc('div.mb-15 > .pos-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.content > .mb-10').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('div.mt-10').text(),\
                })\
        if response.doc('.quality-content > .quality-content-detail'):\
            if not response.doc('.quality-content > .quality-content-detail .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if response.doc('.quality-info > .reply-time'):\
                    create_time = response.doc('.quality-info > .reply-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.quality-content > .quality-content-detail').html().strip(),\
                    "create_time": create_time,\
                    "user_name": response.doc('.q-name > a').text(),\
                })\
        if response.doc('.ec-answer'):\
            #print u'企业回答'\
            if not response.doc('.ec-answer .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                if  response.doc('.ec-time'):\
                    create_time = response.doc('.ec-time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                answers_list.append({\
                    "content":  response.doc('.ec-answer').html().strip(),\
                    "create_time": create_time,\
                    "user_name": '',\
                })\
        if response.doc('.best-related dd > span'):\
            if not response.doc('.best-related dd > span .word-replace'):\
                if response.doc('.question-list-item-tag > a'):\
                    res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
                res_dict['question_detail'] = response.doc('.qb-content').html()\
                if  response.doc('.best-related i > span'):\
                    create_time = response.doc('.best-related i').eq(-1).text().split()[0]\
                    user_name = response.doc('.best-related i').eq(0).text()\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                    user_name = ''\
                answers_list.append({\
                    "content":  response.doc('.best-related dd > span').html().strip(),\
                    "create_time": create_time,\
                    "user_name": user_name,\
                })\
        for each in response.doc('div.answer-text').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            if  each.parent().prev().find('.pos-time'):\
                create_time = each.parent().prev().find('.pos-time').text().split()[0]\
                if not each.parent().prev().find('.pos-time').next():\
                    user_name = each.parent().prev().remove('.pos-time').text()\
                else:\
                    user_name = each.parent().prev().find('.pos-time').next().text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
                user_name = ''\
            answers_list.append({\
                "content":  each.html().strip(),\
                "create_time": create_time,\
                "user_name": user_name,\
            })\
        for each in response.doc('dl.other-answer > .answer').items():\
            if each.find('.word-replace'):\
                continue\
            if response.doc('.question-list-item-tag > a'):\
                res_dict['category_id'] = [v.text() for v in response.doc('.question-list-item-tag > a').items() if v]\
            res_dict['question_detail'] = response.doc('.qb-content').html()\
            if  each.find('.ext-info > i'):\
                create_time = each.find('.ext-info > i').eq(-1).text()\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            answers_list.append({\
                "content":  each.children().eq(0).html().strip(),\
                "create_time": create_time,\
                "user_name": each.find('.ext-info > i').eq(0).text(),\
            })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        res_dict['create_user'] = response.doc('.ask-info > span').eq(1).text()\
        return res_dict$$$$$1471225510.4113
wenda_360$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-04 14:47:05\
# Project: wenda_360\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
import time\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    \
    header = {\
'Cache-Control': 'no-cache',\
'Connection': 'keep-alive',\
'Host': 'wenda.so.com',\
'Pragma': 'no-cache',\
'Upgrade-Insecure-Requests': '1',\
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36',\
'Cookie': 'test_cookie_enable=null; __guid=9114931.2491033014933137000.1459824554175.862; WDTKID=414ac95d77915818; count=1; search_last_sid=33e636c8a753b0bad387d3695c798a7b; search_last_kw=%u513F%u6B4C; erules=p4-3%7Cp2-1'\
   }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        #with open('/apps/home/rd/hexing/data/query.txt','r') as f:\
        #    for line in f:\
        #        if line:\
        #            line = line.strip()\
        line = '儿歌'\
        for index in range(20):\
            self.crawl('http://wenda.so.com/search/?q=%s&pn=%s'%(line,index),save = {'query':line} ,headers = self.header,callback=self.index_page)\
            \
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for  each  in response.doc('.qa-list .item').items():\
                _dict = {}\
                _dict['query'] = response.save.get('query') \
                _dict['title'] = each.find('.qa-i-hd a').text()\
                url = each.find('.qa-i-hd a').attr.href\
                _dict['create_time'] = each.find('.qa-i-ft').text().split()[-1].replace('.', '-')\
                self.crawl(url, save = _dict,headers = self.header,callback=self.detail_page)\
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = '360'\
        res_dict['subject'] = u'主站问答'\
        res_dict['class'] = 34\
        res_dict['question_detail'] = response.doc('.q-cnt').text()\
        if  response.doc('.bd .text > span').text():\
            create_time = response.doc('.bd .text > span').text().split()[-1].replace('.','-')\
        else:\
            create_time = time.strftime('%Y-%m-%d',time.localtime())\
        res_dict['answers'] = {\
            "content":  response.doc('.resolved-cnt').html(),\
            "create_time": create_time,\
            "user_name": response.doc('.text > div a').text(),\
        }\
        res_dict['data_weight'] = 0\
        res_dict['create_user'] = response.doc('.text > span > .ask-author').text()\
        return res_dict\
$$$$$1471335394.1948
wenda_360_jiaoyu$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-04-05 10:49:43\
# Project: qa_360\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    dic = {'http://wenda.so.com/c/2763?pn=0&filt=20': u'\\u6821\\u56ed\\u8bdd\\u9898', 'http://wenda.so.com/c/578?pn=0&filt=20': u'\\u5347\\u5b66\\u5165\\u5b66', 'http://wenda.so.com/c/2790?pn=0&filt=20': u'\\u751f\\u7269', 'http://wenda.so.com/c/2787?pn=0&filt=20': u'\\u5b66\\u4e60\\u65b9\\u6cd5', 'http://wenda.so.com/c/96?pn=0&filt=20': u'\\u7406\\u5de5\\u5b66\\u79d1', 'http://wenda.so.com/c/2794?pn=0&filt=20': u'\\u5316\\u5b66', 'http://wenda.so.com/c/2793?pn=0&filt=20': u'\\u6570\\u5b66', 'http://wenda.so.com/c/2789?pn=0&filt=20': u'\\u7269\\u7406', 'http://wenda.so.com/c/2792?pn=0&filt=20': u'\\u519c\\u4e1a', 'http://wenda.so.com/c/100?pn=0&filt=20': u'\\u516c\\u52a1\\u5458', 'http://wenda.so.com/c/101?pn=0&filt=20': u'\\u7559\\u5b66\\u51fa\\u56fd', 'http://wenda.so.com/c/138?pn=0&filt=20': u'\\u5bb6\\u5ead\\u6559\\u80b2', 'http://wenda.so.com/c/99?pn=0&filt=20': u'\\u8d44\\u683c\\u8003\\u8bd5', 'http://wenda.so.com/c/98?pn=0&filt=20': u'\\u4e2d\\u5c0f\\u5b66\\u4f5c\\u4e1a', 'http://wenda.so.com/c/95?pn=0&filt=20': u'\\u4eba\\u6587\\u793e\\u79d1', 'http://wenda.so.com/c/2788?pn=0&filt=20': u'\\u8bed\\u6587', 'http://wenda.so.com/c/94?pn=0&filt=20': u'\\u9ad8\\u7b49\\u9662\\u6821', 'http://wenda.so.com/c/97?pn=0&filt=20': u'\\u5916\\u8bed', 'http://wenda.so.com/c/2791?pn=0&filt=20': u'\\u5de5\\u7a0b\\u5b66'}\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k, v in self.dic.iteritems():\
            self.crawl(k, save={'type': v}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.question-wrap a').items():\
            \
            self.crawl(each.attr.href,save={'type': response.save['type'],}, callback=self.detail_page)\
        for each in response.doc('.pagination a').items():\
            self.crawl(each.attr.href, save={'type': response.save['type']}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for info in response.doc('.mod-resolved-ans > .bd').items():\
            _dic = {}\
            _dic['name'] = info.find('.text > div a').text()\
            try:\
                _dic['date'] = info.find('.text > span').text().split()[-1].replace('.','-')\
            except:\
                pass\
            _dic['content'] = info.find('.resolved-cnt').html()\
            _dic['avatar'] = info.find('.info img').attr.src\
            content_list.append((_dic))\
        for info in response.doc('.mod-other-ans li').items():\
            try:\
                _dic = {}\
                _dic['name'] = info.find('.text > div a').text()\
                _dic['date'] = info.find('.text > span').text().split()[-1].replace('.','-')\
                _dic['content'] = info.find('.other-ans-cnt').html()\
                _dic['avatar'] = info.find('.info img').attr.src\
                content_list.append((_dic))\
            except:\
                continue\
        if not content_list:\
            return None\
        return {\
            "url": response.url,\
            "title": response.doc('.js-ask-title').text(),\
            "type": response.save['type'],\
            "answers": content_list,\
            "question_detail": response.doc('.q-cnt').text(),\
            "create_date": response.doc('.hd .text > span').text().split()[-1],\
        }\
$$$$$1469623064.8946
wenda_51offer$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-27 17:54:11\
# Project: wenda_51offer\
\
from pyspider.libs.base_handler import *\
import re\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for i in range(48):\
            self.crawl('http://www.51offer.com/wenda/index.html?problemType=1&currentPage=%d'%i, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.s_question').items():\
            self.crawl(each.attr.href, callback=self.detail_page)           \
        # 翻页\
        for each in response.doc('.next').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
          \
    @config(priority=2)\
    def detail_page(self, response):\
        r = re.compile("[^0-9:\\s]")\
        t_str = response.doc('.detail .timeset').eq(0).text()\
        t_arr = r.split(t_str)\
        answer = ''\
        flag = 1\
        for each in response.doc('.editBox  p').items():\
            if u'社区推荐' in each.text() or u'热门内容推荐' in each.text() :\
                flag = 0\
                break\
            else:\
                if each.html():\
                    answer +=  each.html()\
        res_answer = response.doc('.answerContent').eq(0).text() + response.doc('.detail > div > div').eq(0).text() if flag else answer\
        if not res_answer:\
            return None\
        return {\
            "url": response.url,\
            "question": response.doc('.askTitle').text(),\
            "types": response.doc('.askLabel > a').text(),\
            "question_detail": response.doc('.ask > div').eq(1).text(),\
            "create_date": t_arr[0]+'-'+t_arr[1]+'-'+t_arr[2],\
            "answer": res_answer,\
            "answer_date": t_arr[0]+'-'+t_arr[1]+'-'+t_arr[2],\
    }$$$$$1467182441.8102
wenda_cxy_bky$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-19 10:54:58\
# Project: wenda_cxy_bky\
\
\
from pyspider.libs.base_handler import *\
\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'周前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'个月前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'年前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('https://q.cnblogs.com/tag/list?pageindex=1', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('tr td li a').items():\
            self.crawl(each.attr.href+'/solved', callback=self.list_page)\
        #翻页\
        for each in response.doc('#pager a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
            \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.news_item').items():\
            _dict = {}\
            _dict['title'] = each.find('.news_entry > a').text()\
            self.crawl(each.find('.news_entry > a').attr.href, save=_dict,  callback=self.detail_page)\
        #翻页\
        for each in response.doc('#pager a').items():\
            self.crawl(each.attr.href, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for info in response.doc('.qitem_best_answer_inner').items():\
            _dic = {}\
            #print info.find('.answer_author').text()\
            _dic['name'] = info.find('.answer_author > .bluelink').eq(0).text()\
            _dic['date'] = info.find('.answer_author').text().split()[-2]\
            _dic['content'] = info.find('.q_content').html().strip()\
            content_list.append((_dic))\
        for info in response.doc('.qitem_all_answer_inner > .q_answeritem ').items():\
            _dic = {}\
            #print info.find('.answer_author').text()\
            _dic['name'] = info.find('.answer_author > .bluelink').eq(0).text()\
            _dic['date'] = info.find('.answer_author').text().split()[-2]\
            _dic['content'] = info.find('.q_content').html().strip()\
            content_list.append((_dic))\
        if not content_list:\
            return None\
        return {\
            "url": response.url,\
            "title": response.save['title'],\
            #"subject": response.save['subject'],\
            "subject": u'程序员',\
            "answers": content_list,\
            "source": u'博客园',\
            "question_detail": response.doc('#qes_content').html().strip(),\
            "class": 47,\
            "data_weight": 0,\
        }\
$$$$$1471655369.7138
wenda_cxy_bky_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-22 18:08:37\
# Project: wenda_cxy_bky_inc\
\
\
\
from pyspider.libs.base_handler import *\
\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'周前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'个月前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'年前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('https://q.cnblogs.com/tag/list?pageindex=1', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('tr td li a').items():\
            self.crawl(each.attr.href+'/solved', callback=self.list_page)\
        #翻页\
        for each in response.doc('#pager a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
            \
    @config(age=1 * 1)\
    def list_page(self, response):\
        for each in response.doc('.news_item').items():\
            _dict = {}\
            _dict['title'] = each.find('.news_entry > a').text()\
            self.crawl(each.find('.news_entry > a').attr.href, save=_dict,  callback=self.detail_page)\
        #翻页\
        #for each in response.doc('#pager a').items():\
         #   self.crawl(each.attr.href, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for info in response.doc('.qitem_best_answer_inner').items():\
            _dic = {}\
            #print info.find('.answer_author').text()\
            _dic['name'] = info.find('.answer_author > .bluelink').eq(0).text()\
            _dic['date'] = info.find('.answer_author').text().split()[-2]\
            _dic['content'] = info.find('.q_content').html().strip()\
            content_list.append((_dic))\
        for info in response.doc('.qitem_all_answer_inner > .q_answeritem ').items():\
            _dic = {}\
            #print info.find('.answer_author').text()\
            _dic['name'] = info.find('.answer_author > .bluelink').eq(0).text()\
            _dic['date'] = info.find('.answer_author').text().split()[-2]\
            _dic['content'] = info.find('.q_content').html().strip()\
            content_list.append((_dic))\
        if not content_list:\
            return None\
        return {\
            "url": response.url,\
            "title": response.save['title'],\
            #"subject": response.save['subject'],\
            "subject": u'程序员',\
            "answers": content_list,\
            "source": u'博客园',\
            "question_detail": response.doc('#qes_content').html().strip(),\
            "class": 47,\
            "data_weight": 0,\
        }\
$$$$$1471860629.2340
wenda_cxy_csdn$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-19 18:22:17\
# Project: wenda_cxy_csdn\
\
\
\
#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 10:14:04\
# Project: csdn_proxy_change\
\
\
\
from pyspider.libs.base_handler import *\
\
import json,re\
menu = '''{\
    "forumNodes": [{"name":"\\u79fb\\u52a8\\u5f00\\u53d1","url":"/forums/Mobile","children":[{"name":"iOS","url":"/forums/ios"},{"name":"Android","url":"/forums/Android"},{"name":"Swift","url":"/forums/swift"},{"name":"Windows\\u5ba2\\u6237\\u7aef\\u5f00\\u53d1","url":"/forums/WindowsMobile"},{"name":"Symbian","url":"/forums/Symbian"},{"name":"BlackBerry","url":"/forums/BlackBerry"},{"name":"Qt","url":"/forums/Qt"},{"name":"\\u79fb\\u52a8\\u652f\\u4ed8","url":"/forums/PaypalCommunity"},{"name":"\\u79fb\\u52a8\\u5e7f\\u544a","url":"/forums/MobileAD"},{"name":"\\u5fae\\u4fe1\\u5f00\\u53d1","url":"/forums/weixin"},{"name":"\\u79fb\\u52a8\\u5f00\\u53d1\\u5176\\u4ed6\\u95ee\\u9898","url":"/forums/Mobile_Other"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/MobileNonTechnical"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"},{"name":"\\u8054\\u901aWO+\\u5f00\\u653e\\u5e73\\u53f0","url":"/forums/chinaunicom"}]},{"name":"\\u4e91\\u8ba1\\u7b97","url":"/forums/CloudComputing","children":[{"name":"IaaS","children":[{"name":"OpenStack","url":"/forums/OpenStack"}]},{"name":"PaaS/SaaS","children":[{"name":"Cloud Foundry","url":"/forums/CloudFoundry"},{"name":"GAE","url":"/forums/GAE"}]},{"name":"\\u6570\\u636e\\u4e2d\\u5fc3\\u8fd0\\u7ef4","children":[{"name":"\\u670d\\u52a1\\u5668","url":"/forums/server"},{"name":"\\u7f51\\u7edc","url":"/forums/network"},{"name":"\\u865a\\u62df\\u5316","url":"/forums/virtual"}]},{"name":"AWS","url":"/forums/AWS"},{"name":"\\u534e\\u4e3a\\u4e91\\u8ba1\\u7b97","url":"/forums/fusioncloud"},{"name":"\\u5f00\\u653e\\u5e73\\u53f0","url":"/forums/OpenAPI"},{"name":"\\u4e91\\u5b89\\u5168","url":"/forums/ST_Security"},{"name":"\\u5206\\u5e03\\u5f0f\\u8ba1\\u7b97/Hadoop","url":"/forums/hadoop"},{"name":"\\u4e91\\u5b58\\u50a8","url":"/forums/CloudStorage"},{"name":"Docker","url":"/forums/docker"},{"name":"Spark","url":"/forums/spark"},{"name":"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0","url":"/forums/DE"}]},{"name":"\\u4f01\\u4e1aIT","url":"/forums/Enterprise","children":[{"name":"\\u4e2d\\u95f4\\u4ef6","children":[{"name":"\\u4e2d\\u95f4\\u4ef6","url":"/forums/Middleware"},{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"JBoss","url":"/forums/JBoss"}]},{"name":"\\u4f01\\u4e1a\\u7ba1\\u7406\\u8f6f\\u4ef6","children":[{"name":"\\u6d88\\u606f\\u534f\\u4f5c","url":"/forums/ExchangeServer"},{"name":"SharePoint","url":"/forums/SharePoint"}]},{"name":"Atlassian\\u6280\\u672f\\u8bba\\u575b","url":"/forums/atlassian"},{"name":"JetBrains\\u6280\\u672f\\u8bba\\u575b","url":"/forums/JetBrains"},{"name":"\\u5730\\u7406\\u4fe1\\u606f\\u7cfb\\u7edf","url":"/forums/GIS"},{"name":"\\u4f01\\u4e1a\\u4fe1\\u606f\\u5316","url":"/forums/Enterprise_Information"},{"name":"ERP/CRM","url":"/forums/ERP"},{"name":"\\u5176\\u4ed6","url":"/forums/Enterprise_Other"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"},{"name":"Enterprise Architect\\u6280\\u672f\\u8bba\\u575b","url":"/forums/EA"}]},{"name":".NET\\u6280\\u672f","url":"/forums/DotNET","children":[{"name":"C#","url":"/forums/CSharp"},{"name":"ASP.NET","url":"/forums/ASPDotNET"},{"name":".NET Framework","url":"/forums/DotNETFramework"},{"name":"Web Services","url":"/forums/DotNETWebServices"},{"name":"VB.NET","url":"/forums/VBDotNET"},{"name":"VC.NET","url":"/forums/VCDotNet"},{"name":"\\u56fe\\u8868\\u533a","url":"/forums/DotNETReport"},{"name":".NET\\u6280\\u672f\\u524d\\u77bb","url":"/forums/DotNET_NewTech"},{"name":".NET\\u5206\\u6790\\u4e0e\\u8bbe\\u8ba1","url":"/forums/DotNETAnalysisAndDesign"},{"name":"\\u7ec4\\u4ef6/\\u63a7\\u4ef6\\u5f00\\u53d1","url":"/forums/DotNET_Controls"},{"name":"SharePoint","url":"/forums/SharePoint"},{"name":"WPF/Silverlight","url":"/forums/Silverlight"},{"name":"LINQ","url":"/forums/LINQ"},{"name":"VSTS","url":"/forums/VSTS"},{"name":"\\u5176\\u4ed6","url":"/forums/DotNET_Other"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DotNETNonTechnical"},{"name":"Xamarin\\u6280\\u672f","url":"/forums/Xamarin"}]},{"name":"Java \\u6280\\u672f","url":"/forums/Java","children":[{"name":"Java SE","url":"/forums/J2SE"},{"name":"J2ME","url":"/forums/J2ME"},{"name":"Java Web \\u5f00\\u53d1","url":"/forums/Java_WebDevelop"},{"name":"Java EE","url":"/forums/J2EE"},{"name":"Eclipse","url":"/forums/Eclipse"},{"name":"Java\\u5176\\u4ed6\\u76f8\\u5173","url":"/forums/JavaOther"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/JavaNonTechnical"},{"name":"JBoss","url":"/forums/JBoss"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"}]},{"name":"Web \\u5f00\\u53d1","url":"/forums/WebDevelop","children":[{"name":"ASP","url":"/forums/ASP"},{"name":"ASP.NET","url":"/forums/ASPDotNET"},{"name":"JSP","url":"/forums/Java_WebDevelop"},{"name":"PHP","url":"/forums/PHP","children":[{"name":"\\u5f00\\u6e90\\u8d44\\u6e90","url":"/forums/PHPOpenSource"},{"name":"\\u57fa\\u7840\\u7f16\\u7a0b","url":"/forums/PHPBase"},{"name":"Framework","url":"/forums/PHPFramework"}]},{"name":"JavaScript","url":"/forums/JavaScript"},{"name":"\\u641c\\u7d22\\u5f15\\u64ce\\u6280\\u672f","url":"/forums/SearchEngine"},{"name":"Ajax \\u6280\\u672f","url":"/forums/Ajax"},{"name":"VBScript","url":"/forums/vbScript"},{"name":"CGI","url":"/forums/CGI"},{"name":"XML/XSL","url":"/forums/XMLSOAP"},{"name":"IIS","url":"/forums/IIS"},{"name":"Apache","url":"/forums/Apache"},{"name":"HTML(CSS)","url":"/forums/HTMLCSS"},{"name":"ColdFusion","url":"/forums/ColdFusion"},{"name":"Ruby/Rails","url":"/forums/ROR"},{"name":"\\u8de8\\u6d4f\\u89c8\\u5668\\u5f00\\u53d1","url":"/forums/CrossBrowser"},{"name":"\\u5176\\u4ed6","url":"/forums/WebDevelop_Other"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/WebNonTechnical"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"HTML5","url":"/forums/HTML5"}]},{"name":"\\u5f00\\u53d1\\u8bed\\u8a00/\\u6846\\u67b6","children":[{"name":"VC/MFC","url":"/forums/VC","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/VC_Basic"},{"name":"\\u754c\\u9762","url":"/forums/VC_UI"},{"name":"\\u7f51\\u7edc\\u7f16\\u7a0b","url":"/forums/VC_Network"},{"name":"\\u8fdb\\u7a0b/\\u7ebf\\u7a0b/DLL","url":"/forums/VC_Process"},{"name":"ATL/ActiveX/COM","url":"/forums/VC_ActiveX"},{"name":"\\u6570\\u636e\\u5e93","url":"/forums/VC_Database"},{"name":"\\u786c\\u4ef6/\\u7cfb\\u7edf","url":"/forums/VC_Hardware"},{"name":"HTML/XML","url":"/forums/VC_HTML"},{"name":"\\u56fe\\u5f62\\u5904\\u7406/\\u7b97\\u6cd5","url":"/forums/VC_ImageProcessing"},{"name":"\\u8d44\\u6e90","url":"/forums/VCResources"},{"name":"\\u975e\\u6280\\u672f\\u7c7b","url":"/forums/VC_NonTechnical"}]},{"name":"VB","url":"/forums/VB","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/VB_Basic"},{"name":"\\u975e\\u6280\\u672f\\u7c7b","url":"/forums/VB_NonTechnical"},{"name":"\\u63a7\\u4ef6","url":"/forums/VB_Controls"},{"name":"API","url":"/forums/VB_API"},{"name":"\\u6570\\u636e\\u5e93(\\u5305\\u542b\\u6253\\u5370\\uff0c\\u5b89\\u88c5\\uff0c\\u62a5\\u8868)","url":"/forums/VB_Database"},{"name":"\\u591a\\u5a92\\u4f53","url":"/forums/VB_Multimedia"},{"name":"\\u7f51\\u7edc\\u7f16\\u7a0b","url":"/forums/VB_Network"},{"name":"VBA","url":"/forums/VBA"},{"name":"COM/DCOM/COM+","url":"/forums/VB_COM"},{"name":"\\u8d44\\u6e90","url":"/forums/VBResources"}]},{"name":"Delphi","url":"/forums/Delphi","children":[{"name":"VCL\\u7ec4\\u4ef6\\u5f00\\u53d1\\u53ca\\u5e94\\u7528","url":"/forums/DelphiVCL"},{"name":"\\u6570\\u636e\\u5e93\\u76f8\\u5173","url":"/forums/DelphiDB"},{"name":"Windows SDK/API","url":"/forums/DelphiAPI"},{"name":"\\u7f51\\u7edc\\u901a\\u4fe1/\\u5206\\u5e03\\u5f0f\\u5f00\\u53d1","url":"/forums/DelphiNetwork"},{"name":"\\u8bed\\u8a00\\u57fa\\u7840/\\u7b97\\u6cd5/\\u7cfb\\u7edf\\u8bbe\\u8ba1","url":"/forums/DelphiBase"},{"name":"GAME\\uff0c\\u56fe\\u5f62\\u5904\\u7406/\\u591a\\u5a92\\u4f53","url":"/forums/DelphiMultimedia"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DelphiNonTechnical"}]},{"name":"C++ Builder","url":"/forums/BCB","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/BCBBase"},{"name":"\\u6570\\u636e\\u5e93\\u53ca\\u76f8\\u5173\\u6280\\u672f","url":"/forums/BCBDB"},{"name":"VCL\\u7ec4\\u4ef6\\u4f7f\\u7528\\u548c\\u5f00\\u53d1","url":"/forums/BCBVCL"},{"name":"Windows SDK/API","url":"/forums/BCBAPI"},{"name":"\\u7f51\\u7edc\\u53ca\\u901a\\u8baf\\u5f00\\u53d1","url":"/forums/BCBNetwork"},{"name":"ActiveX/COM/DCOM","url":"/forums/BCBCOM"},{"name":"\\u8336\\u9986","url":"/forums/BCBTeaHouses"}]},{"name":"C/C++","url":"/forums/Cpp","children":[{"name":"\\u65b0\\u624b\\u4e50\\u56ed","url":"/forums/Cpp_Freshman"},{"name":"C\\u8bed\\u8a00","url":"/forums/C"},{"name":"C++ \\u8bed\\u8a00","url":"/forums/CPPLanguage"},{"name":"\\u5de5\\u5177\\u5e73\\u53f0\\u548c\\u7a0b\\u5e8f\\u5e93","url":"/forums/Cpp_ToolsPlatform"},{"name":"\\u6a21\\u5f0f\\u53ca\\u5b9e\\u73b0","url":"/forums/Cpp_Model"},{"name":"\\u5176\\u4ed6\\u6280\\u672f\\u95ee\\u9898","url":"/forums/Cpp_Other"},{"name":"Qt","url":"/forums/Qt"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Cpp_NonTechnical"}]},{"name":"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00","url":"/forums/OtherLanguage","open":true,"children":[{"name":"OpenCL\\u548c\\u5f02\\u6784\\u7f16\\u7a0b","url":"/forums/Heterogeneous"},{"name":"Go\\u8bed\\u8a00","url":"/forums/golang"},{"name":"JBoss\\u6280\\u672f\\u4ea4\\u6d41","url":"/forums/JBoss"},{"name":"\\u6c47\\u7f16\\u8bed\\u8a00","url":"/forums/ASM"},{"name":"\\u811a\\u672c\\u8bed\\u8a00\\uff08Perl/Python\\uff09","url":"/forums/OL_Script"},{"name":"Office\\u5f00\\u53d1/ VBA","url":"/forums/OfficeDevelopment"},{"name":"VFP","url":"/forums/VFP"},{"name":"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00","url":"/forums/OtherLanguage_Other"}]}]},{"name":"\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":null,"children":[{"name":"\\u5927\\u6570\\u636e","children":[{"name":"Hadoop","url":"/forums/hadoop"}]},{"name":"MS-SQL Server","url":"/forums/MSSQL","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/MSSQL_Basic"},{"name":"\\u5e94\\u7528\\u5b9e\\u4f8b","url":"/forums/MSSQL_Cases"},{"name":"\\u7591\\u96be\\u95ee\\u9898","url":"/forums/MSSQL_DifficultProblems"},{"name":"\\u65b0\\u6280\\u672f\\u524d\\u6cbf","url":"/forums/MSSQL_NewTech"},{"name":"SQL Server BI","url":"/forums/SQLSERVERBI"},{"name":"\\u975e\\u6280\\u672f\\u7248","url":"/forums/MSSQL_NonTechnical"}]},{"name":"PowerBuilder","url":"/forums/PowerBuilder","children":[{"name":"\\u57fa\\u7840\\u7c7b","url":"/forums/PB_Basic"},{"name":"Pb\\u811a\\u672c\\u8bed\\u8a00","url":"/forums/PBScript"},{"name":"DataWindow","url":"/forums/PB_DataWindow"},{"name":"API \\u8c03\\u7528","url":"/forums/PB_API"},{"name":"\\u63a7\\u4ef6\\u4e0e\\u754c\\u9762","url":"/forums/PB_Controls"},{"name":"Pb Web \\u5e94\\u7528","url":"/forums/PB_WEB"},{"name":"\\u6570\\u636e\\u5e93\\u76f8\\u5173","url":"/forums/PB_Database"},{"name":"\\u9879\\u76ee\\u7ba1\\u7406","url":"/forums/PB_ProjectManagement"},{"name":"\\u975e\\u6280\\u672f\\u7248","url":"/forums/PB_NonTechnical"}]},{"name":"Oracle","url":"/forums/Oracle","children":[{"name":"\\u57fa\\u7840\\u548c\\u7ba1\\u7406","url":"/forums/Oracle_Management"},{"name":"\\u5f00\\u53d1","url":"/forums/Oracle_Develop"},{"name":"\\u9ad8\\u7ea7\\u6280\\u672f","url":"/forums/Oracle_Technology"},{"name":"\\u8ba4\\u8bc1\\u4e0e\\u8003\\u8bd5","url":"/forums/Oracle_Certificate"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Oracle_NonTechnical"}]},{"name":"Informatica","url":"/forums/Informatica"},{"name":"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/OtherDatabase","open":true,"children":[{"name":"IBM DB2","url":"/forums/DB2"},{"name":"MongoDB","url":"/forums/MongoDB"},{"name":"\\u6570\\u636e\\u4ed3\\u5e93","url":"/forums/DataWarehouse"},{"name":"VFP","url":"/forums/VFP"},{"name":"Access","url":"/forums/Access"},{"name":"Sybase","url":"/forums/Sybase"},{"name":"Informix","url":"/forums/Informix"},{"name":"MySQL","url":"/forums/MySQL"},{"name":"PostgreSQL","url":"/forums/PostgreSQL"},{"name":"\\u6570\\u636e\\u5e93\\u62a5\\u8868","url":"/forums/DatabaseReport"},{"name":"\\u5176\\u4ed6\\u6570\\u636e\\u5e93","url":"/forums/OtherDatabase_Other"},{"name":"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/HPDatabase"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/DatabaseNonTechnical"}]}]},{"name":"Linux/Unix\\u793e\\u533a","url":"/forums/Linux","children":[{"name":"\\u7cfb\\u7edf\\u7ef4\\u62a4\\u4e0e\\u4f7f\\u7528\\u533a","url":"/forums/Linux_System"},{"name":"\\u5e94\\u7528\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a","url":"/forums/Linux_Development"},{"name":"\\u5185\\u6838\\u6e90\\u4ee3\\u7801\\u7814\\u7a76\\u533a","url":"/forums/Linux_Kernel"},{"name":"\\u9a71\\u52a8\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a","url":"/forums/Linux_Driver"},{"name":"CPU\\u548c\\u786c\\u4ef6\\u533a","url":"/forums/Linux_Hardware"},{"name":"\\u4e13\\u9898\\u6280\\u672f\\u8ba8\\u8bba\\u533a","url":"/forums/Linux_SpecialTopic"},{"name":"\\u5b9e\\u7528\\u8d44\\u6599\\u53d1\\u5e03\\u533a","url":"/forums/Linux_Information"},{"name":"UNIX\\u6587\\u5316","url":"/forums/Unix_Culture"},{"name":"Solaris","url":"/forums/Solaris"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/LinuxNonTechnical"}]},{"name":"Windows\\u4e13\\u533a","url":"/forums/Windows","children":[{"name":"Windows\\u5ba2\\u6237\\u7aef\\u4f7f\\u7528","url":"/forums/Windows7"},{"name":"Windows Server","url":"/forums/WinNT2000XP2003"},{"name":"\\u7f51\\u7edc\\u7ba1\\u7406\\u4e0e\\u914d\\u7f6e","url":"/forums/NetworkConfiguration"},{"name":"\\u5b89\\u5168\\u6280\\u672f/\\u75c5\\u6bd2","url":"/forums/WindowsSecurity"},{"name":"\\u4e00\\u822c\\u8f6f\\u4ef6\\u4f7f\\u7528","url":"/forums/WindowsBase"},{"name":"Microsoft Office\\u5e94\\u7528","url":"/forums/OfficeBase"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/WindowsNonTechnical"}]},{"name":"\\u786c\\u4ef6/\\u5d4c\\u5165\\u5f00\\u53d1","url":"/forums/Embedded","children":[{"name":"\\u5d4c\\u5165\\u5f00\\u53d1(WinCE)","url":"/forums/WinCE"},{"name":"\\u6c47\\u7f16\\u8bed\\u8a00","url":"/forums/ASM"},{"name":"\\u786c\\u4ef6\\u8bbe\\u8ba1","url":"/forums/Embedded_hardware"},{"name":"\\u9a71\\u52a8\\u5f00\\u53d1/\\u6838\\u5fc3\\u5f00\\u53d1","url":"/forums/Embedded_driver"},{"name":"\\u5355\\u7247\\u673a/\\u5de5\\u63a7","url":"/forums/Embedded_SCM"},{"name":"\\u65e0\\u7ebf","url":"/forums/Embedded_wireless"},{"name":"\\u5176\\u4ed6\\u786c\\u4ef6\\u5f00\\u53d1","url":"/forums/Embedded_Other"},{"name":"VxWorks\\u5f00\\u53d1","url":"/forums/VxWorks"},{"name":"Qt\\u5f00\\u53d1","url":"/forums/Qt"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/EmbeddedNonTechnical"},{"name":"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97","url":"/forums/HPC"},{"name":"\\u667a\\u80fd\\u786c\\u4ef6","url":"/forums/SmartHardware"}]},{"name":"\\u6e38\\u620f\\u5f00\\u53d1","url":"/forums/GameDevelop","children":[{"name":"Cocos2d-x","url":"/forums/GD_Cocos2d-x"},{"name":"Unity3D","url":"/forums/GD_Unity3D"},{"name":"\\u5176\\u4ed6\\u6e38\\u620f\\u5f15\\u64ce","url":"/forums/Othergameengines"},{"name":"\\u6e38\\u620f\\u7b56\\u5212\\u4e0e\\u8fd0\\u8425","url":"/forums/Gdesignoperation"}]},{"name":"\\u7f51\\u7edc\\u4e0e\\u901a\\u4fe1","url":"/forums/network_communication","children":[{"name":"\\u7f51\\u7edc\\u534f\\u8bae\\u4e0e\\u914d\\u7f6e","url":"/forums/IP_Protocolconfiguration"},{"name":"\\u7f51\\u7edc\\u7ef4\\u62a4\\u4e0e\\u7ba1\\u7406","url":"/forums/maintainmanage"},{"name":"\\u4ea4\\u6362\\u53ca\\u8def\\u7531\\u6280\\u672f","url":"/forums/Hardware_SwitchRouter"},{"name":"CDN","url":"/forums/NetworkC_CDN"},{"name":"\\u901a\\u4fe1\\u6280\\u672f","url":"/forums/ST_Network"},{"name":"VOIP\\u6280\\u672f\\u63a2\\u8ba8","url":"/forums/voip"}]},{"name":"\\u6269\\u5145\\u8bdd\\u9898","url":"/forums/Other","children":[{"name":"\\u704c\\u6c34\\u4e50\\u56ed","url":"/forums/FreeZone"},{"name":"\\u7a0b\\u5e8f\\u4eba\\u751f","url":"/forums/ProgrammerStory"},{"name":"\\u7a0b\\u5e8f\\u5a9b\\u4e16\\u754c","url":"/forums/ProgramGirls"},{"name":"\\u7a0b\\u5e8f\\u5458\\u4ea4\\u53cb","url":"/forums/ProgramFriends"},{"name":"\\u4e09\\u5341\\u800c\\u7acb","url":"/forums/30Plus"},{"name":"\\u6e38\\u620f\\u4e13\\u533a","url":"/forums/Game"},{"name":"\\u4e1a\\u754c\\u65b0\\u95fb","url":"/forums/ITnews"},{"name":"\\u7a0b\\u5e8f\\u5458\\u82f1\\u8bed","url":"/forums/English"},{"name":"\\u6c42\\u804c\\u4e0e\\u62db\\u8058","url":"/forums/CAREER"},{"name":"\\u8ba1\\u7b97\\u673a\\u56fe\\u4e66","url":"/forums/Book"},{"name":"\\u5927\\u5b66\\u65f6\\u4ee3","url":"/forums/CollegeTime"},{"name":"\\u8df3\\u86a4\\u5e02\\u573a","url":"/forums/Trade"},{"name":"\\u8f6f\\u4ef6\\u6c42\\u52a9","url":"/forums/Shareware"}]},{"name":"\\u6328\\u8e22\\u804c\\u6daf","url":"/forums/CAREER","children":[{"name":"\\u6c42\\u804c\\u9762\\u8bd5","url":"/forums/WorkplaceCommunication"},{"name":"\\u4f01\\u4e1a\\u70b9\\u8bc4","url":"/forums/TECHHUNT"},{"name":"\\u804c\\u573a\\u8bdd\\u9898","url":"/forums/OFFICELIFE"},{"name":"JOB \\u9a7f\\u7ad9","url":"/forums/jobservice"}]},{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u793e\\u533a","url":"/forums/eSDK","children":[{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b","url":"/forums/DevChallenge2016"},{"name":"\\u4e91\\u8ba1\\u7b97","url":"/forums/hwfsdeveloper"},{"name":"\\u4f01\\u4e1a\\u901a\\u4fe1","url":"/forums/hwucdeveloper"},{"name":"BYOD","url":"/forums/hwbyoddeveloper"},{"name":"\\u5927\\u6570\\u636e","children":[{"name":"FusionInsight HD","url":"/forums/fusioninsightdeveloper"},{"name":"FusionInsight Universe","url":"/forums/hwuniversedeveloper"}]},{"name":"Digital inCloud","url":"/forums/hwswdeveloper"},{"name":"CaaS","url":"/forums/hwcndeveloper"},{"name":"SDN","url":"/forums/hwsdndeveloper"},{"name":"\\u4f01\\u4e1a\\u7f51\\u7edc\\u5f00\\u53d1","url":"/forums/hwendeveloper"},{"name":"\\u654f\\u6377\\u7f51\\u7edc","url":"/forums/hwesightdeveloper"},{"name":"eLTE","url":"/forums/hwbbtdeveloper"},{"name":"\\u7269\\u8054\\u7f51\\u5f00\\u53d1","url":"/forums/hwiotdeveloper"},{"name":"\\u79fb\\u52a8\\u5f00\\u653e\\u5de5\\u573a","url":"/forums/hwwldeveloper"},{"name":"OpenLife\\u667a\\u6167\\u5bb6\\u5ead","url":"/forums/OpenLife"},{"name":"HUAWEI Code Craft","url":"/forums/hwcodecraft"}]},{"name":"IBM \\u6280\\u672f\\u793e\\u533a","children":[{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"DB2","url":"/forums/DB2"},{"name":"Rational","url":"/forums/Rational"},{"name":"Lotus","url":"/forums/Lotus"},{"name":"IBM\\u4e91\\u8ba1\\u7b97","url":"/forums/ibmcloud"},{"name":"IBM \\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"Tivoli","url":"/forums/Tivoli"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"}]},{"name":"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a","children":[{"name":"\\u82f1\\u7279\\u5c14\\u6280\\u672f","url":"/forums/intel"}]},{"name":"Qualcomm\\u5f00\\u53d1\\u8bba\\u575b","children":[{"name":"Qualcomm\\u5f00\\u53d1","url":"/forums/qualcomm"}]},{"name":"\\u4f01\\u4e1a\\u6280\\u672f","children":[{"name":"IBM \\u6280\\u672f\\u793e\\u533a","children":[{"name":"WebSphere","url":"/forums/WebSphere"},{"name":"DB2","url":"/forums/DB2"},{"name":"Rational","url":"/forums/Rational"},{"name":"Lotus","url":"/forums/Lotus"},{"name":"IBM\\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"Tivoli","url":"/forums/Tivoli"},{"name":"IBM AIX","url":"/forums/AIX"},{"name":"Power Linux","url":"/forums/PowerLinux"}]},{"name":"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a","children":[{"name":"\\u82f1\\u7279\\u5c14\\u6280\\u672f","url":"/forums/intel"}]},{"name":"T\\u5ba2\\u8bba\\u575b","url":"/forums/tcl"},{"name":"Paypal\\u5f00\\u53d1\\u8005\\u793e\\u533a","url":"/forums/PaypalCommunity"},{"name":"CUDA","url":"/forums/CUDA","children":[{"name":"CUDA\\u7f16\\u7a0b","url":"/forums/CUDA_Dev"},{"name":"CUDA\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\\u8ba8\\u8bba","url":"/forums/CUDA_Compute"},{"name":"CUDA on Linux","url":"/forums/CUDA_Linux"},{"name":"CUDA on Windows XP","url":"/forums/CUDA_WinXP"}]},{"name":"Google\\u6280\\u672f\\u793e\\u533a","children":[{"name":"Google\\u6280\\u672f\\u793e\\u533a","url":"/forums/GoogleCommunity"},{"name":"Android","url":"/forums/Android"}]},{"name":"Microsoft Office \\u5e94\\u7528\\u4e8e\\u5f00\\u53d1","children":[{"name":"Office\\u5f00\\u53d1","url":"/forums/OfficeDevelopment"},{"name":"Office\\u4f7f\\u7528","url":"/forums/OfficeBase"}]}]},{"name":"\\u5176\\u4ed6\\u6280\\u672f\\u8bba\\u575b","children":[{"name":"\\u8f6f\\u4ef6\\u5de5\\u7a0b/\\u7ba1\\u7406","url":"/forums/SE","children":[{"name":"\\u8f6f\\u4ef6\\u6d4b\\u8bd5","url":"/forums/SE_Quality"},{"name":"\\u7814\\u53d1\\u7ba1\\u7406","url":"/forums/SE_Management"},{"name":"\\u654f\\u6377\\u5f00\\u53d1","url":"/forums/Agile"},{"name":"\\u7248\\u672c\\u63a7\\u5236","url":"/forums/CVS_SVN"},{"name":"\\u8bbe\\u8ba1\\u6a21\\u5f0f","url":"/forums/DesignPatterns"}]},{"name":"\\u9ad8\\u6027\\u80fd\\u5f00\\u53d1","url":"/forums/HPDevelopment","children":[{"name":"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97","url":"/forums/HPC"},{"name":"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1","url":"/forums/HPWebDevelop"},{"name":"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1","url":"/forums/HPDatabase"},{"name":"\\u6d77\\u91cf\\u6570\\u636e\\u5904\\u7406/\\u641c\\u7d22\\u6280\\u672f","url":"/forums/SearchEngine"},{"name":"\\u6570\\u636e\\u7ed3\\u6784\\u4e0e\\u7b97\\u6cd5","url":"/forums/ST_Arithmetic"}]},{"name":"\\u4e13\\u9898\\u5f00\\u53d1/\\u6280\\u672f/\\u9879\\u76ee","url":"/forums/SpecialTopic","children":[{"name":"OpenAPI","url":"/forums/OpenAPI"},{"name":"OpenStack","url":"/forums/OpenStack"},{"name":"\\u673a\\u5668\\u89c6\\u89c9","url":"/forums/ST_Image"},{"name":"OpenCV","url":"/forums/OpenCV"},{"name":"\\u4fe1\\u606f/\\u7f51\\u7edc\\u5b89\\u5168","url":"/forums/ST_Security"},{"name":"\\u4eba\\u5de5\\u667a\\u80fd\\u6280\\u672f","url":"/forums/AI"},{"name":"\\u8d28\\u91cf\\u7ba1\\u7406/\\u8f6f\\u4ef6\\u6d4b\\u8bd5","url":"/forums/SE_Quality"}]},{"name":"\\u591a\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/MediaAndFlash","children":[{"name":"\\u591a\\u5a92\\u4f53/\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/Multimedia"},{"name":"\\u56fe\\u8c61\\u5de5\\u5177\\u4f7f\\u7528","url":"/forums/ImageTools"},{"name":"Flash\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1","url":"/forums/FlashDevelop"},{"name":"\\u4ea4\\u4e92\\u5f0f\\u8bbe\\u8ba1","url":"/forums/InteractiveDesign"},{"name":"WPF/Silverlight","url":"/forums/Silverlight"},{"name":"Flex","url":"/forums/Flex"}]},{"name":"\\u786c\\u4ef6\\u4f7f\\u7528","url":"/forums/HardwareUse","children":[{"name":"\\u6570\\u7801\\u8bbe\\u5907","url":"/forums/Hardware_Digital"},{"name":"\\u7535\\u8111\\u6574\\u673a\\u53ca\\u914d\\u4ef6","url":"/forums/Hardware_Computer"},{"name":"\\u5916\\u8bbe\\u53ca\\u529e\\u516c\\u8bbe\\u5907","url":"/forums/Hardware_Peripheral"},{"name":"\\u88c5\\u673a\\u4e0e\\u5347\\u7ea7\\u53ca\\u5176\\u4ed6","url":"/forums/Hardware_DIY"},{"name":"\\u975e\\u6280\\u672f\\u533a","url":"/forums/Hardware_NonTechnical"}]},{"name":"\\u4ea7\\u54c1/\\u5382\\u5bb6","url":"/forums/ADS","children":[{"name":"IBM \\u5f00\\u53d1\\u8005","url":"/forums/IBMDeveloper"},{"name":"\\u5fae\\u521b\\u8f6f\\u4ef6\\u5f00\\u53d1\\u7ba1\\u7406","url":"/forums/WeiChuang"},{"name":"\\u5176\\u4ed6","url":"/forums/ADSOther"}]}]},{"name":"\\u57f9\\u8bad\\u8ba4\\u8bc1","url":"/forums/Trainning","children":[{"name":"IT\\u57f9\\u8bad","url":"/forums/ITCertificate"}]},{"name":"\\u7ad9\\u52a1\\u4e13\\u533a","url":"/forums/Support","children":[{"name":"\\u793e\\u533a\\u516c\\u544a","url":"/forums/placard"},{"name":"\\u6d3b\\u52a8\\u4e13\\u533a","url":"/forums/Activity"},{"name":"\\u5ba2\\u670d\\u4e13\\u533a","url":"/forums/Service"},{"name":"\\u7248\\u4e3b\\u4e13\\u533a","url":"/forums/Moderator"},{"name":"\\u300a\\u7a0b\\u5e8f\\u5458\\u300b\\u6742\\u5fd7","url":"/forums/Programmer"}]}],\
    "isLogined": "false",\
    "isModerator": "false",\
    "favoriteForumUrls": [],\
    "lastForumNodes": [{"name":"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b ","url":"/forums/DevChallenge2016"},{"name":"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\\u8bba\\u575b","url":"/forums/DE"},{"name":"OpenCV","url":"/forums/OpenCV"},{"name":"FusionInsight HD","url":"/forums/fusioninsightdeveloper"},{"name":"HUAWEI Code Craft","url":"/forums/hwcodecraft"},{"name":"JetBrains\\u6280\\u672f\\u8bba\\u575b","url":"/forums/JetBrains"},{"name":"Enterprise Architect","url":"/forums/EA"}]\
  }'''\
menu = json.loads(menu)\
\
\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'周前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'个月前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if u'年前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
\
class Handler(BaseHandler):\
    crawl_config = {\
'headers':{\
'proxy':'222.219.130.190:8998',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'}\
\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for each in menu['forumNodes']:\
            if each.has_key('children'):\
                for ea in each['children']:\
                    if ea.has_key('url'):\
                            url = ea['url']\
                            self.crawl('http://bbs.csdn.net'+url+'/closed',headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.title > a').items():\
            _dict = {}\
            _dict['title'] = each.text()\
            self.crawl(each.attr.href, save = _dict,headers=self.crawl_config['headers'], callback=self.detail_page)\
        #翻页\
        for each in response.doc('.next').items():\
            self.crawl(each.attr.href, headers=self.crawl_config['headers'],callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        #print len(response.doc('table.post.topic'))\
        content_list = []\
        for info in response.doc('.post').items():\
            if 'topic' in info.attr['class']:\
                continue\
            if u'被管理员删除' in info.find('.post_body').remove('fieldset').html().strip():\
                continue\
            _dic = {}\
            #print info.find('.answer_author').text()\
            _dic['name'] = info.find('.username > a').text()\
            _dic['date'] = info.find('.time').text().split()[-2]\
            _dic['content'] = info.find('.post_body').remove('fieldset').html().strip()\
            content_list.append((_dic))\
        \
        if not content_list:\
            return None\
        return {\
            "url": response.url,\
            "title": response.save['title'],\
            #"subject": response.save['subject'],\
            "subject": u'程序员',\
            "answers": content_list,\
            "source": u'csdn',\
            "question_detail": re.sub('<!--.*','',response.doc('.topic > .post_body').remove('*').html()).strip(),\
            "class": 47,\
            "data_weight": 0,\
        }\
$$$$$1471921965.2890
wenda_eask$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-27 11:18:41\
# Project: wenda_eask\
\
from pyspider.libs.base_handler import *\
import time,datetime\
from pyquery import PyQuery as pq\
import urllib2\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'天之前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u'天之前', '')))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
    \
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
        "headers": {\
        'Referer':'http://wenda.eask.org/',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36',\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://wenda.eask.org/tiwen.html', headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('td strong > font').items():\
            self.crawl(each.parent().parent().attr.href, headers=self.crawl_config['headers'], save={'category_id':[each.text().replace(u'、','')]}, callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for i in range(1,4000):\
            if i == 1:\
                self.crawl(response.url+'index.html', headers=self.crawl_config['headers'], save=response.save, callback=self.list1_page)\
            else:\
                self.crawl(response.url+'index_'+str(i)+'.html', headers=self.crawl_config['headers'], save=response.save, callback=self.list1_page)\
                \
    @config(age=10 * 24 * 60 * 60)\
    def list1_page(self, response):\
        for each in response.doc('.news_list li').items(): \
            _dict = {}\
            _dict['category_id'] = response.save.get('category_id')\
            _dict['title'] = each.find('a').text()\
            _dict['create_time'] = each.find('span').text()\
            _dict['create_user'] = ''\
            self.crawl(each.find('a').attr.href, headers=self.crawl_config['headers'], save=_dict, callback=self.detail_page)\
        \
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = u''\
        res_dict['subject'] = u'主站问答'\
        res_dict['class'] = 34\
        res_dict['question_detail'] = response.doc('#text').html().split('<br />')[0].split(':')[1] if response.doc('#text') else ''\
        #print res_dict['question_detail']\
        answers_list = []\
        ask_more = response.doc('#text script').eq(-1).attr.src\
        content = ''\
        if pq(ask_more).find('table tr').eq(1):\
            content = pq(ask_more).find('table tr').eq(1).find('td').html()\
        if content == '' and response.doc('#text > p'):\
            content = ''.join(v.html() for v in response.doc('#text > p').items())\
        if content == '' or u'上一个问题' in content:\
            return\
        answers_list.append({\
            "content":  content,\
            "create_time": res_dict['create_time'],\
            "user_name": '',\
        })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        return res_dict\
$$$$$1470811084.0423
wenda_gongwuyuan$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-26 15:52:03\
# Project: wenda_gongwuyuan\
\
\
from pyspider.libs.base_handler import *\
import time,datetime\
\
add_words = {\
    #'bj',\
    #'sh',\
    'sd':u'山东公务员',#2\
    #'js',\
    'zj':u'浙江公务员',#2\
    #'ah',\
    #'jl',\
    #'fj',\
    'gd':u'广东公务员',\
    'gx':u'广西公务员',#3\
    'hn':u'河南公务员',\
    #'tj',\
    'hb':u'河北公务员',\
    #'hlj',\
    'sx':u'山西公务员',#3\
    'gs':u'甘肃公务员',\
    #'hu',\
    #'he',\
    'sc':u'四川公务员',\
    'cq':u'重庆公务员',\
    'yn':u'云南公务员',\
    #'gz',\
    #'xz',\
    'nx':u'宁夏公务员',\
    'xj':u'新疆公务员',\
    'qh':u'青海公务员',\
    'ln':u'辽宁公务员',\
    'jx':u'江西公务员',\
    #'nm',\
}\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'天之前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u'天之前', '')))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
    \
class Handler(BaseHandler):\
    crawl_config = {\
        "itag":'0.2',\
        "headers": {\
        'Upgrade-Insecure-Requests':'1',\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.3',\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in add_words.iteritems():\
            self.crawl('http://www.'+k+'gwy.org/ask/', save = {'category_id':v}, headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        if response.doc('.list13 li'):\
            for each in response.doc('.list13 li').items():\
                if 'ivl' in each.outerHtml():\
                    continue\
                _dict = {}\
                _dict['category_id'] = response.save.get('category_id')\
                _dict['title'] = each.find('.con > a').text()\
                _dict['create_user'] = each.find('.role').text()\
                _dict['tag'] = 1\
                self.crawl(each.find('.con > a').attr.href, headers=self.crawl_config['headers'], save=_dict, callback=self.detail_page)\
            #翻页\
            for each in response.doc('.pages > a').items():\
                self.crawl(each.attr.href, headers=self.crawl_config['headers'],  save=response.save, callback=self.index_page)\
        elif response.doc('.list02 li'):\
            for each in response.doc('.list02 li').items():\
                if 'ivl' in each.outerHtml():\
                    continue\
                _dict = {}\
                _dict['category_id'] = response.save.get('category_id')\
                _dict['title'] = each.find('.link02').text()\
                _dict['create_user'] = each.find('.cat').text().replace(u'【','').replace(u'】','')\
                _dict['tag'] = 2\
                self.crawl(each.find('.link02').attr.href, headers=self.crawl_config['headers'], save=_dict, callback=self.detail_page)\
        \
            #翻页\
            for each in response.doc('.next > a').items():\
                self.crawl(each.attr.href, headers=self.crawl_config['headers'],  save=response.save, callback=self.index_page)\
        else:\
            for each in response.doc('tr tr').items():\
                _dict = {}\
                _dict['category_id'] = response.save.get('category_id')\
                _dict['title'] = each.find('.hei13').text()\
                _dict['create_user'] = ''\
                _dict['create_time'] = each.find('.style36 > div').text()\
                _dict['tag'] = 3\
                self.crawl(each.find('.hei13').attr.href, headers=self.crawl_config['headers'], save=_dict, callback=self.detail_page)\
        \
            #翻页\
            for each in response.doc('.page > a').items():\
                self.crawl(each.attr.href, headers=self.crawl_config['headers'],  save=response.save, callback=self.index_page) \
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        if res_dict['tag'] == 1:\
            res_dict['create_time'] = response.doc('.con > .ss > .time').text().split()[0]\
            res_dict['url'] = response.url\
            res_dict['source'] = u'公务员'\
            res_dict['subject'] = u'主站问答'\
            res_dict['class'] = 34\
            res_dict['question_detail'] = response.doc('.details').eq(0).text() if response.doc('.details').eq(0) else ''\
\
            answers_list = []\
\
            for each in response.doc('.r_box').items():\
                if  each.find('.btm .time'):\
                    create_time = each.find('.btm .time').text().split()[0]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                if not each.find('.details'):\
                    continue\
                answers_list.append({\
                    "content":  each.find('.details').html().strip(),\
                    "create_time": create_time,\
                    "user_name": each.find('.btm > .ss').remove('.time').text(),\
                })\
        elif res_dict['tag'] == 2:\
            res_dict['create_time'] = response.doc('.c_l_c_07_1 .q_foot').text().split()[0].split(u'：')[1]\
            res_dict['url'] = response.url\
            res_dict['source'] = u'公务员'\
            res_dict['subject'] = u'主站问答'\
            res_dict['class'] = 34\
            res_dict['question_detail'] = response.doc('.q_con > div').text() if response.doc('.q_con > div') else ''\
\
            answers_list = []\
\
            for each in response.doc('.c_l_c_07_2').items():\
                if  each.find('.solve_date'):\
                    create_time = each.find('.solve_date').text().split()[0].split(u'：')[1]\
                else:\
                    create_time = time.strftime('%Y-%m-%d',time.localtime())\
                if not each.find('.q_con > div'):\
                    continue\
                answers_list.append({\
                    "content":  each.find('.q_con > div').html().strip(),\
                    "create_time": create_time,\
                    "user_name": each.find('.q_title > div').remove('span').text(),\
                })\
        else:\
            res_dict['create_user'] = response.doc('.style8').text().split(':')[2].split(u'】')[0]\
            res_dict['url'] = response.url\
            res_dict['source'] = u'公务员'\
            res_dict['subject'] = u'主站问答'\
            res_dict['class'] = 34\
            res_dict['question_detail'] = response.doc('.ask').text() if response.doc('.ask') else ''\
\
            answers_list = []\
\
            for each in response.doc('.replay').items():\
                if u'待回复' in each.text():\
                    continue\
                create_time = res_dict['create_time']\
                answers_list.append({\
                    "content":  each.html().strip(),\
                    "create_time": create_time,\
                    "user_name": '',\
                })\
        res_dict.pop('tag')\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        return res_dict\
$$$$$1469582458.5090
wenda_liuyouwang$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-27 17:55:22\
# Project: wenda_liuyouwang\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://wenda.liuu.cn/?/home/explore/sort_type-new__day-0', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('h4').items():\
            self.crawl(each.find('a').eq(0).attr.href, callback=self.detail_page)           \
        # 翻页\
        for each in response.doc('.clearfix a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
          \
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "question": response.doc('h1').text(),\
            "types": response.doc('.aw-topic-editor span').text(),\
            "question_detail": response.doc('.aw-mod-body > .markitup-box').text(),\
            "create_date":response.doc('.aw-question-detail-meta > span').text().split()[0],\
            "answer": response.doc('.aw-dynamic-topic-content .markitup-box').text(),\
            "answer_date": response.doc('.aw-dynamic-topic-meta > span.aw-text-color-999').text().split()[0],\
    }\
$$$$$1467093554.0543
wenda_tgnet$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-30 15:59:47\
# Project: wenda_tgnet\
\
\
from pyspider.libs.base_handler import *\
import time,datetime,re\
\
def get_date(d):\
    if not d:\
        return  time.strftime('%Y-%m-%d',time.localtime())\
    if u'分钟' in d or u'小时' in d:\
            return time.strftime('%Y-%m-%d',time.localtime())\
    if u'天之前' in d:\
             now_time = datetime.datetime.now()\
             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u'天之前', '')))\
             return yes_time.strftime('%Y-%m-%d')\
    if not d.startswith(u'20'):\
        return '20'+d\
    else:\
        return d\
    \
class Handler(BaseHandler):\
    crawl_config = {\
        "headers": {\
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
        }\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://wd.tgnet.com/QuestionList/0/1/0/0/0/1/', headers=self.crawl_config['headers'], callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.tableTopic tbody tr').items():\
            #print each.find('.title > a').eq(0).text()\
            _dict = {}\
            _dict['title'] = each.find('.title > a').eq(0).text().strip()\
            _dict['create_time'] = get_date(each.find('td > .cLGray').text())\
            _dict['create_user'] = each.find('td > a').text()\
            self.crawl(each.find('.title > a').eq(0).attr.href, headers=self.crawl_config['headers'],save=_dict, callback=self.detail_page)\
         \
        #翻页\
        for each in response.doc('.p > a').items():\
            self.crawl(each.attr.href, headers=self.crawl_config['headers'],  save=response.save, callback=self.index_page)\
            \
            \
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = u'天工网'\
        res_dict['subject'] = u'主站问答'\
        res_dict['class'] = 34\
        res_dict['question_detail'] = response.doc('#tblInfo .content > div').eq(0).remove('.IsApp').html().strip()\
        if 'strong' in res_dict['question_detail']:\
            res_dict['question_detail'] = response.doc('#tblInfo .content > div').eq(0).remove('.IsApp').find('strong').eq(0).html().strip()\
        res_dict['question_detail'] = re.sub(r'<a[^>]+>','',res_dict['question_detail']).replace('</a>','')\
        answers_list = []\
        \
        for each in response.doc('table.tableReply').items():\
            if 'tblInfo' in each.outerHtml():\
                continue\
            if  each.find('.topInfo'):\
                create_time = get_date(each.find('.topInfo').remove('*').text().split()[2].strip().replace('/','-'))\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            answers_list.append({\
                "content":  re.sub(r'<a[^>]+>','',each.find('.subjectReply').html().strip()).replace('</a>',''),\
                "create_time": create_time,\
                "user_name": each.find('strong > a').text(),\
            })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        res_dict['category_id'] = ['0',]\
        return res_dict$$$$$1472547659.8283
wenda_tiandao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-27 14:40:57\
# Project: wenda_tiandao\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://ask.tiandaoedu.com/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.ptit > a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
        for each in response.doc('.pages > a').items():\
            self.crawl(each.attr.href, callback=self.index_page)\
\
\
    @config(priority=2)\
    def detail_page(self, response):\
        answer = response.doc('.answer_con > span').text()\
        answer = answer.replace(u'如果还有相关的留学问题需要了解，欢迎拨打天道留学的免费咨询热线400-019-0038进行咨询，或者点击页面的“在线咨询”与顾问老师直接对话。', '')\
        if not answer:\
            return None\
        return {\
            "url": response.url,\
            "title": response.doc('.q_con > .yh').text(),\
            "question_detail": response.doc('.zhw_p').text(),\
            "create_date": response.doc('.q_con > span').text().split(u'点击')[0].split(u'：')[-1].strip(' '),\
            "answer": answer,\
        }\
$$$$$1467093550.6124
wenda_tianxing$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-26 10:44:01\
# Project: wenda_tianxing\
\
from pyspider.libs.base_handler import *\
import time\
\
class Handler(BaseHandler):\
    crawl_config = {\
        "headers": {\
            'Host':'www.tesoon.com',\
            'Referer':'http://www.tesoon.com/ask/get_class.php?fatherid=947&status=H',\
            'Upgrade-Insecure-Requests':'1',\
            'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'\
}\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.tesoon.com/ask/catalog.htm', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.lh18 > a').items():\
            if u'更多' not in each.text():\
                self.crawl(each.attr.href, save = {'category_id':each.text()}, headers=self.crawl_config['headers'], callback=self.list_page)\
        self.crawl('http://www.tesoon.com/ask/get_class.php?fatherid=10&classifyflag=0', save = {'category_id':u'考试答案交流区'}, headers=self.crawl_config['headers'], callback=self.list_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.lh25 tr').items():\
            _dict = {}\
            _dict['category_id'] = response.save.get('category_id')\
            _dict['title'] = each.find('a.f14').text()\
            _dict['create_time'] = each.find('td').eq(4).text()\
            _dict['create_user'] = each.find('.c6nul > font').text()\
            self.crawl(each.find('a.f14').attr.href, headers=self.crawl_config['headers'], save=_dict, callback=self.detail_page)\
        \
        #翻页\
        for each in response.doc('.lh25 tr').eq(-2).find('a').items():\
            self.crawl(each.attr.href, headers=self.crawl_config['headers'],  save=response.save, callback=self.list_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['url'] = response.url\
        res_dict['source'] = u'天星教育'\
        res_dict['subject'] = u'主站问答'\
        res_dict['class'] = 34\
        res_dict['question_detail'] = response.doc('tr > .lh13').text() if response.doc('tr > .lh13') else ''\
        \
        answers_list = []\
        \
        for each in response.doc('div.new2_hd').items():\
            #print each.html()\
            if  each.find('.hdsj'):\
                create_time = each.find('.hdsj').text().split()[0]\
            else:\
                create_time = time.strftime('%Y-%m-%d',time.localtime())\
            if not each.find('tr > .lh15'):\
                continue\
            answers_list.append({\
                "content":  each.find('tr > .lh15').remove('a').html().strip(),\
                "create_time": create_time,\
                "user_name": each.find('tr > .c6 > .a05').text(),\
            })\
        if len(answers_list) == 0:\
            return\
        res_dict['answers'] = answers_list\
        res_dict['data_weight'] = 0\
        return res_dict$$$$$1469582395.8999
wenda_yygrammar$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-28 09:57:39\
# Project: wenda_yygrammar\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'proxy':'222.186.30.45:80'\
    }\
\
    header = {\
\
'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\
'Accept-Encoding':'gzip, deflate, sdch',\
'Accept-Language':'zh-CN,zh;q=0.8,en;q=0.6',\
'Cache-Control':'max-age=0',\
'Connection':'keep-alive',\
'Host':'ask.yygrammar.com',\
'Referer':'http://ask.yygrammar.com/c-all/2/1.html?WebShieldSessionVerify=gvLjMxMl5Tyr8APkYMki',\
'Upgrade-Insecure-Requests':'1',\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://ask.yygrammar.com/c-all/2/1.html', proxy='222.186.30.45:80',headers = self.header,callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('table tr').items():\
            if each.find('td'):\
                _dict = {}\
                _dict['title'] = each.find('.title .wrap > a').text()\
                url = each.find('.title .wrap > a').attr.href\
                _dict['create_time'] = each.find('td').eq(2).text().split()[0].replace('/','-')\
                self.crawl(url, save = _dict,callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
       res_dict = response.save        \
       res_dict['category_id'] = [u'英语问答']\
       res_dict['question_detail'] = response.doc('div.description').html()\
       answers_list = []\
       info = response.doc('div.qa-content').remove('.appendcontent').html()\
       if info:\
                 _dict = {}\
                 _dict['content'] = info\
                 _dict['user_name'] = response.doc('div.text > p.name  > a').text()\
                 _dict['create_time'] = response.doc('div.text > div.user-info  > span').eq(4).text().split()[1].replace('/','-')\
                 answers_list.append(_dict)\
       if not answers_list:\
                 return \
       res_dict['answers'] = answers_list\
       res_dict['url'] = response.url\
       res_dict['source'] = 'yygrammar'\
       res_dict['subject'] = u'主站问答'\
       res_dict['class'] = 34\
       res_dict['data_weight'] = 0\
       res_dict['create_user'] = ''         \
       return res_dict\
$$$$$1469674497.8136
xiaoxue_51edu_news$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-15 14:41:21\
# Project: xiaoxue_shiti\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    bread_dic = {'yinianji': u'一年级',\
                 'ernianji': u'二年级',\
                 'sannianji': u'三年级',\
                 'sinianji': u'四年级',\
                 'wunianji': u'五年级',\
                 'liunianji': u'六年级',\
                 'yuwen': u'语文',\
                 'shuxue': u'数学',\
                 'yingyu': u'英语',\
                 }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.51edu.com/xiaoxue/yinianji/', callback=self.index_page)\
        self.crawl('http://www.51edu.com/xiaoxue/ernianji/', callback=self.index_page)\
        self.crawl('http://www.51edu.com/xiaoxue/sannianji/', callback=self.index_page)\
        self.crawl('http://www.51edu.com/xiaoxue/sinianji/', callback=self.index_page)\
        self.crawl('http://www.51edu.com/xiaoxue/wunianji/', callback=self.index_page)\
        self.crawl('http://www.51edu.com/xiaoxue/liunianji/', callback=self.index_page)\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.wdlbt').items():\
            self.crawl(each.find('a').attr.href, callback=self.index2_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index2_page(self, response):\
        for each in response.doc('.wdlbt').items():\
            self.crawl(each.find('a').attr.href, callback=self.index3_page)\
    \
    @config(age=10 * 24 * 60 * 60)\
    def index3_page(self, response):\
        for each in response.doc('li > p > a').items():\
            self.crawl(each.attr.href,  callback=self.detail_page)\
        for each in response.doc('#pages > a').items():\
            self.crawl(each.attr.href,  callback=self.index3_page)\
            \
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = [v.text() for v in response.doc('#endText > p').items()]\
        if not content_list:\
            return None\
        bread = []\
        for k, v in self.bread_dic.iteritems():\
            if k in response.url:\
                bread.append((v))\
        return {\
            "url": response.url,\
            "title": response.doc('h2').text(),\
            "date": response.doc('.wdnrbt > span').text()[:10],\
            "bread": bread,\
            "content": ''.join(['<p>%s</p>'%v for v in content_list]),\
            "subject": u'小学',\
            "class": 33,\
            "source": "51edu",\
            "data_weight": 0,\
        }\
$$$$$1469524077.1551
xiaoxue_lianjia$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-05 17:33:31\
# Project: xiaoxu_lianjia_shanghai\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as pq\
import urllib\
import json\
import sys\
reload(sys)\
sys.setdefaultencoding("utf-8")\
xz_dict = {\
u'公':u'公办',\
u'民':u'民办',\
u'私':u'私立',\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://sz.lianjia.com/xuequfang/', callback=self.index_page)\
        for i in range(2,16):\
            self.crawl('http://sz.lianjia.com/xuequfang/pg%d/'%(i), callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.search-list .fl li').items():\
            url = each.find('a').attr.href\
            icon_img = each.find('a > img').attr.src\
            self.crawl(url, save={'icon_img': icon_img}, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res = {}\
        for each in response.doc('.list li').items():\
            _each = each.text().split()\
            try:\
                res[_each[0]] = _each[1]\
            except:\
                pass\
         \
        _list = []\
        dic = {}\
        for each in response.doc('.li').items():\
            dic = {\
                'community': each.find('.names > a').text(),\
                'building': each.find('.building .tips').text().replace('\\t','').replace('\\n', ',')\
            }\
            if dic['community'] != '':\
                _list.append(dic)\
        res['community'] = json.dumps(_list)\
        res['correspond_school '] = json.dumps([v.text() for v in response.doc('p > a').items() if v])\
        \
        #jz_list = []\
        \
        #for info in response.doc('.jianzhang a').items():\
            #if u'2016年' in info.text():\
            #jz = pq(url=info.attr.href, encoding="utf-8")\
            #jz_list.append(jz.find('.box').html())\
        #res['student_guid'] = list(set(jz_list))\
        res['name'] = response.doc('h1 > b').html()\
        \
        return res$$$$$1472089189.8447
xiaoxue_ruyile$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-18 19:20:58\
# Project: xiaoxue\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    province_dict = {\
        'http://www.ruyile.com/xxlb.aspx?id=2&t=2': '天津', \
        'http://www.ruyile.com/xxlb.aspx?id=3&t=2': '河北', \
        'http://www.ruyile.com/xxlb.aspx?id=4&t=2': '山西', \
        'http://www.ruyile.com/xxlb.aspx?id=5&t=2': '内蒙古', \
        'http://www.ruyile.com/xxlb.aspx?id=6&t=2': '辽宁', \
        'http://www.ruyile.com/xxlb.aspx?id=7&t=2': '吉林', \
        'http://www.ruyile.com/xxlb.aspx?id=8&t=2': '黑龙江', \
        'http://www.ruyile.com/xxlb.aspx?id=9&t=2': '上海', \
        'http://www.ruyile.com/xxlb.aspx?id=10&t=2': '江苏', \
        'http://www.ruyile.com/xxlb.aspx?id=11&t=2': '浙江', \
        'http://www.ruyile.com/xxlb.aspx?id=12&t=2': '安徽', \
        'http://www.ruyile.com/xxlb.aspx?id=13&t=2': '福建', \
        'http://www.ruyile.com/xxlb.aspx?id=14&t=2': '江西', \
        'http://www.ruyile.com/xxlb.aspx?id=15&t=2': '山东', \
        'http://www.ruyile.com/xxlb.aspx?id=16&t=2': '河南', \
        'http://www.ruyile.com/xxlb.aspx?id=17&t=2': '湖北', \
        'http://www.ruyile.com/xxlb.aspx?id=18&t=2': '湖南', \
        'http://www.ruyile.com/xxlb.aspx?id=19&t=2': '广东', \
        'http://www.ruyile.com/xxlb.aspx?id=20&t=2': '广西', \
        'http://www.ruyile.com/xxlb.aspx?id=21&t=2': '海南', \
        'http://www.ruyile.com/xxlb.aspx?id=22&t=2': '重庆', \
        'http://www.ruyile.com/xxlb.aspx?id=23&t=2': '四川', \
        'http://www.ruyile.com/xxlb.aspx?id=24&t=2': '贵州', \
        'http://www.ruyile.com/xxlb.aspx?id=25&t=2': '云南', \
        'http://www.ruyile.com/xxlb.aspx?id=26&t=2': '西藏', \
        'http://www.ruyile.com/xxlb.aspx?id=27&t=2': '陕西', \
        'http://www.ruyile.com/xxlb.aspx?id=28&t=2': '甘肃', \
        'http://www.ruyile.com/xxlb.aspx?id=29&t=2': '青海', \
        'http://www.ruyile.com/xxlb.aspx?id=30&t=2': '宁夏', \
        'http://www.ruyile.com/xxlb.aspx?id=31&t=2': '新疆', \
        'http://www.ruyile.com/xxlb.aspx?id=32&t=2': '台湾', \
        'http://www.ruyile.com/xxlb.aspx?id=33&t=2': '香港', \
        'http://www.ruyile.com/xxlb.aspx?id=34&t=2': '澳门',\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for url, province in self.province_dict.iteritems():\
            \
            self.crawl(url, save={'province': province}, callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        province = response.save['province']\
        #for each in response.doc('.qylb > a').items():\
        #    print "'%s': '%s', "% (each.attr.href, each.text())\
            \
        for each in response.doc('.sk').items():\
            url = each.find('h4 > a').attr.href\
            _save = {\
                'name': each.find('h4 > a').text(),\
                'logo': each.find('img').attr.src,\
            }\
            _save['province'] = province\
            for info in each.remove('h4').text().split():\
                try:\
                    (name, value) = info.split(u'：')\
                    _save[name] = value\
                except:\
                    continue\
            self.crawl(url, save=_save, callback=self.detail_page)\
        for each in response.doc('.fy > a').items():\
            self.crawl(each.attr.href, save={'province': province}, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res = response.save\
        for each in response.doc('.z').items():\
            try:\
                (name, value) = each.text().split(u'：')\
                res[name] = value\
            except:\
                continue\
        res['introduction'] = response.doc('.jj').html()\
        res['url'] = response.url\
        \
        return res\
$$$$$1469524220.4343
xiaoxu_lianjia_shanghai$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-05 17:33:31\
# Project: xiaoxu_lianjia_shanghai\
\
from pyspider.libs.base_handler import *\
from pyquery import PyQuery as pq\
import urllib\
import json\
import sys\
reload(sys)\
sys.setdefaultencoding("utf-8")\
xz_dict = {\
u'公':u'公办',\
u'民':u'民办',\
u'私':u'私立',\
}\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for i in range(1,9):\
            self.crawl('http://sh.lianjia.com/xuequ/d%s'%(i), callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.list-wrap li').items():\
            url = each.find('.info-panel > h2 a').attr.href\
            icon_img = each.find('a > img').attr.src\
            self.crawl(url, save={'icon_img': icon_img}, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res = {}\
        for each in response.doc('#introduction tr').items():\
            try:\
                _each = each.find('td').text().split()\
                res[_each[0]] = _each[1]\
            except:\
                pass\
        for each in response.doc('.aroundInfo td').items():\
            try:\
                _each = each.text().split(u'：')\
                res[_each[0]] = _each[1]\
            except:\
                pass\
            '''\
            if u'别名简称' in each.text():\
                res['nick_name'] = each.remove('.title').text().replace('|',',')\
            if u'升学方式' in each.text():\
                res['attend_way'] = each.remove('.title').text().replace(u'对口','') \
            if u'办学性质' in each.text():\
                res['school_property'] = ''\
                for k,v in xz_dict.iteritems():\
                    if k in each.remove('.title').text():\
                        res['school_property'] = v                       \
\
            if u'名额限制' in each.text():\
                res['numerus_clausus'] = each.remove('.title').text()\
            if u'地址' in each.text():\
                res['address'] = each.find('.addrEllipsis ').text()\
            '''\
         \
        _list = []\
        dic = {}\
        for each in response.doc('#property tr').items():\
            dic = {\
                'community': each.find('.propertyEllipsis').text(),\
                'building': each.find('td').eq(3).find('a').attr['data-buildings']\
            }\
            if dic['community'] != '':\
                _list.append(dic)\
        res['community'] = json.dumps(_list)\
        res['correspond_school '] = json.dumps([v.text() for v in response.doc('td > p').items() if v])\
        print res['community']\
        print res['correspond_school ']\
        \
        #jz_list = []\
        \
        #for info in response.doc('.jianzhang a').items():\
            #if u'2016年' in info.text():\
            #jz = pq(url=info.attr.href, encoding="utf-8")\
            #jz_list.append(jz.find('.box').html())\
        #res['student_guid'] = list(set(jz_list))\
        res['name'] = response.doc('.title-wrapper').text()\
        return res$$$$$1472110993.1273
xingzuo123$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 15:06:21\
# Project: xingzuo123\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    page_dict = {\
        'http://www.xingzuo123.com/12xingzuo/':[u'星座'],\
        'http://www.xingzuo123.com/xingzuoyunshi/':[u'运势'],\
        'http://www.xingzuo123.com/htm/starlove/':[u'配对'],\
        'http://www.xingzuo123.com/12shengxiao/':[u'生肖'],\
        'http://www.xingzuo123.com/smdq/':[u'算命'],\
        'http://www.xingzuo123.com/mianxiang/':[u'面相']\
\
\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.listbox li').items():\
            _dict = {}\
            _dict['title']  = each.find('a').eq(0).text() or each.find('a').eq(1).text() or ''\
            _dict['cover'] = each.find('a').eq(0).find('img').attr.src or ''\
            url =  each.find('a').eq(0).attr.href\
            _dict['date'] = each.find('small').text().split('：')[-1].split()[0]\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('.pagelist a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['class'] = 33\
        res_dict['subject'] = u'星座'\
        res_dict['source'] = 'xingzuo123.com'\
        res_dict['url'] = response.url\
        res_dict['data_weight'] = 0\
        content_list = []\
        for each in response.doc('div.article_content.info_area  p').items():\
            info = each.html()\
            if info:\
                content_list.append(info)\
        res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
        return res_dict\
$$$$$1472438319.4148
xingzuo123_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 15:06:21\
# Project: xingzuo123\
\
from pyspider.libs.base_handler import *\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    page_dict = {\
        'http://www.xingzuo123.com/12xingzuo/':[u'星座'],\
        'http://www.xingzuo123.com/xingzuoyunshi/':[u'运势'],\
        'http://www.xingzuo123.com/htm/starlove/':[u'配对'],\
        'http://www.xingzuo123.com/12shengxiao/':[u'生肖'],\
        'http://www.xingzuo123.com/smdq/':[u'算命'],\
        'http://www.xingzuo123.com/mianxiang/':[u'面相']\
\
\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1)\
    def index_page(self, response):\
        for each in response.doc('.listbox li').items():\
            _dict = {}\
            _dict['title']  = each.find('a').eq(0).text() or each.find('a').eq(1).text() or ''\
            _dict['cover'] = each.find('a').eq(0).find('img').attr.src or ''\
            url =  each.find('a').eq(0).attr.href\
            _dict['date'] = each.find('small').text().split('：')[-1].split()[0]\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        \
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['class'] = 33\
        res_dict['subject'] = u'星座'\
        res_dict['source'] = 'xingzuo123.com'\
        res_dict['url'] = response.url\
        res_dict['data_weight'] = 0\
        content_list = []\
        for each in response.doc('div.article_content.info_area  p').items():\
            info = each.html()\
            if info:\
                content_list.append(info)\
        res_dict['content'] = ''.join(['<p>%s</p>'%s for s in content_list if s and s.strip()])\
        return res_dict\
$$$$$1472546303.4206
xmswim$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 09:56:55\
# Project: xmswim\
\
from pyspider.libs.base_handler import *\
import re\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
    content = content.replace('</a>','')\
    return  content\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1'\
    }\
\
    \
    page_dict = {\
\
        'http://www.xmswim.com/new/':[u'游泳新闻'],\
        'http://www.xmswim.com/jishu/':[u'游泳技术'],\
        'http://www.xmswim.com/jiankang/':[u'游泳健康'],\
        'http://www.xmswim.com/rcswim/':[u'水上救生'],\
        'http://www.xmswim.com/resource/':[u'游泳资源'],\
        'http://www.xmswim.com/wenxian/':[u'游泳文献']\
    }\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('div.deanartice li').items():\
            _dict = {}\
            _dict['title'] = each.find('.deanarticername a').text()\
            _dict['cover'] = each.find('.deanarticel img').attr.src.replace('/new','').replace('/jishu','').replace('/jiankang','').replace('/rcswim','').replace('/resource','').replace('/wenxian','')\
            if 'template'  in _dict['cover']:\
                _dict['cover'] = ''\
            url = each.find('.deanarticername a').attr.href\
            _dict['date'] = each.find('span.deanfabushijian').text().split()[0]\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(url, save = _dict,callback=self.detail_page)\
        for each in response.doc('div.pg a[href]').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href,save = _dict,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['content'] = response.doc('td#article_content').remove('script').remove('embed').html()\
        if res_dict['content'] and '</a>' in res_dict['content']:\
            res_dict['content'] = removeLink(res_dict['content'])\
        res_dict['data_weight'] = 0\
        res_dict['class'] = 33\
        res_dict['subject'] = u'游泳'\
        res_dict['source'] = u'xmswim.com'\
        res_dict['url'] = response.url\
        return res_dict\
$$$$$1472546325.8406
xmswim_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-23 09:56:55\
# Project: xmswim\
\
from pyspider.libs.base_handler import *\
import re\
pattern = re.compile(r'<a.*?">',re.S)\
def removeLink(content):\
    for link in pattern.findall(content):\
        content = content.replace(link,'')\
    content = content.replace('</a>','')\
    return  content\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag':'0.1'\
    }\
\
    \
    page_dict = {\
\
        'http://www.xmswim.com/new/':[u'游泳新闻'],\
        'http://www.xmswim.com/jishu/':[u'游泳技术'],\
        'http://www.xmswim.com/jiankang/':[u'游泳健康'],\
        'http://www.xmswim.com/rcswim/':[u'水上救生'],\
        'http://www.xmswim.com/resource/':[u'游泳资源'],\
        'http://www.xmswim.com/wenxian/':[u'游泳文献']\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_dict.items():\
            self.crawl(k, save = {'bread':v},callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('div.deanartice li').items():\
            _dict = {}\
            _dict['title'] = each.find('.deanarticername a').text()\
            _dict['cover'] = each.find('.deanarticel img').attr.src.replace('/new','').replace('/jishu','').replace('/jiankang','').replace('/rcswim','').replace('/resource','').replace('/wenxian','')\
            if 'template'  in _dict['cover']:\
                _dict['cover'] = ''\
            url = each.find('.deanarticername a').attr.href\
            _dict['date'] = each.find('span.deanfabushijian').text().split()[0]\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(url, save = _dict,callback=self.detail_page)\
       \
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        res_dict['content'] = response.doc('td#article_content').remove('script').remove('embed').html()\
        res_dict['data_weight'] = 0\
        res_dict['class'] = 33\
        res_dict['subject'] = u'游泳'\
        res_dict['source'] = u'xmswim.com'\
        res_dict['url'] = response.url\
        return res_dict\
$$$$$1472546316.9926
xuexiao_baike$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-01 09:37:52\
# Project: xuexiao_baike\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    \
    headers = {\
\
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\
\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://baike.baidu.com/view/466443.htm', headers = self.headers,callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        _dict = {}\
        for each in response.doc('div[label-module="para-title"]').items():\
            key =  each.children('h2').remove('span').text()\
            content_list = list()\
            nx = each.next()        \
            while nx:\
                if nx.attr['label-module'] == 'para-title' or nx.attr['id'] =='open-tag':\
                    break\
                if nx.attr['class'] == 'para':  \
                    content_list.append('<p>'+nx.remove('sup').text()+'</p>')            \
                nx = nx.next()\
            if content_list and key:\
                  _dict[key] = ''.join(content_list)\
        for each in response.doc('dt.basicInfo-item').items():\
            key = ''.join(v for v in each.text().split())\
            nx = each.next()\
            if 'basicInfo-item' in nx.attr['class'] and key:\
                _dict[key] = nx.text()\
        if not _dict:\
            return\
        return _dict\
\
            \
            \
            \
            \
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
        }\
$$$$$1470019435.2355
xuexiao_eol$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-14 09:58:48\
# Project: xuexiao_eol\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8')\
from pyquery import PyQuery as pq\
import requests\
import json\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    set_dict = {\
\
            u'电话':'phone',\
            u'校址':'address',\
            u'网址':'site',\
            u'学段':'period',\
            u'地区':'locate',\
            u'性质':'school_type',\
\
    }\
    #intro.shtml 学校简介\
    #teachers.shtml 师资力量\
    #fest.shtml 办学特色\
    #facilities.shtml  学校风光\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://xuexiao.eol.cn/html4/1100/114000144/index.shtml', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        _dict = {}\
        for each in response.doc('table.line_22').eq(0).find('td').items():     \
              arr =  each.text().split('：')\
              if len(arr) == 2 and arr[0] in self.set_dict:\
                    #print type(arr[1])\
                    _dict[self.set_dict[arr[0]]] = arr[1]\
        \
        \
        baseUrl = response.url.replace('index.shtml','')\
        resp = requests.get(baseUrl+'intro.shtml')  \
        #print len( pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.main'))\
        #print pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.main').eq(0).html()\
        _dict['school_summary'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.main').eq(0).html()\
        resp = requests.get(baseUrl+'teachers.html')  \
        _dict['faculty'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.main').eq(0).html()\
        resp = requests.get(baseUrl+'fest.html')  \
        _dict['feature_course'] = pq(resp.text.encode(resp.encoding).decode('utf-8')).find('div.main').eq(0).html()\
        print _dict\
        \
               \
\
    @config(priority=2)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.doc('title').text(),\
        }\
$$$$$1468465752.3089
yasi_taisha_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-25 17:44:23\
# Project: yasi_taisha\
\
from pyspider.libs.base_handler import *\
import pyquery\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    _dict = {\
    'news': '快讯动态', 'guidance':'分类资讯', 'experience':'经验分享', 'download':'分类资讯', 't-guide':'分类资讯', 'machine':'雅思机经'\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for page in self._dict: \
            self.crawl('http://www.taisha.org/ielts/%s/'%page, save={'type': self._dict[page]}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        type = response.save['type']\
        for each in response.doc('.html_content dd').items():\
            _dict = {}\
            url = each.find('.title > a').attr.href\
            #_dict['url'] = url\
            _dict['tag'] = each.find('.bot > a').text().split(' ')\
            _dict['brief'] = each.find('p').text().replace(u'[详情]', '')\
            _dict['type'] = type\
            self.crawl(url, save=_dict, callback=self.detail_page)\
        #for each in response.doc('span > a').items():\
        #    self.crawl(each.attr.href, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save\
        try:\
            date = response.doc('.txt_title span').text()[-19:]\
        except:\
            date = ''\
        #content = response.doc('.txt_content').html()\
        content_info = response.doc('.txt_content > p')\
        content_list = []\
        for info in content_info:\
            answer = pyquery.PyQuery(info)\
            items = answer.html()\
            if answer.text() == u'太傻网雅思频道精品推荐：':\
                break\
            content_list.append((items.replace(u'<a href="http://www.taisha.org/ielts/" target="_blank" class="keylink">雅思</a>',u'雅思').replace(u'太傻', u'跟谁学')))\
        content_list = content_list[:-1]\
        if not content_list:\
            return None\
        title = response.doc('.txt_title > h2').text()\
        if u'回忆版' in title:\
            return None\
        res_dict['title'] = title \
        res_dict['url'] = response.url\
        res_dict['date'] = date[0:10]\
        res_dict['source'] = u'太傻留学'\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list])\
        res_dict['subject'] = u'雅思'\
        res_dict['bread'] = response.doc('#nav_location a').text().split(' ')[1:]\
        res_dict['bread'].extend(res_dict['tag'])\
        res_dict['bread'].append((response.save['type']))\
        res_dict['bread'] = list(set(res_dict['bread']))\
        res_dict['class'] = 17\
        res_dict.pop('type')\
        res_dict['data_weight'] = 0\
        return res_dict$$$$$1471329896.5925
yasi_xiaozhan_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-03-08 10:48:14\
# Project: yasi_xiaozhan_inc\
\
\
from pyspider.libs.base_handler import *\
import pyquery\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for i in range(1,247):\
            url = 'http://ielts.zhan.com/'\
            self.crawl(url, callback=self.index_page)\
            #self.crawl(url, fetch_type='js', callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc('.pull-right > p > a').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        cnt = 0\
        for each in response.doc('.active > a'):\
            cnt = cnt + 1\
        if cnt == 1:\
            return None\
        \
        url = response.url\
        brief = response.doc('.first-words').text()\
        title = response.doc('title').text()\
        title = title.replace(u'小站', u'跟谁学')\
        date = ''\
        for each in response.doc('.pull-left > span').items():\
            #date =''\
            date = each.text().replace(u'年', '-').replace(u'月','-').replace(u'日','')\
            break\
            \
        bread = []\
        for each in response.doc('.head-crumbs-a-active').items():\
            bread.append(each.text())\
        \
        content_list = []\
        for info in response.doc('.article-content > p'):\
            content_list.append(pyquery.PyQuery(info).html())\
        content = ''.join(["<p>%s</p>"%v for v in content_list])\
        content = content.replace('img src=\\"/uploadfile', 'img src=\\"http://ielts.zhan.com/uploadfile')\
        content = content.replace(u'小站', u'本站')\
        brief = brief.replace(u'小站', u'本站')\
        if len(content) < 10:\
            return None\
        return {\
            "url": response.url,\
            "title": title,\
            "brief": brief,\
            "content": content,\
            "date": date,\
            "source": 'zhan.com',\
            "subject": u'雅思',\
            "bread": bread,\
            "class": 17,\
            "data_weight": 0,\
        }\
$$$$$1471329900.4770
yishu2_58$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-01 09:18:48\
# Project: yishu2_58\
\
from pyspider.libs.base_handler import *\
import time\
import sys\
reload(sys)\
sys.setdefaultencoding='utf-8'\
\
\
nav_yishu = [u'艺术',u'舞蹈',u'乐器',u'美术',u'声乐',u'表演',u'艺考']\
nav_xiqu = [u'兴趣',u'摄影',u'DJ',u'魔术',u'书法',u'风水',u'国学']\
nav_shenghuo = [u'生活',u'礼仪',u'茶艺',u'插花',u'烹饪',u'形体',u'园艺']\
nav = [nav_yishu, nav_xiqu, nav_shenghuo]\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.1'\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1', callback=self.index_page)\
        \
    @config(age=24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('#ObjectType > a').items():\
            if each.text() == u'全部':\
                continue\
            _dict = {}\
            for temp in nav:\
                if each.text() in temp:\
                    _dict['bread'] = [temp[0], each.text()]\
                    \
            _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(age=24 * 60 * 60)\
    def list_page(self, response):\
        if response.doc('#xiaolei > a'):\
            for each in response.doc('#xiaolei > a').items():\
                _dict = {}\
                _dict['bread'] = response.save.get('bread')\
                _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
                if each.text() == u'全部':\
                    _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
                else:\
                    _dict['title'] = u'附近哪里学' + each.text() + u'比较好'\
                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
        else:\
             _dict = {}\
             _dict['bread'] = response.save.get('bread')\
             _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
             _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
             self.crawl(response.url, save = _dict, callback=self.list_page1)       \
            \
        \
    @config(age=10*24*60*60)\
    def list_page1(self, response):\
        for each in response.doc('#local > a').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread',[])\
            #_dict['date'] = response.save.get('date')\
            #_dict['title'] = each.text() + response.save.get('title')\
            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=10*24*60*60)\
    def list_page2(self, response):\
        global flag\
        global cishu\
        for each in response.doc('.subarea > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            _dict['date'] = response.save.get('date')\
            _dict['title'] = each.text() + response.save.get('title')\
            _dict['content'] = ''\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\
            \
    @config(age=10*24*60*60)\
    def list_page3(self, response):\
        global flag\
        global cishu\
        _dict = response.save\
        for each in response.doc('.tdiv').items():\
            _dict['content'] += each.find('a').eq(0).text() + '<br/>' + each.find('div').text()+ '<br/>' + each.find('p').text() + '<br/>'\
\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }\
    \
    @config(priority=3)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }$$$$$1467681597.2940
yishu_58$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-27 11:00:58\
# Project: yishu_58\
\
#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-06-27 10:46:30\
# Project: yishu\
\
from pyspider.libs.base_handler import *\
import time\
import sys\
reload(sys)\
sys.setdefaultencoding='utf-8'\
\
\
nav_yishu = [u'艺术',u'舞蹈',u'乐器',u'美术',u'声乐',u'表演',u'艺考']\
nav_xiqu = [u'兴趣',u'摄影',u'DJ',u'魔术',u'书法',u'风水',u'国学']\
nav_shenghuo = [u'生活',u'礼仪',u'茶艺',u'插花',u'烹饪',u'形体',u'园艺']\
nav = [nav_yishu, nav_xiqu, nav_shenghuo]\
\
class Handler(BaseHandler):\
    crawl_config = {\
        'itag': '0.3'\
    }\
    \
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1', callback=self.index_page)\
        \
    @config(age=24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('#ObjectType > a').items():\
            if each.text() == u'全部':\
                continue\
            _dict = {}\
            for temp in nav:\
                if each.text() in temp:\
                    _dict['bread'] = [temp[0], each.text()]\
                    \
            _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\
\
    @config(age=24 * 60 * 60)\
    def list_page(self, response):\
        if response.doc('#xiaolei > a'):\
            for each in response.doc('#xiaolei > a').items():\
                _dict = {}\
                _dict['bread'] = response.save.get('bread')\
                _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
                if each.text() == u'全部':\
                    _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
                else:\
                    _dict['title'] = u'附近哪里学' + each.text() + u'比较好'\
                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
        else:\
             _dict = {}\
             _dict['bread'] = response.save.get('bread')\
             _dict['date'] = time.strftime('%Y-%m-%d',time.localtime())\
             _dict['title'] = u'附近哪里学' + _dict['bread'][1] + u'比较好'\
             self.crawl(response.url, save = _dict, callback=self.list_page1)       \
            \
        \
    @config(age=10*24*60*60)\
    def list_page1(self, response):\
        for each in response.doc('#local > a').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread',[])\
            #_dict['date'] = response.save.get('date')\
            #_dict['title'] = each.text() + response.save.get('title')\
            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\
            \
    @config(age=10*24*60*60)\
    def list_page2(self, response):\
        global flag\
        global cishu\
        for each in response.doc('.subarea > a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            _dict['date'] = response.save.get('date')\
            _dict['title'] = each.text() + response.save.get('title')\
            _dict['content'] = ''\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\
            \
    @config(age=10*24*60*60)\
    def list_page3(self, response):\
        global flag\
        global cishu\
        _dict = response.save\
        for each in response.doc('.tdiv').items():\
            _dict['content'] += each.find('a').eq(0).text() + '<br/>' + each.find('div').text()+ '<br/>' + each.find('p').text() + '<br/>'\
\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }\
    \
    @config(priority=3)\
    def detail_page(self, response):\
        return {\
            "url": response.url,\
            "title": response.save.get('title'),\
            "bread":response.save.get('bread'),\
            "subject": u'补习班',\
            "class": 46,\
            "date":response.save.get('date'),\
            "content": response.save.get('content'),\
            "source": u'58同城',\
            "data_weight": 0,\
        }$$$$$1467335899.8223
yixueweisheng_zhengbao$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-01 16:03:05\
# Project: yixueweisheng_zhengbao\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
type_dict = {\
    'chuji': [u'财会经济', u'初级会计师'],\
    'chujizhicheng': [u'财会经济', u'初级会计师'],\
    'zhongji': [u'财会经济', u'中级会计师'],\
    'zhongjizhicheng': [u'财会经济', u'中级会计师'],\
    'shiwushi': [u'财会经济', u'注册税务师'],\
    'zhukuai': [u'财会经济', u'注册会计师'],\
    'gaoji': [u'财会经济', u'高级会计师'],\
    'congye': [u'财会经济', u'会计从业'],\
    u'注册会计师': [u'财会经济', u'注册会计师'],\
    u'经济师': [u'财会经济', u'经济师'],\
    u'初级会计职称': [u'财会经济', u'初级会计师'],\
    u'中级会计职称': [u'财会经济', u'中级会计师'],\
    u'税务师': [u'财会经济', u'注册税务师'],\
    u'统计师': [u'财会经济', u'统计师'],\
    u'审计师': [u'财会经济', u'审计师'],\
    u'高级经济师': [u'财会经济', u'经济师'],\
    u'经济师': [u'财会经济', u'经济师'],\
    u'高级会计': [u'财会经济', u'高级会计师'],\
    u'理财规划师': [u'财会经济', u'理财规划师'],\
\
    u'英语四级': [u'外语考试', u'英语四六级'],\
    u'英语六级': [u'外语考试', u'英语四六级'],\
    u'雅思': [u'外语考试', u'雅思'],\
    u'托福': [u'外语考试', u'托福'],\
    u'职称英语': [u'外语考试', u'职称英语'],\
    u'商务英语': [u'外语考试', u'商务英语'],\
    u'公共英语': [u'外语考试', u'公共英语'],\
    u'日语': [u'外语考试', u'日语'],\
    u'GRE考试': [u'外语考试', u'GRE考试'],\
    u'专四专八': [u'外语考试', u'专四专八'],\
    u'口译笔译': [u'外语考试', u'口译笔译'],\
\
    'zaojia': [u'建筑工程', '造价工程师'],\
    u'一级建造师': [u'建筑工程', u'一级建造师'],\
    u'二级建造师': [u'建筑工程', u'二级建造师'],\
    u'咨询工程师': [u'建筑工程', u'咨询工程师'],\
    u'造价工程师': [u'建筑工程', '造价工程师'],\
    u'结构工程师': [u'建筑工程', '结构工程师'],\
    u'物业管理': [u'建筑工程', u'物业管理师'],\
    u'城市规划': [u'建筑工程', u'城市规划师'],\
    u'给排水工程': [u'建筑工程', u'给排水工程'],\
    u'电气工程': [u'建筑工程', u'电气工程师'],\
    u'公路监理师': [u'建筑工程', u'公路监理师'],\
    u'消防工程师': [u'建筑工程', u'消防工程师'],\
    u'消防': [u'建筑工程', u'消防工程师'],\
\
    u'物流师': [u'职业资格', u'物流师'],\
    u'人力资源': [u'职业资格', u'人力资源'],\
    u'心理咨询师': [u'职业资格', u'心理咨询师'],\
    u'公共营养师': [u'职业资格', u'公共营养师'],\
    u'秘书资格': [u'职业资格', u'秘书资格'],\
    u'秘书资格': [u'职业资格', u'秘书资格'],\
    u'证券从业资格': [u'职业资格', u'证券经纪人'],\
    u'电子商务师': [u'职业资格', u'电子商务'],\
    u'期货从业': [u'职业资格', u'期货从业'],\
    u'教师资格': [u'职业资格', u'教师资格'],\
    u'管理咨询师': [u'职业资格', u'管理咨询师'],\
    u'导游证': [u'职业资格', u'导游证'],\
\
    u'英语': [u'学历教育', u'考研'],\
    u'数学': [u'学历教育', u'考研'],\
    u'政治': [u'学历教育', u'考研'],\
    u'专业课': [u'学历教育', u'考研'],\
\
    u'执业医师': [u'医学卫生', u'执业医师'],\
    u'初级药师': [u'医学卫生', u'执业药师'],\
    u'初级药士': [u'医学卫生', u'执业药师'],\
    u'主管药师': [u'医学卫生', u'执业药师'],\
    u'主管中药师': [u'医学卫生', u'执业药师'],\
    u'中药师': [u'医学卫生', u'执业药师'],\
    u'中药士': [u'医学卫生', u'执业药师'],\
    u'临床执业': [u'医学卫生', u'临床执业'],\
    u'临床医师': [u'医学卫生', u'临床执业'],\
    u'中医医师': [u'医学卫生', u'中医执业'],\
    u'中医执业': [u'医学卫生', u'中医执业'],\
    u'中西医医师': [u'医学卫生', u'中西医执业'],\
    u'中西执业': [u'医学卫生', u'中西医执业'],\
    u'中医助理': [u'医学卫生', u'中医助理'],\
    u'中西医助理': [u'医学卫生', u'中西医助理'],\
    u'主治': [u'医学卫生', u'主治'],\
    u'全科主治': [u'医学卫生', u'主治'],\
    u'内科主治': [u'医学卫生', u'主治'],\
    u'外科主治': [u'医学卫生', u'主治'],\
    u'儿科主治': [u'医学卫生', u'主治'],\
    u'妇产科主治': [u'医学卫生', u'主治'],\
    u'检验': [u'医学卫生', u'检验'],\
    u'检验士': [u'医学卫生', u'检验'],\
    u'检验师': [u'医学卫生', u'检验'],\
    u'主管检验师': [u'医学卫生', u'检验'],\
    u'执业护士资格': [u'医学卫生', u'执业护士'],\
    u'护士资格': [u'医学卫生', u'执业护士'],\
\
    u'成人高考': [u'学历教育', u'成人高考'],\
    u'自学考试': [u'学历教育', u'自考'],\
    u'MBA考试': [u'学历教育', u'MBA'],\
    u'法律硕士': [u'学历教育', u'法律硕士'],\
    u'专升本': [u'学历教育', u'专升本'],\
    # u'':[u'学历教育',u'工程硕士'],\
    u'MPA考试': [u'学历教育', u'公共硕士'],\
    # u'':[u'学历教育',u'考研'],\
}\
\
some_url =[\
    'http://www.chinaacc.com/chujizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/chujizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/',\
    'http://www.chinaacc.com/zaojia/zt/',\
    'http://www.chinaacc.com/zaojia/mnst/',    \
]\
\
remove_name = [\
    u'卫生网校',\
    u'医学书店',\
    u'定期考核',\
    u'医学会议',\
    u'继续教育',\
    u'学习卡',\
    u'首页',\
    u'论坛',\
    u'邮箱',\
]\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        self.crawl('http://www.med66.com/', callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('.navItem > div li').items():\
            if each.text().replace(' ', '') in remove_name:\
                continue\
            _dict = {}\
            if each.text().replace(' ', '') in type_dict.keys():\
                _dict['bread'] = type_dict[each.text().replace(' ', '')]\
            else:\
                _dict['bread'] = ['医学卫生', each.text().replace(' ', '')]\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.list_page)\
         \
        \
    @config(age=10 * 24 * 60 * 60)\
    def list_page(self, response):\
        for each in response.doc('.snbody > .subnav').items():\
            if '复习指导' not in each.text():\
                continue\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            for each1 in each.find('a').items():\
                self.crawl(each1.attr.href, save = _dict, callback=self.list_page1)\
        \
    @config(age=10 * 24 * 60 * 60)\
    def list_page1(self, response):\
        for each in response.doc('.xinxi li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('.fl > a').text()\
            if '报名' in _dict['title'] or '名师' in _dict['title'] or '汇总' in _dict['title']:\
                continue\
            _dict['date'] = each.find('.fr').text().replace('[','').replace(']','')\
            self.crawl(each.find('.fl > a').attr.href, save = _dict, callback=self.detail_page)\
        \
        #翻页 \
        for each in response.doc('divpagestr2016 a').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('#fontzoom').items():\
            for each1 in each.find('p').items():\
                if u'全网首发' in each1.text():\
                    continue\
                if u'估分更准确' in each1.text():\
                    continue\
                if u'独家' in each1.text():\
                    continue\
                if u'点击' in each1.text():\
                    continue\
                if u'到建设工程教育网论坛' in each1.text():\
                    continue\
                if u'特色班' in each1.text():\
                    break\
                if u'近几年' in each1.text():\
                    break\
                if u'考友咨询' in each1.text():\
                    break\
                if u'更多' in each1.text() and u'资讯' in each1.text():\
                    break\
                if u'推荐信息' in each1.text() or u'更多推荐' in each1.text() or u'推荐阅读' in each1.text() or u'相关推荐' in each1.text():\
                    break\
                if u'免费在线测试' in each1.text():\
                    continue\
                if u'在线测试系统' in each1.text():\
                    continue\
                if u'转载请注明出处' in each1.text():\
                    continue\
                if u'责任编辑' in each1.text():\
                    break\
                if u'相关链接' in each1.text():\
                    break\
                if u'以上' in each1.text() and u'是中华会计网' in each1.text():\
                    break\
                else:\
                    list.append(each1.remove('a').html())\
            \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            \
            return {\
                "url": response.url,\
                "title": response.save.get('title'),\
                "content": content.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace(u'医学教育网', '').replace('【 】', ''),\
                "subject": u"考证题库",\
                "bread": response.save.get('bread'),\
                "date": response.save.get('date'),\
                "tdk_keywords": str(response.doc('meta[http-equiv="keywords"]').eq(0).attr.content).replace(u'医学教育网', '').replace(\
                    'None', '') + str(response.doc('meta[name="keywords"]').eq(0).attr.content).replace(u'医学教育网', '').replace(\
                    'None', ''),\
                "tdk_desc": str(response.doc('meta[http-equiv="description"]').eq(0).attr.content).replace(u'医学教育网', u'').replace(\
                    '建设网校', '').replace('None', '') + str(\
                    response.doc('meta[name="description"]').eq(0).attr.content).replace(\
                    u'医学教育网', '').replace('建设网校', '').replace('None', ''),\
                "tdk_title": response.doc('head > title').eq(0).text().replace(u'医学教育网', '') + u' 跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"医学教育网",\
            }\
\
$$$$$1471333122.6680
yixue_zhengbao_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-08-16 16:02:22\
# Project: yixue_zhengbao_inc\
\
from pyspider.libs.base_handler import *\
import sys\
reload(sys)\
sys.setdefaultencoding( "utf-8" )\
\
type_dict = {\
    'chuji': [u'财会经济', u'初级会计师'],\
    'chujizhicheng': [u'财会经济', u'初级会计师'],\
    'zhongji': [u'财会经济', u'中级会计师'],\
    'zhongjizhicheng': [u'财会经济', u'中级会计师'],\
    'shiwushi': [u'财会经济', u'注册税务师'],\
    'zhukuai': [u'财会经济', u'注册会计师'],\
    'gaoji': [u'财会经济', u'高级会计师'],\
    'congye': [u'财会经济', u'会计从业'],\
    u'注册会计师': [u'财会经济', u'注册会计师'],\
    u'经济师': [u'财会经济', u'经济师'],\
    u'初级会计职称': [u'财会经济', u'初级会计师'],\
    u'中级会计职称': [u'财会经济', u'中级会计师'],\
    u'税务师': [u'财会经济', u'注册税务师'],\
    u'统计师': [u'财会经济', u'统计师'],\
    u'审计师': [u'财会经济', u'审计师'],\
    u'高级经济师': [u'财会经济', u'经济师'],\
    u'经济师': [u'财会经济', u'经济师'],\
    u'高级会计': [u'财会经济', u'高级会计师'],\
    u'理财规划师': [u'财会经济', u'理财规划师'],\
\
    u'英语四级': [u'外语考试', u'英语四六级'],\
    u'英语六级': [u'外语考试', u'英语四六级'],\
    u'雅思': [u'外语考试', u'雅思'],\
    u'托福': [u'外语考试', u'托福'],\
    u'职称英语': [u'外语考试', u'职称英语'],\
    u'商务英语': [u'外语考试', u'商务英语'],\
    u'公共英语': [u'外语考试', u'公共英语'],\
    u'日语': [u'外语考试', u'日语'],\
    u'GRE考试': [u'外语考试', u'GRE考试'],\
    u'专四专八': [u'外语考试', u'专四专八'],\
    u'口译笔译': [u'外语考试', u'口译笔译'],\
\
    'zaojia': [u'建筑工程', '造价工程师'],\
    u'一级建造师': [u'建筑工程', u'一级建造师'],\
    u'二级建造师': [u'建筑工程', u'二级建造师'],\
    u'咨询工程师': [u'建筑工程', u'咨询工程师'],\
    u'造价工程师': [u'建筑工程', '造价工程师'],\
    u'结构工程师': [u'建筑工程', '结构工程师'],\
    u'物业管理': [u'建筑工程', u'物业管理师'],\
    u'城市规划': [u'建筑工程', u'城市规划师'],\
    u'给排水工程': [u'建筑工程', u'给排水工程'],\
    u'电气工程': [u'建筑工程', u'电气工程师'],\
    u'公路监理师': [u'建筑工程', u'公路监理师'],\
    u'消防工程师': [u'建筑工程', u'消防工程师'],\
    u'消防': [u'建筑工程', u'消防工程师'],\
\
    u'物流师': [u'职业资格', u'物流师'],\
    u'人力资源': [u'职业资格', u'人力资源'],\
    u'心理咨询师': [u'职业资格', u'心理咨询师'],\
    u'公共营养师': [u'职业资格', u'公共营养师'],\
    u'秘书资格': [u'职业资格', u'秘书资格'],\
    u'秘书资格': [u'职业资格', u'秘书资格'],\
    u'证券从业资格': [u'职业资格', u'证券经纪人'],\
    u'电子商务师': [u'职业资格', u'电子商务'],\
    u'期货从业': [u'职业资格', u'期货从业'],\
    u'教师资格': [u'职业资格', u'教师资格'],\
    u'管理咨询师': [u'职业资格', u'管理咨询师'],\
    u'导游证': [u'职业资格', u'导游证'],\
\
    u'英语': [u'学历教育', u'考研'],\
    u'数学': [u'学历教育', u'考研'],\
    u'政治': [u'学历教育', u'考研'],\
    u'专业课': [u'学历教育', u'考研'],\
\
    u'执业医师': [u'医学卫生', u'执业医师'],\
    u'初级药师': [u'医学卫生', u'执业药师'],\
    u'初级药士': [u'医学卫生', u'执业药师'],\
    u'主管药师': [u'医学卫生', u'执业药师'],\
    u'主管中药师': [u'医学卫生', u'执业药师'],\
    u'中药师': [u'医学卫生', u'执业药师'],\
    u'中药士': [u'医学卫生', u'执业药师'],\
    u'临床执业': [u'医学卫生', u'临床执业'],\
    u'临床医师': [u'医学卫生', u'临床执业'],\
    u'中医医师': [u'医学卫生', u'中医执业'],\
    u'中医执业': [u'医学卫生', u'中医执业'],\
    u'中西医医师': [u'医学卫生', u'中西医执业'],\
    u'中西执业': [u'医学卫生', u'中西医执业'],\
    u'中医助理': [u'医学卫生', u'中医助理'],\
    u'中西医助理': [u'医学卫生', u'中西医助理'],\
    u'主治': [u'医学卫生', u'主治'],\
    u'全科主治': [u'医学卫生', u'主治'],\
    u'内科主治': [u'医学卫生', u'主治'],\
    u'外科主治': [u'医学卫生', u'主治'],\
    u'儿科主治': [u'医学卫生', u'主治'],\
    u'妇产科主治': [u'医学卫生', u'主治'],\
    u'检验': [u'医学卫生', u'检验'],\
    u'检验士': [u'医学卫生', u'检验'],\
    u'检验师': [u'医学卫生', u'检验'],\
    u'主管检验师': [u'医学卫生', u'检验'],\
    u'执业护士资格': [u'医学卫生', u'执业护士'],\
    u'护士资格': [u'医学卫生', u'执业护士'],\
\
    u'成人高考': [u'学历教育', u'成人高考'],\
    u'自学考试': [u'学历教育', u'自考'],\
    u'MBA考试': [u'学历教育', u'MBA'],\
    u'法律硕士': [u'学历教育', u'法律硕士'],\
    u'专升本': [u'学历教育', u'专升本'],\
    # u'':[u'学历教育',u'工程硕士'],\
    u'MPA考试': [u'学历教育', u'公共硕士'],\
    # u'':[u'学历教育',u'考研'],\
}\
\
some_url =[\
    'http://www.chinaacc.com/chujizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/chujizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/',\
    'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/',\
    'http://www.chinaacc.com/zaojia/zt/',\
    'http://www.chinaacc.com/zaojia/mnst/',    \
]\
\
remove_name = [\
    u'卫生网校',\
    u'医学书店',\
    u'定期考核',\
    u'医学会议',\
    u'继续教育',\
    u'学习卡',\
    u'首页',\
    u'论坛',\
    u'邮箱',\
]\
\
class Handler(BaseHandler):\
    crawl_config = {\
            'itag':'0.1'\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        self.crawl('http://www.med66.com/', callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        for each in response.doc('.navItem > div li').items():\
            if each.text().replace(' ', '') in remove_name:\
                continue\
            _dict = {}\
            if each.text().replace(' ', '') in type_dict.keys():\
                _dict['bread'] = type_dict[each.text().replace(' ', '')]\
            else:\
                _dict['bread'] = ['医学卫生', each.text().replace(' ', '')]\
            self.crawl(each.find('a').attr.href, save = _dict, callback=self.list_page)\
         \
        \
    @config(age=1*1)\
    def list_page(self, response):\
        for each in response.doc('.snbody > .subnav').items():\
            if '复习指导' not in each.text():\
                continue\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            for each1 in each.find('a').items():\
                self.crawl(each1.attr.href, save = _dict, callback=self.list_page1)\
        \
    @config(age=1*1)\
    def list_page1(self, response):\
        for each in response.doc('.xinxi li').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread')\
            _dict['title'] = each.find('.fl > a').text()\
            if '报名' in _dict['title'] or '名师' in _dict['title'] or '汇总' in _dict['title']:\
                continue\
            _dict['date'] = each.find('.fr').text().replace('[','').replace(']','')\
            self.crawl(each.find('.fl > a').attr.href, save = _dict, callback=self.detail_page)\
        \
        #翻页 \
        #for each in response.doc('divpagestr2016 a').items():\
         #   _dict = {}\
          #  _dict['bread'] = response.save.get('bread')\
           # self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        list = []                             \
        for each in response.doc('#fontzoom').items():\
            for each1 in each.find('p').items():\
                if u'全网首发' in each1.text():\
                    continue\
                if u'估分更准确' in each1.text():\
                    continue\
                if u'独家' in each1.text():\
                    continue\
                if u'点击' in each1.text():\
                    continue\
                if u'到建设工程教育网论坛' in each1.text():\
                    continue\
                if u'特色班' in each1.text():\
                    break\
                if u'近几年' in each1.text():\
                    break\
                if u'考友咨询' in each1.text():\
                    break\
                if u'更多' in each1.text() and u'资讯' in each1.text():\
                    break\
                if u'推荐信息' in each1.text() or u'更多推荐' in each1.text() or u'推荐阅读' in each1.text() or u'相关推荐' in each1.text():\
                    break\
                if u'免费在线测试' in each1.text():\
                    continue\
                if u'在线测试系统' in each1.text():\
                    continue\
                if u'转载请注明出处' in each1.text():\
                    continue\
                if u'责任编辑' in each1.text():\
                    break\
                if u'相关链接' in each1.text():\
                    break\
                if u'以上' in each1.text() and u'是中华会计网' in each1.text():\
                    break\
                else:\
                    list.append(each1.remove('a').html())\
            \
        content = ''.join('<p>%s</p>' % (s) for s in list if s)\
        if len(content.strip()) < 20:\
            pass\
        else:\
            \
            return {\
                "url": response.url,\
                "title": response.save.get('title'),\
                "content": content.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace(u'医学教育网', '').replace('【 】', ''),\
                "subject": u"考证题库",\
                "bread": response.save.get('bread'),\
                "date": response.save.get('date'),\
                "tdk_keywords": str(response.doc('meta[http-equiv="keywords"]').eq(0).attr.content).replace(u'医学教育网', '').replace(\
                    'None', '') + str(response.doc('meta[name="keywords"]').eq(0).attr.content).replace(u'医学教育网', '').replace(\
                    'None', ''),\
                "tdk_desc": str(response.doc('meta[http-equiv="description"]').eq(0).attr.content).replace(u'医学教育网', u'').replace(\
                    '建设网校', '').replace('None', '') + str(\
                    response.doc('meta[name="description"]').eq(0).attr.content).replace(\
                    u'医学教育网', '').replace('建设网校', '').replace('None', ''),\
                "tdk_title": response.doc('head > title').eq(0).text().replace(u'医学教育网', '') + u' 跟谁学',\
                "class": 33,\
                "data_weight": 0,\
                "source": u"医学教育网",\
            }\
\
$$$$$1471334954.4903
youshengxiao_ysxiao_zhongdianxiaoxue$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-03-30 10:41:03\
# Project: youshengxiao_jiaoliu\
\
from pyspider.libs.base_handler import *\
import pyquery\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    page_dict = {\
    'http://www.ysxiao.cn/xiaoxue-haidianqu/': u'重点小学',\
    'http://www.ysxiao.cn/xiaoxue-xichengqu/': u'重点小学',\
    'http://www.ysxiao.cn/xiaoxue-chaoyangqu/': u'重点小学',\
    'http://www.ysxiao.cn/xiaoxue-dongchengqu/': u'重点小学',\
    'http://www.ysxiao.cn/xiaoxue-fengtaiqu/': u'重点小学',\
    'http://www.ysxiao.cn/xiaoxue-qitaqu/': u'重点小学',\
    'http://www.ysxiao.cn/zhongdianxiaoxue/xiaoxue-shijingshanqu/': u'重点小学',\
    'http://www.ysxiao.cn/beijingyoushengxiao/youshengxiaoxianjie/': u'经验交流'\
    }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k, v in self.page_dict.iteritems():\
            self.crawl(k, save={'label': v}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        label = response.save['label']\
        for each in response.doc('.desc').items():\
            _dict = {}\
            url = each.find('a').attr.href\
            _dict['label'] = label\
            _dict['date'] = each.find('p').text()\
            self.crawl(url, save=_dict, callback=self.detail_page)\
        \
        #for each in response.doc('.pages a').items():\
        #    self.crawl(each.attr.href, save={'label': label}, callback=self.index_page)\
            \
            \
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for each in response.doc('p'):\
            _cont = pyquery.PyQuery(each).html().replace('\\r','').replace('\\n','').replace('\\t','')\
            if _cont:\
                content_list.append((_cont))\
        \
        res_dict = response.save\
        #if res_dict['label'] == u'北京幼升小招生简章':\
        content_list = content_list[:-3]\
        if not content_list:\
            return None\
        res_dict["url"] = response.url\
        res_dict["title"] = response.doc('h1').text()\
        res_dict["content"] = ''.join(['<p>%s</p>'%v for v in content_list])\
        #res_dict["tags"] = response.doc('#tags > a').text().split()\
        res_dict['subject'] = u'幼升小'\
        res_dict['bread'] = response.doc('#position a').text().split()\
        res_dict['bread'][0] = res_dict['label']\
        res_dict['bread'] = list(set(res_dict['bread']))\
        res_dict.pop('label')\
        res_dict['brief'] = response.doc('#desc').text()\
        res_dict['class'] = 23\
        res_dict['source'] = 'ysxiao.com'\
        res_dict['data_weight'] = 0\
        return res_dict\
$$$$$1471332766.8603
youshengxiao_ysxiao_zixun_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-03-17 15:15:26\
# Project: youshengxiao_ysxiao_zixun_inc\
\
from pyspider.libs.base_handler import *\
import pyquery\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    zhengce_map = {'haidianqu-youshengxiao': u'幼升小海淀区政策', \
                   'xichengqu-youshengxiao': u'幼升小西城区政策', \
                   'chaoyangqu-youshengxiao': u'幼升小朝阳区政策', \
                   'dongchengqu-youshengxiao': u'幼升小东城区政策', \
                   'qita': u'幼升小其他城区政策', \
                   'youshengxiao-huapian': u'北京幼升小招生划片',\
                   'zhaoshengjianzhang': u'北京幼升小招生简章',\
                   'youshengxiao-xuequfang': u'北京幼升小学区房',\
                   'youshengxiao-zexiao': u'北京幼升小择校技巧',\
                   'ruxuekaoshizhenti': u'北京幼升小名校试题',\
                   'youshengxiao-mianshi': u'北京幼升小面试辅导',\
                   'youshengxiao-jianli': u'北京幼升小简历',\
                   'beijing-youeryuan/youeryuanruyuanjiaoliu': u'北京幼升小经验交流',\
                   'zhaoshengjianzhang': u'招生简章',\
                   }\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for page in self.zhengce_map:\
            self.crawl('http://www.ysxiao.cn/%s/'%page, save={'label': self.zhengce_map[page]}, callback=self.index_page)\
\
    @config(age=1 * 1)\
    def index_page(self, response):\
        label = response.save['label']\
        for each in response.doc('.desc').items():\
            _dict = {}\
            url = each.find('a').attr.href\
            img = each.find('.hasimg img').attr.src\
            if img:\
                _dict['img'] = 'http://www.ysxiao.cn' + img\
            _dict['date'] = each.find('p').text()\
            _dict['label'] = label\
            self.crawl(url, save=_dict, callback=self.detail_page)\
        '''\
        for each in response.doc('.pages a').items():\
            self.crawl(each.attr.href, save={'label': label}, callback=self.index_page)\
        ''' \
            \
    @config(priority=2)\
    def detail_page(self, response):\
        content_list = []\
        for each in response.doc('p'):\
            _cont =pyquery.PyQuery(each).html()\
            if _cont:\
                _cont = _cont.replace('\\r','').replace('\\n','').replace('\\t','').replace('src="/fup/','src="http://www.ysxiao.cn/fup/')\
                content_list.append((_cont))\
        \
        res_dict = response.save\
        if res_dict['label'] in (u'北京幼升小招生简章', u'幼升小海淀区政策', u'幼升小西城区政策', u'幼升小朝阳区政策', u'幼升小东城区政策', u'幼升小其他城区政策', u'北京幼升小经验交流'):\
            content_list = content_list[:-2]\
        if not content_list:\
            return None\
        res_dict["url"] = response.url\
        res_dict["title"] = response.doc('h1').text()\
        \
        res_dict["content"] = ''.join(['<p>%s</p>'%v for v in content_list])\
        res_dict["tag"] = response.doc('#tags > a').text().split()\
        res_dict['subject'] = u'幼升小'\
        res_dict['bread'] = response.doc('#position a').text().split()\
        res_dict['bread'][0] = res_dict['label']\
        res_dict.pop('label')\
        res_dict['bread'] = list(set(res_dict['bread']))\
        res_dict['source'] = 'ysxiao'\
        res_dict['class'] = 23\
        res_dict['data_weight'] = 0\
        return res_dict\
$$$$$1472441331.6511
zhenti3_iciba$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-07-12 16:53:22\
# Project: zhenti3_iciba\
\
from pyspider.libs.base_handler import *\
\
import sys\
import MySQLdb\
import traceback\
reload(sys)\
sys.setdefaultencoding("utf-8")\
\
conn = MySQLdb.connect(host = "127.0.0.1", user = "root", passwd = "123", db = "cidiandb", charset = "utf8")\
cursor = conn.cursor()\
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        sql = 'select id, word from tb_word where flag = 0 limit 10000'\
        try:\
            cursor.execute(sql)\
            for (ci_id, word,) in cursor.fetchall():\
                self.crawl('http://www.iciba.com/'+word, save = {'ci': word, 'id': ci_id}, callback=self.detail_page)\
        except:\
            traceback.print_exc()\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc('a[href^="http"]').items():\
            self.crawl(each.attr.href, callback=self.detail_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        word = response.save.get('ci').strip()\
        ci_id = response.save.get('id')\
        sql = "update tb_word set flag= 1 where id= %s" % (ci_id)\
        try:           \
            cursor.execute(sql)\
            conn.commit()\
        except Exception, e:\
            print e\
        question = {}\
        for each in response.doc('.cet-tab').items():\
            i = 0\
            for each_li in each.find('.article-list li').items():\
                if u'收起' not in each_li.text() and each_li.text().strip() != '': \
                    question[each_li.text()] = each.find('.article').find('.article-section').eq(i).html().replace('\\n','').replace('\\r','')\
                    i += 1\
                    \
        \
        return {\
            "url": response.url,\
            "title": response.doc('title').text().replace(u'爱词霸',u'跟谁学'),\
            "word": word,\
            "question": question,\
        }$$$$$1468316510.0540
zhongkaocom$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-03 18:29:37\
# Project: zhongkaocom\
\
from pyspider.libs.base_handler import *\
import pyquery\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8') \
\
class Handler(BaseHandler):\
    crawl_config = {\
       \
    }\
    \
    page_config = {\
       \
         'http://www.zhongkao.com/baokao/zkzx/':['政策资讯','中考资讯'],\
         'http://www.zhongkao.com/baokao/zkzc/':['政策资讯','中考政策'],\
\
         'http://www.zhongkao.com/beikao/jyjl/':['复习备考','高分经验'],\
         'http://www.zhongkao.com/beikao/zy/zyjy/':['复习备考','高分经验'],\
\
         'http://www.zhongkao.com/beikao/ys/kqys/':['考前调整','中考饮食'],\
         'http://www.zhongkao.com/beikao/jz/jzbd/':['考前调整','中考家长'],\
         'http://www.zhongkao.com/beikao/xlzd/zkxl/':['考前调整','心理辅导'],\
\
         'http://www.zhongkao.com/beikao/zkfx/ywfx/':['科目指导','中考语文'],\
         'http://www.zhongkao.com/beikao/zkfx/sxfx/':['科目指导','中考数学'],\
         'http://www.zhongkao.com/beikao/zkfx/yyfx/':['科目指导','中考英语'],\
         'http://www.zhongkao.com/beikao/zkfx/wlfx/':['科目指导','中考物理'],\
         'http://www.zhongkao.com/beikao/zkfx/hxfx/':['科目指导','中考化学'],\
         'http://www.zhongkao.com/beikao/zkfx/zzfx/':['科目指导','中考政治'],\
\
         'http://www.zhongkao.com/beikao/zkzw/zkmf/':['真题作文','满分作文'],\
         'http://www.zhongkao.com/beikao/zkzw/zwdp/':['真题作文','作文指导'],\
         'http://www.zhongkao.com/beikao/zkzw/zkyy/':['真题作文','英语作文'],\
         'http://www.zhongkao.com/beikao/zkzw/15zkzw/':['真题作文','作文题目'],\
         'http://www.zhongkao.com/beikao/zkzw/14zkzw/':['真题作文','作文题目']\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
        for k,v in self.page_config.items():\
            self.crawl(k,save = {'bread':v},callback=self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
       for each in response.doc('.bk-item').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            url = each.find('h3').find('a').attr.href\
            _dict['title'] = each.find('h3').find('a').html()\
            _dict['date'] =  each.find('.c-b9').text().split()[0]\
            self.crawl(url,save = _dict ,callback=self.detail_page)         \
       for each in response.doc('.pages > a[href]').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            self.crawl(each.attr.href,save = _dict,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save     \
        content_list = []\
        for info in response.doc('.ft14 > p').items():\
               content = pyquery.PyQuery(info)           \
               items = content.remove('a').html()\
               if items and items.find('推荐')>=0 :\
                    break\
               if items and items.strip():\
                    content_list.append(items.replace('中考网',''))        \
        if not content_list:\
            return None\
        res_dict['url'] = response.url\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list])\
        res_dict['source'] = 'zhongkao.com'\
        res_dict['subject'] = u'中考'\
        res_dict['data_weight'] = 0\
        res_dict['class'] = 33\
        res_dict['title'] = res_dict['title'].replace('中考网','')\
        return res_dict\
$$$$$1463030429.2193
zhongkaocom_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-05-03 18:29:37\
# Project: zhongkaocom\
\
from pyspider.libs.base_handler import *\
import pyquery\
import sys\
reload(sys)\
sys.setdefaultencoding('utf-8') \
\
class Handler(BaseHandler):\
    crawl_config = {\
       \
    }\
    \
    page_config = {\
       \
         'http://www.zhongkao.com/baokao/zkzx/':['政策资讯','中考资讯'],\
         'http://www.zhongkao.com/baokao/zkzc/':['政策资讯','中考政策'],\
\
         'http://www.zhongkao.com/beikao/jyjl/':['复习备考','高分经验'],\
         'http://www.zhongkao.com/beikao/zy/zyjy/':['复习备考','高分经验'],\
\
         'http://www.zhongkao.com/beikao/ys/kqys/':['考前调整','中考饮食'],\
         'http://www.zhongkao.com/beikao/jz/jzbd/':['考前调整','中考家长'],\
         'http://www.zhongkao.com/beikao/xlzd/zkxl/':['考前调整','心理辅导'],\
\
         'http://www.zhongkao.com/beikao/zkfx/ywfx/':['科目指导','中考语文'],\
         'http://www.zhongkao.com/beikao/zkfx/sxfx/':['科目指导','中考数学'],\
         'http://www.zhongkao.com/beikao/zkfx/yyfx/':['科目指导','中考英语'],\
         'http://www.zhongkao.com/beikao/zkfx/wlfx/':['科目指导','中考物理'],\
         'http://www.zhongkao.com/beikao/zkfx/hxfx/':['科目指导','中考化学'],\
         'http://www.zhongkao.com/beikao/zkfx/zzfx/':['科目指导','中考政治'],\
\
         'http://www.zhongkao.com/beikao/zkzw/zkmf/':['真题作文','满分作文'],\
         'http://www.zhongkao.com/beikao/zkzw/zwdp/':['真题作文','作文指导'],\
         'http://www.zhongkao.com/beikao/zkzw/zkyy/':['真题作文','英语作文'],\
         'http://www.zhongkao.com/beikao/zkzw/15zkzw/':['真题作文','作文题目'],\
         'http://www.zhongkao.com/beikao/zkzw/14zkzw/':['真题作文','作文题目']\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
        for k,v in self.page_config.items():\
            self.crawl(k,save = {'bread':v},callback=self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
       for each in response.doc('.bk-item').items():\
            _dict = {}\
            _dict['bread'] = response.save.get('bread',[])\
            url = each.find('h3').find('a').attr.href\
            _dict['title'] = each.find('h3').find('a').html()\
            _dict['date'] =  each.find('.c-b9').text().split()[0]\
            self.crawl(url,save = _dict ,callback=self.detail_page)         \
       #for each in response.doc('.pages > a[href]').items():\
            #_dict = {}\
            #_dict['bread'] = response.save.get('bread',[])\
            #self.crawl(each.attr.href,save = _dict,callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save     \
        content_list = []\
        for info in response.doc('.ft14 > p').items():\
               content = pyquery.PyQuery(info)           \
               items = content.remove('a').html()\
               if content.find('img'):\
                    continue\
               if items and items.find('推荐')>=0 :\
                    break\
               if items and items.strip():\
                    content_list.append(items.replace('中考网',''))        \
        if not content_list:\
            return None\
        res_dict['url'] = response.url\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list if v and v.strip()])\
        res_dict['source'] = 'zhongkao.com'\
        res_dict['subject'] = u'中考'\
        res_dict['data_weight'] = 0\
        res_dict['class'] = 33\
        res_dict['title'] = res_dict['title'].replace('中考网','')\
        return res_dict\
$$$$$1471343989.9680
zhongkaosian_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-25 17:44:23\
# Project: zhongkao_sina\
\
from pyspider.libs.base_handler import *\
import pyquery\
reload(sys)\
sys.setdefaultencoding('utf-8') \
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    page_dict = {\
    	'http://roll.edu.sina.com.cn/lm/zk/zkzx/index.shtml':['政策资讯','中考资讯'],\
        'http://roll.edu.sina.com.cn/more/zk/fxjq/index.shtml':['复习备考','复习技巧'],\
        'http://roll.edu.sina.com.cn/more/zk/gfjy/index.shtml':['复习备考','高分经验'],\
        'http://roll.edu.sina.com.cn/lm/zk/zkzy/index.shtml':['复习备考','高分经验'],\
        'http://roll.edu.sina.com.cn/more/zk/zkzd/czyw/index.shtml':['科目指导','中考语文'],\
        'http://roll.edu.sina.com.cn/more/zk/zkzd/czsx/index.shtml':['科目指导','中考数学'],\
        'http://roll.edu.sina.com.cn/more/zk/zkzd/czyy/index.shtml':['科目指导','中考英文'],\
        'http://roll.edu.sina.com.cn/more/zk/zkst/index.shtml':['中考试题','模拟试题'],\
        'http://roll.edu.sina.com.cn/lm/zk/fxjq/index.shtml':['复习备考','复习技巧'],\
    }\
\
    @every(minutes=1 * 60)\
    def on_start(self):\
          for k,v in self.page_dict.iteritems():\
            self.crawl(k,save={'bread': v}, callback = self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc("li").items():\
             _dict = {}\
             url = each.find('a').attr.href \
             _dict['bread'] = response.save.get('bread',[])\
             _dict['title'] = each.find('a').html()\
             _dict['date'] = each.find('span').html().split()[0].replace('(','').replace('年','－').replace('月','－').replace('日','')\
             self.crawl(url, save=_dict, callback=self.detail_page)     \
        #for each in response.doc(".pagebox_num > a[href] , .pagebox_next > a[href]").items():  \
             #_dict = {}\
             #_dict['bread'] = response.save.get('bread',[])\
             #self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save     \
        content_list = []\
        for info in response.doc('#artibody > p').items():\
            content = pyquery.PyQuery(info)\
            items = content.remove('a').html()\
            if content.find('img'):\
                continue\
            if items and items.find('推荐')>=0 :\
                break\
            if  items and (items.find('来源：')>=0 or items.find('更多信息请访问')>=0):\
                break\
            content_list.append(items.replace('新浪',''))\
        if not content_list:\
            return None\
        res_dict['url'] = response.url\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list if v and v.strip()])\
        res_dict['source'] = 'edu.sina.com.cn'\
        res_dict['subject'] = u'中考'\
        res_dict['class'] = 33\
        res_dict['data_weight'] = 0\
        res_dict['title']  = res_dict['title'].replace('新浪','')\
        return res_dict\
$$$$$1471343960.4557
zhongkaoxdfcn$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-25 17:44:23\
# Project: zhongkaoxdfcn\
\
from pyspider.libs.base_handler import *\
import pyquery\
reload(sys)\
sys.setdefaultencoding('utf-8') \
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    page_dict = {\
    	'http://zhongkao.xdf.cn/list_7995_1.html':['政策资讯','志愿填报'],\
        'http://zhongkao.xdf.cn/list_4512_1.html':['政策资讯','中考资讯'],\
        'http://zhongkao.xdf.cn/list_1015_1.html':['科目指导','中考历史'],\
        'http://zhongkao.xdf.cn/list_1014_1.html':['科目指导','中考地理'],\
        'http://zhongkao.xdf.cn/list_1011_1.html':['科目指导','中考生物'],\
\
        'http://zhongkao.xdf.cn/list_1016_1.html':['中考试题','模拟试题'],\
        'http://zhongkao.xdf.cn/list_6132_1.html':['中考试题','期中末试题'],\
        'http://zhongkao.xdf.cn/list_1017_1.html':['中考试题','历年真题'],\
\
\
        'http://zhongkao.xdf.cn/list_1507_1.html':['考前调整','中考家长'],\
        'http://zhongkao.xdf.cn/list_1509_1.html':['考前调整','心理辅导'],\
        'http://zhongkao.xdf.cn/list_1510_1.html':['考前调整','中考饮食'],\
\
        'http://zhongkao.xdf.cn/list_1515_1.html':['复习备考','高分经验'],\
        \
        'http://zhongkao.xdf.cn/list_4809_1.html':['真题作文','英语作文'],\
        'http://zhongkao.xdf.cn/list_1512_1.html':['真题作文','作文题目'],\
\
        'http://zhongkao.xdf.cn/list_1000_1.html':['复习备考','备考策略']\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
          for k,v in self.page_dict.iteritems():\
            self.crawl(k,save={'bread': v}, callback = self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc(".txt_lists01 > li").items():\
             _dict = {}\
             url = each.find('a').attr.href \
             _dict['bread'] = response.save.get('bread',[])\
             _dict['title'] = each.find('a').html()\
             _dict['date'] = each.find('cite').html()\
             self.crawl(url, save=_dict, callback=self.detail_page)     \
        for each in response.doc(".ch_conpage a[href\$='.html']").items():  \
             _dict = {}\
             _dict['bread'] = response.save.get('bread',[])\
             self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save     \
        content_list = []\
        for info in response.doc('.air_con p').items():\
            content = pyquery.PyQuery(info)\
            items = content.remove('a').html()\
            if items and (items.find('推荐')>=0 or items.find('编辑')>=0):\
                    break\
            content_list.append(items.replace('新东方',''))\
        if not content_list:\
            return None\
        res_dict['url'] = response.url\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list])\
        res_dict['source'] = 'zhongkao.xdf.cn'\
        res_dict['subject'] = u'中考'\
        res_dict['class'] = 33\
        res_dict['data_weight'] = 0\
        res_dict['title']  = res_dict['title'].replace('新东方','')\
        return res_dict\
$$$$$1463023518.1454
zhongkaoxdfcn_inc$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-25 17:44:23\
# Project: zhongkaoxdfcn\
\
from pyspider.libs.base_handler import *\
import pyquery\
reload(sys)\
sys.setdefaultencoding('utf-8') \
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    page_dict = {\
    	'http://zhongkao.xdf.cn/list_7995_1.html':['政策资讯','志愿填报'],\
        'http://zhongkao.xdf.cn/list_4512_1.html':['政策资讯','中考资讯'],\
        'http://zhongkao.xdf.cn/list_1015_1.html':['科目指导','中考历史'],\
        'http://zhongkao.xdf.cn/list_1014_1.html':['科目指导','中考地理'],\
        'http://zhongkao.xdf.cn/list_1011_1.html':['科目指导','中考生物'],\
\
        'http://zhongkao.xdf.cn/list_1016_1.html':['中考试题','模拟试题'],\
        'http://zhongkao.xdf.cn/list_6132_1.html':['中考试题','期中末试题'],\
        'http://zhongkao.xdf.cn/list_1017_1.html':['中考试题','历年真题'],\
\
\
        'http://zhongkao.xdf.cn/list_1507_1.html':['考前调整','中考家长'],\
        'http://zhongkao.xdf.cn/list_1509_1.html':['考前调整','心理辅导'],\
        'http://zhongkao.xdf.cn/list_1510_1.html':['考前调整','中考饮食'],\
\
        'http://zhongkao.xdf.cn/list_1515_1.html':['复习备考','高分经验'],\
        \
        'http://zhongkao.xdf.cn/list_4809_1.html':['真题作文','英语作文'],\
        'http://zhongkao.xdf.cn/list_1512_1.html':['真题作文','作文题目'],\
\
        'http://zhongkao.xdf.cn/list_1000_1.html':['复习备考','备考策略']\
    }\
\
    @every(minutes=1*60)\
    def on_start(self):\
          for k,v in self.page_dict.iteritems():\
            self.crawl(k,save={'bread': v}, callback = self.index_page)\
\
    @config(age=1*1)\
    def index_page(self, response):\
        for each in response.doc(".txt_lists01 > li").items():\
             _dict = {}\
             url = each.find('a').attr.href \
             _dict['bread'] = response.save.get('bread',[])\
             _dict['title'] = each.find('a').html()\
             _dict['date'] = each.find('cite').html()\
             self.crawl(url, save=_dict, callback=self.detail_page)     \
        #for each in response.doc(".ch_conpage a[href\$='.html']").items():  \
             #_dict = {}\
             #_dict['bread'] = response.save.get('bread',[])\
             #self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save     \
        content_list = []\
        for info in response.doc('.air_con p').items():\
            content = pyquery.PyQuery(info)\
            items = content.remove('a').html()\
            if content.find('img'):\
                continue\
            if items and (items.find('推荐')>=0 or items.find('编辑')>=0):\
                    break\
            content_list.append(items.replace('新东方',''))\
        if not content_list:\
            return None\
        res_dict['url'] = response.url\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list if v and v.strip()])\
        res_dict['source'] = 'zhongkao.xdf.cn'\
        res_dict['subject'] = u'中考'\
        res_dict['class'] = 33\
        res_dict['data_weight'] = 0\
        res_dict['title']  = res_dict['title'].replace('新东方','')\
        return res_dict\
$$$$$1471343991.9025
zhongkao_sina$$$$$#!/usr/bin/env python\
# -*- encoding: utf-8 -*-\
# Created on 2016-02-25 17:44:23\
# Project: zhongkao_sina\
\
from pyspider.libs.base_handler import *\
import pyquery\
reload(sys)\
sys.setdefaultencoding('utf-8') \
\
class Handler(BaseHandler):\
    crawl_config = {\
    }\
    \
    page_dict = {\
    	'http://roll.edu.sina.com.cn/lm/zk/zkzx/index.shtml':['政策资讯','中考资讯'],\
        'http://roll.edu.sina.com.cn/more/zk/fxjq/index.shtml':['复习备考','复习技巧'],\
        'http://roll.edu.sina.com.cn/more/zk/gfjy/index.shtml':['复习备考','高分经验'],\
        'http://roll.edu.sina.com.cn/lm/zk/zkzy/index.shtml':['复习备考','高分经验'],\
        'http://roll.edu.sina.com.cn/more/zk/zkzd/czyw/index.shtml':['科目指导','中考语文'],\
        'http://roll.edu.sina.com.cn/more/zk/zkzd/czsx/index.shtml':['科目指导','中考数学'],\
        'http://roll.edu.sina.com.cn/more/zk/zkzd/czyy/index.shtml':['科目指导','中考英文'],\
        'http://roll.edu.sina.com.cn/more/zk/zkst/index.shtml':['中考试题','模拟试题'],\
        'http://roll.edu.sina.com.cn/lm/zk/fxjq/index.shtml':['复习备考','复习技巧'],\
    }\
\
    @every(minutes=24 * 60)\
    def on_start(self):\
          for k,v in self.page_dict.iteritems():\
            self.crawl(k,save={'bread': v}, callback = self.index_page)\
\
    @config(age=10 * 24 * 60 * 60)\
    def index_page(self, response):\
        for each in response.doc("li").items():\
             _dict = {}\
             url = each.find('a').attr.href \
             _dict['bread'] = response.save.get('bread',[])\
             _dict['title'] = each.find('a').html()\
             _dict['date'] = each.find('span').html().split()[0].replace('(','').replace('年','－').replace('月','－').replace('日','')\
             self.crawl(url, save=_dict, callback=self.detail_page)     \
        for each in response.doc(".pagebox_num > a[href] , .pagebox_next > a[href]").items():  \
             _dict = {}\
             _dict['bread'] = response.save.get('bread',[])\
             self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\
\
    @config(priority=2)\
    def detail_page(self, response):\
        res_dict = response.save     \
        content_list = []\
        for info in response.doc('#artibody > p').items():\
            content = pyquery.PyQuery(info)\
            items = content.remove('a').html() \
            if items and items.find('推荐')>=0 :\
                break\
            if  items and (items.find('来源：')>=0 or items.find('更多信息请访问')>=0):\
                break\
            content_list.append(items.replace('新浪',''))\
        if not content_list:\
            return None\
        res_dict['url'] = response.url\
        res_dict['content'] = ''.join(["<p>%s</p>"%v for v in content_list])\
        res_dict['source'] = 'edu.sina.com.cn'\
        res_dict['subject'] = u'中考'\
        res_dict['class'] = 33\
        res_dict['data_weight'] = 0\
        res_dict['title']  = res_dict['title'].replace('新浪','')\
        return res_dict\
$$$$$1463030432.8517
