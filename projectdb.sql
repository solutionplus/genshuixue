-- MySQL dump 10.13  Distrib 5.1.73, for redhat-linux-gnu (x86_64)
--
-- Host: localhost    Database: projectdb
-- ------------------------------------------------------
-- Server version	5.1.73

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `projectdb`
--

DROP TABLE IF EXISTS `projectdb`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `projectdb` (
  `name` varchar(64) NOT NULL,
  `group` varchar(64) DEFAULT NULL,
  `status` varchar(16) DEFAULT NULL,
  `script` text,
  `comments` varchar(1024) DEFAULT NULL,
  `rate` float(11,4) DEFAULT NULL,
  `burst` float(11,4) DEFAULT NULL,
  `updatetime` double(16,4) DEFAULT NULL,
  PRIMARY KEY (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `projectdb`
--

LOCK TABLES `projectdb` WRITE;
/*!40000 ALTER TABLE `projectdb` DISABLE KEYS */;
INSERT INTO `projectdb` VALUES ('21cn_org','jigou','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-17 15:01:32\n# Project: 21cn_org\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://edu.21cn.com/news/list_train/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'div.cl_right ul li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').eq(1).text()\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'/\',\'-\')\n            url = each.find(\'a\').eq(1).attr.href\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'div.page a\').items():\n            self.crawl(each.attr.href,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.clt_cont > p\').items():\n            info = each.html()\n            if info:\n                content_list.append(info)\n       for each in response.doc(\'div#p_content > p\').items():\n            info = each.html()\n            if info:\n                content_list.append(info)\n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'class\'] = 46\n       res_dict[\'subject\'] = \'机构\'\n       res_dict[\'source\'] = \'21cn.com\'\n       res_dict[\'bread\'] = [u\'文章资讯\']\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1472615580.0742),('233jzs','jzs','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 16:41:08\n# Project: 233kuaiji_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport re\n\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n       \'http://www.233.com/jzs2/sggl/fudao/\':[\'建筑工程\',\'辅导资料\'],\n       \'http://www.233.com/jzs2/sggl/moniti/\':[\'建筑工程\',\'模拟试题\'],\n       \'http://www.233.com/jzs2/sggl/zhenti/\':[\'建筑工程\',\'历年真题\'],\n       \'http://www.233.com/jzs2/law/fudao/\':[\'工程法规\',\'辅导资料\'],\n       \'http://www.233.com/jzs2/law/moniti/\':[\'工程法规\',\'模拟试题\'],\n       \'http://www.233.com/jzs2/law/zhenti/\':[\'工程法规\',\'历年真题\'],\n       \'http://www.233.com/jzs2/jzgc/\':[\'管理实务\',\'建筑工程\'],\n       \'http://www.233.com/jzs2/szfgc/\':[\'管理实务\',\'市政工程\'],\n       \'http://www.233.com/jzs2/jdgc/\':[\'管理实务\',\'机电工程\'],\n       \'http://www.233.com/jzs2/glgc/\':[\'管理实务\',\'公路工程\'],\n       \'http://www.233.com/jzs2/kygc/\':[\'管理实务\',\'矿业工程\'],\n       \'http://www.233.com/jzs2/slgc/\':[\'管理实务\',\'水电工程\']\n        \n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'ul.fl li\').items():\n            url = each.find(\'a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'dl > dd\').items():\n            url = each.find(\'h4 > a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h4 > a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'.column-info span.fl\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'div.pagebox > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        content_list = []\n        for each in response.doc(\'div.news-body > p\').items():\n            info = each.html()\n            if info and (u\'编辑推荐\' in info or u\'手机用户\' in info or u\'小编\' in info):\n                break\n            if info and (u\'二维码\' in info):\n                break\n            if info and (u\'233网校\' in info):\n                continue         \n            if info:\n                content_list.append(removeLink(info.replace(\'233网校\',\'\')))\n        if not content_list:\n            return\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'233.com\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = \'二级建造师\'\n        return res_dict\n',NULL,1.0000,3.0000,1472615562.4086),('233jzs_inc','jzs','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 16:41:08\n# Project: 233kuaiji_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport re\n\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n       \'http://www.233.com/jzs2/sggl/fudao/\':[\'建筑工程\',\'辅导资料\'],\n       \'http://www.233.com/jzs2/sggl/moniti/\':[\'建筑工程\',\'模拟试题\'],\n       \'http://www.233.com/jzs2/sggl/zhenti/\':[\'建筑工程\',\'历年真题\'],\n       \'http://www.233.com/jzs2/law/fudao/\':[\'工程法规\',\'辅导资料\'],\n       \'http://www.233.com/jzs2/law/moniti/\':[\'工程法规\',\'模拟试题\'],\n       \'http://www.233.com/jzs2/law/zhenti/\':[\'工程法规\',\'历年真题\'],\n       \'http://www.233.com/jzs2/jzgc/\':[\'管理实务\',\'建筑工程\'],\n       \'http://www.233.com/jzs2/szfgc/\':[\'管理实务\',\'市政工程\'],\n       \'http://www.233.com/jzs2/jdgc/\':[\'管理实务\',\'机电工程\'],\n       \'http://www.233.com/jzs2/glgc/\':[\'管理实务\',\'公路工程\'],\n       \'http://www.233.com/jzs2/kygc/\':[\'管理实务\',\'矿业工程\'],\n       \'http://www.233.com/jzs2/slgc/\':[\'管理实务\',\'水电工程\']\n        \n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'ul.fl li\').items():\n            url = each.find(\'a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'dl > dd\').items():\n            url = each.find(\'h4 > a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h4 > a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'.column-info span.fl\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        #for each in response.doc(\'div.pagebox > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\')\n            #self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        content_list = []\n        for each in response.doc(\'div.news-body > p\').items():\n            info = each.html()\n            if info and (u\'编辑推荐\' in info or u\'手机用户\' in info or u\'小编\' in info):\n                break\n            if info and (u\'二维码\' in info):\n                break\n            if info and (u\'233网校\' in info):\n                continue         \n            if info:\n                content_list.append(removeLink(info.replace(\'233网校\',\'\')))\n        if not content_list:\n            return\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'233.com\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = \'二级建造师\'\n        return res_dict\n',NULL,1.0000,3.0000,1472615630.9237),('233kuaiji','kuaiji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 16:41:08\n# Project: 233kuaiji_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport re\n\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n        \'http://www.233.com/cy/zhidao/jichu/fudao/\':[\'会计基础\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/jichu/\':[\'会计基础\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/jichu/\':[\'会计基础\',\'历年真题\'],\n        \'http://www.233.com/cy/zhidao/daode/fudao/\':[\'财经法规\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/law/\':[\'财经法规\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/law/\':[\'财经法规\',\'历年真题\'],\n        \'http://www.233.com/cy/zhidao/diansuanhua/fudao/\':[\'会计电算化\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/diansuanhua/\':[\'会计电算化\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/diansuanhua/\':[\'会计电算化\',\'历年真题\'],\n\n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'ul.column-list-bd li\').items():\n            url = each.find(\'a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'dl > dd\').items():\n            url = each.find(\'h4 > a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h4 > a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'.column-info span.fl\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        #for each in response.doc(\'div.pagebox > a\').items():\n        #    _dict = {}\n        #    _dict[\'bread\'] = response.save.get(\'bread\')\n        #    self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        content_list = []\n        for each in response.doc(\'div.news-body > p\').items():\n            info = each.html()\n            if u\'编辑推荐\' in info or u\'手机用户\' in info or u\'小编\' in info:\n                break\n            if info:\n                content_list.append(removeLink(info.replace(\'233网校\',\'\')))\n        if not content_list:\n            return\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'233.com\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = \'会计\'\n        return res_dict\n',NULL,1.0000,3.0000,1472615680.2540),('233kuaiji_inc','kuaiji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 16:41:08\n# Project: 233kuaiji_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport re\n\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n        \'http://www.233.com/cy/zhidao/jichu/fudao/\':[\'会计基础\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/jichu/\':[\'会计基础\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/jichu/\':[\'会计基础\',\'历年真题\'],\n        \'http://www.233.com/cy/zhidao/daode/fudao/\':[\'财经法规\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/law/\':[\'财经法规\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/law/\':[\'财经法规\',\'历年真题\'],\n        \'http://www.233.com/cy/zhidao/diansuanhua/fudao/\':[\'会计电算化\',\'考试辅导\'],\n        \'http://www.233.com/cy/moniti/diansuanhua/\':[\'会计电算化\',\'模拟试题\'],\n        \'http://www.233.com/cy/zhenti/diansuanhua/\':[\'会计电算化\',\'历年真题\'],\n\n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'ul.column-list-bd li\').items():\n            url = each.find(\'a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'dl > dd\').items():\n            url = each.find(\'h4 > a\').attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h4 > a\').text().replace(\'233网校\',\'\')\n            _dict[\'date\'] = each.find(\'.column-info span.fl\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            self.crawl(url,save = _dict ,callback=self.detail_page)\n        for each in response.doc(\'div.pagebox > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict ,callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        content_list = []\n        for each in response.doc(\'div.news-body > p\').items():\n            info = each.html()\n            if u\'编辑推荐\' in info or u\'手机用户\' in info or u\'小编\' in info:\n                break\n            if info:\n                content_list.append(removeLink(info.replace(\'233网校\',\'\')))\n        if not content_list:\n            return\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'233.com\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = \'会计\'\n        return res_dict\n',NULL,2.0000,3.0000,1472615636.6152),('360_img','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-29 15:24:09\n# Project: 360_img\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        with open(\'/apps/home/rd/hexing/data/zxk\') as f:\n            for line in f:\n                if line:\n                    line = line.strip()\n                    self.crawl(\'http://image.so.com/j?q=\'+line+\'&src=sr&sn=0&pn=10\',save = {\'school_name\':line} ,headers = self.headers,callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n       res_list =  response.json[\'list\']\n       res_dict = {}\n       _list = []\n       for each in res_list:          \n            if each.get(\'img\',\'\'):\n                _list.append(each.get(\'img\'))\n       if _list:\n                res_dict[\'school_name\'] = response.save.get(\'school_name\')\n                res_dict[\'imgs\'] = _list\n                return res_dict\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1472624676.5851),('51job_shixisheng','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-14 14:43:24\n# Project: qiuzhi_51job\n\nfrom pyspider.libs.base_handler import *\nimport traceback\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for i in range(1, 28):\n            self.crawl(\'http://search.51job.com/list/\' + \'%.2d\'%i + \'0000,000000,0000,00,4,99,%2B,2,1.html?lang=c&stype=1&postchannel=0100&workyear=99&cotype=99&degreefrom=99&jobterm=03&companysize=99&lonlat=0%2C0&radius=-1&ord_field=0&confirmdate=9&fromType=&dibiaoid=0&address=&line=&specialarea=00&from=\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.dw_table > .el\').items():\n            title = \'【%s】招聘%s实习生\'%(each.find(\'.t2\').text(), each.find(\'.tg a\').text())\n            #print title\n            date = \'2016-\' + each.find(\'.t5\').text()\n            self.crawl(each.find(\'.tg a\').attr.href,save={\'title\': title, \'date\': date}, callback=self.detail_page)\n        #for each in response.doc(\'.dw_page a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        try:\n            job_msg = response.doc(\'.job_msg\').remove(\'.i_share\').remove(\'.i_note\').html()\n            tmsg = response.doc(\'.tmsg\').html()\n            content = \'%s<p>%s</p>\'%(job_msg, tmsg)\n            content = content.replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n        except Exception, e:\n            traceback.print_exc()\n            return None\n        #print job_msg\n        #print tmsg\n        res[\'content\'] = content\n        res[\"data_weight\"] = 0\n        res[\"subject\"] = u\'求职\'\n        res[\"bread\"] = [u\'实习生\',]\n        res[\"class\"] = 46\n        res[\"source\"] = u\'51job\'\n        return res',NULL,1.0000,3.0000,1471333013.9522),('51sxue_xiaoxue_youshengxiao','xiaoxue','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-15 10:58:27\n# Project: 51sxue_xinwen\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://news.51sxue.com/newsList/classId_65.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.newslistcon li a\').items():\n            \n            self.crawl(each.attr.href, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.page_link\').items():\n            \n            self.crawl(each.attr.href, callback=self.index_page)\n        \'\'\'\n    @config(priority=2)\n    @config(age=1 * 1)\n    def detail_page(self, response):\n        content_list = [v.text() for v in response.doc(\'.news_main > p\').items()]\n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.news > h3\').text(),\n            \"date\": response.doc(\'.news_list > li\').eq(1).text()[:10],\n            \"bread\": [u\'小升初\',],\n            \"content\": \'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n            \"subject\": u\'小学\',\n            \"class\": 33,\n            \"source\": \"51sxue\",\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,1.0000,1472440787.3509),('51sxue_zhongxue','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-23 09:28:49\n# Project: 51sxue_zhongxue\n\nfrom pyspider.libs.base_handler import *\nimport requests\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nfrom pyquery import PyQuery as pq\nimport urllib2\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://xuexiao.51sxue.com/slist/?o=&t=3&areaCodeS=&level=&sp=&score=&order=&areaS=%C8%AB%B9%FA&searchKey=\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.reply_box\').items():\n            _dict = {}\n            _dict[\'school_logo\'] = each.find(\'.school_m_img img\').attr.src\n            _dict[\'school_name\'] = each.find(\'.school_m_img a\').attr.title\n            for info in each.find(\'.school_m_main li\').items():\n                 content = info.text()                 \n                 if content and u\': \' not in content:\n                        _dict[\'school_name\'] = content\n                 elif content and u\': \'  in content:\n                      arr = content.split(\':\')\n                      if u\'地区\' in arr[0]:\n                          _dict[\'province\'] = arr[1].split()[0]\n                          _dict[\'city\'] = arr[1].split()[1].replace(\'市\',\'\')\n                      if u\'属性\' in arr[0]:\n                          _dict[\'school_type\'] = _dict.get(\'school_type\',\'\') + arr[1]\n                      if u\'性质\' in arr[0]:\n                          _dict[\'school_type\'] = _dict.get(\'school_type\',\'\') + arr[1]  \n                          _dict[\'school_type\'] = _dict[\'school_type\'].replace(\'类型\',\'\').strip()\n            _dict[\'address\'] = each.find(\'.school_m_lx .school_dz b\').text()\n            _dict[\'phone\'] = each.find(\'.school_m_lx .school_telephone b\').text()\n            url = each.find(\'.school_m_main h3 a\').attr.href\n            self.crawl(url, save = _dict,callback=self.detail_page)\n            \n        for each in response.doc(\'a.page_link\').items():\n            self.crawl(each.attr.href, save = _dict,callback=self.index_page)    \n   \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save    \n        school_summary =  pq(urllib2.urlopen(response.url.replace(\'detail\',\'content\')).read())\n        res_dict[\'school_summary\'] = school_summary.find(\'.nr_m\').remove(\'img\').remove(\'div\').html()  or \'\'\n        enrollment_condition = pq(urllib2.urlopen(response.url.replace(\'detail\',\'zhaosheng\')).read())\n        res_dict[\'enrollment_condition\'] = enrollment_condition.find(\'.nr_m\').remove(\'img\').remove(\'div\').html() or \'\'\n        res_dict[\'school_url\'] = response.url\n        res_dict[\'site\'] = response.doc(\'#webSite\').text() or \'\'\n        res_dict[\'charge_situation\'] = response.doc(\'.school_y_sf\').text() or \'\'\n        res_dict[\'feature_course\'] = response.doc(\'.school_y_kc\').text() or \'\'\n        school_environment = []    \n        for each in response.doc(\'.school_img_main\').eq(0).find(\'img\').items():\n                school_environment.append(each.attr.src)\n        res_dict[\'school_environment\'] = school_environment\n        #print len(response.doc(\'.school_t\'))\n        #res_dict[\'academic_achievement\']  = response.doc(\'.school_t\').eq(len(response.doc(\'.school_t\')) -1).html() or \'\'\n        return res_dict',NULL,1.0000,3.0000,1472624680.9580),('acca_accazhongguo','jr','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-18 17:38:49\n# Project: acca_accazhongguo\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://cn.accaglobal.com/news/news.html\', save = {\'bread\': [\'ACCA\']}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.newsMContentRMenu > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').eq(1).text()\n            _dict[\'date\'] = each.find(\'.mainList1letter\').eq(0).text()\n            _dict[\'cover\'] = each.find(\'a > img\').attr.src\n            self.crawl(each.find(\'a\').eq(1).attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.next > a\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        cover = response.save.get(\'cover\') \n        pattern = re.compile(r\'src=\".*?\"\')\n        _list = []\n        for each in response.doc(\'.mainList2RCont pre\').children().items():\n            if not each.html():\n                continue\n            #print each.html()\n            if \'<script>\' in each.html():\n                continue\n            if u\'我要纠错\' in each.text() or u\'责任编辑\' in each.text() or u\'编辑推荐\' in each.text() or u\'点击阅读\' in each.text():\n                break\n            if \'<img\' in each.html():\n                if cover == \'\':\n                    cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                _list.append(\'<p>\'+ each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        \n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"cover\": cover,\n            \"content\": content,\n            \"source\": u\'ACCA中国\',\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"金融\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1472615661.6357),('allgwy_zhonggong','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-08 18:37:37\n# Project: allgwy_zhonggong\n\nfrom pyspider.libs.base_handler import *\n\nadd_words = [\n    \'bj\',\n    \'sh\',\n    \'sd\',\n    \'js\',\n    \'zj\',\n    \'ah\',\n    \'jl\',\n    \'fj\',\n    \'gd\',\n    \'gx\',\n    \'hn\',\n    \'tj\',\n    \'hb\',\n    \'hlj\',\n    \'sx\',\n    \'gs\',\n    \'hu\',\n    \'hn\',\n    \'he\',\n    \'sc\',\n    \'cq\',\n    \'yn\',\n    \'gz\',\n    \'xz\',\n    \'nx\',\n    \'xj\',\n    \'qh\',\n    \'sx\',\n    \'ln\',\n    \'jx\',\n    \'nm\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for word in add_words:\n            self.crawl(\'http://www.offcn.com/\'+word+\'gwy/ziliao/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.zg_lm_list > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.zg_lm_2\').text()\n            _dict[\'date\'] = each.find(\'font\').text()\n            self.crawl(each.find(\'.zg_lm_2\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.a1\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):        \n        return {\n            \"url\": response.url,\n            \'bread\': [u\'公务员\'],\n            \'title\': response.save.get(\'title\'),\n            \'date\': response.save.get(\'date\'),\n            \"html\": response.doc(\'.zg_show_word\').html(),\n            \'source\': u\'中公\',\n            \'class\': 36,\n            \'subject\': u\'经验\',\n            \'data_weight\': 0,\n        }\n',NULL,5.0000,5.0000,1472624466.7385),('baidu_index_tiku',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-22 19:59:25\n# Project: baidu_index_tiku\n\nfrom pyspider.libs.base_handler import *\nimport MySQLdb\nimport datetime\n\nconn = MySQLdb.connect(host=\"127.0.0.1\",user=\"root\",passwd=\"123\",port=3306)\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n\n    crawl_config = {\n        \'headers\': {\n            \'Cookie\': \'PSTM=1464750941; BIDUPSID=E86D6D1C5EB675F2CB9CF17431C6D0F8; BDUSS=1h6VmViVUNZcVdVMUk1fnVCV1IzR1ktVWFCQ0ZFUi1LcENJZWRVeXlHWm52MzFYQVFBQUFBJCQAAAAAAAAAAAEAAACa-JFPsNm80rulwaq4-sut0acAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGcyVldnMlZXa; BDSFRCVID=Er4sJeCCxG3CHUcRDzRi_30n8YUjwFHt67Bc3J; H_BDCLCKID_SF=JbIDVCIytDvWe6rxMtTJ-tCE-fTMetJyaR3Wh45bWJ5TMCoLKbOhy5twQh6NqPviWmnNXnAbafP-ShPC-tPV0PDj3n6r2nbCtaRtKUjy3l02VbLRhhQ2Wf3DMMRfatRMW23r0h7mWU5GVKFCe5-Kjj5QepJf-K6hKCoM0n-8Kb7V-P5mhqn5hnLX-U__5to05TTdQbn7KtoHDR-I5JOVBPu8bMrt26ra-D6X0U7tKRTffjQG5-cr-P4H5MoX-Tra2C6yLnIQ34_MqpcNLUbWQTtdybo2Qf3ELD5doKJ8fUTlj43Dyb8-jUDSDPCE5bj2qRFHoC0M3J; lsv=globalTjs_e63380f-wwwTcss_941ce39-routejs_6ede3cf-activityControllerjs_b6f8c66-wwwBcss_cd6b841-framejs_3109ba6-globalBjs_1d5cdae-sugjs_93b1335-wwwjs_609a8a4; H_WISE_SIDS=104381_103996_107047_100615_100040_106465_102431_107196_100098_107290_107285_106666_104340_107065_107185_103759_103999_106926_106890_104671_107325_107116_107042_104613_104638_107044_107060_107092_106795_100458; MSA_WH=320_568; MSA_PBT=92; MSA_ZOOM=1000; BAIDUID=12455DCFF0A2EEB24A1668FB3AC77E9E:FG=1; BD_HOME=1; BDRCVFR[feWj1Vr5u3D]=mk3SLVN4HKm; BD_CK_SAM=1; H_PS_PSSID=20145_18286_1453_20318_18280_20368_20388_19690_20417_19861_15142_11478; BD_UPN=123253; sug=3; sugstore=1; ORIGIN=2; bdime=0; H_PS_645EC=e22ao6ItMqNOF%2BuE7PqRhAQIo36xqOHl9e4ph0bKrJAbiYbAH8SYeru51SMIZV6tCJ%2Bk\',\n        \'Host\':\'www.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\',\n        }\n    }\n\n    @every(minutes=1 * 3)\n    def on_start(self):\n        sql = \"select url_id from zhanqundb.baidu_recruit_info where spider=0 limit 10000\"\n        try:\n            cursor.execute(sql)\n            for (url_id, ) in cursor.fetchall():\n                url = \'http://www.genshuixue.com/tiku/%s.html\'%(url_id)\n                baidu_url = \'https://www.baidu.com/s?wd=\' + url + \'&rsv_spt=1&rsv_iqid=0xdd34d799000450a7&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=7&rsv_sug1=4&rsv_sug7=100&rsv_sug2=0&inputT=1411&rsv_sug4=1412\'\n                self.crawl(baidu_url, save={\'url\': url, \'id\': url_id}, callback=self.detail_page)\n        except Exception, e:\n            print e\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        num = 0\n        for each in response.doc(\'#content_left\').items():\n            num += 1\n        res = response.save\n        \n        if num > 0:\n            sql = \"update zhanqundb.baidu_recruit_info set spider=1, recruit=1, recruit_date=\'%s\' where url_id=\'%s\'\"%(	datetime.datetime.now().strftime(\"%Y-%m-%d\"), res[\'id\'])\n        else:\n            sql = \"update zhanqundb.baidu_recruit_info set spider=1 where url_id=\'%s\'\"%res[\'id\']\n        print sql\n        \n        cursor.execute(sql)\n        conn.commit()\n        res[\'num\'] = num\n        return res\n',NULL,10.0000,10.0000,1468039519.2148),('baidu_jingyan','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-25 16:12:34\n# Project: jingyan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"headers\":{\n        \'Host\':\'jingyan.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\',\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for line in open(\'/apps/home/worker/xuzhihao/jingyan/tags\'):\n            tag = line.strip(\'\\n\')\n            self.crawl(\'http://jingyan.baidu.com/tag?tagName=\'+tag, save={\'tag\': tag},  callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'dt > a\').items():\n            self.crawl(each.attr.href, save={\'tag\': response.save[\'tag\']}, callback=self.detail_page)\n        for each in response.doc(\'.pager a\').items():\n            self.crawl(each.attr.href, save={\'tag\': response.save[\'tag\']}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        answers = []\n        all_content = [v for v in response.doc(\'.exp-content-block\').items()]\n        if len(all_content) == 1:\n            \'\'\'\n            title = [v.text() for v in all_content[0].find(\'h2\').items()]\n            data = [v.text() for v in all_content[0].find(\'p\').items() if v.text().replace(u\'&nbsp; &nbsp;&nbsp;\',\'\')]\n            if len(data) > len(title):\n                data = data[1:]\n            answers = [{\'title\': t, \"steps\": [v,]} for t in title for v in data]\n            print len(title),len(data)\n            abstract = {}\n            print all_content[0].html()\n            \'\'\'\n            answers = {\'title\': \'\', \'data\': [all_content[0].find(\'.content-listblock-text\').remove(\'img\').html(),]}\n            abstract = {}\n        else:\n            for each in all_content[1:]:\n                title = each.find(\'.exp-content-head\').text()\n                _list = []\n                for step in each.find(\'li\').items():\n                    try:\n                        img = step.find(\'a\').html().split(\'data-src=\"\')[-1].split(\'\"\')[0]\n                    except:\n                        img = \'\'\n                    _list.append({\n                        \"img\": img,\n                        \"title\": step.text(),\n                        \"steps\": [],\n                    })\n                if not _list:\n                    _list.append((each.find(\'.content-listblock-text\').text()))\n                answers.append({\"title\": title, \"data\": _list})\n            try:\n                question_desc_img = all_content[0].find(\'.content-listblock-image\').html().split(\'data-src=\"\')[-1].split(\'\"\')[0]\n            except:\n                question_desc_img = \'\'\n            abstract = {\'title\': \'\', \n                        \'data\': all_content[0].find(\'p\').text(),\n                        \'img\': question_desc_img\n                        }\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"methods\": answers,\n            \"abstract\": abstract,  \n            \"date\": response.doc(\'time\').text(),\n            \"bread\": [response.save[\'tag\'],],\n            \"source\": \"baidu\",\n            \"class\": 36,\n            \"subject\": \'经验\',\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1472624686.9946),('buxiban_58',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 15:04:24\n# Project: buxiban_58\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\n\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\'\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1\', callback=self.index_page)\n        \n    @config(age=24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#ObjectType > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            for temp in nav:\n                if each.text() in temp:\n                    _dict[\'bread\'] = [temp[0], each.text()]\n                    \n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=24 * 60 * 60)\n    def list_page(self, response):\n        if response.doc(\'#xiaolei > a\'):\n            for each in response.doc(\'#xiaolei > a\').items():\n                _dict = {}\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n                else:\n                    _dict[\'title\'] = u\'附近哪里学\' + each.text() + u\'比较好\'\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'bread\'] = response.save.get(\'bread\')\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n             self.crawl(response.url, save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=10*24*60*60)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10*24*60*60)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\n        #翻页\n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10*24*60*60)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        for each in response.doc(\'.tdiv\').items():\n            _dict[\'content\'] += each.find(\'a\').eq(0).text() + \'<br/>\' + each.find(\'div\').text()+ \'<br/>\' + each.find(\'p\').text() + \'<br/>\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472120342.9155),('buxiban_58_inc',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 15:25:42\n# Project: buxiban_58_inc\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\n\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\'\n    }\n    \n    @every(minutes=3 * 60)\n    def on_start(self):\n        self.crawl(\'http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1\', callback=self.index_page)\n        \n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'#ObjectType > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            for temp in nav:\n                if each.text() in temp:\n                    _dict[\'bread\'] = [temp[0], each.text()]\n                    \n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=1*1)\n    def list_page(self, response):\n        if response.doc(\'#xiaolei > a\'):\n            for each in response.doc(\'#xiaolei > a\').items():\n                _dict = {}\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n                else:\n                    _dict[\'title\'] = u\'附近哪里学\' + each.text() + u\'比较好\'\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'bread\'] = response.save.get(\'bread\')\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n             self.crawl(response.url, save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=1*1)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=1*1)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\n        #翻页\n        #for each in response.doc(\'.next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=1*1)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        for each in response.doc(\'tr > .t\').items():\n            _dict[\'content\'] += \'<p>\'+each.find(\'a\').eq(0).text() + \'</p><p>\' + each.find(\'div\').text()+ \'</p><p>\' + each.find(\'p\').text() + \'</p>\'\n            _dict[\'content\'] += \'<br/>\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472120339.5478),('buxiban_58_m','bxb','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-25 16:44:08\n# Project: buxiban_58_m\n\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\nfrom pyquery import PyQuery as pq\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\ncity_list = [\n\'bj\',\n\'sh\',\n\'gz\',\n\'sz\',\n\'cd\',\n\'hz\',\n\'nj\',\n\'tj\',\n\'wh\',\n\'cq\',\n\'qd\',\n\'jn\',\n\'yt\',\n\'wf\',\n\'linyi\',\n\'zb\',\n\'jining\',\n\'ta\',\n\'lc\',\n\'weihai\',\n\'zaozhuang\',\n\'dz\',\n\'rizhao\',\n\'dy\',\n\'heze\',\n\'bz\',\n\'lw\',\n\'zhangqiu\',\n\'kl\',\n\'zc\',\n\'shouguang\',\n\'su\',\n\'nj\',\n\'wx\',\n\'cz\',\n\'xz\',\n\'nt\',\n\'yz\',\n\'yancheng\',\n\'ha\',\n\'lyg\',\n\'taizhou\',\n\'suqian\',\n\'zj\',\n\'shuyang\',\n\'dafeng\',\n\'rugao\',\n\'qidong\',\n\'liyang\',\n\'haimen\',\n\'donghai\',\n\'yangzhong\',\n\'xinghuashi\',\n\'xinyishi\',\n\'taixing\',\n\'rudong\',\n\'pizhou\',\n\'xzpeixian\',\n\'jingjiang\',\n\'jianhu\',\n\'haian\',\n\'dongtai\',\n\'danyang\',\n\'hz\',\n\'nb\',\n\'wz\',\n\'jh\',\n\'jx\',\n\'tz\',\n\'sx\',\n\'huzhou\',\n\'lishui\',\n\'quzhou\',\n\'zhoushan\',\n\'yueqingcity\',\n\'ruiancity\',\n\'yiwu\',\n\'yuyao\',\n\'zhuji\',\n\'xiangshanxian\',\n\'wenling\',\n\'tongxiang\',\n\'cixi\',\n\'changxing\',\n\'jiashanx\',\n\'haining\',\n\'deqing\',\n\'hf\',\n\'wuhu\',\n\'bengbu\',\n\'fy\',\n\'hn\',\n\'anqing\',\n\'suzhou\',\n\'la\',\n\'huaibei\',\n\'chuzhou\',\n\'mas\',\n\'tongling\',\n\'xuancheng\',\n\'bozhou\',\n\'huangshan\',\n\'chizhou\',\n\'ch\',\n\'hexian\',\n\'hq\',\n\'tongcheng\',\n\'ningguo\',\n\'tianchang\',\n\'sz\',\n\'gz\',\n\'dg\',\n\'fs\',\n\'zs\',\n\'zh\',\n\'huizhou\',\n\'jm\',\n\'st\',\n\'zhanjiang\',\n\'zq\',\n\'mm\',\n\'jy\',\n\'mz\',\n\'qingyuan\',\n\'yj\',\n\'sg\',\n\'heyuan\',\n\'yf\',\n\'sw\',\n\'chaozhou\',\n\'taishan\',\n\'yangchun\',\n\'sd\',\n\'huidong\',\n\'boluo\',\n\'fz\',\n\'xm\',\n\'qz\',\n\'pt\',\n\'zhangzhou\',\n\'nd\',\n\'sm\',\n\'np\',\n\'ly\',\n\'wuyishan\',\n\'shishi\',\n\'jinjiangshi\',\n\'nananshi\',\n\'nn\',\n\'liuzhou\',\n\'gl\',\n\'yulin\',\n\'wuzhou\',\n\'bh\',\n\'gg\',\n\'qinzhou\',\n\'baise\',\n\'hc\',\n\'lb\',\n\'hezhou\',\n\'fcg\',\n\'chongzuo\',\n\'haikou\',\n\'sanya\',\n\'wzs\',\n\'sansha\',\n\'qh\',\n\'zz\',\n\'luoyang\',\n\'xx\',\n\'ny\',\n\'xc\',\n\'pds\',\n\'ay\',\n\'jiaozuo\',\n\'sq\',\n\'kaifeng\',\n\'puyang\',\n\'zk\',\n\'xy\',\n\'zmd\',\n\'luohe\',\n\'smx\',\n\'hb\',\n\'jiyuan\',\n\'mg\',\n\'yanling\',\n\'yuzhou\',\n\'changge\',\n\'wh\',\n\'yc\',\n\'xf\',\n\'jingzhou\',\n\'shiyan\',\n\'hshi\',\n\'xiaogan\',\n\'hg\',\n\'es\',\n\'jingmen\',\n\'xianning\',\n\'ez\',\n\'suizhou\',\n\'qianjiang\',\n\'tm\',\n\'xiantao\',\n\'snj\',\n\'yidou\',\n\'cs\',\n\'zhuzhou\',\n\'yiyang\',\n\'changde\',\n\'hy\',\n\'xiangtan\',\n\'yy\',\n\'chenzhou\',\n\'shaoyang\',\n\'hh\',\n\'yongzhou\',\n\'ld\',\n\'xiangxi\',\n\'zjj\',\n\'nc\',\n\'ganzhou\',\n\'jj\',\n\'yichun\',\n\'ja\',\n\'sr\',\n\'px\',\n\'fuzhou\',\n\'jdz\',\n\'xinyu\',\n\'yingtan\',\n\'yxx\',\n\'sy\',\n\'dl\',\n\'as\',\n\'jinzhou\',\n\'fushun\',\n\'yk\',\n\'pj\',\n\'cy\',\n\'dandong\',\n\'liaoyang\',\n\'benxi\',\n\'hld\',\n\'tl\',\n\'fx\',\n\'pld\',\n\'wfd\',\n\'hrb\',\n\'dq\',\n\'qqhr\',\n\'mdj\',\n\'suihua\',\n\'jms\',\n\'jixi\',\n\'sys\',\n\'hegang\',\n\'heihe\',\n\'yich\',\n\'qth\',\n\'dxal\',\n\'cc\',\n\'jl\',\n\'sp\',\n\'yanbian\',\n\'songyuan\',\n\'bc\',\n\'th\',\n\'baishan\',\n\'liaoyuan\',\n\'cd\',\n\'mianyang\',\n\'deyang\',\n\'nanchong\',\n\'yb\',\n\'zg\',\n\'ls\',\n\'luzhou\',\n\'dazhou\',\n\'scnj\',\n\'suining\',\n\'panzhihua\',\n\'ms\',\n\'ga\',\n\'zy\',\n\'liangshan\',\n\'guangyuan\',\n\'ya\',\n\'bazhong\',\n\'ab\',\n\'ganzi\',\n\'km\',\n\'qj\',\n\'dali\',\n\'honghe\',\n\'yx\',\n\'lj\',\n\'ws\',\n\'cx\',\n\'bn\',\n\'zt\',\n\'dh\',\n\'pe\',\n\'bs\',\n\'lincang\',\n\'diqing\',\n\'nujiang\',\n\'gy\',\n\'zunyi\',\n\'qdn\',\n\'qn\',\n\'lps\',\n\'bijie\',\n\'tr\',\n\'anshun\',\n\'qxn\',\n\'lasa\',\n\'rkz\',\n\'sn\',\n\'linzhi\',\n\'changdu\',\n\'nq\',\n\'al\',\n\'sjz\',\n\'bd\',\n\'ts\',\n\'lf\',\n\'hd\',\n\'qhd\',\n\'cangzhou\',\n\'xt\',\n\'hs\',\n\'zjk\',\n\'chengde\',\n\'dingzhou\',\n\'gt\',\n\'zhangbei\',\n\'zx\',\n\'zd\',\n\'ty\',\n\'linfen\',\n\'dt\',\n\'yuncheng\',\n\'jz\',\n\'changzhi\',\n\'jincheng\',\n\'yq\',\n\'lvliang\',\n\'xinzhou\',\n\'shuozhou\',\n\'linyixian\',\n\'qingxu\',\n\'hu\',\n\'bt\',\n\'chifeng\',\n\'erds\',\n\'tongliao\',\n\'hlbe\',\n\'bycem\',\n\'wlcb\',\n\'xl\',\n\'xam\',\n\'wuhai\',\n\'alsm\',\n\'hlr\',\n\'xa\',\n\'xianyang\',\n\'baoji\',\n\'wn\',\n\'hanzhong\',\n\'yl\',\n\'yanan\',\n\'ankang\',\n\'sl\',\n\'tc\',\n\'xj\',\n\'changji\',\n\'bygl\',\n\'yili\',\n\'aks\',\n\'ks\',\n\'hami\',\n\'klmy\',\n\'betl\',\n\'tlf\',\n\'ht\',\n\'shz\',\n\'kzls\',\n\'ale\',\n\'wjq\',\n\'tmsk\',\n\'lz\',\n\'tianshui\',\n\'by\',\n\'qingyang\',\n\'pl\',\n\'jq\',\n\'zhangye\',\n\'wuwei\',\n\'dx\',\n\'jinchang\',\n\'ln\',\n\'linxia\',\n\'jyg\',\n\'gn\',\n\'yinchuan\',\n\'wuzhong\',\n\'szs\',\n\'zw\',\n\'guyuan\',\n\'xn\',\n\'hx\',\n\'haibei\',\n\'guoluo\',\n\'haidong\',\n\'huangnan\',\n\'ys\',\n\'hainan\',\n\'hk\',\n\'am\',\n\'tw\',\n\'diaoyudao\',\n\'cn\',\n]\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\'\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for city in city_list:\n            self.crawl(\'http://\'+city+\'.58.com/techang/\',save={\'city\':city}, callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#ObjectType > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            _dict[\'city\'] = response.save[\'city\']\n            for temp in nav:\n                if each.text() in temp:\n                    _dict[\'bread\'] = [temp[0], each.text()]\n                    \n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        if response.doc(\'#xiaolei > a\'):\n            for each in response.doc(\'#xiaolei > a\').items():\n                _dict = {}\n                _dict[\'city\'] = response.save[\'city\']\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n                else:\n                    _dict[\'title\'] = u\'附近哪里学\' + each.text() + u\'比较好\'\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'city\'] = response.save[\'city\']\n             _dict[\'bread\'] = response.save.get(\'bread\')\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n             self.crawl(response.url, save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=10 * 24 * 60 * 60)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            _dict[\'city\'] = response.save[\'city\']\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\n        #翻页\n        #for each in response.doc(\'.next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        _dict[\'content\'] = \'\'\n        for each in response.doc(\'tr > .t\').items():\n            #print each.find(\'a\').outerHtml()\n            if \'http://jump\' in each.find(\'a\').attr.href:\n                url = \'http://m.58.com/\'+_dict[\'city\']+\'/techang/\'+each.find(\'a\').attr[\'name\']+\'x.shtml\'\n            else:\n                url = \'http://m.58.com/\'+_dict[\'city\']+\'/techang/\'+each.find(\'a\').attr[\'href\'].split(\'/\')[-1]\n            #print url\n            _dict[\'content\'] += \'<p>\'+pq(url).find(\'.tit_area h1\').eq(0).text()+\'</p>\'+\'<p>\'+pq(url).find(\'.article li\').eq(-2).text()+\'</p>\' +\'<p>\'+pq(url).find(\'.article li\').eq(-1).text()+\'</p>\'+\'<p>\'+pq(url).find(\'.firm_area h2\').eq(0).text()+\'</p>\'+\'<p>\'+pq(pq(url).find(\'.firm_area a\').eq(0).attr[\'href\']).find(\'.infoitembox li\').eq(-1).text()+\'</p>\'\n            _dict[\'content\'] += \'<br/>\'\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'互联网\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472615704.2366),('bxb_other',NULL,'TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-15 15:04:24\n# Project: buxiban_58\nfrom pyspider.libs.base_handler import *\nimport time\nimport sys\nreload(sys)\nsys.setdefaultencoding=\'utf-8\'\n\n\nnav_yishu = [u\'艺术\',u\'舞蹈\',u\'乐器\',u\'美术\',u\'声乐\',u\'表演\',u\'艺考\']\nnav_xiqu = [u\'兴趣\',u\'摄影\',u\'DJ\',u\'魔术\',u\'书法\',u\'风水\',u\'国学\']\nnav_shenghuo = [u\'生活\',u\'礼仪\',u\'茶艺\',u\'插花\',u\'烹饪\',u\'形体\',u\'园艺\']\nnav = [nav_yishu, nav_xiqu, nav_shenghuo]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.1\'\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://wh.58.com/techang/?utm_source=market&spm=b-31580022738699-me-f-824.bdpz_biaoti&PGTID=0d3037fe-0009-ec0c-d494-94091f157a5f&ClickID=1\', callback=self.index_page)\n        \n    @config(age=24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#ObjectType > a\').items():\n            if each.text() == u\'全部\':\n                continue\n            _dict = {}\n            for temp in nav:\n                if each.text() in temp:\n                    _dict[\'bread\'] = [temp[0], each.text()]\n                    \n            _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(age=24 * 60 * 60)\n    def list_page(self, response):\n        if response.doc(\'#xiaolei > a\'):\n            for each in response.doc(\'#xiaolei > a\').items():\n                _dict = {}\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if each.text() == u\'全部\':\n                    _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n                else:\n                    _dict[\'title\'] = u\'附近哪里学\' + each.text() + u\'比较好\'\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        else:\n             _dict = {}\n             _dict[\'bread\'] = response.save.get(\'bread\')\n             _dict[\'date\'] = time.strftime(\'%Y-%m-%d\',time.localtime())\n             _dict[\'title\'] = u\'附近哪里学\' + _dict[\'bread\'][1] + u\'比较好\'\n             self.crawl(response.url, save = _dict, callback=self.list_page1)       \n            \n        \n    @config(age=10*24*60*60)\n    def list_page1(self, response):\n        for each in response.doc(\'#local > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #_dict[\'date\'] = response.save.get(\'date\')\n            #_dict[\'title\'] = each.text() + response.save.get(\'title\')\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10*24*60*60)\n    def list_page2(self, response):\n        global flag\n        global cishu\n        for each in response.doc(\'.subarea > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            _dict[\'date\'] = response.save.get(\'date\')\n            _dict[\'title\'] = each.text() + response.save.get(\'title\')\n            _dict[\'content\'] = \'\'\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page3)\n        #翻页\n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, save = response.save, callback=self.list_page2)\n            \n    @config(age=10*24*60*60)\n    def list_page3(self, response):\n        global flag\n        global cishu\n        _dict = response.save\n        for each in response.doc(\'.tdiv\').items():\n            _dict[\'content\'] += each.find(\'a\').eq(0).text() + \'<br/>\' + each.find(\'div\').text()+ \'<br/>\' + each.find(\'p\').text() + \'<br/>\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }\n    \n    @config(priority=3)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\":response.save.get(\'bread\'),\n            \"subject\": u\'补习班\',\n            \"class\": 46,\n            \"date\":response.save.get(\'date\'),\n            \"content\": response.save.get(\'content\'),\n            \"source\": u\'58同城\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472113261.5521),('cet46_inc','cet','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-21 14:58:56\n# Project: cet46com\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    page_dict = {\n        \"95\": u\'四级真题及答案\',\n        \"96\": u\'六级真题及答案\',\n        \'91\': u\'四级口语\',\n        \'97\': u\'模拟试题\',\n        \'86\': u\'六级作文\',\n        \'87\': u\'四级听力\',\n        \'88\': u\'六级听力\',\n        \'92\': u\'六级口语\',\n        \'78\': u\'名师指导\',\n        \'79\': u\'复习攻略\',\n        \'75\': u\'样题规定\',\n        \'83\': u\'四级阅读\',\n        \'84\': u\'六级阅读\',\n        \'85\': u\'四级作文\',\n        \'93\': u\'四级翻译\',\n        \'94\': u\'六级翻译\',\n    }\n    \n    @every(minutes=1 * 60)\n    def on_start(self):\n          for k, v in self.page_dict.iteritems():\n            self.crawl(\'http://www.cet-46.com/list%s.html\'%k,save={\'bread\': v}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\".left a\").items():\n             _dict = {}\n             url = each.attr.href  \n             _dict[\'bread\'] = [response.save[\'bread\'], ]\n             _dict[\'brief\'] = each.html()\n             try:\n                 date = each.parent().next(\'span\').find(\'font\').text().split(\'/\')\n                 _dict[\'date\'] = \'%s-%.2d-%2d\'%(date[0], int(date[1]), int(date[2]))\n             except:\n                 _dict[\'date\'] = \'2016-01-01\'\n             self.crawl(url, save=_dict, callback=self.detail_page) \n        \'\'\'\n        for each in response.doc(\".daohang a[href$=\'.html\']\").items(): \n             _dict = {}\n             _dict[\'bread\'] = response.save[\'bread\']\n             self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\n        \'\'\'\n        \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        res_dict[\'title\'] = response.doc(\'.xw_title > span\').html()\n        content_list = []\n        for info in response.doc(\'span > p\').items():\n            content = pyquery.PyQuery(info)\n            items = content.html()\n            content_list.append(items)\n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'source\'] = \'www.cet-46.com\'\n        res_dict[\'subject\'] = u\'四六级\'\n        res_dict[\'class\'] = 46\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,1.0000,3.0000,1471329965.7602),('chazidian_tiku','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-29 17:05:44\n# Project: chazidian_tiku\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\',\n        \'headers\':{\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n\n        }\n    }\n\n    dict_map = {\n\n            u\'注册会计师\':[u\'财会经济\',u\'注册会计师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'初级会计职称\':[u\'财会经济\',u\'初级会计师\'],\n            u\'中级会计职称\':[u\'财会经济\',u\'中级会计师\'],\n            u\'税务师\':[u\'财会经济\',u\'注册税务师\'],\n            u\'统计师\':[u\'财会经济\',u\'统计师\'],\n            u\'审计师\':[u\'财会经济\',u\'审计师\'],\n            u\'高级经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'高级会计\':[u\'财会经济\',u\'高级会计师\'],\n            u\'理财规划师\':[u\'财会经济\',u\'理财规划师\'],\n\n\n            u\'英语四级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'英语六级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'雅思\':[u\'外语考试\',u\'雅思\'],\n            u\'托福\':[u\'外语考试\',u\'托福\'],\n            u\'职称英语\':[u\'外语考试\',u\'职称英语\'],\n            u\'商务英语\':[u\'外语考试\',u\'商务英语\'],\n            u\'公共英语\':[u\'外语考试\',u\'公共英语\'],\n            u\'日语\':[u\'外语考试\',u\'日语\'],\n            u\'GRE考试\':[u\'外语考试\',u\'GRE考试\'],\n            u\'专四专八\':[u\'外语考试\',u\'专四专八\'],\n            u\'口译笔译\':[u\'外语考试\',u\'口译笔译\'],\n\n            u\'一级建造师\':[u\'建筑工程\',u\'一级建造师\'],\n            u\'二级建造师\':[u\'建筑工程\',u\'二级建造师\'],\n            u\'咨询工程师\':[u\'建筑工程\',u\'咨询工程师\'],\n            u\'造价工程师\':[u\'建筑工程\',\'造价工程师\'],\n            u\'结构工程师\':[u\'建筑工程\',\'结构工程师\'],\n            u\'物业管理\':[u\'建筑工程\',u\'物业管理师\'],\n            u\'城市规划\':[u\'建筑工程\',u\'城市规划师\'],\n            u\'给排水工程\':[u\'建筑工程\',u\'给排水工程\'],\n            u\'电气工程\':[u\'建筑工程\',u\'电气工程师\'],\n            u\'公路监理师\':[u\'建筑工程\',u\'公路监理师\'],\n            u\'消防工程师\':[u\'建筑工程\',u\'消防工程师\'],\n\n            u\'物流师\':[u\'职业资格\',u\'物流师\'],\n            u\'人力资源\':[u\'职业资格\',u\'人力资源\'],\n            u\'心理咨询师\':[u\'职业资格\',u\'心理咨询师\'],\n            u\'公共营养师\':[u\'职业资格\',u\'公共营养师\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'证券从业资格\':[u\'职业资格\',u\'证券经纪人\'],\n            u\'电子商务师\':[u\'职业资格\',u\'电子商务\'],\n            u\'期货从业\':[u\'职业资格\',u\'期货从业\'],\n            u\'教师资格\':[u\'职业资格\',u\'教师资格\'],\n            u\'管理咨询师\':[u\'职业资格\',u\'管理咨询师\'],\n            u\'导游证\':[u\'职业资格\',u\'导游证\'],\n            \n            u\'英语\':[u\'学历教育\',u\'考研\'],\n            u\'数学\':[u\'学历教育\',u\'考研\'],\n            u\'政治\':[u\'学历教育\',u\'考研\'],\n            u\'专业课\':[u\'学历教育\',u\'考研\'],\n\n            u\'执业医师\':[u\'医学卫生\',u\'执业医师\'],\n            u\'执业药师\':[u\'医学卫生\',u\'执业药师\'],\n            u\'临床医师\':[u\'医学卫生\',u\'临床执业\'],\n            u\'中医医师\':[u\'医学卫生\',u\'中医执业\'],\n            u\'中西医医师\':[u\'医学卫生\',u\'中西医执业\'],\n            u\'中医助理\':[u\'医学卫生\',u\'中医助理\'],\n            u\'中西医助理\':[u\'医学卫生\',u\'中西医助理\'],\n            u\'主治\':[u\'医学卫生\',u\'主治\'],\n            u\'检验\':[u\'医学卫生\',u\'检验\'],\n            u\'执业护士资格\':[u\'医学卫生\',u\'执业护士\'],\n\n\n            u\'成人高考\':[u\'学历教育\',u\'成人高考\'],\n            u\'自学考试\':[u\'学历教育\',u\'自考\'],\n            u\'MBA考试\':[u\'学历教育\',u\'MBA\'],\n            u\'法律硕士\':[u\'学历教育\',u\'法律硕士\'],\n            u\'专升本\':[u\'学历教育\',u\'专升本\'],\n            #u\'\':[u\'学历教育\',u\'工程硕士\'],\n            u\'MPA考试\':[u\'学历教育\',u\'公共硕士\'],\n            #u\'\':[u\'学历教育\',u\'考研\'],\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://tiku.chazidian.com/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'li dd\').items():         \n                _dict = {}\n                url = each.find(\'a\').attr.href\n                _dict[\'type\'] = each.find(\'a\').text()\n                self.crawl(url, save = _dict,callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.list_sj_xx > a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text()\n            _dict[\'type\'] = response.save.get(\'type\')\n            self.crawl(each.attr.href, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'.page_link\').items():\n            _dict = {}\n            _dict[\'type\'] = response.save.get(\'type\')\n            self.crawl(each.attr.href, save = _dict,callback=self.list_page)\n        \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       res_dict[\'tdk_title\'] = \'跟谁学 \'+response.doc(\'title\').text().replace(\'查字典\',\'\').replace(\'题库网\',\'\')\n       res_dict[\'tdk_desc\'] = response.doc(\'meta[name=\"description\"]\').attr.content.replace(\'查字典\',\'\').replace(\'题库网\',\'\')\n       res_dict[\'tdk_keywords\'] =  response.doc(\'meta[name=\"keywords\"]\').attr.content.replace(\'查字典\',\'\').replace(\'题库网\',\'\')\n       content_list = []\n       for each in response.doc(\'p\').items():\n            info = each.remove(\'a\').html()\n            if info and  info.find(\'查字典\') <0 and info.find(\'题库网\') <0:\n                content_list.append(info)\n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%(s) for s in content_list if s and s.strip()])\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'考证题库\'\n       res_dict[\'source\'] = \'chazidian.com\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       res_dict[\'date\'] = u\'2016-06-29\'\n       if res_dict[\'type\'] in self.dict_map:\n            res_dict[\'bread\'] = self.dict_map[res_dict[\'type\']]\n            del res_dict[\'type\']\n       else:\n            return \n       return res_dict\n\n',NULL,1.0000,3.0000,1472624620.8286),('chengxuyuan_bkyshengqu','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-13 16:09:10\n# Project: chengxuyuan_bkyshengqu\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.cnblogs.com/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#cate_item a\').items():\n            if u\'所有评论\' in each.text():\n                continue\n            _dict = {}\n            _dict[\'bread\'] = [each.text().split(\'(\')[0]]\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n    \n    def list_page(self, response):\n        for each in response.doc(\'.post_item_body\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h3 > a\').text()\n            _dict[\'date\'] = [v for v in each.find(\'.post_item_foot\').remove(\'a\').text().split() if \'-\' in v][0]\n            self.crawl(each.find(\'h3 > a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页    \n        for each in response.doc(\'#paging_block a\').items():\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'#cnblogs_post_body\').items():\n            #print each.html()\n            if \'img\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif \'<pre>\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\'),\n            \"source\": u\"博客园\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"程序员\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,2.0000,3.0000,1472624474.4894),('chengxuyuan_bkyuan','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-13 14:37:27\n# Project: chengxuyuan_bkyuan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.4\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://kb.cnblogs.com/zt/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'span > a\').items():\n            if u\'知识库\' in each.text() or u\'专题\' in each.text() or u\'最新文章\' in each.text():\n                continue\n            _dict = {}\n            _dict[\'bread\'] = [each.text()]\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n    \n    def list_page(self, response):\n        for each in response.doc(\'#list_block > div\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.list_title\').text()\n            _dict[\'date\'] = [v for v in each.find(\'.listfooter\').remove(\'a\').text().split() if \'-\' in v][0]\n            self.crawl(each.find(\'.list_title > a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页    \n        for each in response.doc(\'#pager_block_top a\').items():\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'.contents_main > div\').items(): \n            if \'img\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif \'<pre>\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"content\": content,\n            \"source\": u\"博客园\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"程序员\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,2.0000,3.0000,1472624477.2426),('chengxuyuan_boke','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-14 11:27:09\n# Project: chengxuyuan_boke\n\nfrom pyspider.libs.base_handler import *\nimport sys\nimport traceback\nfrom pyquery import PyQuery\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\nsub_items = u\'<div id=\"cate_content_block_108698\" onmouseover=\"cateShow(108698)\" onmouseout=\"cateHidden(108698)\" class=\"cate_content_block_wrapper\" style=\"top:30px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/beginner/\">.NET新手区(1)</a></li><li><a href=\"/cate/aspnet/\">ASP.NET(1)</a></li><li><a href=\"/cate/csharp/\">C#(1)</a></li><li><a href=\"/cate/dotnetcore/\">.NET Core(0)</a></li><li><a href=\"/cate/winform/\">WinForm(0)</a></li><li><a href=\"/cate/silverlight/\">Silverlight(0)</a></li><li><a href=\"/cate/wcf/\">WCF(0)</a></li><li><a href=\"/cate/clr/\">CLR(0)</a></li><li><a href=\"/cate/wpf/\">WPF(0)</a></li><li><a href=\"/cate/xna/\">XNA(0)</a></li><li><a href=\"/cate/vs2010/\">Visual Studio(0)</a></li><li><a href=\"/cate/mvc/\">ASP.NET MVC(2)</a></li><li><a href=\"/cate/control/\">控件开发(0)</a></li><li><a href=\"/cate/ef/\">Entity Framework(0)</a></li><li><a href=\"/cate/nhibernate/\">NHibernate(0)</a></li><li><a href=\"/cate/winrt_metro/\">WinRT/Metro(1)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_2\" onmouseover=\"cateShow(2)\" onmouseout=\"cateHidden(2)\" class=\"cate_content_block_wrapper\" style=\"top:58px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/java/\">Java(4)</a></li><li><a href=\"/cate/cpp/\">C++(1)</a></li><li><a href=\"/cate/php/\">PHP(1)</a></li><li><a href=\"/cate/delphi/\">Delphi(0)</a></li><li><a href=\"/cate/python/\">Python(2)</a></li><li><a href=\"/cate/ruby/\">Ruby(0)</a></li><li><a href=\"/cate/c/\">C语言(0)</a></li><li><a href=\"/cate/erlang/\">Erlang(0)</a></li><li><a href=\"/cate/go/\">Go(0)</a></li><li><a href=\"/cate/swift/\">Swift(0)</a></li><li><a href=\"/cate/scala/\">Scala(0)</a></li><li><a href=\"/cate/r/\">R语言(0)</a></li><li><a href=\"/cate/verilog/\">Verilog(0)</a></li><li><a href=\"/cate/otherlang/\">其它语言(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108701\" onmouseover=\"cateShow(108701)\" onmouseout=\"cateHidden(108701)\" class=\"cate_content_block_wrapper\" style=\"top:86px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/design/\">架构设计(0)</a></li><li><a href=\"/cate/108702/\">面向对象(0)</a></li><li><a href=\"/cate/dp/\">设计模式(0)</a></li><li><a href=\"/cate/ddd/\">领域驱动设计(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108703\" onmouseover=\"cateShow(108703)\" onmouseout=\"cateHidden(108703)\" class=\"cate_content_block_wrapper\" style=\"top:114px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/web/\">Html/Css(1)</a></li><li><a href=\"/cate/javascript/\">JavaScript(3)</a></li><li><a href=\"/cate/jquery/\">jQuery(1)</a></li><li><a href=\"/cate/html5/\">HTML5(1)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108704\" onmouseover=\"cateShow(108704)\" onmouseout=\"cateHidden(108704)\" class=\"cate_content_block_wrapper\" style=\"top:142px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/sharepoint/\">SharePoint(0)</a></li><li><a href=\"/cate/gis/\">GIS技术(0)</a></li><li><a href=\"/cate/sap/\">SAP(0)</a></li><li><a href=\"/cate/OracleERP/\">Oracle ERP(0)</a></li><li><a href=\"/cate/dynamics/\">Dynamics CRM(0)</a></li><li><a href=\"/cate/k2/\">K2 BPM(0)</a></li><li><a href=\"/cate/infosec/\">信息安全(0)</a></li><li><a href=\"/cate/3/\">企业信息化其他(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108705\" onmouseover=\"cateShow(108705)\" onmouseout=\"cateHidden(108705)\" class=\"cate_content_block_wrapper\" style=\"top:170px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/android/\">Android开发(1)</a></li><li><a href=\"/cate/ios/\">iOS开发(3)</a></li><li><a href=\"/cate/wp/\">Windows Phone(0)</a></li><li><a href=\"/cate/wm/\">Windows Mobile(0)</a></li><li><a href=\"/cate/mobile/\">其他手机开发(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108709\" onmouseover=\"cateShow(108709)\" onmouseout=\"cateHidden(108709)\" class=\"cate_content_block_wrapper\" style=\"top:198px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/agile/\">敏捷开发(0)</a></li><li><a href=\"/cate/pm/\">项目与团队管理(0)</a></li><li><a href=\"/cate/Engineering/\">软件工程其他(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108712\" onmouseover=\"cateShow(108712)\" onmouseout=\"cateHidden(108712)\" class=\"cate_content_block_wrapper\" style=\"top:226px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/sqlserver/\">SQL Server(0)</a></li><li><a href=\"/cate/oracle/\">Oracle(0)</a></li><li><a href=\"/cate/mysql/\">MySQL(1)</a></li><li><a href=\"/cate/nosql/\">NoSQL(0)</a></li><li><a href=\"/cate/bigdata/\">大数据(0)</a></li><li><a href=\"/cate/database/\">其它数据库(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108724\" onmouseover=\"cateShow(108724)\" onmouseout=\"cateHidden(108724)\" class=\"cate_content_block_wrapper\" style=\"top:254px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/win7/\">Windows(0)</a></li><li><a href=\"/cate/winserver/\">Windows Server(0)</a></li><li><a href=\"/cate/linux/\">Linux(0)</a></li><li><a href=\"/cate/osx/\">OS X(0)</a></li><li><a href=\"/cate/eos/\">嵌入式(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_4\" onmouseover=\"cateShow(4)\" onmouseout=\"cateHidden(4)\" class=\"cate_content_block_wrapper\" style=\"top:282px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/life/\">非技术区(0)</a></li><li><a href=\"/cate/testing/\">软件测试(0)</a></li><li><a href=\"/cate/software/\">代码与软件发布(0)</a></li><li><a href=\"/cate/cg/\">计算机图形学(0)</a></li><li><a href=\"/cate/google/\">Google开发(0)</a></li><li><a href=\"/cate/gamedev/\">游戏开发(1)</a></li><li><a href=\"/cate/codelife/\">程序人生(0)</a></li><li><a href=\"/cate/job/\">求职面试(0)</a></li><li><a href=\"/cate/book/\">读书区(0)</a></li><li><a href=\"/cate/quoted/\">转载区(0)</a></li><li><a href=\"/cate/wince/\">Windows CE(0)</a></li><li><a href=\"/cate/translate/\">翻译区(0)</a></li><li><a href=\"/cate/opensource/\">开源研究(0)</a></li><li><a href=\"/cate/flex/\">Flex(0)</a></li><li><a href=\"/cate/cloud/\">云计算(0)</a></li><li><a href=\"/cate/algorithm/\">算法与数据结构(0)</a></li><li><a href=\"/cate/misc/\">其他技术区(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div>\'\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.cnblogs.com/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        global sub_items\n        i = 0\n        for each in response.doc(\'#cate_item a\').items():\n            if u\'所有评论\' in each.text():\n                continue\n            sub_items = PyQuery(sub_items)\n            for each_sub in sub_items(\'.cate_content_block\').eq(i).find(\'li > a\').items():\n                _dict = {}\n                _dict[\'bread\'] = [each.text().split(\'(\')[0]]\n                _dict[\'bread\'].append(each_sub.text().split(\'(\')[0])\n                self.crawl(\'http://www.cnblogs.com\'+each_sub.attr.href, save = _dict, callback=self.list_page)\n            i += 1\n    \n    def list_page(self, response):\n        for each in response.doc(\'.post_item_body\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h3 > a\').text()\n            _dict[\'date\'] = [v for v in each.find(\'.post_item_foot\').remove(\'a\').text().split() if \'-\' in v][0]\n            self.crawl(each.find(\'h3 > a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页    \n        #for each in response.doc(\'#paging_block a\').items():\n         #   self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'#cnblogs_post_body\').items():\n            #print each.html()\n            if \'img\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif \'<pre>\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\'),\n            \"source\": u\"博客园\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"程序员\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1472624479.9484),('chengxuyuan_boke_inc','cxy','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-10 14:12:20\n# Project: chengxuyuan_boke_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nimport traceback\nfrom pyquery import PyQuery\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\nsub_items = u\'<div id=\"cate_content_block_108698\" onmouseover=\"cateShow(108698)\" onmouseout=\"cateHidden(108698)\" class=\"cate_content_block_wrapper\" style=\"top:30px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/beginner/\">.NET新手区(1)</a></li><li><a href=\"/cate/aspnet/\">ASP.NET(1)</a></li><li><a href=\"/cate/csharp/\">C#(1)</a></li><li><a href=\"/cate/dotnetcore/\">.NET Core(0)</a></li><li><a href=\"/cate/winform/\">WinForm(0)</a></li><li><a href=\"/cate/silverlight/\">Silverlight(0)</a></li><li><a href=\"/cate/wcf/\">WCF(0)</a></li><li><a href=\"/cate/clr/\">CLR(0)</a></li><li><a href=\"/cate/wpf/\">WPF(0)</a></li><li><a href=\"/cate/xna/\">XNA(0)</a></li><li><a href=\"/cate/vs2010/\">Visual Studio(0)</a></li><li><a href=\"/cate/mvc/\">ASP.NET MVC(2)</a></li><li><a href=\"/cate/control/\">控件开发(0)</a></li><li><a href=\"/cate/ef/\">Entity Framework(0)</a></li><li><a href=\"/cate/nhibernate/\">NHibernate(0)</a></li><li><a href=\"/cate/winrt_metro/\">WinRT/Metro(1)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_2\" onmouseover=\"cateShow(2)\" onmouseout=\"cateHidden(2)\" class=\"cate_content_block_wrapper\" style=\"top:58px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/java/\">Java(4)</a></li><li><a href=\"/cate/cpp/\">C++(1)</a></li><li><a href=\"/cate/php/\">PHP(1)</a></li><li><a href=\"/cate/delphi/\">Delphi(0)</a></li><li><a href=\"/cate/python/\">Python(2)</a></li><li><a href=\"/cate/ruby/\">Ruby(0)</a></li><li><a href=\"/cate/c/\">C语言(0)</a></li><li><a href=\"/cate/erlang/\">Erlang(0)</a></li><li><a href=\"/cate/go/\">Go(0)</a></li><li><a href=\"/cate/swift/\">Swift(0)</a></li><li><a href=\"/cate/scala/\">Scala(0)</a></li><li><a href=\"/cate/r/\">R语言(0)</a></li><li><a href=\"/cate/verilog/\">Verilog(0)</a></li><li><a href=\"/cate/otherlang/\">其它语言(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108701\" onmouseover=\"cateShow(108701)\" onmouseout=\"cateHidden(108701)\" class=\"cate_content_block_wrapper\" style=\"top:86px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/design/\">架构设计(0)</a></li><li><a href=\"/cate/108702/\">面向对象(0)</a></li><li><a href=\"/cate/dp/\">设计模式(0)</a></li><li><a href=\"/cate/ddd/\">领域驱动设计(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108703\" onmouseover=\"cateShow(108703)\" onmouseout=\"cateHidden(108703)\" class=\"cate_content_block_wrapper\" style=\"top:114px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/web/\">Html/Css(1)</a></li><li><a href=\"/cate/javascript/\">JavaScript(3)</a></li><li><a href=\"/cate/jquery/\">jQuery(1)</a></li><li><a href=\"/cate/html5/\">HTML5(1)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108704\" onmouseover=\"cateShow(108704)\" onmouseout=\"cateHidden(108704)\" class=\"cate_content_block_wrapper\" style=\"top:142px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/sharepoint/\">SharePoint(0)</a></li><li><a href=\"/cate/gis/\">GIS技术(0)</a></li><li><a href=\"/cate/sap/\">SAP(0)</a></li><li><a href=\"/cate/OracleERP/\">Oracle ERP(0)</a></li><li><a href=\"/cate/dynamics/\">Dynamics CRM(0)</a></li><li><a href=\"/cate/k2/\">K2 BPM(0)</a></li><li><a href=\"/cate/infosec/\">信息安全(0)</a></li><li><a href=\"/cate/3/\">企业信息化其他(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108705\" onmouseover=\"cateShow(108705)\" onmouseout=\"cateHidden(108705)\" class=\"cate_content_block_wrapper\" style=\"top:170px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/android/\">Android开发(1)</a></li><li><a href=\"/cate/ios/\">iOS开发(3)</a></li><li><a href=\"/cate/wp/\">Windows Phone(0)</a></li><li><a href=\"/cate/wm/\">Windows Mobile(0)</a></li><li><a href=\"/cate/mobile/\">其他手机开发(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108709\" onmouseover=\"cateShow(108709)\" onmouseout=\"cateHidden(108709)\" class=\"cate_content_block_wrapper\" style=\"top:198px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/agile/\">敏捷开发(0)</a></li><li><a href=\"/cate/pm/\">项目与团队管理(0)</a></li><li><a href=\"/cate/Engineering/\">软件工程其他(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108712\" onmouseover=\"cateShow(108712)\" onmouseout=\"cateHidden(108712)\" class=\"cate_content_block_wrapper\" style=\"top:226px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/sqlserver/\">SQL Server(0)</a></li><li><a href=\"/cate/oracle/\">Oracle(0)</a></li><li><a href=\"/cate/mysql/\">MySQL(1)</a></li><li><a href=\"/cate/nosql/\">NoSQL(0)</a></li><li><a href=\"/cate/bigdata/\">大数据(0)</a></li><li><a href=\"/cate/database/\">其它数据库(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_108724\" onmouseover=\"cateShow(108724)\" onmouseout=\"cateHidden(108724)\" class=\"cate_content_block_wrapper\" style=\"top:254px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/win7/\">Windows(0)</a></li><li><a href=\"/cate/winserver/\">Windows Server(0)</a></li><li><a href=\"/cate/linux/\">Linux(0)</a></li><li><a href=\"/cate/osx/\">OS X(0)</a></li><li><a href=\"/cate/eos/\">嵌入式(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div><div id=\"cate_content_block_4\" onmouseover=\"cateShow(4)\" onmouseout=\"cateHidden(4)\" class=\"cate_content_block_wrapper\" style=\"top:282px\"><div class=\"cate_content_top\"></div><div class=\"cate_content_block\"><ul><li><a href=\"/cate/life/\">非技术区(0)</a></li><li><a href=\"/cate/testing/\">软件测试(0)</a></li><li><a href=\"/cate/software/\">代码与软件发布(0)</a></li><li><a href=\"/cate/cg/\">计算机图形学(0)</a></li><li><a href=\"/cate/google/\">Google开发(0)</a></li><li><a href=\"/cate/gamedev/\">游戏开发(1)</a></li><li><a href=\"/cate/codelife/\">程序人生(0)</a></li><li><a href=\"/cate/job/\">求职面试(0)</a></li><li><a href=\"/cate/book/\">读书区(0)</a></li><li><a href=\"/cate/quoted/\">转载区(0)</a></li><li><a href=\"/cate/wince/\">Windows CE(0)</a></li><li><a href=\"/cate/translate/\">翻译区(0)</a></li><li><a href=\"/cate/opensource/\">开源研究(0)</a></li><li><a href=\"/cate/flex/\">Flex(0)</a></li><li><a href=\"/cate/cloud/\">云计算(0)</a></li><li><a href=\"/cate/algorithm/\">算法与数据结构(0)</a></li><li><a href=\"/cate/misc/\">其他技术区(0)</a></li></ul></div><div class=\"cate_content_bottom\"></div></div>\'\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.cnblogs.com/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        global sub_items\n        i = 0\n        for each in response.doc(\'#cate_item a\').items():\n            if u\'所有评论\' in each.text():\n                continue\n            sub_items = PyQuery(sub_items)\n            for each_sub in sub_items(\'.cate_content_block\').eq(i).find(\'li > a\').items():\n                _dict = {}\n                _dict[\'bread\'] = [each.text().split(\'(\')[0]]\n                _dict[\'bread\'].append(each_sub.text().split(\'(\')[0])\n                self.crawl(\'http://www.cnblogs.com\'+each_sub.attr.href, save = _dict, callback=self.list_page)\n            i += 1\n    \n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.post_item_body\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h3 > a\').text()\n            _dict[\'date\'] = [v for v in each.find(\'.post_item_foot\').remove(\'a\').text().split() if \'-\' in v][0]\n            self.crawl(each.find(\'h3 > a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页    \n        #for each in response.doc(\'#paging_block a\').items():\n         #   self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        _list = []\n        for each in response.doc(\'#cnblogs_post_body\').items():\n            #print each.html()\n            if \'img\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif \'<pre>\' in each.html():\n                _list.append(\'<p>\'+each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\'),\n            \"source\": u\"博客园\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"程序员\",\n            \"date\": response.save.get(\'date\'),\n\n        }',NULL,1.0000,3.0000,1471334415.9426),('chinadance','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 09:55:12\n# Project: chinadance\n\nfrom pyspider.libs.base_handler import *\nimport re\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    page_dict = {\n\n        \'http://www.chinadance.cn/article/wudaozhishi/\':[u\'舞蹈知识\'],\n        \'http://www.chinadance.cn/article/wudaojiaoxue/\':[u\'舞蹈教学\'],\n        \'http://www.chinadance.cn/article/wudaoshangxi/\':[u\'舞蹈鉴赏\'],\n        \'http://www.chinadance.cn/article/wudaorensheng/\':[u\'舞蹈人生\'],\n        \'http://www.chinadance.cn/article/wudaoshi/\':[u\'舞蹈史论\'],\n        \'http://www.chinadance.cn/news/\':[u\'舞蹈资讯\'],\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'li.cl\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h3 > a\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            url = each.find(\'h3 > a\').attr.href\n            _dict[\'date\'] = each.find(\'p.info > span\').text().split(\' \')[0]\n            _dict[\'cover\'] = each.children(\'a\').find(\'img\').attr.src or \'\'\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'div.pg > a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       if \'</a>\' in response.doc(\'td#article_content\').remove(\'embed\').remove(\'script\').html():   \n            res_dict[\'content\'] = removeLink(response.doc(\'td#article_content\').remove(\'embed\').remove(\'script\').html())\n       else:\n             res_dict[\'content\'] = response.doc(\'td#article_content\').remove(\'embed\').remove(\'script\').html()   \n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'舞蹈\'\n       res_dict[\'source\'] = \'chinadance\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict',NULL,1.0000,3.0000,1472624643.9256),('chinadance_inc','wudao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 09:55:12\n# Project: chinadance\n\nfrom pyspider.libs.base_handler import *\nimport re\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    page_dict = {\n\n        \'http://www.chinadance.cn/article/wudaozhishi/\':[u\'舞蹈知识\'],\n        \'http://www.chinadance.cn/article/wudaojiaoxue/\':[u\'舞蹈教学\'],\n        \'http://www.chinadance.cn/article/wudaoshangxi/\':[u\'舞蹈鉴赏\'],\n        \'http://www.chinadance.cn/article/wudaorensheng/\':[u\'舞蹈人生\'],\n        \'http://www.chinadance.cn/article/wudaoshi/\':[u\'舞蹈史论\'],\n        \'http://www.chinadance.cn/news/\':[u\'舞蹈资讯\'],\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'li.cl\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h3 > a\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            url = each.find(\'h3 > a\').attr.href\n            _dict[\'date\'] = each.find(\'p.info > span\').text().split(\' \')[0]\n            _dict[\'cover\'] = each.children(\'a\').find(\'img\').attr.src or \'\'\n            self.crawl(url, save = _dict,callback=self.detail_page)\n      \n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       res_dict[\'content\'] = response.doc(\'td#article_content\').remove(\'embed\').remove(\'script\').html()   \n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'舞蹈\'\n       res_dict[\'source\'] = \'chinadance\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict',NULL,1.0000,3.0000,1472611411.9489),('cidian2_haici',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-15 09:34:36\n# Project: cidian2_haici\n\nfrom pyspider.libs.base_handler import *\nimport sys\nimport MySQLdb\nimport traceback\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"cidiandb\", charset = \"utf8\")\ncursor = conn.cursor()\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 30)\n    def on_start(self):\n        sql = \'select id, word from tb_word where final_flag = 0 limit 10000\'\n        try:\n            cursor.execute(sql)\n            for (ci_id, word,) in cursor.fetchall():\n                #word = \'good\'\n                self.crawl(\'http://dict.cn/\'+word, save = {\'ci\': word, \'id\': ci_id}, callback=self.detail_page)\n        except:\n            traceback.print_exc()\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        word = response.save.get(\'ci\').strip()\n        ci_id = response.save.get(\'id\')\n        sql = \"update tb_word set final_flag= 1 where id= %s\" % (ci_id)\n        try:           \n            cursor.execute(sql)\n            conn.commit()\n        except Exception, e:\n            print e\n        paraphrase = {}\n        example = {}\n        annotation = {}\n        related_word = {}\n        recommend = {}\n        \n        for each in response.doc(\'.sent > h3\').items():\n            if u\'词汇搭配\' in each.text():\n                related_word[each.text().strip()] = {}\n                _dict = {}\n\n                for each_span in each.next().find(\'div\').items():\n                    #_dict = {}\n                    temp = each_span.find(\'b\').text()\n                    _dict[temp] = []\n                    _dict2 = {}\n                    while each_span.next():\n                        \n                        if \'<b>\' in each_span.next().html():\n                            break\n                        if \'<li>\' not in each_span.next().html():\n                            _dict2[\'title\'] = each_span.next().text()\n                        else:\n                            _dict2[\'words\'] = {}\n                            for each_li in each_span.next().find(\'li\').items():\n                                #这两句话不能放一行！！，不然remove(\'a\')会先执行\n                                k = each_li.find(\'a\').text()\n                                v = each_li.remove(\'a\').text()\n                                _dict2[\'words\'][k] = v\n                                #print _dict2    \n                            _dict[temp].append(_dict2)\n                            _dict2 = {}\n                        #print _dict2        \n                        each_span = each_span.next()\n                        #_dict[temp].append(_dict2)\n                if not each.next().find(\'b\'):\n                    _dict[\'all\'] = []\n                    _dict1 ={}\n                    for each_li in each.next().find(\'li\').items():\n                        k = each_li.find(\'a\').text()\n                        v = each_li.remove(\'a\').html()\n                        _dict1[k] = v\n                    _dict[\'all\'].append(_dict1)\n                related_word[each.text().strip()] = _dict\n                #continue\n            elif u\'例句\' in each.text():    \n                example[each.text().strip()] = {}\n                _dict = {}\n\n                for each_span in each.next().find(\'div\').items():\n                    #_dict = {}\n                    if not each_span.find(\'b\'):\n                        continue\n                    _dict[each_span.find(\'b\').text()] = []\n                    \n                    for each_li in each_span.next().find(\'li\').items():\n                        _dict1 = {}\n                        _dict1[\"chinese\"] = each_li.remove(\'i\').html().split(\'<br/>\')[1].replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'&#13;\',\'\').strip()\n                        _dict1[\"english\"] = each_li.remove(\'i\').html().split(\'<br/>\')[0].replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'&#13;\',\'\').strip()\n                        #print _dict1\n                        _dict[each_span.find(\'b\').text()].append(_dict1)\n                example[each.text().strip()] = _dict\n            elif u\'常见句型\' in each.text():\n                example[each.text().strip()] = {}\n                _dict = {}\n\n                for each_span in each.next().find(\'div\').items():\n                    #_dict = {}\n                    if not each_span.find(\'b\'):\n                        continue\n                    temp = each_span.find(\'b\').text()\n                    _dict[temp] = []\n                    #_dict1 = {}\n                    while each_span.next():\n                        if \'<b>\' in each_span.next().html():\n                            break\n                        if \'<li>\' in each_span.next().html():\n                            for each_li in each_span.next().find(\'li\').items():\n                                _dict1 = {}\n                                #print each_li.remove(\'i\').html().split(\'<br/>\')[0]\n                                _dict1[\"chinese\"] = each_li.remove(\'i\').html().split(\'<br/>\')[1].replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'&#13;\',\'\').strip()\n                                _dict1[\"english\"] = each_li.remove(\'i\').html().split(\'<br/>\')[0].replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'&#13;\',\'\').strip()\n                                _dict[temp].append(_dict1)     \n                        each_span = each_span.next()\n                example[each.text().strip()] = _dict\n            elif u\'经典引文\' in each.text():    \n                example[each.text().strip()] = []\n                for each_li in each.next().find(\'li\').items():\n                    _dict1 = {}\n                    _dict1[\"source\"] = each_li.find(\'b\').text().strip()\n                    _dict1[\"sentence\"] = each_li.find(\'p\').text().strip()\n                    #print _dict1\n                    example[each.text().strip()].append(_dict1)\n        for each in response.doc(\'.rel > h3\').items():\n            if u\'近反义词\' in each.text():\n                related_word[each.text().strip()] = {}\n                _dict = {}\n\n                for each_span in each.next().find(\'div\').items():\n                    #_dict = {}\n                    _dict[each_span.text()] = {}\n                    _dict1 = {}\n                    #_list = []\n                    for each_li in each_span.next().find(\'li\').items():\n                        if each_li.find(\'span\'):\n                            _dict1[each_li.find(\'span\').text()] = [v.text() for v in each_li.find(\'a\').items() if v]\n                        #else:\n                         #   _list.append(each_li.text())\n                    #if not each.next().find(\'div\').find(\'span\'):\n                     #   _dict1[\'all\'] = _list\n                    _dict[each_span.text()] = _dict1\n                if not each.next().find(\'span\'):\n                    _list = []\n                    for each_li in each.next().find(\'li\').items():\n                         _list.append(each_li.text())\n                    _dict1[\'all\'] = _list\n                    _dict[each_span.text()] = _dict1\n                related_word[each.text().strip()] = _dict\n                continue\n            \n        \n        for each in response.doc(\'.def > h3\').items():\n            if u\'行业释义\' in each.text():\n                continue\n            if u\'英英释义\' in each.text():\n                paraphrase[each.text().strip()] = []\n                #_dict = {}\n\n                for each_span in each.next().find(\'span\').items():\n                    _dict = {}\n                    meanings = {}\n                    _dict[\"name\"] = each_span.find(\'bdo\').text()\n                    _dict[\"english_name\"] = each_span.remove(\'bdo\').text()\n                    for each_li in each_span.next().find(\'li\').items():\n                        val = each_li.find(\'p\').html()\n                        k = each_li.remove(\'p\').text()\n                        meanings[k] = [v for v in val.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'&#13;\',\'\').split(\'<br/>\') if v]\n                    _dict[\"meanings\"] = meanings\n                    paraphrase[each.text().strip()].append(_dict)\n                continue\n                \n            paraphrase[each.text().strip()] = []\n            #_dict = {}\n            \n            for each_span in each.next().find(\'span\').items():\n                _dict = {}\n                meanings = []\n                _dict[\"name\"] = each_span.find(\'bdo\').text()\n                _dict[\"english_name\"] = each_span.remove(\'bdo\').text()\n                for each_li in each_span.next().find(\'li\').items():\n                    meanings.append(each_li.text())\n                _dict[\"meanings\"] = meanings\n                paraphrase[each.text().strip()].append(_dict)\n        \n        for each in response.doc(\'.learn > h3\').items():\n            if u\'词义辨析\' in each.text():\n                annotation[each.text().strip()] = []\n                #_dict = {}\n\n                for each_span in each.next().find(\'span\').items():\n                    phrase_detail = []\n                    _dict = {}\n                    _dict[\"name\"] = each_span.find(\'bdo\').text()\n                    #print each_spann.find(\'bdo\').text()\n                    _dict[\"english_name\"] = each_span.remove(\'bdo\').text()\n                    #print each_span.next().next().html()\n                    #break\n                    while each_span.next():\n                        if \'dd\' not in each_span.next().html():\n                            break\n                        _dict1 = {}\n                        _dict1[\'phrase\'] = each_span.next().find(\'dt\').text()\n                        _dict1[\'sentences\'] = []\n                        for each_div in each_span.next().find(\'dd > div\').items():\n                            _dict1[\'sentences\'].append(each_div.text())\n                        for each_div in each_span.next().find(\'dd li\').items():\n                            _dict1[\'sentences\'].append(each_div.text())\n                        each_span = each_span.next()\n                        #print each_span.html()\n                        phrase_detail.append(_dict1)\n                    _dict[\"phrase_detail\"] = phrase_detail\n                    annotation[each.text().strip()].append(_dict)\n            elif u\'词语用法\' in each.text():\n                annotation[each.text().strip()] = []\n                #_dict = {}\n\n                for each_span in each.next().find(\'span\').items():\n                    _dict = {}\n                    _dict[\"name\"] = each_span.find(\'bdo\').text()\n                    _dict[\"english_name\"] = each_span.remove(\'bdo\').text()\n                    _dict[\"sentences\"] = []\n                    for each_div in each_span.next().find(\'li\').items():\n                        _dict[\'sentences\'].append(each_div.text())\n                    annotation[each.text().strip()].append(_dict)\n                    \n        \n        \n        for each in response.doc(\'.rel > h3\').items():\n            if u\'缩略词\' in each.text():\n                recommend[each.text().strip()] = {}\n                _dict = {}\n                _dict[each.next().find(\'div\').text().split(u\'，\')[0]] = []\n                for each_li in each.next().find(\'div\').next().find(\'li\').items():\n                    _dict[each.next().find(\'div\').text().split(u\'，\')[0]].append(each_li.text())\n                recommend[each.text().strip()] = _dict\n            elif u\'临近单词\' in each.text():\n                recommend[each.text().strip()] = []\n                #_dict = {}\n\n                for each_a in each.next().find(\'a\').items():\n                    recommend[each.text().strip()].append(each_a.text())\n\n    \n            \n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text().replace(u\'海词\',u\'跟谁学\'),\n            \"word\": response.save.get(\'ci\'),\n            \"paraphrase\": paraphrase,\n            \"example\": example,\n            \"annotation\": annotation,\n            \"related_word\": related_word,\n            \"recommend\": recommend,\n        }',NULL,2.0000,3.0000,1468912645.7863),('cidian_haici','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-12 09:36:09\n# Project: cidian_haici\n\nfrom pyspider.libs.base_handler import *\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        i = 0\n        with open(\'/apps/home/rd/zengsheng/ciku.txt\') as f:\n            for line in f:\n                #print line\n                ci = line.strip().replace(\'\\n\',\'\')\n                #ci = \'good\'\n                #i += 1\n                #if i<44:\n                self.crawl(\'http://dict.cn/\'+ci, save = {\'ci\': ci}, callback=self.detail_page)\n                #else:\n                 #  break\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        paraphrase = {}\n        example = {}\n        annotation = {}\n        related_word = {}\n        for each in response.doc(\'.def > h3\').items():\n            if u\'行业释义\' in each.text():\n                continue\n            paraphrase[each.text()] = each.next().remove(\'.sound\').html().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n        \n        for each in response.doc(\'.sent > h3\').items():\n            if u\'常用短语\' in each.text():\n                annotation[each.text()] = each.next().remove(\'.sound\').html().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n                continue\n            if u\'词汇搭配\' in each.text():\n                related_word[each.text()] = each.next().remove(\'.sound\').text().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n                continue\n            example[each.text()] = each.next().remove(\'.sound\').remove(\'.more\').html().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n        \n        for each in response.doc(\'.learn > h3\').items():\n            annotation[each.text()] = each.next().remove(\'.sound\').html().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n        \n        for each in response.doc(\'.rel > h3\').items():\n            if u\'近反义词\' in each.text():\n                related_word[each.text()] = each.next().remove(\'.sound\').text().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n            \n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text().replace(u\'海词\',u\'跟谁学\'),\n            \"word\": response.save.get(\'ci\'),\n            \"paraphrase\": paraphrase,\n            \"example\": example,\n            \"annotation\": annotation,\n            \"related_word\": related_word,\n        }',NULL,5.0000,5.0000,1472624695.9217),('cidian_iciba','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-11 14:20:25\n# Project: cidian_iciba\n\nfrom pyspider.libs.base_handler import *\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        i = 0\n        with open(\'/apps/home/rd/zengsheng/ciku.txt\') as f:\n            for line in f:\n                #print line\n                ci = line.strip().replace(\'\\n\',\'\')\n                #ci = \'good\'\n                #i += 1\n                #if i<44:\n                self.crawl(\'http://www.iciba.com/\'+ci, save = {\'ci\': ci}, callback=self.detail_page)\n                #else:\n                 #   break\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        level = response.doc(\'.word-rate > p > i\').size()\n        flag_first = True\n        other_meaning = {}\n        character = {}\n        pronunciation = {}\n        for each in response.doc(\'.base-speak span\').items():\n            pronunciation[each.text()] = each.next().attr.onmouseover.split(\"(\'\")[1].split(\"\')\")[0]\n        for each in response.doc(\'.base-list\').items():\n            if flag_first:\n                flag_first = False\n                for each_li in each.find(\'li\').items():\n                    character[each_li.find(\'span\').text()] = each_li.find(\'p\').text()\n            else:\n                other_meaning[each.prev().text()] = each.text()\n        if len(character) == 0:\n            character[\'\'] = response.doc(\'.base-word-long\').text()\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text().replace(u\'爱词霸\',u\'跟谁学\'),\n            \"word\": response.save.get(\'ci\'),\n            \"type\": response.doc(\'.base-level > p\').text(),\n            \"pronunciation\": pronunciation,\n            \"character\":character,\n            \"other_meaning\": other_meaning,\n            \"change\": response.doc(\'.change\').find(\'p\').text(),\n            \"level\": level,\n        }',NULL,5.0000,5.0000,1472624698.3008),('cizu2_youdao','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-20 16:54:44\n# Project: cizu2_youdao\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'Accept\': \'*/*\',\n    \'Accept-Encoding\':\'gzip, deflate, sdch\',\n    \'Accept-Language\':\'zh-CN,zh;q=0.8\',\n    \'Cache-Control\':\'max-age=0\',\n    \'Connection\':\'keep-alive\',\n    \'Cookie\':\'_ntes_nnid=0bcf1ec80a7f728f778add2cf07877d5,1466150794390; OUTFOX_SEARCH_USER_ID_NCOO=1559550182.3308268; YOUDAO_EAD_UUID=bb83f57c-3189-401e-a9d5-6b6f45759154; search-popup-show=-1; tabRecord.webTrans=%23tEETrans; tabRecord.examples=%23authority; OUTFOX_SEARCH_USER_ID=-550718429@113.57.47.225; JSESSIONID=abcC4xkadz5D_5SeoFhyv; PICUGC_FLASH=; PICUGC_SESSION=90fbe28fda40b5ff04f343a261160ce83d3439fc-%00_TS%3Asession%00\',\n    \'Host\':\'dict.youdao.com\',\n    \'Referer\':\'http://dict.youdao.com/w/p_guest_professor/\',\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\',\n    \'X-Requested-With\':\'XMLHttpRequest\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://dict.youdao.com/map/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'h2\').eq(0).next().find(\'a\').items():\n            self.crawl(each.attr.href,headers = self.header,  callback=self.list_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'li > a\').items():\n            self.crawl(each.attr.href, headers = self.header, callback=self.list2_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list2_page(self, response):\n        for each in response.doc(\'td > a\').items():\n            self.crawl(each.attr.href, save = {\'word\': each.text()}, headers = self.header, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        paraphrase = {}\n        example = {}\n        annotation = {}\n        related_word = {}\n        recommend = {}\n        \n        for each in response.doc(\'#webTrans > h3\').items():\n            if u\'网络释义\' in each.text():\n                paraphrase[u\'网络释义\'] = []\n                for each_div in each.next().next().find(\'#tWebTrans > div\').items():\n                    _dict = {}\n                    #print \'<p>\'+each_div.html()+\'</p>\'\n                    if u\'短语\' in each_div.text():\n                        annotation[u\'常用短语\'] = {}\n                        for each_p in each_div.find(\'p\').items():\n                            k = each_p.find(\'span\').text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                            v = each_p.remove(\'span\').text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                            _dict[k] = v    \n                        annotation[u\'常用短语\'] = _dict\n                    else:\n                        _dict[\'name\'] = each_div.find(\'span\').text().strip().split()[0]\n                        _dict[\'meanings\'] = [each_div.find(\'p\').eq(0).text().replace(\'  \', \'\').replace(\'\\n\',\'\')]\n                        _dict[\'english_name\'] = \'\'\n                        paraphrase[u\'网络释义\'].append(_dict)\n            if u\'英英释义\' in each.text():\n                paraphrase[u\'英英释义\'] = []\n                for each_div in each.next().next().find(\'#tEETrans > div\').items():\n                    _dict = {}\n                    _dict[\'name\'] = \'\'\n                    _dict[\'english_name\'] = each_div.find(\'.ol\').prev().text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    _dict1 = {}\n                    for each_li in each_div.find(\'.ol > li\').items():\n                        _dict1[each_li.find(\'span\').text().replace(\'\\n\',\'\')] = [each_li.find(\'p\').text().replace(\'\\n\',\'\')]\n                    _dict[\'meanings\'] = _dict1\n                    paraphrase[u\'英英释义\'].append(_dict)\n        for each in response.doc(\'#examples > h3\').items():\n            \n            if u\'双语例句\' in each.text():\n                example[u\'双语例句\'] = []\n                for each_div in each.next().find(\'#bilingual li\').items():\n                    _dict = {}\n                    _dict[\'chinese\'] = each_div.find(\'p\').eq(1).text().replace(\' \', \'\').replace(\'\\n\',\'\')\n                    _dict[\'english\'] =  each_div.find(\'p\').eq(0).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    example[u\'双语例句\'].append(_dict)\n                    \n            if u\'原声例句\' in each.text():\n                example[u\'经典引文\'] = []\n                for each_div in each.next().find(\'#originalSound li\').items():\n                    _dict = {}\n                    _dict[\'source\'] = each_div.find(\'p\').eq(-1).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    _dict[\'sentence\'] =  each_div.find(\'p\').eq(0).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    example[u\'经典引文\'].append(_dict)\n                for each_div in each.next().find(\'#authority li\').items():\n                    _dict = {}\n                    _dict[\'source\'] = each_div.find(\'p\').eq(-1).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    _dict[\'sentence\'] =  each_div.find(\'p\').eq(0).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    example[u\'经典引文\'].append(_dict)\n                    \n        for each in response.doc(\'#rel-search\').items():\n            \n            if u\'相关搜索\' in each.text():\n                recommend[u\'相关搜索\'] = []\n                for each_a in each.find(\'a\').items():\n                    recommend[u\'相关搜索\'].append(each_a.text())\n              \n        ci_character = {}\n        if response.doc(\'.clearfix li\'):\n            ci_character[u\'词义\'] = response.doc(\'.clearfix li\').text()\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text().replace(u\'有道词典\',u\'跟谁学\'),\n            \"paraphrase\": paraphrase,\n            \"example\": example,\n            \"annotation\": annotation,\n            \"related_word\": related_word,\n            \"recommend\": recommend,  \n            \"dict\": {},\n            \"question\": {},\n            \"ci_type\": {},\n            \"ci_character\": ci_character,\n            \"ci_change\": {},\n            \"word\": response.save.get(\'word\'),\n            \"ci_level\": 0,\n            \"pronunciation\": {},\n            \"other_meaning\": {},\n        }',NULL,1.0000,3.0000,1472624779.0534),('cizu_youdao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-19 16:40:25\n# Project: cizu_youdao\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'Accept\': \'*/*\',\n    \'Accept-Encoding\':\'gzip, deflate, sdch\',\n    \'Accept-Language\':\'zh-CN,zh;q=0.8\',\n    \'Cache-Control\':\'max-age=0\',\n    \'Connection\':\'keep-alive\',\n    \'Cookie\':\'_ntes_nnid=0bcf1ec80a7f728f778add2cf07877d5,1466150794390; OUTFOX_SEARCH_USER_ID_NCOO=1559550182.3308268; YOUDAO_EAD_UUID=bb83f57c-3189-401e-a9d5-6b6f45759154; search-popup-show=-1; tabRecord.webTrans=%23tEETrans; tabRecord.examples=%23authority; OUTFOX_SEARCH_USER_ID=-550718429@113.57.47.225; JSESSIONID=abcC4xkadz5D_5SeoFhyv; PICUGC_FLASH=; PICUGC_SESSION=90fbe28fda40b5ff04f343a261160ce83d3439fc-%00_TS%3Asession%00\',\n    \'Host\':\'dict.youdao.com\',\n    \'Referer\':\'http://dict.youdao.com/w/p_guest_professor/\',\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\',\n    \'X-Requested-With\':\'XMLHttpRequest\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://dict.youdao.com/map/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'h2\').eq(0).next().find(\'a\').items():\n            self.crawl(each.attr.href,headers = self.header,  callback=self.list_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'li > a\').items():\n            self.crawl(each.attr.href, headers = self.header, callback=self.list2_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list2_page(self, response):\n        for each in response.doc(\'td > a\').items():\n            self.crawl(each.attr.href, save = {\'word\': each.text()}, headers = self.header, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        paraphrase = {}\n        example = {}\n        annotation = {}\n        related_word = {}\n        recommend = {}\n        \n        for each in response.doc(\'#webTrans > h3\').items():\n            if u\'网络释义\' in each.text():\n                paraphrase[u\'网络释义\'] = []\n                for each_div in each.next().next().find(\'#tWebTrans > div\').items():\n                    _dict = {}\n                    #print \'<p>\'+each_div.html()+\'</p>\'\n                    if u\'短语\' in each_div.text():\n                        annotation[u\'常用短语\'] = {}\n                        for each_p in each_div.find(\'p\').items():\n                            k = each_p.find(\'span\').text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                            v = each_p.remove(\'span\').text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                            _dict[k] = v    \n                        annotation[u\'常用短语\'] = _dict\n                    else:\n                        _dict[\'name\'] = each_div.find(\'span\').text().strip().split()[0]\n                        _dict[\'meanings\'] = [each_div.find(\'p\').eq(0).text().replace(\'  \', \'\').replace(\'\\n\',\'\')]\n                        _dict[\'english_name\'] = \'\'\n                        paraphrase[u\'网络释义\'].append(_dict)\n            if u\'英英释义\' in each.text():\n                paraphrase[u\'英英释义\'] = []\n                for each_div in each.next().next().find(\'#tEETrans > div\').items():\n                    _dict = {}\n                    _dict[\'name\'] = \'\'\n                    _dict[\'english_name\'] = each_div.find(\'.ol\').prev().text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    _dict1 = {}\n                    for each_li in each_div.find(\'.ol > li\').items():\n                        _dict1[each_li.find(\'span\').text().replace(\'\\n\',\'\')] = [each_li.find(\'p\').text().replace(\'\\n\',\'\')]\n                    _dict[\'meanings\'] = _dict1\n                    paraphrase[u\'英英释义\'].append(_dict)\n        for each in response.doc(\'#examples > h3\').items():\n            \n            if u\'双语例句\' in each.text():\n                example[u\'双语例句\'] = []\n                for each_div in each.next().find(\'#bilingual li\').items():\n                    _dict = {}\n                    _dict[\'chinese\'] = each_div.find(\'p\').eq(1).text().replace(\' \', \'\').replace(\'\\n\',\'\')\n                    _dict[\'english\'] =  each_div.find(\'p\').eq(0).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    example[u\'双语例句\'].append(_dict)\n                    \n            if u\'原声例句\' in each.text():\n                example[u\'经典引文\'] = []\n                for each_div in each.next().find(\'#originalSound li\').items():\n                    _dict = {}\n                    _dict[\'source\'] = each_div.find(\'p\').eq(-1).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    _dict[\'sentence\'] =  each_div.find(\'p\').eq(0).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    example[u\'经典引文\'].append(_dict)\n                for each_div in each.next().find(\'#authority li\').items():\n                    _dict = {}\n                    _dict[\'source\'] = each_div.find(\'p\').eq(-1).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    _dict[\'sentence\'] =  each_div.find(\'p\').eq(0).text().replace(\'  \', \'\').replace(\'\\n\',\'\')\n                    example[u\'经典引文\'].append(_dict)\n                    \n        for each in response.doc(\'#rel-search\').items():\n            \n            if u\'相关搜索\' in each.text():\n                recommend[u\'相关搜索\'] = []\n                for each_a in each.find(\'a\').items():\n                    recommend[u\'相关搜索\'].append(each_a.text())\n              \n        ci_character = {}\n        if response.doc(\'.clearfix li\'):\n            ci_character[u\'词义\'] = response.doc(\'.clearfix li\').text()\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text().replace(u\'有道词典\',u\'跟谁学\'),\n            \"paraphrase\": paraphrase,\n            \"example\": example,\n            \"annotation\": annotation,\n            \"related_word\": related_word,\n            \"recommend\": recommend,  \n            \"dict\": {},\n            \"question\": {},\n            \"ci_type\": {},\n            \"ci_character\": ci_character,\n            \"ci_change\": {},\n            \"word\": response.save.get(\'word\'),\n            \"ci_level\": 0,\n            \"pronunciation\": {},\n            \"other_meaning\": {},\n        }',NULL,5.0000,5.0000,1472624488.3861),('csdn_proxy','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 10:14:04\n# Project: csdn_proxy\n\n\n\nfrom pyspider.libs.base_handler import *\n\nimport json,re\nmenu = \'\'\'{\n    \"forumNodes\": [{\"name\":\"\\u79fb\\u52a8\\u5f00\\u53d1\",\"url\":\"/forums/Mobile\",\"children\":[{\"name\":\"iOS\",\"url\":\"/forums/ios\"},{\"name\":\"Android\",\"url\":\"/forums/Android\"},{\"name\":\"Swift\",\"url\":\"/forums/swift\"},{\"name\":\"Windows\\u5ba2\\u6237\\u7aef\\u5f00\\u53d1\",\"url\":\"/forums/WindowsMobile\"},{\"name\":\"Symbian\",\"url\":\"/forums/Symbian\"},{\"name\":\"BlackBerry\",\"url\":\"/forums/BlackBerry\"},{\"name\":\"Qt\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u79fb\\u52a8\\u652f\\u4ed8\",\"url\":\"/forums/PaypalCommunity\"},{\"name\":\"\\u79fb\\u52a8\\u5e7f\\u544a\",\"url\":\"/forums/MobileAD\"},{\"name\":\"\\u5fae\\u4fe1\\u5f00\\u53d1\",\"url\":\"/forums/weixin\"},{\"name\":\"\\u79fb\\u52a8\\u5f00\\u53d1\\u5176\\u4ed6\\u95ee\\u9898\",\"url\":\"/forums/Mobile_Other\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/MobileNonTechnical\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"},{\"name\":\"\\u8054\\u901aWO+\\u5f00\\u653e\\u5e73\\u53f0\",\"url\":\"/forums/chinaunicom\"}]},{\"name\":\"\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/CloudComputing\",\"children\":[{\"name\":\"IaaS\",\"children\":[{\"name\":\"OpenStack\",\"url\":\"/forums/OpenStack\"}]},{\"name\":\"PaaS/SaaS\",\"children\":[{\"name\":\"Cloud Foundry\",\"url\":\"/forums/CloudFoundry\"},{\"name\":\"GAE\",\"url\":\"/forums/GAE\"}]},{\"name\":\"\\u6570\\u636e\\u4e2d\\u5fc3\\u8fd0\\u7ef4\",\"children\":[{\"name\":\"\\u670d\\u52a1\\u5668\",\"url\":\"/forums/server\"},{\"name\":\"\\u7f51\\u7edc\",\"url\":\"/forums/network\"},{\"name\":\"\\u865a\\u62df\\u5316\",\"url\":\"/forums/virtual\"}]},{\"name\":\"AWS\",\"url\":\"/forums/AWS\"},{\"name\":\"\\u534e\\u4e3a\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/fusioncloud\"},{\"name\":\"\\u5f00\\u653e\\u5e73\\u53f0\",\"url\":\"/forums/OpenAPI\"},{\"name\":\"\\u4e91\\u5b89\\u5168\",\"url\":\"/forums/ST_Security\"},{\"name\":\"\\u5206\\u5e03\\u5f0f\\u8ba1\\u7b97/Hadoop\",\"url\":\"/forums/hadoop\"},{\"name\":\"\\u4e91\\u5b58\\u50a8\",\"url\":\"/forums/CloudStorage\"},{\"name\":\"Docker\",\"url\":\"/forums/docker\"},{\"name\":\"Spark\",\"url\":\"/forums/spark\"},{\"name\":\"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\",\"url\":\"/forums/DE\"}]},{\"name\":\"\\u4f01\\u4e1aIT\",\"url\":\"/forums/Enterprise\",\"children\":[{\"name\":\"\\u4e2d\\u95f4\\u4ef6\",\"children\":[{\"name\":\"\\u4e2d\\u95f4\\u4ef6\",\"url\":\"/forums/Middleware\"},{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"JBoss\",\"url\":\"/forums/JBoss\"}]},{\"name\":\"\\u4f01\\u4e1a\\u7ba1\\u7406\\u8f6f\\u4ef6\",\"children\":[{\"name\":\"\\u6d88\\u606f\\u534f\\u4f5c\",\"url\":\"/forums/ExchangeServer\"},{\"name\":\"SharePoint\",\"url\":\"/forums/SharePoint\"}]},{\"name\":\"Atlassian\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/atlassian\"},{\"name\":\"JetBrains\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/JetBrains\"},{\"name\":\"\\u5730\\u7406\\u4fe1\\u606f\\u7cfb\\u7edf\",\"url\":\"/forums/GIS\"},{\"name\":\"\\u4f01\\u4e1a\\u4fe1\\u606f\\u5316\",\"url\":\"/forums/Enterprise_Information\"},{\"name\":\"ERP/CRM\",\"url\":\"/forums/ERP\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/Enterprise_Other\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"},{\"name\":\"Enterprise Architect\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/EA\"}]},{\"name\":\".NET\\u6280\\u672f\",\"url\":\"/forums/DotNET\",\"children\":[{\"name\":\"C#\",\"url\":\"/forums/CSharp\"},{\"name\":\"ASP.NET\",\"url\":\"/forums/ASPDotNET\"},{\"name\":\".NET Framework\",\"url\":\"/forums/DotNETFramework\"},{\"name\":\"Web Services\",\"url\":\"/forums/DotNETWebServices\"},{\"name\":\"VB.NET\",\"url\":\"/forums/VBDotNET\"},{\"name\":\"VC.NET\",\"url\":\"/forums/VCDotNet\"},{\"name\":\"\\u56fe\\u8868\\u533a\",\"url\":\"/forums/DotNETReport\"},{\"name\":\".NET\\u6280\\u672f\\u524d\\u77bb\",\"url\":\"/forums/DotNET_NewTech\"},{\"name\":\".NET\\u5206\\u6790\\u4e0e\\u8bbe\\u8ba1\",\"url\":\"/forums/DotNETAnalysisAndDesign\"},{\"name\":\"\\u7ec4\\u4ef6/\\u63a7\\u4ef6\\u5f00\\u53d1\",\"url\":\"/forums/DotNET_Controls\"},{\"name\":\"SharePoint\",\"url\":\"/forums/SharePoint\"},{\"name\":\"WPF/Silverlight\",\"url\":\"/forums/Silverlight\"},{\"name\":\"LINQ\",\"url\":\"/forums/LINQ\"},{\"name\":\"VSTS\",\"url\":\"/forums/VSTS\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/DotNET_Other\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DotNETNonTechnical\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"}]},{\"name\":\"Java \\u6280\\u672f\",\"url\":\"/forums/Java\",\"children\":[{\"name\":\"Java SE\",\"url\":\"/forums/J2SE\"},{\"name\":\"J2ME\",\"url\":\"/forums/J2ME\"},{\"name\":\"Java Web \\u5f00\\u53d1\",\"url\":\"/forums/Java_WebDevelop\"},{\"name\":\"Java EE\",\"url\":\"/forums/J2EE\"},{\"name\":\"Eclipse\",\"url\":\"/forums/Eclipse\"},{\"name\":\"Java\\u5176\\u4ed6\\u76f8\\u5173\",\"url\":\"/forums/JavaOther\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/JavaNonTechnical\"},{\"name\":\"JBoss\",\"url\":\"/forums/JBoss\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"}]},{\"name\":\"Web \\u5f00\\u53d1\",\"url\":\"/forums/WebDevelop\",\"children\":[{\"name\":\"ASP\",\"url\":\"/forums/ASP\"},{\"name\":\"ASP.NET\",\"url\":\"/forums/ASPDotNET\"},{\"name\":\"JSP\",\"url\":\"/forums/Java_WebDevelop\"},{\"name\":\"PHP\",\"url\":\"/forums/PHP\",\"children\":[{\"name\":\"\\u5f00\\u6e90\\u8d44\\u6e90\",\"url\":\"/forums/PHPOpenSource\"},{\"name\":\"\\u57fa\\u7840\\u7f16\\u7a0b\",\"url\":\"/forums/PHPBase\"},{\"name\":\"Framework\",\"url\":\"/forums/PHPFramework\"}]},{\"name\":\"JavaScript\",\"url\":\"/forums/JavaScript\"},{\"name\":\"\\u641c\\u7d22\\u5f15\\u64ce\\u6280\\u672f\",\"url\":\"/forums/SearchEngine\"},{\"name\":\"Ajax \\u6280\\u672f\",\"url\":\"/forums/Ajax\"},{\"name\":\"VBScript\",\"url\":\"/forums/vbScript\"},{\"name\":\"CGI\",\"url\":\"/forums/CGI\"},{\"name\":\"XML/XSL\",\"url\":\"/forums/XMLSOAP\"},{\"name\":\"IIS\",\"url\":\"/forums/IIS\"},{\"name\":\"Apache\",\"url\":\"/forums/Apache\"},{\"name\":\"HTML(CSS)\",\"url\":\"/forums/HTMLCSS\"},{\"name\":\"ColdFusion\",\"url\":\"/forums/ColdFusion\"},{\"name\":\"Ruby/Rails\",\"url\":\"/forums/ROR\"},{\"name\":\"\\u8de8\\u6d4f\\u89c8\\u5668\\u5f00\\u53d1\",\"url\":\"/forums/CrossBrowser\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/WebDevelop_Other\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/WebNonTechnical\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"HTML5\",\"url\":\"/forums/HTML5\"}]},{\"name\":\"\\u5f00\\u53d1\\u8bed\\u8a00/\\u6846\\u67b6\",\"children\":[{\"name\":\"VC/MFC\",\"url\":\"/forums/VC\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/VC_Basic\"},{\"name\":\"\\u754c\\u9762\",\"url\":\"/forums/VC_UI\"},{\"name\":\"\\u7f51\\u7edc\\u7f16\\u7a0b\",\"url\":\"/forums/VC_Network\"},{\"name\":\"\\u8fdb\\u7a0b/\\u7ebf\\u7a0b/DLL\",\"url\":\"/forums/VC_Process\"},{\"name\":\"ATL/ActiveX/COM\",\"url\":\"/forums/VC_ActiveX\"},{\"name\":\"\\u6570\\u636e\\u5e93\",\"url\":\"/forums/VC_Database\"},{\"name\":\"\\u786c\\u4ef6/\\u7cfb\\u7edf\",\"url\":\"/forums/VC_Hardware\"},{\"name\":\"HTML/XML\",\"url\":\"/forums/VC_HTML\"},{\"name\":\"\\u56fe\\u5f62\\u5904\\u7406/\\u7b97\\u6cd5\",\"url\":\"/forums/VC_ImageProcessing\"},{\"name\":\"\\u8d44\\u6e90\",\"url\":\"/forums/VCResources\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7c7b\",\"url\":\"/forums/VC_NonTechnical\"}]},{\"name\":\"VB\",\"url\":\"/forums/VB\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/VB_Basic\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7c7b\",\"url\":\"/forums/VB_NonTechnical\"},{\"name\":\"\\u63a7\\u4ef6\",\"url\":\"/forums/VB_Controls\"},{\"name\":\"API\",\"url\":\"/forums/VB_API\"},{\"name\":\"\\u6570\\u636e\\u5e93(\\u5305\\u542b\\u6253\\u5370\\uff0c\\u5b89\\u88c5\\uff0c\\u62a5\\u8868)\",\"url\":\"/forums/VB_Database\"},{\"name\":\"\\u591a\\u5a92\\u4f53\",\"url\":\"/forums/VB_Multimedia\"},{\"name\":\"\\u7f51\\u7edc\\u7f16\\u7a0b\",\"url\":\"/forums/VB_Network\"},{\"name\":\"VBA\",\"url\":\"/forums/VBA\"},{\"name\":\"COM/DCOM/COM+\",\"url\":\"/forums/VB_COM\"},{\"name\":\"\\u8d44\\u6e90\",\"url\":\"/forums/VBResources\"}]},{\"name\":\"Delphi\",\"url\":\"/forums/Delphi\",\"children\":[{\"name\":\"VCL\\u7ec4\\u4ef6\\u5f00\\u53d1\\u53ca\\u5e94\\u7528\",\"url\":\"/forums/DelphiVCL\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u76f8\\u5173\",\"url\":\"/forums/DelphiDB\"},{\"name\":\"Windows SDK/API\",\"url\":\"/forums/DelphiAPI\"},{\"name\":\"\\u7f51\\u7edc\\u901a\\u4fe1/\\u5206\\u5e03\\u5f0f\\u5f00\\u53d1\",\"url\":\"/forums/DelphiNetwork\"},{\"name\":\"\\u8bed\\u8a00\\u57fa\\u7840/\\u7b97\\u6cd5/\\u7cfb\\u7edf\\u8bbe\\u8ba1\",\"url\":\"/forums/DelphiBase\"},{\"name\":\"GAME\\uff0c\\u56fe\\u5f62\\u5904\\u7406/\\u591a\\u5a92\\u4f53\",\"url\":\"/forums/DelphiMultimedia\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DelphiNonTechnical\"}]},{\"name\":\"C++ Builder\",\"url\":\"/forums/BCB\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/BCBBase\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u53ca\\u76f8\\u5173\\u6280\\u672f\",\"url\":\"/forums/BCBDB\"},{\"name\":\"VCL\\u7ec4\\u4ef6\\u4f7f\\u7528\\u548c\\u5f00\\u53d1\",\"url\":\"/forums/BCBVCL\"},{\"name\":\"Windows SDK/API\",\"url\":\"/forums/BCBAPI\"},{\"name\":\"\\u7f51\\u7edc\\u53ca\\u901a\\u8baf\\u5f00\\u53d1\",\"url\":\"/forums/BCBNetwork\"},{\"name\":\"ActiveX/COM/DCOM\",\"url\":\"/forums/BCBCOM\"},{\"name\":\"\\u8336\\u9986\",\"url\":\"/forums/BCBTeaHouses\"}]},{\"name\":\"C/C++\",\"url\":\"/forums/Cpp\",\"children\":[{\"name\":\"\\u65b0\\u624b\\u4e50\\u56ed\",\"url\":\"/forums/Cpp_Freshman\"},{\"name\":\"C\\u8bed\\u8a00\",\"url\":\"/forums/C\"},{\"name\":\"C++ \\u8bed\\u8a00\",\"url\":\"/forums/CPPLanguage\"},{\"name\":\"\\u5de5\\u5177\\u5e73\\u53f0\\u548c\\u7a0b\\u5e8f\\u5e93\",\"url\":\"/forums/Cpp_ToolsPlatform\"},{\"name\":\"\\u6a21\\u5f0f\\u53ca\\u5b9e\\u73b0\",\"url\":\"/forums/Cpp_Model\"},{\"name\":\"\\u5176\\u4ed6\\u6280\\u672f\\u95ee\\u9898\",\"url\":\"/forums/Cpp_Other\"},{\"name\":\"Qt\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Cpp_NonTechnical\"}]},{\"name\":\"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00\",\"url\":\"/forums/OtherLanguage\",\"open\":true,\"children\":[{\"name\":\"OpenCL\\u548c\\u5f02\\u6784\\u7f16\\u7a0b\",\"url\":\"/forums/Heterogeneous\"},{\"name\":\"Go\\u8bed\\u8a00\",\"url\":\"/forums/golang\"},{\"name\":\"JBoss\\u6280\\u672f\\u4ea4\\u6d41\",\"url\":\"/forums/JBoss\"},{\"name\":\"\\u6c47\\u7f16\\u8bed\\u8a00\",\"url\":\"/forums/ASM\"},{\"name\":\"\\u811a\\u672c\\u8bed\\u8a00\\uff08Perl/Python\\uff09\",\"url\":\"/forums/OL_Script\"},{\"name\":\"Office\\u5f00\\u53d1/ VBA\",\"url\":\"/forums/OfficeDevelopment\"},{\"name\":\"VFP\",\"url\":\"/forums/VFP\"},{\"name\":\"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00\",\"url\":\"/forums/OtherLanguage_Other\"}]}]},{\"name\":\"\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":null,\"children\":[{\"name\":\"\\u5927\\u6570\\u636e\",\"children\":[{\"name\":\"Hadoop\",\"url\":\"/forums/hadoop\"}]},{\"name\":\"MS-SQL Server\",\"url\":\"/forums/MSSQL\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/MSSQL_Basic\"},{\"name\":\"\\u5e94\\u7528\\u5b9e\\u4f8b\",\"url\":\"/forums/MSSQL_Cases\"},{\"name\":\"\\u7591\\u96be\\u95ee\\u9898\",\"url\":\"/forums/MSSQL_DifficultProblems\"},{\"name\":\"\\u65b0\\u6280\\u672f\\u524d\\u6cbf\",\"url\":\"/forums/MSSQL_NewTech\"},{\"name\":\"SQL Server BI\",\"url\":\"/forums/SQLSERVERBI\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7248\",\"url\":\"/forums/MSSQL_NonTechnical\"}]},{\"name\":\"PowerBuilder\",\"url\":\"/forums/PowerBuilder\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/PB_Basic\"},{\"name\":\"Pb\\u811a\\u672c\\u8bed\\u8a00\",\"url\":\"/forums/PBScript\"},{\"name\":\"DataWindow\",\"url\":\"/forums/PB_DataWindow\"},{\"name\":\"API \\u8c03\\u7528\",\"url\":\"/forums/PB_API\"},{\"name\":\"\\u63a7\\u4ef6\\u4e0e\\u754c\\u9762\",\"url\":\"/forums/PB_Controls\"},{\"name\":\"Pb Web \\u5e94\\u7528\",\"url\":\"/forums/PB_WEB\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u76f8\\u5173\",\"url\":\"/forums/PB_Database\"},{\"name\":\"\\u9879\\u76ee\\u7ba1\\u7406\",\"url\":\"/forums/PB_ProjectManagement\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7248\",\"url\":\"/forums/PB_NonTechnical\"}]},{\"name\":\"Oracle\",\"url\":\"/forums/Oracle\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u548c\\u7ba1\\u7406\",\"url\":\"/forums/Oracle_Management\"},{\"name\":\"\\u5f00\\u53d1\",\"url\":\"/forums/Oracle_Develop\"},{\"name\":\"\\u9ad8\\u7ea7\\u6280\\u672f\",\"url\":\"/forums/Oracle_Technology\"},{\"name\":\"\\u8ba4\\u8bc1\\u4e0e\\u8003\\u8bd5\",\"url\":\"/forums/Oracle_Certificate\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Oracle_NonTechnical\"}]},{\"name\":\"Informatica\",\"url\":\"/forums/Informatica\"},{\"name\":\"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/OtherDatabase\",\"open\":true,\"children\":[{\"name\":\"IBM DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"MongoDB\",\"url\":\"/forums/MongoDB\"},{\"name\":\"\\u6570\\u636e\\u4ed3\\u5e93\",\"url\":\"/forums/DataWarehouse\"},{\"name\":\"VFP\",\"url\":\"/forums/VFP\"},{\"name\":\"Access\",\"url\":\"/forums/Access\"},{\"name\":\"Sybase\",\"url\":\"/forums/Sybase\"},{\"name\":\"Informix\",\"url\":\"/forums/Informix\"},{\"name\":\"MySQL\",\"url\":\"/forums/MySQL\"},{\"name\":\"PostgreSQL\",\"url\":\"/forums/PostgreSQL\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u62a5\\u8868\",\"url\":\"/forums/DatabaseReport\"},{\"name\":\"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\",\"url\":\"/forums/OtherDatabase_Other\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/HPDatabase\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DatabaseNonTechnical\"}]}]},{\"name\":\"Linux/Unix\\u793e\\u533a\",\"url\":\"/forums/Linux\",\"children\":[{\"name\":\"\\u7cfb\\u7edf\\u7ef4\\u62a4\\u4e0e\\u4f7f\\u7528\\u533a\",\"url\":\"/forums/Linux_System\"},{\"name\":\"\\u5e94\\u7528\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a\",\"url\":\"/forums/Linux_Development\"},{\"name\":\"\\u5185\\u6838\\u6e90\\u4ee3\\u7801\\u7814\\u7a76\\u533a\",\"url\":\"/forums/Linux_Kernel\"},{\"name\":\"\\u9a71\\u52a8\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a\",\"url\":\"/forums/Linux_Driver\"},{\"name\":\"CPU\\u548c\\u786c\\u4ef6\\u533a\",\"url\":\"/forums/Linux_Hardware\"},{\"name\":\"\\u4e13\\u9898\\u6280\\u672f\\u8ba8\\u8bba\\u533a\",\"url\":\"/forums/Linux_SpecialTopic\"},{\"name\":\"\\u5b9e\\u7528\\u8d44\\u6599\\u53d1\\u5e03\\u533a\",\"url\":\"/forums/Linux_Information\"},{\"name\":\"UNIX\\u6587\\u5316\",\"url\":\"/forums/Unix_Culture\"},{\"name\":\"Solaris\",\"url\":\"/forums/Solaris\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/LinuxNonTechnical\"}]},{\"name\":\"Windows\\u4e13\\u533a\",\"url\":\"/forums/Windows\",\"children\":[{\"name\":\"Windows\\u5ba2\\u6237\\u7aef\\u4f7f\\u7528\",\"url\":\"/forums/Windows7\"},{\"name\":\"Windows Server\",\"url\":\"/forums/WinNT2000XP2003\"},{\"name\":\"\\u7f51\\u7edc\\u7ba1\\u7406\\u4e0e\\u914d\\u7f6e\",\"url\":\"/forums/NetworkConfiguration\"},{\"name\":\"\\u5b89\\u5168\\u6280\\u672f/\\u75c5\\u6bd2\",\"url\":\"/forums/WindowsSecurity\"},{\"name\":\"\\u4e00\\u822c\\u8f6f\\u4ef6\\u4f7f\\u7528\",\"url\":\"/forums/WindowsBase\"},{\"name\":\"Microsoft Office\\u5e94\\u7528\",\"url\":\"/forums/OfficeBase\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/WindowsNonTechnical\"}]},{\"name\":\"\\u786c\\u4ef6/\\u5d4c\\u5165\\u5f00\\u53d1\",\"url\":\"/forums/Embedded\",\"children\":[{\"name\":\"\\u5d4c\\u5165\\u5f00\\u53d1(WinCE)\",\"url\":\"/forums/WinCE\"},{\"name\":\"\\u6c47\\u7f16\\u8bed\\u8a00\",\"url\":\"/forums/ASM\"},{\"name\":\"\\u786c\\u4ef6\\u8bbe\\u8ba1\",\"url\":\"/forums/Embedded_hardware\"},{\"name\":\"\\u9a71\\u52a8\\u5f00\\u53d1/\\u6838\\u5fc3\\u5f00\\u53d1\",\"url\":\"/forums/Embedded_driver\"},{\"name\":\"\\u5355\\u7247\\u673a/\\u5de5\\u63a7\",\"url\":\"/forums/Embedded_SCM\"},{\"name\":\"\\u65e0\\u7ebf\",\"url\":\"/forums/Embedded_wireless\"},{\"name\":\"\\u5176\\u4ed6\\u786c\\u4ef6\\u5f00\\u53d1\",\"url\":\"/forums/Embedded_Other\"},{\"name\":\"VxWorks\\u5f00\\u53d1\",\"url\":\"/forums/VxWorks\"},{\"name\":\"Qt\\u5f00\\u53d1\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/EmbeddedNonTechnical\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\",\"url\":\"/forums/HPC\"},{\"name\":\"\\u667a\\u80fd\\u786c\\u4ef6\",\"url\":\"/forums/SmartHardware\"}]},{\"name\":\"\\u6e38\\u620f\\u5f00\\u53d1\",\"url\":\"/forums/GameDevelop\",\"children\":[{\"name\":\"Cocos2d-x\",\"url\":\"/forums/GD_Cocos2d-x\"},{\"name\":\"Unity3D\",\"url\":\"/forums/GD_Unity3D\"},{\"name\":\"\\u5176\\u4ed6\\u6e38\\u620f\\u5f15\\u64ce\",\"url\":\"/forums/Othergameengines\"},{\"name\":\"\\u6e38\\u620f\\u7b56\\u5212\\u4e0e\\u8fd0\\u8425\",\"url\":\"/forums/Gdesignoperation\"}]},{\"name\":\"\\u7f51\\u7edc\\u4e0e\\u901a\\u4fe1\",\"url\":\"/forums/network_communication\",\"children\":[{\"name\":\"\\u7f51\\u7edc\\u534f\\u8bae\\u4e0e\\u914d\\u7f6e\",\"url\":\"/forums/IP_Protocolconfiguration\"},{\"name\":\"\\u7f51\\u7edc\\u7ef4\\u62a4\\u4e0e\\u7ba1\\u7406\",\"url\":\"/forums/maintainmanage\"},{\"name\":\"\\u4ea4\\u6362\\u53ca\\u8def\\u7531\\u6280\\u672f\",\"url\":\"/forums/Hardware_SwitchRouter\"},{\"name\":\"CDN\",\"url\":\"/forums/NetworkC_CDN\"},{\"name\":\"\\u901a\\u4fe1\\u6280\\u672f\",\"url\":\"/forums/ST_Network\"},{\"name\":\"VOIP\\u6280\\u672f\\u63a2\\u8ba8\",\"url\":\"/forums/voip\"}]},{\"name\":\"\\u6269\\u5145\\u8bdd\\u9898\",\"url\":\"/forums/Other\",\"children\":[{\"name\":\"\\u704c\\u6c34\\u4e50\\u56ed\",\"url\":\"/forums/FreeZone\"},{\"name\":\"\\u7a0b\\u5e8f\\u4eba\\u751f\",\"url\":\"/forums/ProgrammerStory\"},{\"name\":\"\\u7a0b\\u5e8f\\u5a9b\\u4e16\\u754c\",\"url\":\"/forums/ProgramGirls\"},{\"name\":\"\\u7a0b\\u5e8f\\u5458\\u4ea4\\u53cb\",\"url\":\"/forums/ProgramFriends\"},{\"name\":\"\\u4e09\\u5341\\u800c\\u7acb\",\"url\":\"/forums/30Plus\"},{\"name\":\"\\u6e38\\u620f\\u4e13\\u533a\",\"url\":\"/forums/Game\"},{\"name\":\"\\u4e1a\\u754c\\u65b0\\u95fb\",\"url\":\"/forums/ITnews\"},{\"name\":\"\\u7a0b\\u5e8f\\u5458\\u82f1\\u8bed\",\"url\":\"/forums/English\"},{\"name\":\"\\u6c42\\u804c\\u4e0e\\u62db\\u8058\",\"url\":\"/forums/CAREER\"},{\"name\":\"\\u8ba1\\u7b97\\u673a\\u56fe\\u4e66\",\"url\":\"/forums/Book\"},{\"name\":\"\\u5927\\u5b66\\u65f6\\u4ee3\",\"url\":\"/forums/CollegeTime\"},{\"name\":\"\\u8df3\\u86a4\\u5e02\\u573a\",\"url\":\"/forums/Trade\"},{\"name\":\"\\u8f6f\\u4ef6\\u6c42\\u52a9\",\"url\":\"/forums/Shareware\"}]},{\"name\":\"\\u6328\\u8e22\\u804c\\u6daf\",\"url\":\"/forums/CAREER\",\"children\":[{\"name\":\"\\u6c42\\u804c\\u9762\\u8bd5\",\"url\":\"/forums/WorkplaceCommunication\"},{\"name\":\"\\u4f01\\u4e1a\\u70b9\\u8bc4\",\"url\":\"/forums/TECHHUNT\"},{\"name\":\"\\u804c\\u573a\\u8bdd\\u9898\",\"url\":\"/forums/OFFICELIFE\"},{\"name\":\"JOB \\u9a7f\\u7ad9\",\"url\":\"/forums/jobservice\"}]},{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u793e\\u533a\",\"url\":\"/forums/eSDK\",\"children\":[{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b\",\"url\":\"/forums/DevChallenge2016\"},{\"name\":\"\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/hwfsdeveloper\"},{\"name\":\"\\u4f01\\u4e1a\\u901a\\u4fe1\",\"url\":\"/forums/hwucdeveloper\"},{\"name\":\"BYOD\",\"url\":\"/forums/hwbyoddeveloper\"},{\"name\":\"\\u5927\\u6570\\u636e\",\"children\":[{\"name\":\"FusionInsight HD\",\"url\":\"/forums/fusioninsightdeveloper\"},{\"name\":\"FusionInsight Universe\",\"url\":\"/forums/hwuniversedeveloper\"}]},{\"name\":\"Digital inCloud\",\"url\":\"/forums/hwswdeveloper\"},{\"name\":\"CaaS\",\"url\":\"/forums/hwcndeveloper\"},{\"name\":\"SDN\",\"url\":\"/forums/hwsdndeveloper\"},{\"name\":\"\\u4f01\\u4e1a\\u7f51\\u7edc\\u5f00\\u53d1\",\"url\":\"/forums/hwendeveloper\"},{\"name\":\"\\u654f\\u6377\\u7f51\\u7edc\",\"url\":\"/forums/hwesightdeveloper\"},{\"name\":\"eLTE\",\"url\":\"/forums/hwbbtdeveloper\"},{\"name\":\"\\u7269\\u8054\\u7f51\\u5f00\\u53d1\",\"url\":\"/forums/hwiotdeveloper\"},{\"name\":\"\\u79fb\\u52a8\\u5f00\\u653e\\u5de5\\u573a\",\"url\":\"/forums/hwwldeveloper\"},{\"name\":\"OpenLife\\u667a\\u6167\\u5bb6\\u5ead\",\"url\":\"/forums/OpenLife\"},{\"name\":\"HUAWEI Code Craft\",\"url\":\"/forums/hwcodecraft\"}]},{\"name\":\"IBM \\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"Rational\",\"url\":\"/forums/Rational\"},{\"name\":\"Lotus\",\"url\":\"/forums/Lotus\"},{\"name\":\"IBM\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/ibmcloud\"},{\"name\":\"IBM \\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"Tivoli\",\"url\":\"/forums/Tivoli\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"}]},{\"name\":\"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a\",\"children\":[{\"name\":\"\\u82f1\\u7279\\u5c14\\u6280\\u672f\",\"url\":\"/forums/intel\"}]},{\"name\":\"Qualcomm\\u5f00\\u53d1\\u8bba\\u575b\",\"children\":[{\"name\":\"Qualcomm\\u5f00\\u53d1\",\"url\":\"/forums/qualcomm\"}]},{\"name\":\"\\u4f01\\u4e1a\\u6280\\u672f\",\"children\":[{\"name\":\"IBM \\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"Rational\",\"url\":\"/forums/Rational\"},{\"name\":\"Lotus\",\"url\":\"/forums/Lotus\"},{\"name\":\"IBM\\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"Tivoli\",\"url\":\"/forums/Tivoli\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"}]},{\"name\":\"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a\",\"children\":[{\"name\":\"\\u82f1\\u7279\\u5c14\\u6280\\u672f\",\"url\":\"/forums/intel\"}]},{\"name\":\"T\\u5ba2\\u8bba\\u575b\",\"url\":\"/forums/tcl\"},{\"name\":\"Paypal\\u5f00\\u53d1\\u8005\\u793e\\u533a\",\"url\":\"/forums/PaypalCommunity\"},{\"name\":\"CUDA\",\"url\":\"/forums/CUDA\",\"children\":[{\"name\":\"CUDA\\u7f16\\u7a0b\",\"url\":\"/forums/CUDA_Dev\"},{\"name\":\"CUDA\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\\u8ba8\\u8bba\",\"url\":\"/forums/CUDA_Compute\"},{\"name\":\"CUDA on Linux\",\"url\":\"/forums/CUDA_Linux\"},{\"name\":\"CUDA on Windows XP\",\"url\":\"/forums/CUDA_WinXP\"}]},{\"name\":\"Google\\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"Google\\u6280\\u672f\\u793e\\u533a\",\"url\":\"/forums/GoogleCommunity\"},{\"name\":\"Android\",\"url\":\"/forums/Android\"}]},{\"name\":\"Microsoft Office \\u5e94\\u7528\\u4e8e\\u5f00\\u53d1\",\"children\":[{\"name\":\"Office\\u5f00\\u53d1\",\"url\":\"/forums/OfficeDevelopment\"},{\"name\":\"Office\\u4f7f\\u7528\",\"url\":\"/forums/OfficeBase\"}]}]},{\"name\":\"\\u5176\\u4ed6\\u6280\\u672f\\u8bba\\u575b\",\"children\":[{\"name\":\"\\u8f6f\\u4ef6\\u5de5\\u7a0b/\\u7ba1\\u7406\",\"url\":\"/forums/SE\",\"children\":[{\"name\":\"\\u8f6f\\u4ef6\\u6d4b\\u8bd5\",\"url\":\"/forums/SE_Quality\"},{\"name\":\"\\u7814\\u53d1\\u7ba1\\u7406\",\"url\":\"/forums/SE_Management\"},{\"name\":\"\\u654f\\u6377\\u5f00\\u53d1\",\"url\":\"/forums/Agile\"},{\"name\":\"\\u7248\\u672c\\u63a7\\u5236\",\"url\":\"/forums/CVS_SVN\"},{\"name\":\"\\u8bbe\\u8ba1\\u6a21\\u5f0f\",\"url\":\"/forums/DesignPatterns\"}]},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u5f00\\u53d1\",\"url\":\"/forums/HPDevelopment\",\"children\":[{\"name\":\"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\",\"url\":\"/forums/HPC\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/HPDatabase\"},{\"name\":\"\\u6d77\\u91cf\\u6570\\u636e\\u5904\\u7406/\\u641c\\u7d22\\u6280\\u672f\",\"url\":\"/forums/SearchEngine\"},{\"name\":\"\\u6570\\u636e\\u7ed3\\u6784\\u4e0e\\u7b97\\u6cd5\",\"url\":\"/forums/ST_Arithmetic\"}]},{\"name\":\"\\u4e13\\u9898\\u5f00\\u53d1/\\u6280\\u672f/\\u9879\\u76ee\",\"url\":\"/forums/SpecialTopic\",\"children\":[{\"name\":\"OpenAPI\",\"url\":\"/forums/OpenAPI\"},{\"name\":\"OpenStack\",\"url\":\"/forums/OpenStack\"},{\"name\":\"\\u673a\\u5668\\u89c6\\u89c9\",\"url\":\"/forums/ST_Image\"},{\"name\":\"OpenCV\",\"url\":\"/forums/OpenCV\"},{\"name\":\"\\u4fe1\\u606f/\\u7f51\\u7edc\\u5b89\\u5168\",\"url\":\"/forums/ST_Security\"},{\"name\":\"\\u4eba\\u5de5\\u667a\\u80fd\\u6280\\u672f\",\"url\":\"/forums/AI\"},{\"name\":\"\\u8d28\\u91cf\\u7ba1\\u7406/\\u8f6f\\u4ef6\\u6d4b\\u8bd5\",\"url\":\"/forums/SE_Quality\"}]},{\"name\":\"\\u591a\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/MediaAndFlash\",\"children\":[{\"name\":\"\\u591a\\u5a92\\u4f53/\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/Multimedia\"},{\"name\":\"\\u56fe\\u8c61\\u5de5\\u5177\\u4f7f\\u7528\",\"url\":\"/forums/ImageTools\"},{\"name\":\"Flash\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/FlashDevelop\"},{\"name\":\"\\u4ea4\\u4e92\\u5f0f\\u8bbe\\u8ba1\",\"url\":\"/forums/InteractiveDesign\"},{\"name\":\"WPF/Silverlight\",\"url\":\"/forums/Silverlight\"},{\"name\":\"Flex\",\"url\":\"/forums/Flex\"}]},{\"name\":\"\\u786c\\u4ef6\\u4f7f\\u7528\",\"url\":\"/forums/HardwareUse\",\"children\":[{\"name\":\"\\u6570\\u7801\\u8bbe\\u5907\",\"url\":\"/forums/Hardware_Digital\"},{\"name\":\"\\u7535\\u8111\\u6574\\u673a\\u53ca\\u914d\\u4ef6\",\"url\":\"/forums/Hardware_Computer\"},{\"name\":\"\\u5916\\u8bbe\\u53ca\\u529e\\u516c\\u8bbe\\u5907\",\"url\":\"/forums/Hardware_Peripheral\"},{\"name\":\"\\u88c5\\u673a\\u4e0e\\u5347\\u7ea7\\u53ca\\u5176\\u4ed6\",\"url\":\"/forums/Hardware_DIY\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Hardware_NonTechnical\"}]},{\"name\":\"\\u4ea7\\u54c1/\\u5382\\u5bb6\",\"url\":\"/forums/ADS\",\"children\":[{\"name\":\"IBM \\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"\\u5fae\\u521b\\u8f6f\\u4ef6\\u5f00\\u53d1\\u7ba1\\u7406\",\"url\":\"/forums/WeiChuang\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/ADSOther\"}]}]},{\"name\":\"\\u57f9\\u8bad\\u8ba4\\u8bc1\",\"url\":\"/forums/Trainning\",\"children\":[{\"name\":\"IT\\u57f9\\u8bad\",\"url\":\"/forums/ITCertificate\"}]},{\"name\":\"\\u7ad9\\u52a1\\u4e13\\u533a\",\"url\":\"/forums/Support\",\"children\":[{\"name\":\"\\u793e\\u533a\\u516c\\u544a\",\"url\":\"/forums/placard\"},{\"name\":\"\\u6d3b\\u52a8\\u4e13\\u533a\",\"url\":\"/forums/Activity\"},{\"name\":\"\\u5ba2\\u670d\\u4e13\\u533a\",\"url\":\"/forums/Service\"},{\"name\":\"\\u7248\\u4e3b\\u4e13\\u533a\",\"url\":\"/forums/Moderator\"},{\"name\":\"\\u300a\\u7a0b\\u5e8f\\u5458\\u300b\\u6742\\u5fd7\",\"url\":\"/forums/Programmer\"}]}],\n    \"isLogined\": \"false\",\n    \"isModerator\": \"false\",\n    \"favoriteForumUrls\": [],\n    \"lastForumNodes\": [{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b \",\"url\":\"/forums/DevChallenge2016\"},{\"name\":\"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\\u8bba\\u575b\",\"url\":\"/forums/DE\"},{\"name\":\"OpenCV\",\"url\":\"/forums/OpenCV\"},{\"name\":\"FusionInsight HD\",\"url\":\"/forums/fusioninsightdeveloper\"},{\"name\":\"HUAWEI Code Craft\",\"url\":\"/forums/hwcodecraft\"},{\"name\":\"JetBrains\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/JetBrains\"},{\"name\":\"Enterprise Architect\",\"url\":\"/forums/EA\"}]\n  }\'\'\'\nmenu = json.loads(menu)\n\n\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'周前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'个月前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'年前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'headers\':{\n#\'proxy\':\'123.161.133.18:63574\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n},\n\'proxy\':\'171.38.166.177:8123\',\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for each in menu[\'forumNodes\']:\n            if each.has_key(\'children\'):\n                for ea in each[\'children\']:\n                    if ea.has_key(\'url\'):\n                            url = ea[\'url\']\n                            self.crawl(\'http://bbs.csdn.net\'+url+\'/closed\',headers=self.crawl_config[\'headers\'],proxy=self.crawl_config[\'proxy\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.title > a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text()\n            self.crawl(each.attr.href, save = _dict,headers=self.crawl_config[\'headers\'],proxy=self.crawl_config[\'proxy\'], callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],proxy=self.crawl_config[\'proxy\'],callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        #print len(response.doc(\'table.post.topic\'))\n        content_list = []\n        for info in response.doc(\'.post\').items():\n            if \'topic\' in info.attr[\'class\']:\n                continue\n            if u\'被管理员删除\' in info.find(\'.post_body\').remove(\'fieldset\').html().strip():\n                continue\n            _dic = {}\n            #print info.find(\'.answer_author\').text()\n            _dic[\'name\'] = info.find(\'.username > a\').text()\n            _dic[\'date\'] = info.find(\'.time\').text().split()[-2]\n            _dic[\'content\'] = info.find(\'.post_body\').remove(\'fieldset\').html().strip()\n            content_list.append((_dic))\n        \n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'程序员\',\n            \"answers\": content_list,\n            \"source\": u\'csdn\',\n            \"question_detail\": re.sub(\'<!--.*\',\'\',response.doc(\'.topic > .post_body\').remove(\'*\').html()).strip(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1472624485.3056),('csdn_proxy2',NULL,'RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 10:14:04\n# Project: csdn_proxy2\n\n\n\nfrom pyspider.libs.base_handler import *\n\nimport json,re\nmenu = \'\'\'{\n    \"forumNodes\": [{\"name\":\"\\u79fb\\u52a8\\u5f00\\u53d1\",\"url\":\"/forums/Mobile\",\"children\":[{\"name\":\"iOS\",\"url\":\"/forums/ios\"},{\"name\":\"Android\",\"url\":\"/forums/Android\"},{\"name\":\"Swift\",\"url\":\"/forums/swift\"},{\"name\":\"Windows\\u5ba2\\u6237\\u7aef\\u5f00\\u53d1\",\"url\":\"/forums/WindowsMobile\"},{\"name\":\"Symbian\",\"url\":\"/forums/Symbian\"},{\"name\":\"BlackBerry\",\"url\":\"/forums/BlackBerry\"},{\"name\":\"Qt\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u79fb\\u52a8\\u652f\\u4ed8\",\"url\":\"/forums/PaypalCommunity\"},{\"name\":\"\\u79fb\\u52a8\\u5e7f\\u544a\",\"url\":\"/forums/MobileAD\"},{\"name\":\"\\u5fae\\u4fe1\\u5f00\\u53d1\",\"url\":\"/forums/weixin\"},{\"name\":\"\\u79fb\\u52a8\\u5f00\\u53d1\\u5176\\u4ed6\\u95ee\\u9898\",\"url\":\"/forums/Mobile_Other\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/MobileNonTechnical\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"},{\"name\":\"\\u8054\\u901aWO+\\u5f00\\u653e\\u5e73\\u53f0\",\"url\":\"/forums/chinaunicom\"}]},{\"name\":\"\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/CloudComputing\",\"children\":[{\"name\":\"IaaS\",\"children\":[{\"name\":\"OpenStack\",\"url\":\"/forums/OpenStack\"}]},{\"name\":\"PaaS/SaaS\",\"children\":[{\"name\":\"Cloud Foundry\",\"url\":\"/forums/CloudFoundry\"},{\"name\":\"GAE\",\"url\":\"/forums/GAE\"}]},{\"name\":\"\\u6570\\u636e\\u4e2d\\u5fc3\\u8fd0\\u7ef4\",\"children\":[{\"name\":\"\\u670d\\u52a1\\u5668\",\"url\":\"/forums/server\"},{\"name\":\"\\u7f51\\u7edc\",\"url\":\"/forums/network\"},{\"name\":\"\\u865a\\u62df\\u5316\",\"url\":\"/forums/virtual\"}]},{\"name\":\"AWS\",\"url\":\"/forums/AWS\"},{\"name\":\"\\u534e\\u4e3a\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/fusioncloud\"},{\"name\":\"\\u5f00\\u653e\\u5e73\\u53f0\",\"url\":\"/forums/OpenAPI\"},{\"name\":\"\\u4e91\\u5b89\\u5168\",\"url\":\"/forums/ST_Security\"},{\"name\":\"\\u5206\\u5e03\\u5f0f\\u8ba1\\u7b97/Hadoop\",\"url\":\"/forums/hadoop\"},{\"name\":\"\\u4e91\\u5b58\\u50a8\",\"url\":\"/forums/CloudStorage\"},{\"name\":\"Docker\",\"url\":\"/forums/docker\"},{\"name\":\"Spark\",\"url\":\"/forums/spark\"},{\"name\":\"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\",\"url\":\"/forums/DE\"}]},{\"name\":\"\\u4f01\\u4e1aIT\",\"url\":\"/forums/Enterprise\",\"children\":[{\"name\":\"\\u4e2d\\u95f4\\u4ef6\",\"children\":[{\"name\":\"\\u4e2d\\u95f4\\u4ef6\",\"url\":\"/forums/Middleware\"},{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"JBoss\",\"url\":\"/forums/JBoss\"}]},{\"name\":\"\\u4f01\\u4e1a\\u7ba1\\u7406\\u8f6f\\u4ef6\",\"children\":[{\"name\":\"\\u6d88\\u606f\\u534f\\u4f5c\",\"url\":\"/forums/ExchangeServer\"},{\"name\":\"SharePoint\",\"url\":\"/forums/SharePoint\"}]},{\"name\":\"Atlassian\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/atlassian\"},{\"name\":\"JetBrains\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/JetBrains\"},{\"name\":\"\\u5730\\u7406\\u4fe1\\u606f\\u7cfb\\u7edf\",\"url\":\"/forums/GIS\"},{\"name\":\"\\u4f01\\u4e1a\\u4fe1\\u606f\\u5316\",\"url\":\"/forums/Enterprise_Information\"},{\"name\":\"ERP/CRM\",\"url\":\"/forums/ERP\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/Enterprise_Other\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"},{\"name\":\"Enterprise Architect\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/EA\"}]},{\"name\":\".NET\\u6280\\u672f\",\"url\":\"/forums/DotNET\",\"children\":[{\"name\":\"C#\",\"url\":\"/forums/CSharp\"},{\"name\":\"ASP.NET\",\"url\":\"/forums/ASPDotNET\"},{\"name\":\".NET Framework\",\"url\":\"/forums/DotNETFramework\"},{\"name\":\"Web Services\",\"url\":\"/forums/DotNETWebServices\"},{\"name\":\"VB.NET\",\"url\":\"/forums/VBDotNET\"},{\"name\":\"VC.NET\",\"url\":\"/forums/VCDotNet\"},{\"name\":\"\\u56fe\\u8868\\u533a\",\"url\":\"/forums/DotNETReport\"},{\"name\":\".NET\\u6280\\u672f\\u524d\\u77bb\",\"url\":\"/forums/DotNET_NewTech\"},{\"name\":\".NET\\u5206\\u6790\\u4e0e\\u8bbe\\u8ba1\",\"url\":\"/forums/DotNETAnalysisAndDesign\"},{\"name\":\"\\u7ec4\\u4ef6/\\u63a7\\u4ef6\\u5f00\\u53d1\",\"url\":\"/forums/DotNET_Controls\"},{\"name\":\"SharePoint\",\"url\":\"/forums/SharePoint\"},{\"name\":\"WPF/Silverlight\",\"url\":\"/forums/Silverlight\"},{\"name\":\"LINQ\",\"url\":\"/forums/LINQ\"},{\"name\":\"VSTS\",\"url\":\"/forums/VSTS\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/DotNET_Other\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DotNETNonTechnical\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"}]},{\"name\":\"Java \\u6280\\u672f\",\"url\":\"/forums/Java\",\"children\":[{\"name\":\"Java SE\",\"url\":\"/forums/J2SE\"},{\"name\":\"J2ME\",\"url\":\"/forums/J2ME\"},{\"name\":\"Java Web \\u5f00\\u53d1\",\"url\":\"/forums/Java_WebDevelop\"},{\"name\":\"Java EE\",\"url\":\"/forums/J2EE\"},{\"name\":\"Eclipse\",\"url\":\"/forums/Eclipse\"},{\"name\":\"Java\\u5176\\u4ed6\\u76f8\\u5173\",\"url\":\"/forums/JavaOther\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/JavaNonTechnical\"},{\"name\":\"JBoss\",\"url\":\"/forums/JBoss\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"}]},{\"name\":\"Web \\u5f00\\u53d1\",\"url\":\"/forums/WebDevelop\",\"children\":[{\"name\":\"ASP\",\"url\":\"/forums/ASP\"},{\"name\":\"ASP.NET\",\"url\":\"/forums/ASPDotNET\"},{\"name\":\"JSP\",\"url\":\"/forums/Java_WebDevelop\"},{\"name\":\"PHP\",\"url\":\"/forums/PHP\",\"children\":[{\"name\":\"\\u5f00\\u6e90\\u8d44\\u6e90\",\"url\":\"/forums/PHPOpenSource\"},{\"name\":\"\\u57fa\\u7840\\u7f16\\u7a0b\",\"url\":\"/forums/PHPBase\"},{\"name\":\"Framework\",\"url\":\"/forums/PHPFramework\"}]},{\"name\":\"JavaScript\",\"url\":\"/forums/JavaScript\"},{\"name\":\"\\u641c\\u7d22\\u5f15\\u64ce\\u6280\\u672f\",\"url\":\"/forums/SearchEngine\"},{\"name\":\"Ajax \\u6280\\u672f\",\"url\":\"/forums/Ajax\"},{\"name\":\"VBScript\",\"url\":\"/forums/vbScript\"},{\"name\":\"CGI\",\"url\":\"/forums/CGI\"},{\"name\":\"XML/XSL\",\"url\":\"/forums/XMLSOAP\"},{\"name\":\"IIS\",\"url\":\"/forums/IIS\"},{\"name\":\"Apache\",\"url\":\"/forums/Apache\"},{\"name\":\"HTML(CSS)\",\"url\":\"/forums/HTMLCSS\"},{\"name\":\"ColdFusion\",\"url\":\"/forums/ColdFusion\"},{\"name\":\"Ruby/Rails\",\"url\":\"/forums/ROR\"},{\"name\":\"\\u8de8\\u6d4f\\u89c8\\u5668\\u5f00\\u53d1\",\"url\":\"/forums/CrossBrowser\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/WebDevelop_Other\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/WebNonTechnical\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"HTML5\",\"url\":\"/forums/HTML5\"}]},{\"name\":\"\\u5f00\\u53d1\\u8bed\\u8a00/\\u6846\\u67b6\",\"children\":[{\"name\":\"VC/MFC\",\"url\":\"/forums/VC\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/VC_Basic\"},{\"name\":\"\\u754c\\u9762\",\"url\":\"/forums/VC_UI\"},{\"name\":\"\\u7f51\\u7edc\\u7f16\\u7a0b\",\"url\":\"/forums/VC_Network\"},{\"name\":\"\\u8fdb\\u7a0b/\\u7ebf\\u7a0b/DLL\",\"url\":\"/forums/VC_Process\"},{\"name\":\"ATL/ActiveX/COM\",\"url\":\"/forums/VC_ActiveX\"},{\"name\":\"\\u6570\\u636e\\u5e93\",\"url\":\"/forums/VC_Database\"},{\"name\":\"\\u786c\\u4ef6/\\u7cfb\\u7edf\",\"url\":\"/forums/VC_Hardware\"},{\"name\":\"HTML/XML\",\"url\":\"/forums/VC_HTML\"},{\"name\":\"\\u56fe\\u5f62\\u5904\\u7406/\\u7b97\\u6cd5\",\"url\":\"/forums/VC_ImageProcessing\"},{\"name\":\"\\u8d44\\u6e90\",\"url\":\"/forums/VCResources\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7c7b\",\"url\":\"/forums/VC_NonTechnical\"}]},{\"name\":\"VB\",\"url\":\"/forums/VB\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/VB_Basic\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7c7b\",\"url\":\"/forums/VB_NonTechnical\"},{\"name\":\"\\u63a7\\u4ef6\",\"url\":\"/forums/VB_Controls\"},{\"name\":\"API\",\"url\":\"/forums/VB_API\"},{\"name\":\"\\u6570\\u636e\\u5e93(\\u5305\\u542b\\u6253\\u5370\\uff0c\\u5b89\\u88c5\\uff0c\\u62a5\\u8868)\",\"url\":\"/forums/VB_Database\"},{\"name\":\"\\u591a\\u5a92\\u4f53\",\"url\":\"/forums/VB_Multimedia\"},{\"name\":\"\\u7f51\\u7edc\\u7f16\\u7a0b\",\"url\":\"/forums/VB_Network\"},{\"name\":\"VBA\",\"url\":\"/forums/VBA\"},{\"name\":\"COM/DCOM/COM+\",\"url\":\"/forums/VB_COM\"},{\"name\":\"\\u8d44\\u6e90\",\"url\":\"/forums/VBResources\"}]},{\"name\":\"Delphi\",\"url\":\"/forums/Delphi\",\"children\":[{\"name\":\"VCL\\u7ec4\\u4ef6\\u5f00\\u53d1\\u53ca\\u5e94\\u7528\",\"url\":\"/forums/DelphiVCL\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u76f8\\u5173\",\"url\":\"/forums/DelphiDB\"},{\"name\":\"Windows SDK/API\",\"url\":\"/forums/DelphiAPI\"},{\"name\":\"\\u7f51\\u7edc\\u901a\\u4fe1/\\u5206\\u5e03\\u5f0f\\u5f00\\u53d1\",\"url\":\"/forums/DelphiNetwork\"},{\"name\":\"\\u8bed\\u8a00\\u57fa\\u7840/\\u7b97\\u6cd5/\\u7cfb\\u7edf\\u8bbe\\u8ba1\",\"url\":\"/forums/DelphiBase\"},{\"name\":\"GAME\\uff0c\\u56fe\\u5f62\\u5904\\u7406/\\u591a\\u5a92\\u4f53\",\"url\":\"/forums/DelphiMultimedia\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DelphiNonTechnical\"}]},{\"name\":\"C++ Builder\",\"url\":\"/forums/BCB\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/BCBBase\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u53ca\\u76f8\\u5173\\u6280\\u672f\",\"url\":\"/forums/BCBDB\"},{\"name\":\"VCL\\u7ec4\\u4ef6\\u4f7f\\u7528\\u548c\\u5f00\\u53d1\",\"url\":\"/forums/BCBVCL\"},{\"name\":\"Windows SDK/API\",\"url\":\"/forums/BCBAPI\"},{\"name\":\"\\u7f51\\u7edc\\u53ca\\u901a\\u8baf\\u5f00\\u53d1\",\"url\":\"/forums/BCBNetwork\"},{\"name\":\"ActiveX/COM/DCOM\",\"url\":\"/forums/BCBCOM\"},{\"name\":\"\\u8336\\u9986\",\"url\":\"/forums/BCBTeaHouses\"}]},{\"name\":\"C/C++\",\"url\":\"/forums/Cpp\",\"children\":[{\"name\":\"\\u65b0\\u624b\\u4e50\\u56ed\",\"url\":\"/forums/Cpp_Freshman\"},{\"name\":\"C\\u8bed\\u8a00\",\"url\":\"/forums/C\"},{\"name\":\"C++ \\u8bed\\u8a00\",\"url\":\"/forums/CPPLanguage\"},{\"name\":\"\\u5de5\\u5177\\u5e73\\u53f0\\u548c\\u7a0b\\u5e8f\\u5e93\",\"url\":\"/forums/Cpp_ToolsPlatform\"},{\"name\":\"\\u6a21\\u5f0f\\u53ca\\u5b9e\\u73b0\",\"url\":\"/forums/Cpp_Model\"},{\"name\":\"\\u5176\\u4ed6\\u6280\\u672f\\u95ee\\u9898\",\"url\":\"/forums/Cpp_Other\"},{\"name\":\"Qt\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Cpp_NonTechnical\"}]},{\"name\":\"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00\",\"url\":\"/forums/OtherLanguage\",\"open\":true,\"children\":[{\"name\":\"OpenCL\\u548c\\u5f02\\u6784\\u7f16\\u7a0b\",\"url\":\"/forums/Heterogeneous\"},{\"name\":\"Go\\u8bed\\u8a00\",\"url\":\"/forums/golang\"},{\"name\":\"JBoss\\u6280\\u672f\\u4ea4\\u6d41\",\"url\":\"/forums/JBoss\"},{\"name\":\"\\u6c47\\u7f16\\u8bed\\u8a00\",\"url\":\"/forums/ASM\"},{\"name\":\"\\u811a\\u672c\\u8bed\\u8a00\\uff08Perl/Python\\uff09\",\"url\":\"/forums/OL_Script\"},{\"name\":\"Office\\u5f00\\u53d1/ VBA\",\"url\":\"/forums/OfficeDevelopment\"},{\"name\":\"VFP\",\"url\":\"/forums/VFP\"},{\"name\":\"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00\",\"url\":\"/forums/OtherLanguage_Other\"}]}]},{\"name\":\"\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":null,\"children\":[{\"name\":\"\\u5927\\u6570\\u636e\",\"children\":[{\"name\":\"Hadoop\",\"url\":\"/forums/hadoop\"}]},{\"name\":\"MS-SQL Server\",\"url\":\"/forums/MSSQL\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/MSSQL_Basic\"},{\"name\":\"\\u5e94\\u7528\\u5b9e\\u4f8b\",\"url\":\"/forums/MSSQL_Cases\"},{\"name\":\"\\u7591\\u96be\\u95ee\\u9898\",\"url\":\"/forums/MSSQL_DifficultProblems\"},{\"name\":\"\\u65b0\\u6280\\u672f\\u524d\\u6cbf\",\"url\":\"/forums/MSSQL_NewTech\"},{\"name\":\"SQL Server BI\",\"url\":\"/forums/SQLSERVERBI\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7248\",\"url\":\"/forums/MSSQL_NonTechnical\"}]},{\"name\":\"PowerBuilder\",\"url\":\"/forums/PowerBuilder\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/PB_Basic\"},{\"name\":\"Pb\\u811a\\u672c\\u8bed\\u8a00\",\"url\":\"/forums/PBScript\"},{\"name\":\"DataWindow\",\"url\":\"/forums/PB_DataWindow\"},{\"name\":\"API \\u8c03\\u7528\",\"url\":\"/forums/PB_API\"},{\"name\":\"\\u63a7\\u4ef6\\u4e0e\\u754c\\u9762\",\"url\":\"/forums/PB_Controls\"},{\"name\":\"Pb Web \\u5e94\\u7528\",\"url\":\"/forums/PB_WEB\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u76f8\\u5173\",\"url\":\"/forums/PB_Database\"},{\"name\":\"\\u9879\\u76ee\\u7ba1\\u7406\",\"url\":\"/forums/PB_ProjectManagement\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7248\",\"url\":\"/forums/PB_NonTechnical\"}]},{\"name\":\"Oracle\",\"url\":\"/forums/Oracle\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u548c\\u7ba1\\u7406\",\"url\":\"/forums/Oracle_Management\"},{\"name\":\"\\u5f00\\u53d1\",\"url\":\"/forums/Oracle_Develop\"},{\"name\":\"\\u9ad8\\u7ea7\\u6280\\u672f\",\"url\":\"/forums/Oracle_Technology\"},{\"name\":\"\\u8ba4\\u8bc1\\u4e0e\\u8003\\u8bd5\",\"url\":\"/forums/Oracle_Certificate\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Oracle_NonTechnical\"}]},{\"name\":\"Informatica\",\"url\":\"/forums/Informatica\"},{\"name\":\"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/OtherDatabase\",\"open\":true,\"children\":[{\"name\":\"IBM DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"MongoDB\",\"url\":\"/forums/MongoDB\"},{\"name\":\"\\u6570\\u636e\\u4ed3\\u5e93\",\"url\":\"/forums/DataWarehouse\"},{\"name\":\"VFP\",\"url\":\"/forums/VFP\"},{\"name\":\"Access\",\"url\":\"/forums/Access\"},{\"name\":\"Sybase\",\"url\":\"/forums/Sybase\"},{\"name\":\"Informix\",\"url\":\"/forums/Informix\"},{\"name\":\"MySQL\",\"url\":\"/forums/MySQL\"},{\"name\":\"PostgreSQL\",\"url\":\"/forums/PostgreSQL\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u62a5\\u8868\",\"url\":\"/forums/DatabaseReport\"},{\"name\":\"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\",\"url\":\"/forums/OtherDatabase_Other\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/HPDatabase\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DatabaseNonTechnical\"}]}]},{\"name\":\"Linux/Unix\\u793e\\u533a\",\"url\":\"/forums/Linux\",\"children\":[{\"name\":\"\\u7cfb\\u7edf\\u7ef4\\u62a4\\u4e0e\\u4f7f\\u7528\\u533a\",\"url\":\"/forums/Linux_System\"},{\"name\":\"\\u5e94\\u7528\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a\",\"url\":\"/forums/Linux_Development\"},{\"name\":\"\\u5185\\u6838\\u6e90\\u4ee3\\u7801\\u7814\\u7a76\\u533a\",\"url\":\"/forums/Linux_Kernel\"},{\"name\":\"\\u9a71\\u52a8\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a\",\"url\":\"/forums/Linux_Driver\"},{\"name\":\"CPU\\u548c\\u786c\\u4ef6\\u533a\",\"url\":\"/forums/Linux_Hardware\"},{\"name\":\"\\u4e13\\u9898\\u6280\\u672f\\u8ba8\\u8bba\\u533a\",\"url\":\"/forums/Linux_SpecialTopic\"},{\"name\":\"\\u5b9e\\u7528\\u8d44\\u6599\\u53d1\\u5e03\\u533a\",\"url\":\"/forums/Linux_Information\"},{\"name\":\"UNIX\\u6587\\u5316\",\"url\":\"/forums/Unix_Culture\"},{\"name\":\"Solaris\",\"url\":\"/forums/Solaris\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/LinuxNonTechnical\"}]},{\"name\":\"Windows\\u4e13\\u533a\",\"url\":\"/forums/Windows\",\"children\":[{\"name\":\"Windows\\u5ba2\\u6237\\u7aef\\u4f7f\\u7528\",\"url\":\"/forums/Windows7\"},{\"name\":\"Windows Server\",\"url\":\"/forums/WinNT2000XP2003\"},{\"name\":\"\\u7f51\\u7edc\\u7ba1\\u7406\\u4e0e\\u914d\\u7f6e\",\"url\":\"/forums/NetworkConfiguration\"},{\"name\":\"\\u5b89\\u5168\\u6280\\u672f/\\u75c5\\u6bd2\",\"url\":\"/forums/WindowsSecurity\"},{\"name\":\"\\u4e00\\u822c\\u8f6f\\u4ef6\\u4f7f\\u7528\",\"url\":\"/forums/WindowsBase\"},{\"name\":\"Microsoft Office\\u5e94\\u7528\",\"url\":\"/forums/OfficeBase\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/WindowsNonTechnical\"}]},{\"name\":\"\\u786c\\u4ef6/\\u5d4c\\u5165\\u5f00\\u53d1\",\"url\":\"/forums/Embedded\",\"children\":[{\"name\":\"\\u5d4c\\u5165\\u5f00\\u53d1(WinCE)\",\"url\":\"/forums/WinCE\"},{\"name\":\"\\u6c47\\u7f16\\u8bed\\u8a00\",\"url\":\"/forums/ASM\"},{\"name\":\"\\u786c\\u4ef6\\u8bbe\\u8ba1\",\"url\":\"/forums/Embedded_hardware\"},{\"name\":\"\\u9a71\\u52a8\\u5f00\\u53d1/\\u6838\\u5fc3\\u5f00\\u53d1\",\"url\":\"/forums/Embedded_driver\"},{\"name\":\"\\u5355\\u7247\\u673a/\\u5de5\\u63a7\",\"url\":\"/forums/Embedded_SCM\"},{\"name\":\"\\u65e0\\u7ebf\",\"url\":\"/forums/Embedded_wireless\"},{\"name\":\"\\u5176\\u4ed6\\u786c\\u4ef6\\u5f00\\u53d1\",\"url\":\"/forums/Embedded_Other\"},{\"name\":\"VxWorks\\u5f00\\u53d1\",\"url\":\"/forums/VxWorks\"},{\"name\":\"Qt\\u5f00\\u53d1\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/EmbeddedNonTechnical\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\",\"url\":\"/forums/HPC\"},{\"name\":\"\\u667a\\u80fd\\u786c\\u4ef6\",\"url\":\"/forums/SmartHardware\"}]},{\"name\":\"\\u6e38\\u620f\\u5f00\\u53d1\",\"url\":\"/forums/GameDevelop\",\"children\":[{\"name\":\"Cocos2d-x\",\"url\":\"/forums/GD_Cocos2d-x\"},{\"name\":\"Unity3D\",\"url\":\"/forums/GD_Unity3D\"},{\"name\":\"\\u5176\\u4ed6\\u6e38\\u620f\\u5f15\\u64ce\",\"url\":\"/forums/Othergameengines\"},{\"name\":\"\\u6e38\\u620f\\u7b56\\u5212\\u4e0e\\u8fd0\\u8425\",\"url\":\"/forums/Gdesignoperation\"}]},{\"name\":\"\\u7f51\\u7edc\\u4e0e\\u901a\\u4fe1\",\"url\":\"/forums/network_communication\",\"children\":[{\"name\":\"\\u7f51\\u7edc\\u534f\\u8bae\\u4e0e\\u914d\\u7f6e\",\"url\":\"/forums/IP_Protocolconfiguration\"},{\"name\":\"\\u7f51\\u7edc\\u7ef4\\u62a4\\u4e0e\\u7ba1\\u7406\",\"url\":\"/forums/maintainmanage\"},{\"name\":\"\\u4ea4\\u6362\\u53ca\\u8def\\u7531\\u6280\\u672f\",\"url\":\"/forums/Hardware_SwitchRouter\"},{\"name\":\"CDN\",\"url\":\"/forums/NetworkC_CDN\"},{\"name\":\"\\u901a\\u4fe1\\u6280\\u672f\",\"url\":\"/forums/ST_Network\"},{\"name\":\"VOIP\\u6280\\u672f\\u63a2\\u8ba8\",\"url\":\"/forums/voip\"}]},{\"name\":\"\\u6269\\u5145\\u8bdd\\u9898\",\"url\":\"/forums/Other\",\"children\":[{\"name\":\"\\u704c\\u6c34\\u4e50\\u56ed\",\"url\":\"/forums/FreeZone\"},{\"name\":\"\\u7a0b\\u5e8f\\u4eba\\u751f\",\"url\":\"/forums/ProgrammerStory\"},{\"name\":\"\\u7a0b\\u5e8f\\u5a9b\\u4e16\\u754c\",\"url\":\"/forums/ProgramGirls\"},{\"name\":\"\\u7a0b\\u5e8f\\u5458\\u4ea4\\u53cb\",\"url\":\"/forums/ProgramFriends\"},{\"name\":\"\\u4e09\\u5341\\u800c\\u7acb\",\"url\":\"/forums/30Plus\"},{\"name\":\"\\u6e38\\u620f\\u4e13\\u533a\",\"url\":\"/forums/Game\"},{\"name\":\"\\u4e1a\\u754c\\u65b0\\u95fb\",\"url\":\"/forums/ITnews\"},{\"name\":\"\\u7a0b\\u5e8f\\u5458\\u82f1\\u8bed\",\"url\":\"/forums/English\"},{\"name\":\"\\u6c42\\u804c\\u4e0e\\u62db\\u8058\",\"url\":\"/forums/CAREER\"},{\"name\":\"\\u8ba1\\u7b97\\u673a\\u56fe\\u4e66\",\"url\":\"/forums/Book\"},{\"name\":\"\\u5927\\u5b66\\u65f6\\u4ee3\",\"url\":\"/forums/CollegeTime\"},{\"name\":\"\\u8df3\\u86a4\\u5e02\\u573a\",\"url\":\"/forums/Trade\"},{\"name\":\"\\u8f6f\\u4ef6\\u6c42\\u52a9\",\"url\":\"/forums/Shareware\"}]},{\"name\":\"\\u6328\\u8e22\\u804c\\u6daf\",\"url\":\"/forums/CAREER\",\"children\":[{\"name\":\"\\u6c42\\u804c\\u9762\\u8bd5\",\"url\":\"/forums/WorkplaceCommunication\"},{\"name\":\"\\u4f01\\u4e1a\\u70b9\\u8bc4\",\"url\":\"/forums/TECHHUNT\"},{\"name\":\"\\u804c\\u573a\\u8bdd\\u9898\",\"url\":\"/forums/OFFICELIFE\"},{\"name\":\"JOB \\u9a7f\\u7ad9\",\"url\":\"/forums/jobservice\"}]},{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u793e\\u533a\",\"url\":\"/forums/eSDK\",\"children\":[{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b\",\"url\":\"/forums/DevChallenge2016\"},{\"name\":\"\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/hwfsdeveloper\"},{\"name\":\"\\u4f01\\u4e1a\\u901a\\u4fe1\",\"url\":\"/forums/hwucdeveloper\"},{\"name\":\"BYOD\",\"url\":\"/forums/hwbyoddeveloper\"},{\"name\":\"\\u5927\\u6570\\u636e\",\"children\":[{\"name\":\"FusionInsight HD\",\"url\":\"/forums/fusioninsightdeveloper\"},{\"name\":\"FusionInsight Universe\",\"url\":\"/forums/hwuniversedeveloper\"}]},{\"name\":\"Digital inCloud\",\"url\":\"/forums/hwswdeveloper\"},{\"name\":\"CaaS\",\"url\":\"/forums/hwcndeveloper\"},{\"name\":\"SDN\",\"url\":\"/forums/hwsdndeveloper\"},{\"name\":\"\\u4f01\\u4e1a\\u7f51\\u7edc\\u5f00\\u53d1\",\"url\":\"/forums/hwendeveloper\"},{\"name\":\"\\u654f\\u6377\\u7f51\\u7edc\",\"url\":\"/forums/hwesightdeveloper\"},{\"name\":\"eLTE\",\"url\":\"/forums/hwbbtdeveloper\"},{\"name\":\"\\u7269\\u8054\\u7f51\\u5f00\\u53d1\",\"url\":\"/forums/hwiotdeveloper\"},{\"name\":\"\\u79fb\\u52a8\\u5f00\\u653e\\u5de5\\u573a\",\"url\":\"/forums/hwwldeveloper\"},{\"name\":\"OpenLife\\u667a\\u6167\\u5bb6\\u5ead\",\"url\":\"/forums/OpenLife\"},{\"name\":\"HUAWEI Code Craft\",\"url\":\"/forums/hwcodecraft\"}]},{\"name\":\"IBM \\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"Rational\",\"url\":\"/forums/Rational\"},{\"name\":\"Lotus\",\"url\":\"/forums/Lotus\"},{\"name\":\"IBM\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/ibmcloud\"},{\"name\":\"IBM \\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"Tivoli\",\"url\":\"/forums/Tivoli\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"}]},{\"name\":\"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a\",\"children\":[{\"name\":\"\\u82f1\\u7279\\u5c14\\u6280\\u672f\",\"url\":\"/forums/intel\"}]},{\"name\":\"Qualcomm\\u5f00\\u53d1\\u8bba\\u575b\",\"children\":[{\"name\":\"Qualcomm\\u5f00\\u53d1\",\"url\":\"/forums/qualcomm\"}]},{\"name\":\"\\u4f01\\u4e1a\\u6280\\u672f\",\"children\":[{\"name\":\"IBM \\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"Rational\",\"url\":\"/forums/Rational\"},{\"name\":\"Lotus\",\"url\":\"/forums/Lotus\"},{\"name\":\"IBM\\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"Tivoli\",\"url\":\"/forums/Tivoli\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"}]},{\"name\":\"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a\",\"children\":[{\"name\":\"\\u82f1\\u7279\\u5c14\\u6280\\u672f\",\"url\":\"/forums/intel\"}]},{\"name\":\"T\\u5ba2\\u8bba\\u575b\",\"url\":\"/forums/tcl\"},{\"name\":\"Paypal\\u5f00\\u53d1\\u8005\\u793e\\u533a\",\"url\":\"/forums/PaypalCommunity\"},{\"name\":\"CUDA\",\"url\":\"/forums/CUDA\",\"children\":[{\"name\":\"CUDA\\u7f16\\u7a0b\",\"url\":\"/forums/CUDA_Dev\"},{\"name\":\"CUDA\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\\u8ba8\\u8bba\",\"url\":\"/forums/CUDA_Compute\"},{\"name\":\"CUDA on Linux\",\"url\":\"/forums/CUDA_Linux\"},{\"name\":\"CUDA on Windows XP\",\"url\":\"/forums/CUDA_WinXP\"}]},{\"name\":\"Google\\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"Google\\u6280\\u672f\\u793e\\u533a\",\"url\":\"/forums/GoogleCommunity\"},{\"name\":\"Android\",\"url\":\"/forums/Android\"}]},{\"name\":\"Microsoft Office \\u5e94\\u7528\\u4e8e\\u5f00\\u53d1\",\"children\":[{\"name\":\"Office\\u5f00\\u53d1\",\"url\":\"/forums/OfficeDevelopment\"},{\"name\":\"Office\\u4f7f\\u7528\",\"url\":\"/forums/OfficeBase\"}]}]},{\"name\":\"\\u5176\\u4ed6\\u6280\\u672f\\u8bba\\u575b\",\"children\":[{\"name\":\"\\u8f6f\\u4ef6\\u5de5\\u7a0b/\\u7ba1\\u7406\",\"url\":\"/forums/SE\",\"children\":[{\"name\":\"\\u8f6f\\u4ef6\\u6d4b\\u8bd5\",\"url\":\"/forums/SE_Quality\"},{\"name\":\"\\u7814\\u53d1\\u7ba1\\u7406\",\"url\":\"/forums/SE_Management\"},{\"name\":\"\\u654f\\u6377\\u5f00\\u53d1\",\"url\":\"/forums/Agile\"},{\"name\":\"\\u7248\\u672c\\u63a7\\u5236\",\"url\":\"/forums/CVS_SVN\"},{\"name\":\"\\u8bbe\\u8ba1\\u6a21\\u5f0f\",\"url\":\"/forums/DesignPatterns\"}]},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u5f00\\u53d1\",\"url\":\"/forums/HPDevelopment\",\"children\":[{\"name\":\"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\",\"url\":\"/forums/HPC\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/HPDatabase\"},{\"name\":\"\\u6d77\\u91cf\\u6570\\u636e\\u5904\\u7406/\\u641c\\u7d22\\u6280\\u672f\",\"url\":\"/forums/SearchEngine\"},{\"name\":\"\\u6570\\u636e\\u7ed3\\u6784\\u4e0e\\u7b97\\u6cd5\",\"url\":\"/forums/ST_Arithmetic\"}]},{\"name\":\"\\u4e13\\u9898\\u5f00\\u53d1/\\u6280\\u672f/\\u9879\\u76ee\",\"url\":\"/forums/SpecialTopic\",\"children\":[{\"name\":\"OpenAPI\",\"url\":\"/forums/OpenAPI\"},{\"name\":\"OpenStack\",\"url\":\"/forums/OpenStack\"},{\"name\":\"\\u673a\\u5668\\u89c6\\u89c9\",\"url\":\"/forums/ST_Image\"},{\"name\":\"OpenCV\",\"url\":\"/forums/OpenCV\"},{\"name\":\"\\u4fe1\\u606f/\\u7f51\\u7edc\\u5b89\\u5168\",\"url\":\"/forums/ST_Security\"},{\"name\":\"\\u4eba\\u5de5\\u667a\\u80fd\\u6280\\u672f\",\"url\":\"/forums/AI\"},{\"name\":\"\\u8d28\\u91cf\\u7ba1\\u7406/\\u8f6f\\u4ef6\\u6d4b\\u8bd5\",\"url\":\"/forums/SE_Quality\"}]},{\"name\":\"\\u591a\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/MediaAndFlash\",\"children\":[{\"name\":\"\\u591a\\u5a92\\u4f53/\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/Multimedia\"},{\"name\":\"\\u56fe\\u8c61\\u5de5\\u5177\\u4f7f\\u7528\",\"url\":\"/forums/ImageTools\"},{\"name\":\"Flash\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/FlashDevelop\"},{\"name\":\"\\u4ea4\\u4e92\\u5f0f\\u8bbe\\u8ba1\",\"url\":\"/forums/InteractiveDesign\"},{\"name\":\"WPF/Silverlight\",\"url\":\"/forums/Silverlight\"},{\"name\":\"Flex\",\"url\":\"/forums/Flex\"}]},{\"name\":\"\\u786c\\u4ef6\\u4f7f\\u7528\",\"url\":\"/forums/HardwareUse\",\"children\":[{\"name\":\"\\u6570\\u7801\\u8bbe\\u5907\",\"url\":\"/forums/Hardware_Digital\"},{\"name\":\"\\u7535\\u8111\\u6574\\u673a\\u53ca\\u914d\\u4ef6\",\"url\":\"/forums/Hardware_Computer\"},{\"name\":\"\\u5916\\u8bbe\\u53ca\\u529e\\u516c\\u8bbe\\u5907\",\"url\":\"/forums/Hardware_Peripheral\"},{\"name\":\"\\u88c5\\u673a\\u4e0e\\u5347\\u7ea7\\u53ca\\u5176\\u4ed6\",\"url\":\"/forums/Hardware_DIY\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Hardware_NonTechnical\"}]},{\"name\":\"\\u4ea7\\u54c1/\\u5382\\u5bb6\",\"url\":\"/forums/ADS\",\"children\":[{\"name\":\"IBM \\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"\\u5fae\\u521b\\u8f6f\\u4ef6\\u5f00\\u53d1\\u7ba1\\u7406\",\"url\":\"/forums/WeiChuang\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/ADSOther\"}]}]},{\"name\":\"\\u57f9\\u8bad\\u8ba4\\u8bc1\",\"url\":\"/forums/Trainning\",\"children\":[{\"name\":\"IT\\u57f9\\u8bad\",\"url\":\"/forums/ITCertificate\"}]},{\"name\":\"\\u7ad9\\u52a1\\u4e13\\u533a\",\"url\":\"/forums/Support\",\"children\":[{\"name\":\"\\u793e\\u533a\\u516c\\u544a\",\"url\":\"/forums/placard\"},{\"name\":\"\\u6d3b\\u52a8\\u4e13\\u533a\",\"url\":\"/forums/Activity\"},{\"name\":\"\\u5ba2\\u670d\\u4e13\\u533a\",\"url\":\"/forums/Service\"},{\"name\":\"\\u7248\\u4e3b\\u4e13\\u533a\",\"url\":\"/forums/Moderator\"},{\"name\":\"\\u300a\\u7a0b\\u5e8f\\u5458\\u300b\\u6742\\u5fd7\",\"url\":\"/forums/Programmer\"}]}],\n    \"isLogined\": \"false\",\n    \"isModerator\": \"false\",\n    \"favoriteForumUrls\": [],\n    \"lastForumNodes\": [{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b \",\"url\":\"/forums/DevChallenge2016\"},{\"name\":\"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\\u8bba\\u575b\",\"url\":\"/forums/DE\"},{\"name\":\"OpenCV\",\"url\":\"/forums/OpenCV\"},{\"name\":\"FusionInsight HD\",\"url\":\"/forums/fusioninsightdeveloper\"},{\"name\":\"HUAWEI Code Craft\",\"url\":\"/forums/hwcodecraft\"},{\"name\":\"JetBrains\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/JetBrains\"},{\"name\":\"Enterprise Architect\",\"url\":\"/forums/EA\"}]\n  }\'\'\'\nmenu = json.loads(menu)\n\n\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'周前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'个月前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'年前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'headers\':{\n#\'proxy\':\'123.161.133.18:63574\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n},\n\'proxy\':\'124.88.67.7:843\',\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for each in menu[\'forumNodes\']:\n            if each.has_key(\'children\'):\n                for ea in each[\'children\']:\n                    if ea.has_key(\'url\'):\n                            url = ea[\'url\']\n                            self.crawl(\'http://bbs.csdn.net\'+url+\'/closed\',headers=self.crawl_config[\'headers\'],proxy=self.crawl_config[\'proxy\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.title > a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text()\n            self.crawl(each.attr.href, save = _dict,headers=self.crawl_config[\'headers\'],proxy=self.crawl_config[\'proxy\'], callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],proxy=self.crawl_config[\'proxy\'],callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        #print len(response.doc(\'table.post.topic\'))\n        content_list = []\n        for info in response.doc(\'.post\').items():\n            if \'topic\' in info.attr[\'class\']:\n                continue\n            if u\'被管理员删除\' in info.find(\'.post_body\').remove(\'fieldset\').html().strip():\n                continue\n            _dic = {}\n            #print info.find(\'.answer_author\').text()\n            _dic[\'name\'] = info.find(\'.username > a\').text()\n            _dic[\'date\'] = info.find(\'.time\').text().split()[-2]\n            _dic[\'content\'] = info.find(\'.post_body\').remove(\'fieldset\').html().strip()\n            content_list.append((_dic))\n        \n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'程序员\',\n            \"answers\": content_list,\n            \"source\": u\'csdn\',\n            \"question_detail\": re.sub(\'<!--.*\',\'\',response.doc(\'.topic > .post_body\').remove(\'*\').html()).strip(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1472035810.0612),('cxy_openkfjyk','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 16:20:13\n# Project: cxy_openkfjyk\n\nfrom pyspider.libs.base_handler import *\nimport sys\nimport traceback\nfrom pyquery import PyQuery\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\ncxy_dict = {\n    u\'软件开发\':[u\'软件设计\'],\n    u\'开发语言与工具\':[u\'编程语言\'],\n    u\'网站系统\':[u\'软件设计\'],\n    u\'企业应用\':[u\'软件设计\'],\n    u\'服务器软件\':[u\'软件设计\'],\n    u\'插件和扩展\':[u\'软件设计\'],\n    u\'数据库相关\':[u\'数据库技术\'],\n    u\'操作系统\':[u\'操作系统\'],\n    u\'软件开发管理\':[u\'软件设计\'],\n    u\'管理和监控\':[u\'软件设计\'],\n    u\'应用工具\':[u\'软件设计\'],\n    u\'游戏相关\':[u\'软件设计\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.open-open.com/lib/list/all\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'h4 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = cxy_dict[each.text()]\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n    \n    def list_page(self, response):\n        for each in response.doc(\'a > h3\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.text()\n            #print each.parent().outerHtml()\n            self.crawl(each.parent().attr.href, save = _dict, callback=self.detail_page)\n        #翻页    \n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content = response.doc(\'article\').html().strip()\n        if not content:\n            return\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"content\": content,\n            \"source\": u\"open开发经验库\",\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"程序员\",\n            \"date\": response.doc(\'.meta > .item\').text().split()[0].strip(),\n\n        }',NULL,1.0000,3.0000,1472624601.0032),('dead_link_check','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-14 10:30:47\n# Project: dead_link_check\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.genshuixue.com/bj/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1472624598.5702),('dongman_missevan','dongman','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-18 15:50:19\n# Project: dongman_missevan\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport cPickle\nimport time\n\nmax_pageno = 5\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".newslist\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret:\n                a = P(url_node).find(\".newstitle\").find(\"a\")\n                ret.append(P(a))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        page_node = P(response.doc(\".pagelist\"))\n        try:\n            next_page = page_node.find(\".next\")\n            if next_page:\n                current_pageno = page_node.find(\".selected a\").text()\n                next_pageno = re.findall(r\"p=(\\d+)\", next_page.find(\"a\").attr.href)\n                if not next_pageno:\n                    return False\n                next_pageno = next_pageno[0]\n                    \n                if int(next_pageno) > int(current_pageno):\n                    # 限制只抓前20页\n                    #if int(next_pageno) > max_pageno:\n                    #    return False\n                    return P(\'\'\'<a href=\"%s\">next</a>\'\'\' % next_page.find(\"a\").attr.href)\n                return False\n\n            else:\n                return False\n        except Exception as info:\n            return False\n    \n    @staticmethod\n    def parse_detail(response):\n        ret = {}\n        try:\n            content_node = response.doc(\"#articlebox\")\n            if not content_node:\n                return False\n            title = P(content_node).find(\"#articletitle\").text()\n            \n            detail = P(content_node).find(\"#articlecontent\").html().strip()\n            \n            article_info = P(content_node).find(\"#articleinfo\").text()\n            info = re.findall(u\"来源:\\s+([^\\s]+?)\\s+时间:\\s+(\\d{4}-\\d{2}-\\d{2})\", article_info)\n            source = False\n            date_string = time.strftime(\"%Y-%m-%d\", time.localtime(time.time()))\n            if info:\n                info = info[0]\n                source = info[0]\n                date_string = info[1]\n\n            bread = response.save.get(\"bread\")\n        except:\n            return False\n                \n        url = response.url\n        if not detail.strip():\n            return False\n        \n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n        return {\"title\": title,\n                \"url\": url,\n                \"content\": detail,\n                \"bread\": bread,\n                \"source\": source if source else u\"news.missevan.com\",\n                \"data_weight\": 0,\n                \"class\": 46,\n                \"subject\": u\"动漫\",\n                \"date\": date_string,\n                \"image_list\": image_list,\n                }\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\n        \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=2\",\n            \"bread\": [\"动画\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=3\",\n            \"bread\": [\"音乐\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=4\",\n            \"bread\": [\"游戏\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=5\",\n            \"bread\": [\"声优\"],\n        },\n\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=6\",\n            \"bread\": [\"小说\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=7\",\n            \"bread\": [\"漫画\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=8\",\n            \"bread\": [\"Cosplay\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=9\",\n            \"bread\": [\"杂志\"],\n        },\n\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=10\",\n            \"bread\": [\"周边\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=11\",\n            \"bread\": [\"展会\"],\n        },\n\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=12\",\n            \"bread\": [\"电影\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=13\",\n            \"bread\": [\"萌宅\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=14\",\n            \"bread\": [\"杂谈\"],\n        },\n\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=15\",\n            \"bread\": [\"DVD/BD\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://news.missevan.com/news/index?ntype=16\",\n            \"bread\": [\"其他\"],\n        },\n        \n    ]\n    \n    @every(minutes=1 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                           \'base_url\': crawler[\"url\"],\n                       },\n                       callback = self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n',NULL,1.0000,3.0000,1471330340.9388),('dongman_qiongkong','dongman','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-11 17:15:33\n# Project: dongman\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://qingkong.net/anime/dmzx/\', callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.nLink\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        #for each in response.doc(\'.p_current > a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        data = response.doc(\'.zxtitle td\').text()\n        source = data.split(u\'：\')[1].split(\'[\')[0].strip()\n        date = data.split(\'[\')[-1].split(\']\')[0]\n        content = response.doc(\'#content > div > div\').html()\n        if not content:\n            content = response.doc(\'#content\').html()\n        if not content:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.hzxnr\').text(),\n            \'content\': content,\n            \"bread\": [u\'动漫资讯\'],\n            \"source\": source if source else u\"qingkong.net\",\n            \"data_weight\": 0,\n            \"class\": 46,\n            \"subject\": u\"动漫\",\n            \"date\": date,\n        }\n',NULL,1.0000,3.0000,1471330343.5662),('dota_guanwang_news','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-11 12:14:31\n# Project: dota_news\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport time\n\ndef full_img_url(url):\n    if url.startswith(\"http\"):\n        return url\n    \n    return \"/\".join([\"http://www.dota2.com.cn\", url.strip(\"/\")])\n\nmax_pageno = 3\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".hd_list\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\".hd_li dl dt a\"):\n                ret.append(P(url_node))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        next_page = P(response.doc(\".hd_list .page\"))\n        next_page = next_page.find(\"span\").next()\n        if next_page:\n            current_page = re.findall(r\"index(\\d+)\\.htm\", response.url)\n            cpo = \"1\"\n            if current_page:\n                cpo = current_page[0]\n            npo = re.findall(r\"index(\\d+)\\.htm\", next_page.attr.href)\n            if npo:\n                npo = npo[0]\n            else:\n                return False\n            if int(cpo) < int(npo):\n                if int(npo) > max_pageno:\n                    return False\n                return P(next_page)\n            else:\n                return False\n        return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        url = response.url\n        if not url.startswith(\"http://www.dota2.com.cn/\"):\n            return False\n        detail_node = response.doc(\".newsart\")\n        title = P(detail_node).find(\".art_title h1\").text()\n        date_string = re.findall(r\"\\d{4}-\\d{2}-\\d{2}\", P(detail_node).find(\".art_title h3\").text())\n        if date_string:\n            date_string = date_string[0]\n        else:\n            time.strftime(\"%Y-%m-%d\", time.localtime(time.time()))\n            \n        detail = P(detail_node).find(\".font_style\").html().strip()\n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n        if image_list:\n            image_list = [full_img_url(img) for img in image_list]\n            \n        return {\n                \"url\": url,\n                \"title\": title,\n                \"content\": detail,\n                \"bread\": response.save.get(\"bread\"),\n                \"image_list\": image_list,\n                \"source\": \"www.dota2.com.cn\",\n                \"class\": 33,\n                \"date\": date_string,\n                \"subject\": u\"刀塔\",\n                \"data_weight\": 0,\n                }\n    \nclass Parser2(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".newlist\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\".ullist li .imgpic a\"):\n                ret.append(P(url_node))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        next_page = P(response.doc(\".newlist .page\"))\n        next_page = next_page.find(\"span\").next()\n        if next_page:\n            current_page = re.findall(r\"index(\\d+)\\.htm\", response.url)\n            cpo = \"1\"\n            if current_page:\n                cpo = current_page[0]\n            npo = re.findall(r\"index(\\d+)\\.htm\", next_page.attr.href)\n            if npo:\n                npo = npo[0]\n            else:\n                return False\n            if int(cpo) < int(npo):\n                if int(npo) > max_pageno:\n                    return False\n                return P(next_page)\n            else:\n                return False\n        return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        url = response.url\n        if not url.startswith(\"http://www.dota2.com.cn/\"):\n            return False\n        detail_node = response.doc(\".newsart\")\n        title = P(detail_node).find(\".art_title h1\").text()\n        date_string = re.findall(r\"\\d{4}-\\d{2}-\\d{2}\", P(detail_node).find(\".art_title h3\").text())\n        if date_string:\n            date_string = date_string[0]\n        else:\n            time.strftime(\"%Y-%m-%d\", time.localtime(time.time()))\n            \n        detail = P(detail_node).find(\".font_style\").html().strip()\n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n        if image_list:\n            image_list = [full_img_url(img) for img in image_list]\n            \n        return {\n                \"url\": url,\n                \"title\": title,\n                \"content\": detail,\n                \"bread\": response.save.get(\"bread\"),\n                \"image_list\": image_list,\n                \"source\": \"www.dota2.com.cn\",\n                \"class\": 33,\n                \"date\": date_string,\n                \"subject\": u\"刀塔\",\n                \"data_weight\": 0,\n                }\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\n        \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(spider_handle, parser, response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        },\n        \"2\": {\n            \"processor\": Processor,\n            \"parser\": Parser2,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.dota2.com.cn/raiders/index.htm\",\n            \"bread\": [\"dota攻略\"],\n        },\n        {   \n            \"key\": \"2\",\n            \"url\": \"http://www.dota2.com.cn/news/index.htm\",\n            \"bread\": [\"dota新闻\"],\n        },\n    ]\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                           \'base_url\': crawler[\"url\"],\n                       },\n                       callback = self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n',NULL,1.0000,3.0000,1472624594.6489),('dota_replays_news','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-11 12:14:31\n# Project: dota_news\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport time\n\nmax_pageno = 3\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".newslist\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\".newstitle a\"):\n                ret.append(P(url_node))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        next_page = P(response.doc(\".nweslist_Page .pagination\"))\n        next_page = next_page.find(\".act\").next().find(\"a\")\n        if next_page:\n            try:\n                npo = int(next_page.text().strip())\n                if npo > max_pageno:\n                    return False\n                return P(next_page)\n            except:\n                return False\n        return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        url = response.url\n        if url.find(\"replays.net\") == -1:\n            return False\n        \n        detail_node = response.doc(\".News-content\")\n        title = P(detail_node).find(\".News-top h2\").text()\n        \n        date_string = re.findall(ur\"\\d{4}-\\d{2}-\\d{2}\", P(detail_node).find(\".News-address\").text())\n        if date_string:\n            date_string = date_string[0]\n        else:\n            date_string = time.strftime(\"%Y-%m-%d\", time.localtime(time.time()))\n            \n        detail = P(detail_node).find(\".News-main-table\").html().strip().replace(\"&#13;\", \"\")\n        if len(detail) < 50:\n            return False\n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n            \n        return {\n                \"url\": url,\n                \"title\": title,\n                \"content\": detail,\n                \"bread\": response.save.get(\"bread\"),\n                \"image_list\": image_list,\n                \"source\": \"replays.net\",\n                \"class\": 33,\n                \"date\": date_string,\n                \"subject\": u\"刀塔\",\n                \"data_weight\": 0,\n                }\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\n        \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(spider_handle, parser, response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://dota2.replays.net/news/list_3.html\",\n            \"bread\": [\"dota资讯\"],\n        },\n    ]\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                           \'base_url\': crawler[\"url\"],\n                       },\n                       callback = self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n',NULL,10.0000,10.0000,1472624590.3817),('dota_tgbus_news','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-11 12:14:31\n# Project: dota_news\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport time\n\nmax_pageno = 3\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".list\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\".list_tit p a\"):\n                db_id = re.findall(r\"(\\d+)\\.shtml$\", P(url_node).attr.href)\n                if db_id:\n                    db_id = db_id[0]\n                    ret.append(P(\'\'\'<a href=\"http://app.tgbus.com/saver/read.aspx?type=olgame&id=%s\"></a>\'\'\' % db_id))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        next_page = P(response.doc(\".page\"))\n        next_page = next_page.find(\"font\").next()\n        if next_page:\n            try:\n                npo = int(next_page.text().strip())\n                if npo > max_pageno:\n                    return False\n                return P(next_page)\n            except:\n                return False\n        return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        url = response.url\n        if url.find(\"tgbus.com\") == -1:\n            return False\n        \n        detail_node = response.doc(\".text\")\n        title = P(detail_node).find(\"h1\").text()\n        \n        date_string = re.findall(ur\"(\\d{4})年(\\d{1,2})月(\\d{1,2})日\", P(detail_node).find(\"h2\").text())\n        if date_string:\n            date_string = date_string[0]\n            year = date_string[0]\n            month = date_string[1]\n            day = date_string[2]\n            if len(month) == 1:\n                month = \"0\" + month\n            if len(day) == 1:\n                day = \"0\" + day    \n            date_string = \"-\".join([year, month, day])\n        else:\n            date_string = time.strftime(\"%Y-%m-%d\", time.localtime(time.time()))\n            \n        detail = P(detail_node).find(\"#ct\").html().strip().replace(\"&#13;\", \"\")\n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n            \n        return {\n                \"url\": url,\n                \"title\": title,\n                \"content\": detail,\n                \"bread\": response.save.get(\"bread\"),\n                \"image_list\": image_list,\n                \"source\": \"tgbus.com\",\n                \"class\": 33,\n                \"date\": date_string,\n                \"subject\": u\"刀塔\",\n                \"data_weight\": 0,\n                }\n    \nclass Parser2(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".indexlist\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\"tr td a\"):\n                db_id = re.findall(r\"(\\d+)\\.shtml$\", P(url_node).attr.href)\n                if db_id:\n                    db_id = db_id[0]\n                    ret.append(P(\'\'\'<a href=\"http://app.tgbus.com/saver/read.aspx?type=olgame&id=%s\"></a>\'\'\' % db_id))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        next_page = P(response.doc(\".showpage\"))\n        next_page = next_page.find(\"font\").next()\n        if next_page:\n            try:\n                npo = int(next_page.text().strip())\n                if npo > max_pageno:\n                    return False\n                return P(next_page)\n            except:\n                return False\n        return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        url = response.url\n        if url.find(\"tgbus.com\") == -1:\n            return False\n        \n        detail_node = response.doc(\".text\")\n        title = P(detail_node).find(\"h1\").text()\n        \n        date_string = re.findall(ur\"(\\d{4})年(\\d{1,2})月(\\d{1,2})日\", P(detail_node).find(\"h2\").text())\n        if date_string:\n            date_string = date_string[0]\n            year = date_string[0]\n            month = date_string[1]\n            day = date_string[2]\n            if len(month) == 1:\n                month = \"0\" + month\n            if len(day) == 1:\n                day = \"0\" + day    \n            date_string = \"-\".join([year, month, day])\n        else:\n            date_string = time.strftime(\"%Y-%m-%d\", time.localtime(time.time()))\n            \n        detail = P(detail_node).find(\"#ct\").html().strip().replace(\"&#13;\", \"\")\n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n            \n        return {\n                \"url\": url,\n                \"title\": title,\n                \"content\": detail,\n                \"bread\": response.save.get(\"bread\"),\n                \"image_list\": image_list,\n                \"source\": \"tgbus.com\",\n                \"class\": 33,\n                \"date\": date_string,\n                \"subject\": u\"刀塔\",\n                \"data_weight\": 0,\n                }\n    \nclass Parser3(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".heroes\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\"a\"):\n                ret.append(P(url_node))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        url = response.url\n        if url.find(\"tgbus.com\") == -1:\n            return False\n       \n        title = response.doc(\"title\").text().split(\"-\")[0]\n        date_string = time.strftime(\"%Y-%m-%d\", time.localtime(time.time()))\n            \n        detail_node = response.doc(\".contentd\")\n        detail_node.find(\".rightContent\").remove()\n        detail_node.find(\".cl\").remove()\n        detail_node.find(\".overall\").remove()\n        detail = detail_node.html().strip().replace(\"&#13;\", \"\")\n        detail = re.sub(r\"display\\s*:\\s*none\", \"\", detail)\n        \n        \n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n        for img in image_list:\n            if img.startswith(\"http:\"):\n                continue\n            detail = detail.replace(img, \"/\".join([url.strip(\"/\"), img.strip(\"/\")]))\n            \n        return {\n                \"url\": url,\n                \"title\": title,\n                \"content\": detail,\n                \"bread\": response.save.get(\"bread\"),\n                \"image_list\": [],\n                \"source\": \"tgbus.com\",\n                \"class\": 33,\n                \"date\": date_string,\n                \"subject\": u\"刀塔\",\n                \"data_weight\": 0,\n                }\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\n        \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(spider_handle, parser, response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        },\n        \"2\": {\n            \"processor\": Processor,\n            \"parser\": Parser2,\n        },\n        \"3\": {\n            \"processor\": Processor,\n            \"parser\": Parser3,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://dota2.tgbus.com/strategy/\",\n            \"bread\": [\"dota资讯\"],\n        },\n        {   \n            \"key\": \"2\",\n            \"url\": \"http://dota2.tgbus.com/news/\",\n            \"bread\": [\"dota资讯\"],\n        },\n        {   \n            \"key\": \"2\",\n            \"url\": \"http://dota2.tgbus.com/wenda/\",\n            \"bread\": [\"dota资讯\"],\n        },\n        {   \n            \"key\": \"3\",\n            \"url\": \"http://dota2.tgbus.com/heroes/\",\n            \"bread\": [\"dota英雄\", \"出装加点\"],\n        },\n    ]\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                           \'base_url\': crawler[\"url\"],\n                       },\n                       callback = self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n',NULL,1.0000,3.0000,1472624587.9508),('eoffcn_gongwuyuan','gongwuyuan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-29 10:07:32\n# Project: eoffcn_tiku\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/zhenti/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/moniti/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/lxt/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        \n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/zhenti/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/moniti/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/lxt/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        \n        \n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/zhenti/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/moniti/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/lxt/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        \n        \n        \n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.xm_gwy_l_mainbottom li\').items():\n            _dict = {}\n            url = each.find(\'a\').attr.href\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'中公\',\'\').replace(\'网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save =_dict,callback=self.detail_page)\n        for each in response.doc(\'#pages > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save =_dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       res_dict[\'subject\'] = u\'公务员\'\n       res_dict[\'class\'] = 46\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'source\'] = \'eoffcn.com\'\n       res_dict[\'url\'] = response.url\n       res_dict[\'tdk_title\'] = u\'跟谁学 \'+response.doc(\'title\').text().replace(\'中公\',\'\').replace(\'网校\',\'\')\n       res_dict[\'tdk_desc\'] = response.doc(\'meta[name=\"description\"]\').attr.content.replace(\'中公\',\'\').replace(\'网校\',\'\')\n       res_dict[\'tdk_keywords\'] = response.doc(\'meta[name=\"keywords\"]\').attr.content.replace(\'中公\',\'\').replace(\'网校\',\'\')\n       content_list = []\n       for each in response.doc(\'.main_l_cont > p\').items():\n            if each and u\'相关推荐\' in each.text():\n                continue\n            if each and u\'中公\' in each.text():\n                continue\n            if each and u\'责任编辑\' in each.text():\n                break\n            if each:\n                content_list.append(each.remove(\'a\').html())\n            pass\n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%(s) for s in content_list if s])\n       return res_dict\n',NULL,1.0000,3.0000,1472615989.1984),('eoffcn_gongwuyuan_inc','gongwuyuan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-29 10:07:32\n# Project: eoffcn_tiku\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/zhenti/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/moniti/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/xingce/lxt/\',save = {\'bread\':[u\'行测\']}, callback=self.index_page)\n        \n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/zhenti/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/moniti/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/shenlun/lxt/\',save = {\'bread\':[u\'申论\']}, callback=self.index_page)\n        \n        \n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/zhenti/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/moniti/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        self.crawl(\'http://www.eoffcn.com/gwy/stixz/ms/lxt/\',save = {\'bread\':[u\'面试\']}, callback=self.index_page)\n        \n        \n        \n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.xm_gwy_l_mainbottom li\').items():\n            _dict = {}\n            url = each.find(\'a\').attr.href\n            _dict[\'title\'] = each.find(\'a\').text().replace(\'中公\',\'\').replace(\'网校\',\'\')\n            _dict[\'date\'] = each.find(\'span\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save =_dict,callback=self.detail_page)\n        #for each in response.doc(\'#pages > a\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\')\n            #self.crawl(each.attr.href, save =_dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       res_dict[\'subject\'] = u\'公务员\'\n       res_dict[\'class\'] = 46\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'source\'] = \'eoffcn.com\'\n       res_dict[\'url\'] = response.url\n       res_dict[\'tdk_title\'] = u\'跟谁学 \'+response.doc(\'title\').text().replace(\'中公\',\'\').replace(\'网校\',\'\')\n       res_dict[\'tdk_desc\'] = response.doc(\'meta[name=\"description\"]\').attr.content.replace(\'中公\',\'\').replace(\'网校\',\'\')\n       res_dict[\'tdk_keywords\'] = response.doc(\'meta[name=\"keywords\"]\').attr.content.replace(\'中公\',\'\').replace(\'网校\',\'\')\n       content_list = []\n       for each in response.doc(\'.main_l_cont > p\').items():\n            if each and u\'相关推荐\' in each.text():\n                continue\n            if each and u\'中公\' in each.text():\n                continue\n            if each and u\'责任编辑\' in each.text():\n                break\n            if each:\n                content_list.append(each.remove(\'a\').html())\n            pass\n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%(s) for s in content_list if s])\n       return res_dict\n',NULL,1.0000,3.0000,1472611407.8003),('eol_gaozhong','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-13 14:04:45\n# Project: eol_gaozhong\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport requests\nfrom pyquery import PyQuery as pq\n\n\n\ndef getdistrict(s):\n    if u\'地区：\' in s.text():\n        return True\n    else:\n        return False\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.2\',\n        \'headers\':{\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n\n        }\n    }\n\n    set_dict = {\n        u\'学校类型\':\'school_type\',\n        u\'电话\':\'phone\',\n        u\'校址\':\'address\',\n        u\'网址\':\'site\',\n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for index in range(332):\n            self.crawl(\'http://haogaozhong.eol.cn/school_area.php?page=%s\'%(index+1), callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'div.mar_t_30\').items():\n            _dict = {}\n            arr =  each.find(\'.w_360 table td\').eq(0).text().replace(\'&nbsp\',\' \').split()\n            _dict[\'province\'] = arr[0]\n            _dict[\'city\'] = arr[1].replace(\'市\',\'\')\n            if  _dict[\'province\'] == u\'北京\' or  _dict[\'province\'] == u\'上海\' or  _dict[\'province\'] == u\'天津\' or  _dict[\'province\'] == u\'重庆\':\n                _dict[\'city\'] = arr[2]\n            url = each.find(\'.img_160 p a\').attr.href\n            self.crawl(url,save = _dict, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       district =  filter(getdistrict,response.doc(\'.w_460 .font_14 td\').items())[0].text()\n       res_dict[\'district\'] = district.split(\'：\')[-1].strip() if district!=u\'地区：\' else \'\'\n       school_type =  response.doc(\'div.w_460 table td\').eq(0).text().split(\'：\')\n       phone =  response.doc(\'div.w_460 table td\').eq(2).text().split(\'：\')\n       address = response.doc(\'div.w_460 table td\').eq(4).text().split(\'：\')\n       site = response.doc(\'div.w_460 table td\').eq(5).text().split(\'：\')\n       if school_type[0] in self.set_dict:\n           res_dict[self.set_dict[school_type[0]]] = school_type[1]\n       if phone[0] in self.set_dict:\n           res_dict[self.set_dict[phone[0]]] = phone[1]\n       if address[0] in self.set_dict:\n           res_dict[self.set_dict[address[0]]] = address[1]\n       if site[0] in self.set_dict:\n           res_dict[self.set_dict[site[0]]] = site[1]\n       res_dict[\'school_url\'] = response.url \n       res_dict[\'school_name\'] = response.doc(\'div.w_260 h2 a\').text()\n       #res_dict[\'enrol_plan\'] = response.doc(\'div.mar_t_20 > table.t_table\').html()\n       res_dict[\'school_imgs\'] = []\n       for each in response.doc(\'ul.slider li img\').items():\n                     res_dict[\'school_imgs\'].append(each.attr.src)\n       baseUrl = response.url.replace(\'index.html\',\'\')\n       resp = requests.get(baseUrl+\'intro.html\')\n       res_dict[\'school_summary\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.mar_t_20\').eq(3).html()\n       resp = requests.get(baseUrl+\'gaokao.html\')                     \n       res_dict[\'gaokao_socre\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div#gaokao\').html()\n       #res_dict[\'enrol_socre\'] = pq(requests.get(baseUrl+\'score.html\').text).find(\'div#gaokao\').html()\n       resp = requests.get(baseUrl+\'plan.html\')                    \n       res_dict[\'enrol_plan\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.mar_t_10\').eq(1).html()\n       resp = requests.get(baseUrl+\'teacher.html\')  \n       res_dict[\'faculty\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.mar_t_20\').eq(3).html()\n       resp = requests.get(baseUrl+\'honor.html\')  \n       res_dict[\'academic_achievement\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.mar_t_10\').eq(1).html()    \n       resp = requests.get(baseUrl+\'tese.html\')  \n       res_dict[\'feature_course\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.mar_t_10\').eq(1).html() \n       resp = requests.get(baseUrl+\'xiaoyou.html\')  \n       res_dict[\'xiaoyou\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.pad_l_20\').html() \n       resp = requests.get(baseUrl+\'chuguo.html\')  \n       #print resp.encoding\n       res_dict[\'chuguo\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.mar_t_10\').eq(1).html()\n       return res_dict\n',NULL,1.0000,3.0000,1472624496.3314),('erge_sohu','erge','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-14 16:21:18\n# Project: guitar_china\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        _list = []\n        with open (\'/apps/home/rd/xuzhihao/sohu_url2\') as f:\n            for line in f:\n                _list.append(line.strip(\'\\n\'))\n        for line in _list:\n            self.crawl(line, callback=self.detail_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'td td td td td a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.normalfont > a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n        \'\'\'\n    @config(priority=2)\n    def detail_page(self, response):\n        #content = \'\'.join([\'<p>%s</p>\'%v.html() for v in response.doc(\'p\').items()])\n        content = response.doc(\'#contentText\').html()\n        title = response.doc(\'h1\').text()\n        if u\'儿歌\' not in title and u\'儿歌\' not in content:\n            return None\n        \n        return {\n            \"url\": response.url,\n            \"subject\": u\'儿歌\',\n            \"source\": response.doc(\'.writer > a\').text(),\n            \"content\": content,\n            \"title\": title,\n            \"date\": response.doc(\'[itemprop=\"datePublished\"]\').text()[:10],\n            \"class\": 46,\n            \"bread\": response.doc(\'.tag > a\').text().split(),\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471341414.3742),('gangqin_new_163_inc','gangqin','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-25 15:04:32\n# Project: gangqin_new_163_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://edu.163.com/keywords/9/a/94a27434/1.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.titleBar > h3 > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content = response.doc(\'.post_text\').remove(\'.ep-source\').html()\n        if not content:\n            content = response.doc(\'.end-text\').remove(\'.ep-source\').html()\n        if not content:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"content\": content.replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'\\r\',\'\'),\n            \"subject\": u\'钢琴\',\n            \"source\": \'163.com\',\n            \"date\": response.doc(\'.post_time_source\').text()[:10],\n            \"bread\": [u\'钢琴资讯\',],\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471329973.5625),('gaokao_51test_inc','gaokao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-07 11:22:37\n# Project: gmat_51test\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.51test.net/gaokao/st/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.news-list-left-content li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'span\').text()\n            self.crawl(url, save={\'date\': date}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        date = response.save[\'date\']\n        \'\'\'\n        for each in response.doc(\'.show_content_next > a\').items():\n            self.crawl(each.attr.href,save={\'date\': date}, callback=self.detail_page)\n        \'\'\'\n\n        content = response.doc(\'.show_content\').remove(\'div\').html().replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'【无忧考网 - GMAT研究生管理考试试题】\',\'\')\n        if not content:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"date\": date,\n            \"subject\": \'高考\',\n            \"source\": \'51test\',\n            \"bread\": [u\'模拟真题\',],\n            \"class\": 26,\n            \"data_weight\": 0,\n            \"content\": content, #\'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n        }\n',NULL,1.0000,3.0000,1470815072.2272),('gaozhongtiku_weipan','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-04 19:31:20\n# Project: gaozhongtiku_weipan\n\nfrom pyspider.libs.base_handler import *\n\n#fw = open(\'/Users/bjhl/Documents/gaozhongtiku/tiku_urls.log\', \'a\')\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://vdisk.weibo.com/u/2234165192\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.sort_name_intro a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.vd_page_btn\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n        \n    @config(priority=2)\n    def detail_page(self, response):\n        for each in response.doc(\'script\').items():\n            if \'download_list\' in each.text():\n                url = each.text().split(\'\"download_list\":[\"\')[1].split(\'\"]\')[0].replace(\'\\\\\',\'\')\n                #print url\n                #fw.write(url + \'\\n\')\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n            \"download_url\":url\n        }\n',NULL,5.0000,5.0000,1472624499.2351),('gmat_tiandao_inc','gmat','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-05 15:21:53\n# Project: gmat_tiandao_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    page_dict = {\n        \"advice\": u\'GMAT机经\',\n        \'read\': u\'GMAT阅读\',\n        \'syntax\': u\'GMAT语法\',\n        \'write\': u\'GMAT写作\',\n        \'math\': u\'GMAT数学\',\n        \'logic\': u\'GMAT逻辑\',\n        \'news\': u\'快讯动态\',\n        \'comments\': u\'冲刺宝典\',\n        \'experience\': u\'备考计划\',\n    }\n    @every(minutes=12 * 60)\n    def on_start(self):\n        for k, v in self.page_dict.iteritems():\n            self.crawl(\'http://gmat.tiandaoedu.com/%s/\'%k,save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://tiandaoedu.com/yyxx/syyd/\', save={\'bread\': u\'GMAT阅读\'}, callback=self.index_page)\n        self.crawl(\'http://tiandaoedu.com/yyxx/xcxy/\', save={\'bread\': u\'GMAT词汇\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.sty_two > .ptit > a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n        for each in response.doc(\'.rf > .ptit > a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n        #for each in response.doc(\'.pages > a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.wzlist > .yh\').text(),\n            \"date\": response.doc(\'.p_span > span\').text().split()[2].split(u\'：\')[-1],\n            \"brief\": response.doc(\'.zhw_p\').text(),\n            \"subject\": \'GMAT\',\n            \'source\': \'tiandao\',\n            \'content\': response.doc(\'.wzy_bot\').html().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\').strip(\' \').replace(u\'天道\',\'\'),\n            \"bread\": [response.save[\'bread\'],],\n            \"class\": 30,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471334123.6224),('gmat_zhan_inc','gmat','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 18:19:22\n# Project: gmat_xiaozhan_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    type_dict = {\n        \'yufa\': u\'GMAT语法\',\n        \'cihui\': u\'GMAT词汇\', \n        \'yuedu\': \'GMAT阅读\', \n        \'luoji\': u\'GMAT逻辑\',\n        \'tuili\': u\'GMAT逻辑\',\n        \'zuowen\': u\'GMAT作文\',\n        \'shuxue\': u\'GMAT数学\', \n        \'jihua\': u\'备考计划\', \n        \'tifen/beikao\': u\'备考计划\', \n        \'gaofen\': u\'高分心得\',\n        \'fuxi\': u\'复习攻略\',  \n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for type, v in self.type_dict.iteritems():\n            self.crawl(\'http://gmat.zhan.com/%s/\'%type, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gmat/kaoqian/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gmat/fukao/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gmat/beikao/\', save={\'bread\': u\'备考计划\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.experience-item\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.index-middle-info-3 > .pull-left\').text().split()[0]\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.index-middle-info-2\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        for each in response.doc(\'.things_list > .pull-right\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.padding_right_10\').text()\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.text\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \n        #for each in response.doc(\'nav a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        content_list = []\n        for each in  response.doc(\'.article-content > p\').items():\n            content_list.append((each.html().replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'小站\',\'\')))\n        content_list = content_list[:-1]\n        if not content_list:\n            return None\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'% v for v in content_list])\n        res_dict[\'subject\'] = u\'GMAT\'\n        res_dict[\'source\'] = u\'小站\'\n        res_dict[\'tag\'] = response.doc(\'.tag > a\').text().split(\' \')\n        bread = [res_dict[\'bread\'], ]\n        bread.extend(response.doc(\'.head-crumbs-a-active\').text().split(\' \'))\n        if res_dict[\'date\'][:3] != \'201\':\n            res_dict[\'date\'] = response.doc(\'.pull-left > span\').text().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        res_dict[\'bread\'] = list(set(bread))\n        res_dict[\'class\'] = 30\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471334127.3871),('gre_tiandao_inc','gre','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-05 15:21:53\n# Project: gre_tiandao_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    page_dict = {\n        \"advice\": u\'GRE机经\',\n        \"glossary\": u\'GRE词汇\',\n        \'read\': u\'GRE阅读\',\n        \'completion\': u\'GRE填空\',\n        \'write\': u\'GRE作文\',\n        \'math\': u\'GRE数学\',\n        \'news\': u\'快讯动态\',\n        \'comments\': u\'冲刺宝典\',\n        \'experience\': u\'备考计划\',\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in self.page_dict.iteritems():\n            self.crawl(\'http://gre.tiandaoedu.com/%s/\'%k,save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://tiandaoedu.com/yyxx/syyd/\', save={\'bread\': u\'GRE阅读\'}, callback=self.index_page)\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.sty_two > .ptit > a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n        for each in response.doc(\'.rf > .ptit > a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n        #for each in response.doc(\'.pages > a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.wzlist > .yh\').text(),\n            \"date\": response.doc(\'.p_span > span\').text().split()[2].split(u\'：\')[-1],\n            \"brief\": response.doc(\'.zhw_p\').text(),\n            \"subject\": \'GRE\',\n            \'source\': \'tiandao\',\n            \'content\': response.doc(\'.wzy_bot\').html().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\'),\n            \"bread\": [response.save[\'bread\'],],\n            \"class\": 29,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471334156.2028),('gre_xiaozhan_inc','gre','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 18:19:22\n# Project: gre_xiaozhan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    type_dict = {\n        \'zhenti\': u\'GRE机经\', \'cihui\': u\'GRE词汇\', \'yuedu\': \'GRE阅读\', \'tiankong\': u\'GRE填空\', \'zuowen\': u\'GRE作文\',\n        \'shuxue\': u\'GRE数学\', \'jihua\': u\'备考计划\', \'tifen/beikao\': u\'备考计划\', \'gaofen\': u\'高分心得\',\n        \'fuxi\': u\'复习攻略\',  \n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for type, v in self.type_dict.iteritems():\n            self.crawl(\'http://gre.zhan.com/%s/\'%type, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gre/kaoqian/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gre/fukao/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/gre/beikao/\', save={\'bread\': u\'备考计划\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.experience-item\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.index-middle-info-3 > .pull-left\').text().split()[0]\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.index-middle-info-2\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        for each in response.doc(\'.things_list > .pull-right\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.padding_right_10\').text()\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.text\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.col-sm-9\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.move-top > .pull-left\').text().split()[0]\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.index-middle-info-5\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \'\'\'\n        #for each in response.doc(\'nav a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        content_list = []\n        for each in  response.doc(\'.article-content > p\').items():\n            content_list.append((each.html().replace(\'src=\"/uploadfile\',\'src=\"http://gre.zhan.com/uploadfile\').replace(\'\\t\',\'\').replace(\'\\r\',\'\').replace(\'\\n\',\'\')))\n        if not content_list:\n            return None\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'% v for v in content_list])\n        res_dict[\'subject\'] = u\'GRE\'\n        res_dict[\'source\'] = u\'小站\'\n        res_dict[\'tag\'] = response.doc(\'.tag > a\').text().split(\' \')\n        bread = [res_dict[\'bread\'], ]\n        bread.extend(response.doc(\'.head-crumbs-a-active\').text().split(\' \'))\n        if res_dict[\'date\'][:3] != \'201\':\n            res_dict[\'date\'] = response.doc(\'.pull-left > span\').text().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        res_dict[\'bread\'] = list(set(bread))\n        res_dict[\'class\'] = 29\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471334159.2649),('guitarchina_news','guitar','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-14 16:21:18\n# Project: guitar_china\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://news.guitarchina.com/sort/1.html\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'td td td td td a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.normalfont > a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n        \'\'\'\n    @config(priority=2)\n    def detail_page(self, response):\n        #content = \'\'.join([\'<p>%s</p>\'%v.html() for v in response.doc(\'p\').items()])\n        content = response.doc(\'.content\').html()\n        if not content:\n            return None\n        return {\n            \"url\": response.url,\n            \"subject\": u\'吉他\',\n            \"source\": u\'吉他中国\',\n            \"content\": content,\n            \"title\": response.doc(\'.bigfont > b\').text(),\n            \"date\": response.doc(\'.normalfont > font\').text(),\n            \"class\": 16,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471329638.5885),('guojiasifa_zhengbao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-01 15:29:58\n# Project: guojiasifa_zhengbao\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ntype_dict = {\n    u\'民法\': [u\'职业资格\', u\'国家司法\'],\n    u\'刑法\': [u\'职业资格\', u\'国家司法\'],\n    u\'行政法\': [u\'职业资格\', u\'国家司法\'],\n    u\'民事诉讼法\': [u\'职业资格\', u\'国家司法\'],\n    u\'刑事诉讼法\': [u\'职业资格\', u\'国家司法\'],\n    u\'行政诉讼法\': [u\'职业资格\', u\'国家司法\'],\n    u\'商法\': [u\'职业资格\', u\'国家司法\'],\n    u\'经济法\': [u\'职业资格\', u\'国家司法\'],\n    u\'三国法\': [u\'职业资格\', u\'国家司法\'],\n    u\'宪法\': [u\'职业资格\', u\'国家司法\'],\n    u\'法理学\': [u\'职业资格\', u\'国家司法\'],\n    u\'法制史\': [u\'职业资格\', u\'国家司法\'],\n    u\'其他\': [u\'职业资格\', u\'国家司法\'],\n}\n\nsome_url =[\n    \'http://www.chinaacc.com/chujizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/chujizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/\',\n    \'http://www.chinaacc.com/zaojia/zt/\',\n    \'http://www.chinaacc.com/zaojia/mnst/\',    \n]\n\nspecial_name = [\n    \'民法\',\n    \'刑法\',\n    \'行政法\',\n]\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.chinalawedu.com/sifakaoshi/ziliao/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.cnav1 > a\').items():\n            if \'选课\' in each.text():\n                continue\n            if each.text() in special_name:\n                _dict = {}\n                if each.text().replace(\' \', \'\') in type_dict.keys():\n                    _dict[\'bread\'] = type_dict[each.text().replace(\' \', \'\')]\n                else:\n                    _dict[\'bread\'] = [\'职业资格\', each.text().replace(\' \', \'\')]\n                self.crawl(each.attr.href, save = _dict, callback=self.special_list_page)\n            else:\n                _dict = {}\n                if each.text().replace(\' \', \'\') in type_dict.keys():\n                    _dict[\'bread\'] = type_dict[each.text().replace(\' \', \'\')]\n                else:\n                    _dict[\'bread\'] = [\'职业资格\', each.text().replace(\' \', \'\')]\n                self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n      \n    @config(age=10 * 24 * 60 * 60)\n    def special_list_page(self, response):\n        for each in response.doc(\'.w666 > .tr a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)          \n     \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.info-list li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            if \'报名\' in _dict[\'title\'] or \'名师\' in _dict[\'title\'] or \'汇总\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.text().replace(_dict[\'title\'],\'\').replace(\'[\',\'\').replace(\']\',\'\').replace(\'·\',\'\').replace(\' \',\'\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页 \n        for each in response.doc(\'.p1 > a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'#fontzoom\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'全网首发\' in each1.text():\n                    continue\n                if u\'估分更准确\' in each1.text():\n                    continue\n                if u\'独家\' in each1.text():\n                    continue\n                if u\'点击\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text() or u\'推荐阅读\' in each1.text() or u\'相关推荐\' in each1.text() or u\'精彩推荐\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'欢迎考生\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                if u\'相关链接\' in each1.text() or u\'精彩链接\' in each1.text():\n                    break\n                if u\'以上\' in each1.text() and u\'是法律教育网\' in each1.text():\n                    break\n                else:\n                    list.append(each1.text().replace(\' \',\'\'))\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            \n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\', \'\').replace(\'\\n\', \'\').replace(\'\\t\', \'\').replace(u\'法律教育网\', \'\').replace(\'【 】\', \'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'),\n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'法律教育网\', \'\').replace(\n                    \'None\', \'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'法律教育网\', \'\').replace(\n                    \'None\', \'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'法律教育网\', u\'\').replace(\n                    \'建设网校\', \'\').replace(\'None\', \'\') + str(\n                    response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(\n                    u\'法律教育网\', \'\').replace(\'建设网校\', \'\').replace(\'None\', \'\'),\n                \"tdk_title\": response.doc(\'head > title\').eq(0).text().replace(u\'法律教育网\', \'\') + u\' 跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"法律教育网\",\n            }\n\n\n',NULL,1.0000,3.0000,1472624503.8121),('gu_drumchina','gu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-24 14:10:09\n# Project: gu_drumchina\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.2\'\n    }\n    \n    page_dict = {\n\n        \'http://news.drumchina.com/sort/1.html\':[u\'鼓资讯\'],\n        \'http://news.drumchina.com/sort/4.html\':[u\'鼓手志\'],\n        \'http://news.drumchina.com/sort/6.html\':[u\'鼓教室\'],\n        \'http://news.drumchina.com/sort/5.html\':[u\'鼓世界\'],\n        \'http://news.drumchina.com/sort/7.html\':[u\'鼓谱台\'],\n        \'http://news.drumchina.com/sort/9.html\':[u\'鼓服务\']\n\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k,save = {\'bread\':v}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'td td td td td td\').items():\n            if each.find(\'a\') and each.find(\'font\').text().split():\n                _dict = {}\n                _dict[\'title\'] = each.find(\'a\').text()\n                url = each.find(\'a\').attr.href\n                _dict[\'date\'] = each.find(\'font\').text().split()[0]\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'td[align=\"right\"] > .normalfont > a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict,callback=self.index_page)    \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'.content\').remove(\'script\').remove(\'embed\').html()\n        if \'</a>\' in res_dict[\'content\']:\n            res_dict[\'content\'] = removeLink(res_dict[\'content\'])\n        if res_dict[\'content\'] == \'<p/>\':\n            return\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'鼓\'\n        res_dict[\'source\'] = \'drumchina.com\'\n        res_dict[\'url\'] = response.url\n        return res_dict\n        \n',NULL,1.0000,3.0000,1472615979.1136),('gu_inc','gu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-24 14:10:09\n# Project: gu_drumchina\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.2\'\n    }\n    \n    page_dict = {\n\n        \'http://news.drumchina.com/sort/1.html\':[u\'鼓资讯\'],\n        \'http://news.drumchina.com/sort/4.html\':[u\'鼓手志\'],\n        \'http://news.drumchina.com/sort/6.html\':[u\'鼓教室\'],\n        \'http://news.drumchina.com/sort/5.html\':[u\'鼓世界\'],\n        \'http://news.drumchina.com/sort/7.html\':[u\'鼓谱台\'],\n        \'http://news.drumchina.com/sort/9.html\':[u\'鼓服务\']\n\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k,save = {\'bread\':v}, callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'td td td td td td\').items():\n            if each.find(\'a\') and each.find(\'font\').text().split():\n                _dict = {}\n                _dict[\'title\'] = each.find(\'a\').text()\n                url = each.find(\'a\').attr.href\n                _dict[\'date\'] = each.find(\'font\').text().split()[0]\n                _dict[\'bread\'] = response.save.get(\'bread\')\n                self.crawl(url, save = _dict,callback=self.detail_page)\n       \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'.content\').remove(\'script\').remove(\'embed\').html()\n        if res_dict[\'content\'] == \'<p/>\':\n            return\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'鼓\'\n        res_dict[\'source\'] = \'drumchina.com\'\n        res_dict[\'url\'] = response.url\n        return res_dict\n        \n',NULL,1.0000,3.0000,1472615976.0705),('gwyjingyan_zhonggong','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-08 18:25:49\n# Project: gwyjingyan_zhonggong\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.offcn.com/gjgwy/ziliao/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.zg_lm_list > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.zg_lm_2\').text()\n            _dict[\'date\'] = each.find(\'font\').text()\n            self.crawl(each.find(\'.zg_lm_2\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.a1\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):        \n        return {\n            \"url\": response.url,\n            \'bread\': [u\'职场分享\'],\n            \'title\': response.save.get(\'title\'),\n            \'date\': response.save.get(\'date\'),\n            \"html\": response.doc(\'.zg_show_word\').html(),\n            \'source\': u\'敲墙简历\',\n            \'class\': 36,\n            \'subject\': u\'经验\',\n            \'data_weight\': 0,\n        }',NULL,1.0000,3.0000,1472624506.2435),('hujiang_deyu','deyu','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 14:18:29\n# Project: hujiang_deyu\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'v0.2\'\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.hujiang.com/new/rumen/\':[u\'德语基础\'],\n            \'http://de.hujiang.com/new/shiyong/\':[u\'实用德语\'],\n            \'http://de.hujiang.com/new/yule/\':[u\'德语文化\'],\n            \'http://de.hujiang.com/new/topic/627/\':[u\'德国留学\'],\n            \'http://de.hujiang.com/new/topic/1024/\':[u\'德语考试\']\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            _dict[\'date\'] = each.find(\'p.article_list_moreinfo\').find(\'span.green\').text().split()[0]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        for each in response.doc(\'div.page_list > a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info and \'</a>\' in info:\n                content_list.append(removeLink(info))\n            elif info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'德语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1472456349.3527),('hujiang_deyu_inc','deyu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 14:18:29\n# Project: hujiang_deyu\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n        contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://de.hujiang.com/new/rumen/\':[u\'德语基础\'],\n            \'http://de.hujiang.com/new/shiyong/\':[u\'实用德语\'],\n            \'http://de.hujiang.com/new/yule/\':[u\'德语文化\'],\n            \'http://de.hujiang.com/new/topic/627/\':[u\'德国留学\'],\n            \'http://de.hujiang.com/new/topic/1024/\':[u\'德语考试\']\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            _dict[\'date\'] = each.find(\'p.article_list_moreinfo\').find(\'span.green\').text().split()[0]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n      \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'德语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1472615768.5101),('hujiang_riyu','riyu','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 14:18:29\n# Project: hujiang_deyu\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://jp.hjenglish.com/new/c4010/\':[u\'日语基础\'],\n            \'http://jp.hjenglish.com/new/c4040/\':[u\'实用日语\'],\n            \'http://jp.hjenglish.com/new/c4020/\':[u\'日语考试\'],\n            \'http://jp.hjenglish.com/new/c4070/\':[u\'日语文化\'],\n            \'http://jp.hjenglish.com/new/c4090/\':[u\'商务日语\']\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            _dict[\'date\'] = each.find(\'p.article_list_moreinfo\').find(\'span.green\').text().split()[0]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n        for each in response.doc(\'div.page_list > a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict ,headers = self.headers,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').remove(\'script\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info and \'</a>\' in info:\n                content_list.append(removeLink(info))\n            elif info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'德语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1472615746.0475),('hujiang_riyu_inc','riyu','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 14:18:29\n# Project: hujiang_deyu\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    headers = {\n\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n\n    }\n    page_dict = {\n            \'http://jp.hjenglish.com/new/c4010/\':[u\'日语基础\'],\n            \'http://jp.hjenglish.com/new/c4040/\':[u\'实用日语\'],\n            \'http://jp.hjenglish.com/new/c4020/\':[u\'日语考试\'],\n            \'http://jp.hjenglish.com/new/c4070/\':[u\'日语文化\'],\n            \'http://jp.hjenglish.com/new/c4090/\':[u\'商务日语\']\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'ul#article_list > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a[title]\').text().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            url =  each.find(\'h2 > a.a_article_title \').attr.href\n            _dict[\'date\'] = each.find(\'p.article_list_moreinfo\').find(\'span.green\').text().split()[0]\n            self.crawl(url,save = _dict ,headers = self.headers,callback=self.detail_page)\n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.main_article > p\').items():\n            info = each.remove(\'img\').remove(\'embed\').remove(\'script\').html().replace(\'沪江\',\'\').replace(\'网校\',\'\')\n            if u\'小编推荐\'  in info:\n                break\n            if u\'沪江\' in info or \'转载\' in info or \'声明：\' in info:\n                continue\n            if info:\n                content_list.append(info)\n                \n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n       if not res_dict[\'content\'] and len(content_list)<=1:\n            return\n       if res_dict[\'content\'] and not res_dict[\'content\'].strip():\n            return\n       if len(res_dict[\'content\'])<=10:\n            return\n       res_dict[\'class\'] = 33\n       res_dict[\'subject\'] = u\'德语\'\n       res_dict[\'source\'] = u\'沪江\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'url\'] = response.url\n       return res_dict\n',NULL,1.0000,3.0000,1472615761.9062),('iask_question',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-08 13:59:11\n# Project: iask_wenda\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nimport traceback\nimport MySQLdb\nclass ConnectionUtil(object):\n\n    def __init__(self,connection):\n        self.connection = connection\n\n    def cursor(self):\n        if self.connection:\n            self.cursor  = self.connection.cursor()\n            return self.cursor\n        else:\n            return  None\n\n    def __enter__(self):\n        return self.cursor()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n            #print \'ok\'\n            if self.connection:\n                self.connection.commit()\n            if self.cursor:\n                self.cursor.close()\n            if self.connection:\n                self.connection.close()\n            if exc_type is not None:\n                #print \'errror\'\n                print exc_val\n                #traceback.print_exc()\n                return True\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\',\n        \'headers\':{\n        \'Accept\':\'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\',\n\'Accept-Encoding\':\'gzip, deflate, sdch\',\n\'Accept-Language\':\'zh-CN,zh;q=0.8,en;q=0.6\',\n\'Cache-Control\':\'max-age=0\',\n\'Connection\':\'keep-alive\',\n\'Host\':\'iask.sina.com.cn\',\n#Referer:http://iask.sina.com.cn/c/997-essence-1-new.html\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\',\n\n        }\n    }\n\n    @every(minutes=1 * 20)\n    def on_start(self):\n       sql = \'select * from iask where flag = 0 limit 5\'\n       con = MySQLdb.connect(db=\'test\',host=\'127.0.0.1\',user=\'root\',passwd=\'123\',charset=\'utf8\')\n       with ConnectionUtil(con) as cursor:\n           cursor.execute(sql)\n           result =  cursor.fetchall()\n       if result:\n           for each in result:\n               _dict = {}\n               _dict[\'id\'] = each[1]\n               self.crawl(each[2],save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def index_page(self, response):\n       res_dict = response.save\n       sql = \'update iask set flag = 1 where iask_id=%s\'\n       con = MySQLdb.connect(db=\'test\',host=\'127.0.0.1\',user=\'root\',passwd=\'123\',charset=\'utf8\')\n       with ConnectionUtil(con) as cursor:\n           cursor.execute(sql,(res_dict[\'id\']))              \n       question_other = response.doc(\'span.ask-time \').text()\n       res_dict[\'create_time\'] = get_date(question_other)\n       res_dict[\'bread\'] = response.doc(\'div.dib span a\').text().split()\n       res_dict[\'title\'] = response.doc(\'h3.title-f22\').text()\n       if u\'title\' not in res_dict or not res_dict[\'title\']:\n                res_dict[\'title\'] = response.doc(\'div.question_text > .pre_img\').remove(\'div.link_layer\').text()\n       res_dict[\'question_detail\'] = response.doc(\'.question_text\').remove(\'.link_layer\').text()\n       answers_list = []\n       for each in response.doc(\'li.clearfix\').items():\n            info = each.find(\'.answer_txt\').text()\n            if info:\n                 _dict = {}\n                 _dict[\'content\'] = info\n                 _dict[\'user_name\'] = each.find(\'.user_wrap > a\').text()\n                 question_time = each.find(\'.answer_t\').text()\n                 _dict[\'create_time\'] = get_date(question_time)\n                 answers_list.append(_dict)\n       for each in response.doc(\'.good_answer\').items():\n            info = each.find(\'.answer_text\').text()\n            if info:\n                 _dict = {}\n                 _dict[\'content\'] = info\n                 _dict[\'user_name\'] = each.find(\'.answer_tip > a\').text()\n                 question_time = each.find(\'.answer_tip .time\').text().replace(\'|\',\'\').strip()\n                 _dict[\'create_time\'] = get_date(question_time)\n                 answers_list.append(_dict)\n       if not answers_list:\n                 return \n       res_dict[\'answers\'] = answers_list\n       res_dict[\'url\'] = response.url\n       res_dict[\'source\'] = \'sina\'\n       res_dict[\'subject\'] = u\'主站问答\'\n       res_dict[\'class\'] = 34\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'create_user\'] = response.doc(\'.ask_autho span.user_wrap\').children(\'a\').text()               \n       return res_dict\n\n            \n    @config(priority=2)\n    def detail_page(self, response):\n       pass',NULL,5.0000,5.0000,1469524369.5301),('iask_sina','wenda','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-08 13:59:11\n# Project: iask_sina\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nimport datetime\nimport MySQLdb\n\nclass ConnectionUtil(object):\n\n    def __init__(self,connection):\n        self.connection = connection\n\n    def cursor(self):\n        if self.connection:\n            self.cursor  = self.connection.cursor()\n            return self.cursor\n        else:\n            return  None\n\n    def __enter__(self):\n        return self.cursor()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n            #print \'ok\'\n            if self.connection:\n                self.connection.commit()\n            if self.cursor:\n                self.cursor.close()\n            if self.connection:\n                self.connection.close()\n            if exc_type is not None:\n                #print \'errror\'\n                print exc_val\n                #traceback.print_exc()\n                return True\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'v0.2\'\n        #\'headers\':self.headers\n    }\n    \n    headers = {\n     \'Accept\':\'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\',\n\'Accept-Encoding\':\'gzip, deflate, sdch\',\n\'Accept-Language\':\'zh-CN,zh;q=0.8,en;q=0.6\',\n\'Cache-Control\':\'max-age=0\',\n\'Connection\':\'keep-alive\',\n\'Host\':\'iask.sina.com.cn\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\',\n\n    }\n\n    @every(minutes=1*30)\n    def on_start(self):\n       sql = \'select * from tb_query where sina_flag = 0 limit 5000\'\n       con = MySQLdb.connect(db=\'querydb\',host=\'127.0.0.1\',user=\'root\',passwd=\'123\',charset=\'utf8\')\n       with ConnectionUtil(con) as cursor:\n           cursor.execute(sql)\n           result =  cursor.fetchall()\n       for each in result:\n           _dict = {}\n           _dict[\'query\'] = each[2]\n           _dict[\'id\'] = each[0]\n           line = each[2]\n           for index in range(3):\n               self.crawl(\'http://iask.sina.com.cn/search?searchWord=%s&page=%s\'%(line,index),save = _dict ,headers=self.headers,callback=self.index_page)\n                            \n                       \n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.search_list .search_item\').items():\n            _dict = {}\n            #_dict[\'category_id\'] = response.save.get(\'category_id\')\n            _dict[\'title\'] = each.find(\'h2 a\').text()\n            _dict[\'id\'] = response.save.get(\'id\')\n            url = each.find(\'h2 a\').attr.href\n            question_other = each.find(\'.answer_det a\').eq(0).text()\n            _dict[\'create_time\'] = get_date(question_other)\n            self.crawl(url, save = _dict,headers=self.headers,callback=self.detail_page)\n        \n            \n    @config(priority=2)\n    def detail_page(self, response):\n       sql = \'update tb_query  set sina_flag = %s  where id = %s\'\n       con = MySQLdb.connect(db=\'querydb\',host=\'127.0.0.1\',user=\'root\',passwd=\'123\',charset=\'utf8\')\n       with ConnectionUtil(con) as cursor:\n           cursor.execute(sql,(1,int(response.save.get(\'id\'))))\n       res_dict = response.save\n       question_other = response.doc(\'span.ask-time\').text().strip()\n       res_dict[\'create_time\'] = get_date(question_other)\n       res_dict[\'category_id\'] = response.doc(\'div.dib span a\').text().split()\n       res_dict[\'title\'] = response.doc(\'h3.title-f22\').text()\n       if u\'title\' not in res_dict or not res_dict[\'title\']:\n                res_dict[\'title\'] = response.doc(\'div.question_text > .pre_img\').remove(\'div.link_layer\').text()\n       res_dict[\'question_detail\'] = response.doc(\'.question_text\').remove(\'.link_layer\').text()\n       answers_list = []\n                \n       for each in response.doc(\'li.clearfix\').items():\n            info = each.find(\'.answer_txt\').text()\n            if info:\n                 _dict = {}\n                 _dict[\'content\'] = info\n                 _dict[\'user_name\'] = each.find(\'.user_wrap > a\').text()\n                 question_time = each.find(\'.answer_t\').text().strip()\n                 _dict[\'create_time\'] = get_date(question_time)\n                 answers_list.append(_dict)\n                    \n       for each in response.doc(\'.good_answer\').items():\n            info = each.find(\'.answer_text\').text()\n            if info:\n                 _dict = {}\n                 _dict[\'content\'] = info\n                 _dict[\'user_name\'] = each.find(\'.answer_tip > a\').text()\n                 question_time = each.find(\'.answer_tip .time\').text().replace(\'|\',\'\').strip()\n                 _dict[\'create_time\'] = get_date(question_time)\n                 answers_list.append(_dict)\n                    \n       if not answers_list:\n                 return \n       res_dict[\'answers\'] = answers_list\n       res_dict[\'url\'] = response.url\n       res_dict[\'source\'] = \'sina\'\n       res_dict[\'subject\'] = u\'主站问答\'\n       res_dict[\'class\'] = 34\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'create_user\'] = response.doc(\'.ask_autho span.user_wrap\').children(\'a\').text() \n       del res_dict[\'id\']\n       return res_dict\n',NULL,1.0000,3.0000,1469670208.5139),('inc_10_youku','guitar','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-30 16:44:48\n# Project: inc_10_youku\n\nfrom pyspider.libs.base_handler import *\nimport time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=5 * 60)\n    def on_start(self):\n        self.crawl(\'http://i.youku.com/i/UNDg1NzQyODA=/videos\', callback=self.index_page)\n        self.crawl(\'http://i.youku.com/u/UMjg2MDY1OTYxMg==\', callback=self.index_page)\n        self.crawl(\'http://www.soku.com/search_video/q_%E5%90%89%E4%BB%96_orderby_3_lengthtype_1_hd_7?site=14&_lg=10&limitdate=0\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.v\').items():\n            cover = each.find(\'img\').attr.src\n           # print cover\n            vlink_title = each.find(\'.v-link > a\').attr.title\n            if vlink_title == u\'该视频已被发布者加密\':\n                continue\n            url = each.find(\'.v-meta-title > a\').attr.href\n            title = each.find(\'.v-meta-title > a\').text()\n            if title.find(u\'吉他\') == -1 and title.find(u\'吉它\') == -1:\n                pass\n            else:\n                self.crawl(url, save={\'title\': title, \'cover\': cover},  callback=self.detail_page)\n\n   # @config(priority=2)\n    @config(age=1*1)\n    def detail_page(self, response):\n        try:\n            url = response.url\n        #http://v.youku.com/v_show/id_XMTUxNzMxODAyMA==.html?from=s1.8-1-1.2\n            id = url.split(\'id_\')[1].split(\'==\')[0]\n            video_url_element = \'http://player.youku.com/embed/%s\' %(id)\n            video_url = []\n            video_url.append(video_url_element)\n            title = response.save[\'title\']\n            cover = response.save[\'cover\']\n           # vlink_title = response.save[\'vlink_title\']\n           # title = response.doc(\'.base_info > .title\').text()\n            subject = u\'吉他\'\n            source = \'youku.com\'\n            publish_time = time.strftime(\'%Y-%m-%d\',time.localtime(time.time()))\n        except:\n            return None\n        return {\n            \"url\": url,\n            \"video_url\":video_url,\n            \"title\": title,\n            \"subject\": subject,\n            \"source\": source,\n            \"publish_time\": publish_time,\n            \"cover\": cover,\n            \"class\": 10,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471335003.5497),('inc_16_guitarworld','guitar','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-30 14:13:13\n# Project: guitar_news_inc\n\nfrom pyspider.libs.base_handler import *\nimport re\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.guitarworld.com.cn/news\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'h3 > a\').items():\n            url = each.attr.href\n            if re.match(\'http://www.guitarworld.com.cn/news/\', url):\n                self.crawl(url, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        url = response.url\n        subject = u\'吉他\'\n        source = \'guitarworld.com.cn\'\n        title = response.doc(\'.news-title\').text()\n        content = response.doc(\'.news-content\').html()\n        info_arr = []\n        for item in  response.doc(\'.news-link > span\').items():\n            info_arr.append(item.text())\n        content = content.replace(\'news/data/attachment\', \'data/attachment\')\n        return {\n            \"url\": url,\n            \"subject\": subject,\n            \"source\": source,\n            \"content\": content,\n            \"title\": title,\n            \"date\": info_arr[1].split(\' \')[0],\n            \"class\": 16,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471329630.2764),('inc_gangqin_youku_6','gangqin','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-19 11:22:03\n# Project: inc_gangqin_youku_6\n\n\nfrom pyspider.libs.base_handler import *\nimport time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.soku.com/search_video/q_%E9%92%A2%E7%90%B4_limitdate_0?site=14&_lg=10&orderby=2\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.v\').items():\n            cover = each.find(\'img\').attr.src\n           # print cover\n            url = each.find(\'.v-meta-title > a\').attr.href\n            title = each.find(\'.v-meta-title > a\').text()\n            self.crawl(url, save={\'title\': title, \'cover\': cover},  callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        try:\n            url = response.url\n            id = url.split(\'id_\')[1].split(\'==\')[0]\n            video_url_element = \'http://player.youku.com/embed/%s\' %(id)\n            video_url = []\n            video_url.append(video_url_element)\n            title = response.save[\'title\']\n            cover = response.save[\'cover\']\n           # title = response.doc(\'.base_info > .title\').text()\n            subject = u\'钢琴\'\n            source = \'youku.com\'\n            publish_time = time.strftime(\'%Y-%m-%d\',time.localtime(time.time()))\n        except:\n            return None\n        return {\n            \"url\": url,\n            \"video_url\":video_url,\n            \"title\": title,\n            \"subject\": subject,\n            \"source\": source,\n            \"publish_time\": publish_time,\n            \"cover\": cover,\n            \"class\": 6,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471335046.7183),('jianlijingyan_qqjl','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-08 18:06:20\n# Project: jianlijingyan_qqjl\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.qqjianli.com/blog/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'h2 > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.nextpostslink\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        methods = []\n        content = []\n        for each in response.doc(\'.post_content p\').items():\n            if each.text().strip() != \'\':\n                content.append(each.html())   \n        if len(content) == 0:\n            return \n        img = \'\'\n        title = \'\'\n        index = 0\n        if len(content[0].strip()) > 1 and content[0].strip()[1] != u\'、\' and \'--\' != content[0].strip()[0:2]:\n            title = content[0]\n            index = 1\n        _list = []  \n        step_title = \'\'\n        substeps = []\n        flag_first = True\n        flag_mothod = False\n        for each in content[index:]:\n            if each.strip() == \'\':\n                continue\n            if each.strip()[1] == u\'、\' or \'--\' == each.strip()[0:2]:\n                flag_mothod = True\n                if not flag_first:\n                    _list.append({\n                        \'img\': \'\',\n                        \'title\': step_title,\n                        \'substeps\': substeps,\n                    })\n                    #print each\n                    if \'<br\' in each:\n                        step_title = \'<strong>\'+each.split(\'<br />\\n\')[0]+\'</strong>\'\n                        substeps = [each.split(\'<br />\\n\')[1]]\n                    else:\n                        step_title = \'<strong>\'+each+\'</strong>\'\n                        substeps = []\n                if flag_first:\n                    if \'<br\' in each:\n                        step_title = \'<strong>\'+each.split(\'<br />\\n\')[0]+\'</strong>\'\n                        substeps = [each.split(\'<br />\\n\')[1]]\n                    else:\n                        step_title = \'<strong>\'+each+\'</strong>\'\n                        substeps = []\n                    flag_first = False\n            else:\n                if flag_mothod:\n                    substeps.append(\'<p>\'+each+\'</p>\')\n                else:\n                    step_title = \'<strong>\'+each+\'</strong>\'\n            _list.append({\n                \'img\': \'\',\n                \'title\': step_title,\n                \'substeps\': substeps,\n            })\n        methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n                \n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h2\').text(),\n            \"abstract\": {\n                \'title\': \'\',\n                \'steps\': [\'<strong>\'+title+\'</strong>\'],\n                \'img\': \'\',\n            },\n            \"methods\": methods,\n            \'bread\': [u\'职场分享\'],\n            \'date\': response.doc(\'.date\').text(),\n            \'source\': u\'敲墙简历\',\n            \'class\': 36,\n            \'subject\': u\'经验\',\n            \'data_weight\': 0,\n        }\n',NULL,1.0000,3.0000,1472624511.2697),('jianli_jianlisky','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-07 11:01:22\n# Project: qiuzhi_kuaiji\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.jianli-sky.com/\', callback=self.list_page)\n        \'\'\'\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/gerenjianlimoban/\', callback=self.index_page)\n\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/yingwenjianlimoban/\', callback=self.index_page)\n\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/qiuzhijianlimoban/\', callback=self.index_page)\n        \'\'\'\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.pleft strong > a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.w1 > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        for each in response.doc(\'.dede_pages a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = [\'<p>%s</p>\'%v.text() for v in response.doc(\'p\').items()][:-1]\n        if not content_list:\n            return None\n        content = \'\'.join(content_list)\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"content\": content, \n            \"data_weight\": 0,\n            \"subject\": u\'求职\',\n            \"bread\": [u\'简历\',],\n            \"class\": 46,\n            \"source\": u\'jianli-sky\',\n        }\n',NULL,1.0000,3.0000,1472624509.1076),('jingyan','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-25 16:12:34\n# Project: jingyan\n\nfrom pyspider.libs.base_handler import *\nimport time\n\nclass Handler(BaseHandler):\n\n    \n    page_dict = {\n\n        \'1\':u\'电脑数码\',\n        \'2\':u\'美食烹饪\',\n        \'3\':u\'健康养生\',\n        \'4\':u\'时尚美容\',\n        \'5\':u\'情感家庭\',\n        \'6\':u\'游戏攻略\',\n        \'7\':u\'职场理财\',\n        \'8\':u\'生活技巧\',\n        \'9\':u\'体育运动\',\n\n        \n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        \'\'\'\n        for line in open(\'/apps/home/worker/xuzhihao/jingyan/tags\'):\n            tag = line.strip(\'\\n\')\n            self.crawl(\'http://jingyan.baidu.com/tag?tagName=\'+tag, save={\'tag\': tag},  callback=self.index_page)\n        \'\'\'\n        for k,v in self.page_dict.items():\n            self.crawl(\'http://xinzhi.wenda.so.com/home/list?cid=%s\'%(k),save={\'tag\': v,\'cid\':k}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        url = \'http://xinzhi.wenda.so.com/home/partList?type=1&cid=%s&pn=%s&t=%s\'\n        for each in range(300):\n            _dict = {}\n            _dict[\'tag\'] = response.save.get(\'tag\')\n            _dict[\'cid\'] = response.save.get(\'cid\')\n            timestr =  \'%f\'%(time.time()*1000)\n            self.crawl(url%(response.save.get(\'cid\'),each,timestr.split(\'.\')[0]), save=_dict, callback=self.list_page)\n            \n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'h3 > a\').items():\n            self.crawl(each.attr.href, save={\'tag\': response.save[\'tag\']}, callback=self.detail_page)\n        \n\n    @config(priority=2)\n    def detail_page(self, response):\n        methods = []\n        for each in response.doc(\'.steps\').items():\n            title = each.find(\'.title\').text() #u\'方法\'\n            print title\n            _list = []\n            for step in each.find(\'li\').items():\n                try:\n                    img = step.find(\'.pic > img\').attr.src\n                except:\n                    img = \'\'\n                _list.append({\n                    \"title\": step.find(\'.text\').html(),\n                    \"img\": img,\n                    #\"title\": step.text(),\n                    \"substeps\": [],\n                })\n\n            methods.append({\"title\": title, \"steps\": _list})\n        abstract = {\'title\': \'\',\n                    \'steps\': [response.doc(\'.brief > div\').text(),],\n                    \'img\': \'\'\n                    }\n        prepare = {\'title\': response.doc(\'.tools .title\').text(),\n                   \'steps\': [response.doc(\'.tools .content\').text(), ],\n                   }\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.art-title\').text(),\n            \"methods\": methods,\n            \"abstract\": abstract,\n            \"prepare\": prepare,\n            \"date\": response.doc(\'.art-time\').text().replace(u\'创建于\', \'\')[:10],\n            \"bread\": [response.save[\'tag\'],],\n            \"source\": \"360\",\n            \"class\": 36,\n            \"subject\": \'经验\',\n            \"data_weight\": 0,\n        }\n',NULL,5.0000,5.0000,1472624415.1851),('jingyan2_gaosanwang','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-24 09:39:15\n# Project: jingyan2_gaosanwang\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\nurl_dict = {\n\'http://www.gaosan.com/zhukao/1/\': u\'高考\',\n\'http://www.gaosan.com/zhuanyejiedu/1/\': u\'大学\',\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url,bread in url_dict.iteritems():\n            self.crawl(url,save = {\'bread\':bread},  callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.showMoreNChildren > li\').items():\n            if \'index.html\' in each.find(\'a\').attr.href:\n                continue\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'title\'] = each.find(\'a > b\').text().strip()\n            _dict[\'date\'] = each.find(\'i\').text().strip().split()[-1].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'#AspNetPager1 > a\').items():\n            self.crawl(each.attr.href, save = response.save, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        #print pattern.match(str).group(0)\n        #print response.doc(\'.content div.left.d > div\').eq(-2).html()\n        if response.doc(\'.content div.left.d > div\').eq(-2).find(\'.lcontent\'):\n            con = response.doc(\'.content div.left.d > div\').eq(-2).find(\'.lcontent\')\n        else:\n            con = response.doc(\'.content div.left.d > div\').eq(-2)\n        for each in con.children().items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if each.text() == \'None\' or not each.html():\n                continue\n            if u\'高三网小编推荐你\' in each.text() or u\'扫一扫\' in each.text() or u\'相关链接\' in each.text():\n                break  \n            if u\'点击查看\' in each.text() or u\'查看更多\' in each.text() or u\'相关链接\' in each.text():\n                continue  \n            if pattern.match(each.text().strip()) or each.html().strip().startswith(\'<strong>\') or \'<h\' in each.outerHtml():\n                flag_abstract = False\n                flag_method = True\n            if \'<img\' in each.html():\n                #print each.html()\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                    #print desc_img\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if not each.text():\n                continue\n            if flag_abstract:\n                if \'<table\' in each.outerHtml():\n                    steps.append(each.outerHtml().replace(u\'高三网\',\'\'))\n                else:\n                    steps.append(each.text().replace(u\'高三网\',\'\'))\n            else:\n                if  pattern.match(each.text().strip()) or each.html().strip().startswith(\'<strong>\') or \'<h\' in each.outerHtml():\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'高三网\',\'\'))    \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [response.save.get(\'bread\'),],\n            \"source\": u\"高三网\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,3.0000,5.0000,1472624514.7545),('jingyan_dida','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-11 16:02:50\n# Project: jingyan_dida\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\nimport sys,json\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nurls_list = [\n\'http://studyabroad.tigtag.com/application/\',   \n\'http://studyabroad.tigtag.com/schoolguide/\',                                    \n\'http://studyabroad.tigtag.com/rank/\',                                         \n\'http://studyabroad.tigtag.com/schoolfile/\',    \n\'http://studyabroad.tigtag.com/major/\',   \n\'http://studyabroad.tigtag.com/scholarship/\',  \n\'http://studyabroad.tigtag.com/writing/\',    \n\'http://studyabroad.tigtag.com/visa/\',                                          \n\'http://studyabroad.tigtag.com/experience/\',\n]\n\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url in urls_list:\n            self.crawl(url,headers = self.header, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.col-list li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip()\n            self.crawl(each.find(\'a\').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.page-control > a\').items():\n            self.crawl(each.attr.href,headers = self.header, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        pattern2 = re.compile(ur\'[0123456789]{1,2}[：].*\')\n        pattern3 = re.compile(r\'[0123456789]{2}.*\')\n        pattern4 = re.compile(r\'<strong>\')\n        if response.doc(\'#articon\'):\n            #print \'ok\'\n            for each in response.doc(\'#articon > *\').items():\n                #print each\n                #print dir(each)\n                #print each.outerHtml()\n                #print each.html()\n                if not each.text().strip().replace(u\'滴答网\',\'\'):\n                    continue\n                \n                if u\'专注免费\' in each.text() or u\'版权声明\' in each.text() or u\'上一页\' in each.text():\n                    break \n                #print i,each.html()\n                if \'<img\' in each.html():\n                    if desc_img == \'\':\n                        desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n                if each.text() == \'None\' or each.text().strip() == \'\':\n                    continue\n                #if each.find(\'strong\'):\n                 #   print each.find(\'strong\').outerHtml()\n                if \'【\' in each.text() or \'strong\' in each.outerHtml() or each.text().strip()[-1] == u\'：\':\n                    flag_abstract = False\n                    flag_method = True\n                if u\'讯\' in each.text():\n                    flag_abstract = True\n                if flag_abstract:\n                    steps.append(each.text().replace(u\'滴答网\',\'\'))\n\n                else:\n                    if \'【\' in each.text() or \'strong\' in each.outerHtml() or each.text().strip()[-1] == u\'：\':\n                        #print each.strip()[1]\n                        #print each.html()\n                        if not flag_first:\n                            _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                            })\n                            img = \'\'\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            substeps = []\n                        if flag_first:\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            flag_first = False    \n\n                    else:\n                        substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')) \n                        \n                        \n        elif response.doc(\'.articon > span\') and response.doc(\'.articon > p\').size() < 5 and response.doc(\'.articon > div\').size() < 5:\n            for each in response.doc(\'.articon > *\').items():\n                #print dir(each)\n                #print each.outerHtml()\n                #print each.html()\n                if not each.html():\n                    continue\n                if u\'专注免费\' in each.text() or u\'版权声明\' in each.text() or u\'上一页\' in each.text():\n                    break \n\n                if \'<img\' in each.html():\n                    if desc_img == \'\':\n                        desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n                if each.text() == \'None\' or each.text().strip() == \'\':\n                    continue\n                #if each.find(\'strong\'):\n                 #   print each.find(\'strong\').outerHtml()\n                if pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                    flag_abstract = False\n                    flag_method = True\n\n                if flag_abstract:\n                    steps.append(each.text().replace(u\'滴答网\',\'\'))\n\n                else:\n                    if pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                        #print each.strip()[1]\n                        #print each.html()\n                        con_list = each.html().split(\'<br style=\"padding: 0px; margin: 0px; border: none;\"/>&#13;\')\n                        for ea in con_list:\n                            #print ea\n                            if not ea.strip():\n                                continue\n                            if \'<strong\' in ea:\n                                if not flag_first:\n                                    _list.append({\n                                        \'img\': img,\n                                        \'title\': step_title,\n                                        \'substeps\': substeps,\n                                    })\n                                    img = \'\'\n                                    step_title = \'<strong>\'+pq(ea).text().replace(u\'留学益网\',\'\')+\'</strong>\'\n                                    substeps = []\n                                if flag_first:\n                                    step_title = \'<strong>\'+pq(ea).text().replace(u\'留学益网\',\'\')+\'</strong>\'\n                                    flag_first = False\n                            else:\n                                substeps.append(ea.replace(\'\\n\',\'\').replace(u\'留学益网\',\'\'))    \n\n                    else:\n                        substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'留学益网\',\'\')) \n                        \n        elif response.doc(\'.articon > p\').size() < 5 and response.doc(\'.articon > div\').size() < 5:\n            #print response.doc(\'.articon\').html()\n            con_list = response.doc(\'.articon\').html().split(\'<br/>&#13;\')\n            i = 0\n            #print len(con_list)\n            for each in con_list:\n                if not each.strip():\n                    continue\n                #print each\n                each = pq(each)\n                #print dir(each)\n                #print each.outerHtml()\n                #print each.html()\n                if not each.html():\n                    continue\n                \n                if u\'专注免费\' in each.text() or u\'版权声明\' in each.text() or u\'上一页\' in each.text():\n                    break \n                i += 1\n                #print i,each.html()\n                if \'<img\' in each.html():\n                    if desc_img == \'\':\n                        desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n                if each.text() == \'None\' or each.text().strip() == \'\':\n                    continue\n                #if each.find(\'strong\'):\n                 #   print each.find(\'strong\').outerHtml()\n                if \'【\' in each.text() or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.outerHtml()) or each.text().strip()[-1] == u\'：\':\n                    flag_abstract = False\n                    flag_method = True\n                if u\'讯\' in each.text():\n                    flag_abstract = True\n                if flag_abstract:\n                    steps.append(each.text().replace(u\'滴答网\',\'\'))\n\n                else:\n                    if \'【\' in each.text() or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.outerHtml()) or each.text().strip()[-1] == u\'：\':\n                        #print each.strip()[1]\n                        #print each.html()\n                        if not flag_first:\n                            _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                            })\n                            img = \'\'\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            substeps = []\n                        if flag_first:\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            flag_first = False    \n\n                    else:\n                        substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')) \n         \n        else:\n            #print \'ok\'\n            for each in response.doc(\'.articon > *\').items():\n                #print each\n                #print dir(each)\n                #print each.outerHtml()\n                #print each.html()\n                if not each.text().strip().replace(u\'滴答网\',\'\'):\n                    continue\n                \n                if u\'专注免费\' in each.text() or u\'版权声明\' in each.text() or u\'上一页\' in each.text():\n                    break \n                #print i,each.html()\n                if \'<img\' in each.html():\n                    if desc_img == \'\':\n                        desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n                if each.text() == \'None\' or each.text().strip() == \'\':\n                    continue\n                #if each.find(\'strong\'):\n                 #   print each.find(\'strong\').outerHtml()\n                if \'【\' in each.text() or \'strong\' in each.outerHtml() or each.text().strip()[-1] == u\'：\':\n                    flag_abstract = False\n                    flag_method = True\n                if u\'讯\' in each.text():\n                    flag_abstract = True\n                if flag_abstract:\n                    steps.append(each.text().replace(u\'滴答网\',\'\'))\n\n                else:\n                    if \'【\' in each.text() or \'strong\' in each.outerHtml() or each.text().strip()[-1] == u\'：\':\n                        #print each.strip()[1]\n                        #print each.html()\n                        if not flag_first:\n                            _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                            })\n                            img = \'\'\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            substeps = []\n                        if flag_first:\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            flag_first = False    \n\n                    else:\n                        substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')) \n            \n                        \n         \n                        \n        \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [\'\'.join(steps)],\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        if len(methods[0][\'steps\']) == 0:\n           return \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'留学\',],\n            \"source\": u\"滴答\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472624410.8653),('jingyan_dida2',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-12 13:25:00\n# Project: jingyan_dida2\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\nimport sys,json\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nurls_list = [\n\'http://studyabroad.tigtag.com/application/\',   \n\'http://studyabroad.tigtag.com/schoolguide/\',                                    \n\'http://studyabroad.tigtag.com/rank/\',                                         \n\'http://studyabroad.tigtag.com/schoolfile/\',    \n\'http://studyabroad.tigtag.com/major/\',   \n\'http://studyabroad.tigtag.com/scholarship/\',  \n\'http://studyabroad.tigtag.com/writing/\',    \n\'http://studyabroad.tigtag.com/visa/\',                                          \n\'http://studyabroad.tigtag.com/experience/\',\n]\n\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url in urls_list:\n            self.crawl(url,headers = self.header, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.col-list li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'span\').text().strip()\n            self.crawl(each.find(\'a\').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.page-control > a\').items():\n            self.crawl(each.attr.href,headers = self.header, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        pattern2 = re.compile(ur\'[0123456789]{1,2}[：].*\')\n        pattern3 = re.compile(r\'[0123456789]{2}.*\')\n        pattern4 = re.compile(r\'<strong>\')\n        if response.doc(\'#articon\'):\n            #print \'ok\'\n            for each in response.doc(\'#articon > *\').items():\n                #print each\n                #print dir(each)\n                #print each.outerHtml()\n                #print each.html()\n                if not each.text().strip().replace(u\'滴答网\',\'\'):\n                    continue\n                \n                if u\'专注免费\' in each.text() or u\'版权声明\' in each.text() or u\'上一页\' in each.text():\n                    break \n                #print i,each.html()\n                if \'<img\' in each.html():\n                    if desc_img == \'\':\n                        desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n                if each.text() == \'None\' or each.text().strip() == \'\':\n                    continue\n                #if each.find(\'strong\'):\n                 #   print each.find(\'strong\').outerHtml()\n                if \'【\' in each.text() or \'strong\' in each.outerHtml() or each.text().strip()[-1] == u\'：\':\n                    flag_abstract = False\n                    flag_method = True\n                if u\'讯\' in each.text():\n                    flag_abstract = True\n                if flag_abstract:\n                    steps.append(each.text().replace(u\'滴答网\',\'\'))\n\n                else:\n                    if \'【\' in each.text() or \'strong\' in each.outerHtml() or each.text().strip()[-1] == u\'：\':\n                        #print each.strip()[1]\n                        #print each.html()\n                        if not flag_first:\n                            _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                            })\n                            img = \'\'\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            substeps = []\n                        if flag_first:\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            flag_first = False    \n\n                    else:\n                        substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')) \n                        \n                        \n        elif response.doc(\'.articon > span\') and response.doc(\'.articon > p\').size() < 5 and response.doc(\'.articon > div\').size() < 5:\n            for each in response.doc(\'.articon > *\').items():\n                #print dir(each)\n                #print each.outerHtml()\n                #print each.html()\n                if not each.html():\n                    continue\n                if u\'专注免费\' in each.text() or u\'版权声明\' in each.text() or u\'上一页\' in each.text():\n                    break \n\n                if \'<img\' in each.html():\n                    if desc_img == \'\':\n                        desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n                if each.text() == \'None\' or each.text().strip() == \'\':\n                    continue\n                #if each.find(\'strong\'):\n                 #   print each.find(\'strong\').outerHtml()\n                if pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                    flag_abstract = False\n                    flag_method = True\n\n                if flag_abstract:\n                    steps.append(each.text().replace(u\'滴答网\',\'\'))\n\n                else:\n                    if pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                        #print each.strip()[1]\n                        #print each.html()\n                        con_list = each.html().split(\'<br style=\"padding: 0px; margin: 0px; border: none;\"/>&#13;\')\n                        for ea in con_list:\n                            #print ea\n                            if not ea.strip():\n                                continue\n                            if \'<strong\' in ea:\n                                if not flag_first:\n                                    _list.append({\n                                        \'img\': img,\n                                        \'title\': step_title,\n                                        \'substeps\': substeps,\n                                    })\n                                    img = \'\'\n                                    step_title = \'<strong>\'+pq(ea).text().replace(u\'留学益网\',\'\')+\'</strong>\'\n                                    substeps = []\n                                if flag_first:\n                                    step_title = \'<strong>\'+pq(ea).text().replace(u\'留学益网\',\'\')+\'</strong>\'\n                                    flag_first = False\n                            else:\n                                substeps.append(ea.replace(\'\\n\',\'\').replace(u\'留学益网\',\'\'))    \n\n                    else:\n                        substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'留学益网\',\'\')) \n                        \n        elif response.doc(\'.articon > p\').size() < 5 and response.doc(\'.articon > div\').size() < 5:\n            #print response.doc(\'.articon\').html()\n            con_list = response.doc(\'.articon\').html().split(\'<br/>&#13;\')\n            i = 0\n            #print len(con_list)\n            for each in con_list:\n                if not each.strip():\n                    continue\n                #print each\n                each = pq(each)\n                #print dir(each)\n                #print each.outerHtml()\n                #print each.html()\n                if not each.html():\n                    continue\n                \n                if u\'专注免费\' in each.text() or u\'版权声明\' in each.text() or u\'上一页\' in each.text():\n                    break \n                i += 1\n                #print i,each.html()\n                if \'<img\' in each.html():\n                    if desc_img == \'\':\n                        desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n                if each.text() == \'None\' or each.text().strip() == \'\':\n                    continue\n                #if each.find(\'strong\'):\n                 #   print each.find(\'strong\').outerHtml()\n                if \'【\' in each.text() or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.outerHtml()) or each.text().strip()[-1] == u\'：\':\n                    flag_abstract = False\n                    flag_method = True\n                if u\'讯\' in each.text():\n                    flag_abstract = True\n                if flag_abstract:\n                    steps.append(each.text().replace(u\'滴答网\',\'\'))\n\n                else:\n                    if \'【\' in each.text() or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.outerHtml()) or each.text().strip()[-1] == u\'：\':\n                        #print each.strip()[1]\n                        #print each.html()\n                        if not flag_first:\n                            _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                            })\n                            img = \'\'\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            substeps = []\n                        if flag_first:\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            flag_first = False    \n\n                    else:\n                        substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')) \n         \n        else:\n            #print \'ok\'\n            for each in response.doc(\'.articon > *\').items():\n                #print each\n                #print dir(each)\n                #print each.outerHtml()\n                #print each.html()\n                if not each.text().strip().replace(u\'滴答网\',\'\'):\n                    continue\n                \n                if u\'专注免费\' in each.text() or u\'版权声明\' in each.text() or u\'上一页\' in each.text():\n                    break \n                #print i,each.html()\n                if \'<img\' in each.html():\n                    if desc_img == \'\':\n                        desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n                if each.text() == \'None\' or each.text().strip() == \'\':\n                    continue\n                #if each.find(\'strong\'):\n                 #   print each.find(\'strong\').outerHtml()\n                if \'【\' in each.text() or \'strong\' in each.outerHtml() or each.text().strip()[-1] == u\'：\':\n                    flag_abstract = False\n                    flag_method = True\n                if u\'讯\' in each.text():\n                    flag_abstract = True\n                if flag_abstract:\n                    steps.append(each.text().replace(u\'滴答网\',\'\'))\n\n                else:\n                    if \'【\' in each.text() or \'strong\' in each.outerHtml() or each.text().strip()[-1] == u\'：\':\n                        #print each.strip()[1]\n                        #print each.html()\n                        if not flag_first:\n                            _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                            })\n                            img = \'\'\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            substeps = []\n                        if flag_first:\n                            step_title = \'<strong>\'+each.text().replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')+\'</strong>\'\n                            flag_first = False    \n\n                    else:\n                        substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'滴答网\',\'\').replace(u\'留学益网\',\'\')) \n            \n                        \n         \n                        \n        \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [\'\'.join(steps)],\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        if len(methods[0][\'steps\']) == 0:\n           return \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'留学\',],\n            \"source\": u\"滴答\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,5.0000,5.0000,1470996047.8872),('jingyan_gaosanwang','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 21:50:33\n# Project: jingyan_gaosanwang\n\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\nurl_dict = {\n\'http://www.gaosan.com/zhukao/1/\': u\'高考\',\n\'http://www.gaosan.com/zhuanyejiedu/1/\': u\'大学\',\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url,bread in url_dict.iteritems():\n            self.crawl(url,save = {\'bread\':bread},  callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.showMoreNChildren > li\').items():\n            if \'index.html\' in each.find(\'a\').attr.href:\n                continue\n            _dict = {}\n            _dict[\'bread\'] = response.save[\'bread\']\n            _dict[\'title\'] = each.find(\'a > b\').text().strip()\n            _dict[\'date\'] = each.find(\'i\').text().strip().split()[-1].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'#AspNetPager1 > a\').items():\n            self.crawl(each.attr.href, save = response.save, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        #print pattern.match(str).group(0)\n        #print response.doc(\'.content div.left.d > div\').eq(-2).html()\n        for each in response.doc(\'.content div.left.d > div\').eq(-2).children().items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if each.text() == \'None\' or not each.html():\n                continue\n            if u\'高三网小编推荐你\' in each.text() or u\'扫一扫\' in each.text() or u\'相关链接\' in each.text():\n                break  \n            if u\'点击查看\' in each.text() or u\'查看更多\' in each.text() or u\'相关链接\' in each.text():\n                continue  \n            if pattern.match(each.text().strip()) or \'<strong>\' in each.html() or \'<h\' in each.outerHtml():\n                flag_abstract = False\n                flag_method = True\n            if \'<img\' in each.html():\n                #print each.html()\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                    #print desc_img\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if not each.text():\n                continue\n            if flag_abstract:\n                if \'<table\' in each.outerHtml():\n                    steps.append(each.outerHtml().replace(u\'高三网\',\'\'))\n                else:\n                    steps.append(each.text().replace(u\'高三网\',\'\'))\n            else:\n                if  pattern.match(each.text().strip()) or \'<strong>\' in each.html() or \'<h\' in each.outerHtml():\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'高三网\',\'\'))    \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [response.save.get(\'bread\'),],\n            \"source\": u\"高三网\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472624519.0763),('jingyan_sohu_gaokao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-20 18:05:47\n# Project: jingyan_sohu_gaokao\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://learning.sohu.com/tag/0313/000000313.shtml\',headers = self.header, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.published\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.content-title > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.time\').text().strip().split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            self.crawl(each.find(\'.content-title > a\').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\n        #翻页\n        for i in range(263,362,1):\n            self.crawl(\'http://learning.sohu.com/tag/0313/000000313_\'+str(i)+\'.shtml\',headers = self.header, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        pattern2 = re.compile(ur\'[0123456789]{1,2}[：].*\')\n        pattern3 = re.compile(r\'[0123456789]{2}.*\')\n        pattern4 = re.compile(r\'<strong>\')\n        for each in response.doc(\'#contentText > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break \n            \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\'))    \n        for each in response.doc(\'.text > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break  \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\')) \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        if len(methods[0][\'steps\']) == 0:\n           return \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'高考\',],\n            \"source\": u\"搜狐\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472624531.0857),('jingyan_sohu_gaokao_inc',NULL,'RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 15:36:33\n# Project: jingyan_sohu_gaokao_inc\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://learning.sohu.com/tag/0313/000000313.shtml\',headers = self.header, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.published\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.content-title > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.time\').text().strip().split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            self.crawl(each.find(\'.content-title > a\').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\n        #翻页\n        #for i in range(263,362,1):\n            #self.crawl(\'http://learning.sohu.com/tag/0313/000000313_\'+str(i)+\'.shtml\',headers = self.header, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        pattern2 = re.compile(ur\'[0123456789]{1,2}[：].*\')\n        pattern3 = re.compile(r\'[0123456789]{2}.*\')\n        pattern4 = re.compile(r\'<strong>\')\n        for each in response.doc(\'#contentText > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break \n            \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\'))    \n        for each in response.doc(\'.text > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break  \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\')) \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        if len(methods[0][\'steps\']) == 0:\n           return \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'高考\',],\n            \"source\": u\"搜狐\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1471592283.3611),('jingyan_sohu_liuxue','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-20 18:13:10\n# Project: jingyan_sohu_liuxue\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://learning.sohu.com/tag/0073/000000073.shtml\',save = {\'tag\':1}, headers = self.header, callback=self.index_page)\n        self.crawl(\'http://learning.sohu.com/tag/0249/000000249.shtml\',save = {\'tag\':2},headers = self.header, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.published\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.content-title > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.time\').text().strip().split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            self.crawl(each.find(\'.content-title > a\').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\n        #翻页\n        if response.save.get(\'tag\') == 1:\n            for i in range(356,455,1):\n                self.crawl(\'http://learning.sohu.com/tag/0073/000000073_\'+str(i)+\'.shtml\',save = response.save, headers = self.header, callback=self.index_page)\n        if response.save.get(\'tag\') == 2:\n            for i in range(44,143,1):\n                self.crawl(\'http://learning.sohu.com/tag/0249/000000249_\'+str(i)+\'.shtml\',save = response.save, headers = self.header, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        pattern2 = re.compile(ur\'[0123456789]{1,2}[：].*\')\n        pattern3 = re.compile(r\'[0123456789]{2}.*\')\n        pattern4 = re.compile(r\'<strong>\')\n        for each in response.doc(\'#contentText > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break \n            \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\'))    \n        for each in response.doc(\'.text > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break  \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\')) \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        if len(methods[0][\'steps\']) == 0:\n           return \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'留学\',],\n            \"source\": u\"新东方\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472624527.9107),('jingyan_sohu_liuxue_inc',NULL,'RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 15:38:51\n# Project: jingyan_sohu_liuxue_inc\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    header = {\n    \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://learning.sohu.com/tag/0073/000000073.shtml\',save = {\'tag\':1}, headers = self.header, callback=self.index_page)\n        self.crawl(\'http://learning.sohu.com/tag/0249/000000249.shtml\',save = {\'tag\':2},headers = self.header, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.published\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.content-title > a\').text().strip()\n            _dict[\'date\'] = each.find(\'.time\').text().strip().split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            self.crawl(each.find(\'.content-title > a\').attr.href,headers = self.header, save = _dict, callback=self.detail_page)\n        #翻页\n        #if response.save.get(\'tag\') == 1:\n         #   for i in range(356,455,1):\n                #self.crawl(\'http://learning.sohu.com/tag/0073/000000073_\'+str(i)+\'.shtml\',save = response.save, headers = self.header, callback=self.index_page)\n #       if response.save.get(\'tag\') == 2:\n  #          for i in range(44,143,1):\n                #self.crawl(\'http://learning.sohu.com/tag/0249/000000249_\'+str(i)+\'.shtml\',save = response.save, headers = self.header, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        pattern2 = re.compile(ur\'[0123456789]{1,2}[：].*\')\n        pattern3 = re.compile(r\'[0123456789]{2}.*\')\n        pattern4 = re.compile(r\'<strong>\')\n        for each in response.doc(\'#contentText > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break \n            \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\'))    \n        for each in response.doc(\'.text > p\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if not each.html():\n                continue\n            if u\'编辑推荐\' in each.text() or u\'聚铭师教育\' in each.text() or u\'相关链接\' in each.text():\n                break  \n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            #if each.find(\'strong\'):\n             #   print each.find(\'strong\').outerHtml()\n            if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                flag_abstract = False\n                flag_method = True\n            \n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if pattern.match(each.text().strip()) or pattern2.match(each.text().strip()) or pattern3.match(each.text().strip()) or pattern4.match(each.find(\'strong\').outerHtml().strip() if each.find(\'strong\') else each.text()) or \'<em>\' in each.html() or each.text().strip()[-1] == u\'：\':\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\')) \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        if len(methods[0][\'steps\']) == 0:\n           return \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'留学\',],\n            \"source\": u\"搜狐\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1471592434.2140),('jingyan_wikiHow','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-27 10:30:13\n# Project: wikiHow_jingyan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://zh.wikihow.com/Special:Sitemap\', callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'#catentry a\').items():\n            self.crawl(each.attr.href, save={\'tag\': each.text()}, callback=self.index_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'td a\').items():\n            \n            self.crawl(each.attr.href, save={\'tag\': response.save[\'tag\']}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        steps = []\n        for each in response.doc(\'.steps\').items():\n            method = each.find(\'h3\').text()\n            if not method:\n                method = each.find(\'h2\').text()\n\n            _list = []\n            for step in each.find(\'ol > li\').items():\n                try:\n                    img = step.find(\'a\').html().split(\'data-src=\"\')[-1].split(\'\"\')[0]\n                except:\n                    img = \'\'\n                step_title = step.find(\'.whb\').text()\n                step_list = []\n                for _step in step.find(\'ul > li\').items():\n                    step_list.append((_step.text()))\n                _list.append({\n                    \"img\": img,\n                    \"title\": step_title,\n                    \"substeps\": step_list,\n                })\n            steps.append({\"data\": _list, \"title\": method})\n        summary = {\'title\': response.doc(u\'.小提示 h2\').text(),\n                   \'steps\': [v.text() for v in response.doc(u\'.小提示 li\').items()]\n                   }\n        abstract = {\'title\': \'\',\n                    \'img\': \'\',\n                    \'steps\': [v.text() for v in response.doc(\'#intro > p\').items()][-1]\n                    }\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'[itemprop=\"name\"] > a\').text(),\n            \"methods\": steps,\n            \"abstract\": abstract,\n            \"summary\": summary,,\n            \"date\": \'\',\n            \"bread\": [response.save[\'tag\'],],\n            \"source\": \"wikiHow\",\n            \"class\": 36,\n            \"subject\": \'经验\',\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1472624402.6569),('jingyan_xdf','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-20 14:23:32\n# Project: jingyan_xdf\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://xiaoxue.xdf.cn/list_1220_1.html\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.txt_lists01 > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text().strip()\n            _dict[\'date\'] = each.find(\'.time\').text().strip()\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.teacherNum > a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        #print pattern.match(str).group(0)\n        for each in response.doc(\'.air_con > *\').items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if each.text() == \'None\' or each.text().strip() == \'\':\n                continue\n            if u\'编辑推荐\' in each.text() or u\'扫一扫\' in each.text() or u\'相关链接\' in each.text():\n                break    \n            if pattern.match(each.text().strip()) or \'<strong>\' in each.html() or \'<h\' in each.outerHtml():\n                flag_abstract = False\n                flag_method = True\n            if \'<img\' in each.html():\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                    img = each.find(\'img\').attr.src\n                    each.remove(\'img\')\n            if flag_abstract:\n                steps.append(each.text())\n\n            else:\n                if  pattern.match(each.text().strip()) or \'<strong>\' in each.html() or \'<h\' in each.outerHtml():\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\'))    \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.save.get(\'date\'),\n            \"bread\": [u\'小学\',],\n            \"source\": u\"新东方\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472624750.7798),('jingyan_xuexila','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-24 10:00:50\n# Project: jingyan_xuexila\n\nfrom pyspider.libs.base_handler import *\nimport random\nfrom pyquery import PyQuery as pq\nimport re\n\nurl_list = [\n\'http://www.xuexila.com/shenghuo/\',\n\'http://www.xuexila.com/naoli/\',\n\'http://www.xuexila.com/zhishi/\',\n]\n\nurl2_list = [\n\'http://www.xuexila.com/diannao/\',\n]\nurl3_list = [\n\'http://www.xuexila.com/tiyu/\',\n]\nbread_dict = {\n    u\'生活小常识\':[u\'生活技巧\'],\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n    \'itag\':\'2\',\n    \'headers\':{\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url in url_list:\n            bread = [u\'生活技巧\',]\n            self.crawl(url,headers = self.crawl_config[\'headers\'], save = {\'bread\':bread}, callback=self.index_page)\n        for url in url2_list:\n            bread = [u\'电脑数码\',]\n            self.crawl(url,headers = self.crawl_config[\'headers\'], save = {\'bread\':bread}, callback=self.index_page)\n        for url in url3_list:\n            bread = [u\'体育运动\',]\n            self.crawl(url,headers = self.crawl_config[\'headers\'], save = {\'bread\':bread}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.box > div\').items():\n            if u\'最强大脑\' in each.find(\'b > a\').text() or u\'一站到底\' in each.find(\'b > a\').text():\n                continue\n            self.crawl(each.find(\'.i_more > a\').attr.href, save = response.save,  headers = self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.r_list a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href, save = _dict,headers = self.crawl_config[\'headers\'], callback=self.detail_page)\n            \n        for each in response.doc(\'.box > div\').items():\n            self.crawl(each.find(\'.i_more > a\').attr.href, save = response.save,  headers = self.crawl_config[\'headers\'], callback=self.list1_page)\n        \n        #翻页\n        for each in response.doc(\'div > li > a\').items():\n            self.crawl(each.attr.href, save = response.save, headers = self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.r_list a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text().strip()\n            _dict[\'bread\'] = response.save[\'bread\']\n            self.crawl(each.attr.href, save = _dict,headers = self.crawl_config[\'headers\'], callback=self.detail_page)\n\n        #翻页\n        for each in response.doc(\'div > li > a\').items():\n            self.crawl(each.attr.href, save = response.save, headers = self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        abstract = {}\n        #abstract[\'steps\'] = []\n        #abstract[\'img\'] = \'\'\n        #abstract[\'title\'] = \'\'\n        methods = []\n        #_dict = {}\n        #_dict[\'steps\'] = []\n        #_dict[\'title\'] = u\'方法/步骤\'\n        flag_abstract = True\n        flag_method = False\n        flag_first = True\n        steps = []\n        _list = []\n        img = \'\'\n        desc_img = \'\'\n        step_title = \'\'\n        substeps = []\n        pattern = re.compile(ur\'[一二三四五六七八九十][、].*\')\n        #print pattern.match(str).group(0)\n        #print response.doc(\'.content div.left.d > div\').eq(-2).html()\n        \n        for each in response.doc(\'#contentText\').children().items():\n            #print dir(each)\n            #print each.outerHtml()\n            #print each.html()\n            if u\'高三网小编推荐你\' in each.text() or \'<hr />\'==each.outerHtml().strip() or u\'猜你喜欢\' in each.text() or \'BAIDU_CLB\' in each.text() or u\'点击进入\' in each.text() or u\'相关文章：\' in each.text() or u\'的人还看了：\' in each.text():\n                break \n            if each.text() == \'None\' or not each.html():\n                continue\n             \n            if u\'点击查看\' in each.text() or u\'查看更多\' in each.text() or u\'相关链接\' in each.text():\n                continue  \n            if pattern.match(each.text().strip()) or each.html().strip().startswith(\'<strong>\') or each.text().strip().startswith(u\'【\') or \'<h\' in each.outerHtml():\n                flag_abstract = False\n                flag_method = True\n            if \'<img\' in each.html():\n                #print each.html()\n                if desc_img == \'\':\n                    desc_img = each.find(\'img\').attr.src\n                    #print desc_img\n                img = each.find(\'img\').attr.src\n                each.remove(\'img\')\n            if not each.text():\n                continue\n            if flag_abstract:\n                if \'<table\' in each.outerHtml():\n                    steps.append(each.outerHtml().replace(u\'高三网\',\'\'))\n                else:\n                    steps.append(each.text().replace(u\'高三网\',\'\'))\n            else:\n                if  pattern.match(each.text().strip()) or each.html().strip().startswith(\'<strong>\') or each.text().strip().startswith(u\'【\') or \'<h\' in each.outerHtml():\n                    #print each.strip()[1]\n                    if not flag_first:\n                        _list.append({\n                            \'img\': img,\n                            \'title\': step_title,\n                            \'substeps\': substeps,\n                        })\n                        img = \'\'\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        substeps = []\n                    if flag_first:\n                        step_title = \'<strong>\'+each.text()+\'</strong>\'\n                        flag_first = False\n                else:\n                    substeps.append(each.text().replace(\'\\n\',\'\').replace(u\'高三网\',\'\'))    \n        _list.append({\n            \'img\': img,\n            \'title\': step_title,\n            \'substeps\': substeps,\n        })\n        if desc_img == \'\':\n            desc_img = \'http://img.gsxservice.com/zhanqun/jingyan%.2d.jpg\'%(random.randrange(1, 20))\n        if flag_method:\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list})\n            abstract = {\n                \'title\': \'\',\n                \'steps\': steps,\n                \'img\': desc_img,\n            }\n        else:\n            _list1 = []\n            for v in steps[1:]:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': v,\n                    \'substeps\': \'\',\n                })\n            if len(steps) == 1:\n                _list1.append({\n                    \'img\': \'\',\n                    \'title\': steps[0],\n                    \'substeps\': \'\',\n                })\n            methods.append({\'title\': u\'方法/步骤\', \'steps\': _list1})\n            if len(steps) == 0:\n                steps = [\'\']\n            abstract = {\n                \'title\': \'\',\n                \'steps\': [steps[0]],\n                \'img\': desc_img,\n            }\n        #_dict1 = {}\n        #_dict1[\'substeps\'] = []\n        #_dict1[\'img\'] = \'\'\n        \n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"methods\": methods,\n            \"abstract\": abstract,  \n            \"date\": response.doc(\'.read_people_time\').text().split(u\'：\')[-1],\n            \"bread\": response.save.get(\'bread\'),\n            \"source\": u\"学习啦\",\n            \"class\": 36,\n            \"subject\": u\'经验\',\n            \"data_weight\": 0,\n        }',NULL,1.0000,3.0000,1472624398.4466),('jinrong_chinaacc','jinrong','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-18 17:20:52\n# Project: jinrong_chinaacc\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nurls = {\n    \'http://www.chinaacc.com/acca/hyxw/\': [\'ACCA\'],\n    \'http://www.chinaacc.com/zhongjijingjishi/zhengcetiaojian/\': [u\'经济师\'],\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url,bread in urls.iteritems():\n            self.crawl(url, save = {\'bread\': bread}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'li > .fl\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            if u\'中华会计\' in each.find(\'a\').text():\n                continue\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        for each in response.doc(\'.nr li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            if u\'中华会计\' in each.find(\'a\').text():\n                continue\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'divpagestrcjzc a\').items():\n        #    self.crawl(each.attr.href, save = response.save, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        date = \'\'\n        source = u\'中华会计网校\'\n        if response.doc(\'.mark\'):\n            date = response.doc(\'.mark\').text().strip().split()[0]\n            source = response.doc(\'.mark\').text().strip().split(u\'来源：\')[1].split()[0]\n        \n        cover = \'\' \n        pattern = re.compile(r\'src=\".*?\"\')\n        _list = []\n        for each in response.doc(\'#fontzoom\').children().items():\n            if not each.html():\n                continue\n            #print each.html()\n            if \'<script>\' in each.html() or u\'中华会计网校\' in each.text():\n                continue\n            if u\'我要纠错\' in each.text() or u\'责任编辑\' in each.text() or u\'编辑推荐\' in each.text() or u\'点击阅读\' in each.text():\n                break\n            if \'<img\' in each.html():\n                if cover == \'\':\n                    cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                _list.append(\'<p>\'+ each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        \n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"cover\": cover,\n            \"content\": content,\n            \"source\": source,\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"金融\",\n            \"date\": date,\n\n        }',NULL,1.0000,3.0000,1471334568.9845),('jinrong_xinlang','jinrong','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-18 15:32:42\n# Project: jinrong_xinlang\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nurls = {\n    \'http://roll.finance.sina.com.cn/finance/zq1/index_\': [u\'证券从业\'],\n    \'http://roll.finance.sina.com.cn/finance/yh/index_\': [u\'银行从业\'],\n    \'http://roll.finance.sina.com.cn/finance/jj4/index_\': [u\'基金从业\'],\n}\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.2\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url,bread in urls.iteritems():\n            for i in range(1, 2):\n                self.crawl(url + str(i) +\'.shtml\', save = {\'bread\': bread}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.list_009 > li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        date = \'\'\n        source = u\'新浪财经\'\n        if response.doc(\'#pub_date\'):\n            date = response.doc(\'#pub_date\').text().strip().split()[0].split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            source = response.doc(\'#media_name\').text().strip()\n        elif response.doc(\'.time-source\'):\n            date = response.doc(\'.time-source\').text().strip().split()[0].split(u\'日\')[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\')\n            source = response.doc(\'.time-source\').text().strip().split()[1]\n        elif response.doc(\'.articalTitle > .SG_txtc\'):\n            date = response.doc(\'.articalTitle > .SG_txtc\').text().strip().split()[0].replace(\'(\',\'\')\n        cover = \'\' \n        pattern = re.compile(r\'src=\".*?\"\')\n        _list = []\n        flag_first = False\n        for each in response.doc(\'.article_16\').children().items():\n            if not each.html():\n                continue\n            #print each.html()\n            flag_first = True\n            if \'<script>\' in each.html() or u\'新浪财经\' in each.text() or \'finance_app_zqtg\' in each.html():\n                continue\n            if u\'新浪声明\' in each.text() or u\'新浪财经股吧\' in each.text():\n                break\n            if \'<img\' in each.html():\n                if cover == \'\':\n                    cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                _list.append(\'<p>\'+ each.html()+\'</p>\')\n            elif each.text().strip() != \'\':\n                _list.append(\'<p>\'+each.text()+\'</p>\')\n        if not flag_first:\n            for each in response.doc(\'#artibody\').children().items():\n                if not each.html():\n                    continue\n                flag_first = True\n                #print each.html()\n                if \'<script>\' in each.html() or u\'新浪财经\' in each.text() or \'finance_app_zqtg\' in each.html():\n                    continue\n                if u\'新浪声明\' in each.text() or u\'新浪财经股吧\' in each.text():\n                    break\n                if \'<img\' in each.html():\n                    if cover == \'\':\n                        cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                    _list.append(\'<p>\'+ each.html()+\'</p>\')\n                elif each.text().strip() != \'\':\n                    _list.append(\'<p>\'+each.text()+\'</p>\')\n        if not flag_first:\n            for each in response.doc(\'.newfont_family\').children().items():\n                if not each.html():\n                    continue\n                #print each.html()\n                if \'<script>\' in each.html() or u\'新浪财经\' in each.text() or \'finance_app_zqtg\' in each.html():\n                    continue\n                if u\'新浪声明\' in each.text() or u\'新浪财经股吧\' in each.text():\n                    break\n                if \'<img\' in each.html():\n                    if cover == \'\':\n                        cover = pattern.search(each.html()).group(0).replace(\'src=\"\',\'\').replace(\'\"\',\'\')\n                    _list.append(\'<p>\'+ each.html()+\'</p>\')\n                elif each.text().strip() != \'\':\n                    _list.append(\'<p>\'+each.text()+\'</p>\')\n        content = \'\'.join([v for v in _list if v])\n        #print content\n        return {\n            \"url\": response.url,\n            \"title\": response.save.get(\'title\'),\n            \"bread\": response.save.get(\"bread\"),\n            \"cover\": cover,\n            \"content\": content,\n            \"source\": source,\n            \"data_weight\": 0,\n            \"class\": 33,\n            \"subject\": u\"金融\",\n            \"date\": date,\n\n        }',NULL,1.0000,3.0000,1471334586.9698),('jita_10_youku_base',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-19 14:57:23\n# Project: jita_10_youku_base\n\nfrom pyspider.libs.base_handler import *\nimport time\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.soku.com/search_video/q_%E5%90%89%E4%BB%96_limitdate_0?site=14&_lg=10&orderby=3\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.sk_pager a\').items():\n            url = each.attr.href\n            self.crawl(url, callback=self.index_page)\n        for each in response.doc(\'.v\').items():\n            cover = each.find(\'img\').attr.src\n           # print cover\n            url = each.find(\'.v-meta-title > a\').attr.href\n            title = each.find(\'.v-meta-title > a\').text()\n            self.crawl(url, save={\'title\': title, \'cover\': cover},  callback=self.detail_page)\n\n   # @config(priority=2)\n    @config(age=1*1)\n    def detail_page(self, response):\n        try:\n            url = response.url\n        #http://v.youku.com/v_show/id_XMTUxNzMxODAyMA==.html?from=s1.8-1-1.2\n            id = url.split(\'id_\')[1].split(\'==\')[0]\n            video_url_element = \'http://player.youku.com/embed/%s\' %(id)\n            video_url = []\n            video_url.append(video_url_element)\n            title = response.save[\'title\']\n            cover = response.save[\'cover\']\n           # title = response.doc(\'.base_info > .title\').text()\n            subject = u\'吉他\'\n            source = \'youku.com\'\n            publish_time = time.strftime(\'%Y-%m-%d\',time.localtime(time.time()))\n        except:\n            return None\n        if len(video_url) == 0:\n            return None\n        return {\n            \"url\": url,\n            \"video_url\":video_url,\n            \"title\": title,\n            \"subject\": subject,\n            \"source\": source,\n            \"publish_time\": publish_time,\n            \"cover\": cover,\n            \"class\": 10,\n            \"data_weight\":0,\n        }\n',NULL,5.0000,3.0000,1468374380.6862),('jzgc_zhengbao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-30 18:17:48\n# Project: jzgc_zhengbao\n\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ntype_dict = {\n            \'chuji\':[u\'财会经济\',u\'初级会计师\'],\n            \'zhongji\':[u\'财会经济\',u\'中级会计师\'],\n            \'shiwushi\':[u\'财会经济\',u\'注册税务师\'],\n            \'zhukuai\':[u\'财会经济\',u\'注册会计师\'],\n            \'gaoji\':[u\'财会经济\',u\'高级会计师\'],\n            \'congye\':[u\'财会经济\',u\'会计从业\'],\n            u\'注册会计师\':[u\'财会经济\',u\'注册会计师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'初级会计职称\':[u\'财会经济\',u\'初级会计师\'],\n            u\'中级会计职称\':[u\'财会经济\',u\'中级会计师\'],\n            u\'税务师\':[u\'财会经济\',u\'注册税务师\'],\n            u\'统计师\':[u\'财会经济\',u\'统计师\'],\n            u\'审计师\':[u\'财会经济\',u\'审计师\'],\n            u\'高级经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'高级会计\':[u\'财会经济\',u\'高级会计师\'],\n            u\'理财规划师\':[u\'财会经济\',u\'理财规划师\'],\n\n\n            u\'英语四级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'英语六级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'雅思\':[u\'外语考试\',u\'雅思\'],\n            u\'托福\':[u\'外语考试\',u\'托福\'],\n            u\'职称英语\':[u\'外语考试\',u\'职称英语\'],\n            u\'商务英语\':[u\'外语考试\',u\'商务英语\'],\n            u\'公共英语\':[u\'外语考试\',u\'公共英语\'],\n            u\'日语\':[u\'外语考试\',u\'日语\'],\n            u\'GRE考试\':[u\'外语考试\',u\'GRE考试\'],\n            u\'专四专八\':[u\'外语考试\',u\'专四专八\'],\n            u\'口译笔译\':[u\'外语考试\',u\'口译笔译\'],\n\n            u\'一级建造师\':[u\'建筑工程\',u\'一级建造师\'],\n            u\'二级建造师\':[u\'建筑工程\',u\'二级建造师\'],\n            u\'咨询工程师\':[u\'建筑工程\',u\'咨询工程师\'],\n            u\'造价工程师\':[u\'建筑工程\',\'造价工程师\'],\n            u\'结构工程师\':[u\'建筑工程\',\'结构工程师\'],\n            u\'物业管理\':[u\'建筑工程\',u\'物业管理师\'],\n            u\'城市规划\':[u\'建筑工程\',u\'城市规划师\'],\n            u\'给排水工程\':[u\'建筑工程\',u\'给排水工程\'],\n            u\'电气工程\':[u\'建筑工程\',u\'电气工程师\'],\n            u\'公路监理师\':[u\'建筑工程\',u\'公路监理师\'],\n            u\'消防工程师\':[u\'建筑工程\',u\'消防工程师\'],\n            u\'消防\':[u\'建筑工程\',u\'消防工程师\'],\n\n            u\'物流师\':[u\'职业资格\',u\'物流师\'],\n            u\'人力资源\':[u\'职业资格\',u\'人力资源\'],\n            u\'心理咨询师\':[u\'职业资格\',u\'心理咨询师\'],\n            u\'公共营养师\':[u\'职业资格\',u\'公共营养师\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'证券从业资格\':[u\'职业资格\',u\'证券经纪人\'],\n            u\'电子商务师\':[u\'职业资格\',u\'电子商务\'],\n            u\'期货从业\':[u\'职业资格\',u\'期货从业\'],\n            u\'教师资格\':[u\'职业资格\',u\'教师资格\'],\n            u\'管理咨询师\':[u\'职业资格\',u\'管理咨询师\'],\n            u\'导游证\':[u\'职业资格\',u\'导游证\'],\n            \n            u\'英语\':[u\'学历教育\',u\'考研\'],\n            u\'数学\':[u\'学历教育\',u\'考研\'],\n            u\'政治\':[u\'学历教育\',u\'考研\'],\n            u\'专业课\':[u\'学历教育\',u\'考研\'],\n\n            u\'执业医师\':[u\'医学卫生\',u\'执业医师\'],\n            u\'执业药师\':[u\'医学卫生\',u\'执业药师\'],\n            u\'临床医师\':[u\'医学卫生\',u\'临床执业\'],\n            u\'中医医师\':[u\'医学卫生\',u\'中医执业\'],\n            u\'中西医医师\':[u\'医学卫生\',u\'中西医执业\'],\n            u\'中医助理\':[u\'医学卫生\',u\'中医助理\'],\n            u\'中西医助理\':[u\'医学卫生\',u\'中西医助理\'],\n            u\'主治\':[u\'医学卫生\',u\'主治\'],\n            u\'检验\':[u\'医学卫生\',u\'检验\'],\n            u\'执业护士资格\':[u\'医学卫生\',u\'执业护士\'],\n\n\n            u\'成人高考\':[u\'学历教育\',u\'成人高考\'],\n            u\'自学考试\':[u\'学历教育\',u\'自考\'],\n            u\'MBA考试\':[u\'学历教育\',u\'MBA\'],\n            u\'法律硕士\':[u\'学历教育\',u\'法律硕士\'],\n            u\'专升本\':[u\'学历教育\',u\'专升本\'],\n            #u\'\':[u\'学历教育\',u\'工程硕士\'],\n            u\'MPA考试\':[u\'学历教育\',u\'公共硕士\'],\n            #u\'\':[u\'学历教育\',u\'考研\'],\n}\n\n\nadd_url =[\n    \'zhenti/\',\n    \'moniti/\',\n    \'ziliao/\',\n    \'jingyan/\',\n]\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.jianshe99.com/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.category > a\').items():\n            if each.text() == \'其他\' or each.text() == \'消防\':\n                continue\n            _dict = {}\n            if each.text() in type_dict.keys():\n                _dict[\'bread\'] = type_dict[each.text()]\n            else:\n                _dict[\'bread\'] = [\'建筑工程\',each.text()]\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for val in add_url:\n            self.crawl(response.url + val, save = response.save, callback=self.list_page1)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page1(self, response):      \n        for each in response.doc(\'.mleft li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.fl > a\').text()\n            if \'报名\' in _dict[\'title\'] or \'名师\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'.fr\').text().replace(\'[\',\'\').replace(\']\',\'\')\n            self.crawl(each.find(\'.fl > a\').attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页 \n        for each in response.doc(\'divpagestr2016 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n        \n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'#fontzoom\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'全网首发\' in each1.text():\n                    continue\n                if u\'估分更准确\' in each1.text():\n                    continue\n                if u\'独家\' in each1.text():\n                    continue\n                if u\'点击查看\' in each1.text():\n                    continue\n                if u\'点击参与\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())\n            for each1 in each.find(\'div\').items():\n                if u\'点击查看\' in each1.text():\n                    continue\n                if u\'点击参与\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'建设工程教育网\',\'\').replace(\'【 】\',\'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'), \n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'建设工程教育网\',\'\').replace(\'None\',\'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'建设工程教育网\',\'\').replace(\'None\',\'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'建设工程教育网\',u\'\').replace(\'建设网校\', \'\').replace(\'None\',\'\') + str(response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(u\'建设工程教育网\',\'\').replace(\'建设网校\', \'\').replace(\'None\',\'\'),\n                \"tdk_title\":response.doc(\'head > title\').eq(0).text().replace(u\'建设工程教育网\',\'\') + u\' 跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"建设工程教育网\",\n            }\n',NULL,1.0000,3.0000,1472624389.7836),('kaoyan_chinakaoyan_inc','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-11 19:41:56\n# Project: kaoyan_chinakaoyan\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        page_list = {\'11\': [u\'政策新闻\'],\n                     \'12\': [u\'报考指南\'],\n                     \'14\': [u\'招生信息\'],\n                     \'13\': [u\'择选院校\'],\n                     \'91\': [u\'专业介绍\'],\n                     \'22\': [u\'考研分数线\'],\n                     \'24\': [u\'调剂指南\'],\n                     \'26\': [u\'考研英语\'],\n                     \'25\': [u\'考研政治\'],\n                     \'27\': [u\'考研数学\'],\n                     \'32\': [u\'计算机\',u\'专业课\'],\n                     \'33\': [u\'教育学\',u\'专业课\'],\n                     \'34\': [u\'心理学\',u\'专业课\'],\n                     \'35\': [u\'历史学\',u\'专业课\'],\n                     \'47\': [u\'其他\',u\'专业课\'],\n                     \'65\': [u\'大纲解析\'],\n                     \'64\': [u\'大纲解析\'],\n                     \'63\': [u\'大纲解析\'],\n                     \'62\': [u\'大纲解析\'],\n                     \'61\': [u\'大纲解析\'],\n                     \'49\': [u\'成绩查询\'],\n                     \'50\': [u\'研招动态\'],\n                     \'9\': [u\'研招动态\'],\n                     \'31\': [u\'研招动态\'],\n                     \'10\': [u\'研招动态\'],\n                     }\n        for k,v in page_list.iteritems():\n            self.crawl(\'http://www.chinakaoyan.com/info/list/ClassID/%s.shtml\'%k, save={\'bread\': v}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.uc_ulbox a\').items():\n            self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.detail_page)\n\n        #for each in response.doc(\'.dajax > a\').items():\n        #    self.crawl(each.attr.href,  save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        for each in response.doc(\'.arcont > .cont\').items():\n            content = each.html().replace(\'\\t\',\'\').replace(\'\\r\',\'\').replace(\'\\n\',\'\').strip()\n        if not content:\n            return None\n        dates = response.doc(\'.time\').text().split()\n        for d in dates:\n            if d[:3] == \'201\':\n                break\n        #bread = set(response.doc(\'.change_con a\').text().split()[1:-1])\n        #bread.add(response.save[\'bread\'])\n        try:\n            source = response.doc(\'.time\').remove(\'a\').text().split()[0].split(u\'：\')[-1]\n        except:\n            source = u\'中国考研网\'\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"date\": d,\n            \"subject\": u\'考研\',\n            \"source\": source,\n            \"content\": content,\n            \"bread\": response.save[\'bread\'],\n            \"cover\": \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471332804.5242),('kaoyan_chsi','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-31 17:01:19\n# Project: gaokao_chsi_com\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    url_content = {\n        \'kyzx/kydt/\': u\'研招动态\',\n        \'kyzx/jyxd/\': u\'备考经验\',\n        \'kyzx/politics/\': u\'考研政治\',\n        \'kyzx/en/\': u\'考研英语\',\n        \'kyzx/math/\': u\'考研数学\',\n        \'kyzx/zyk/\': u\'专业课\',\n        \'kyzx/yxzc/\': u\'政策新闻\',\n        \'kyzx/zcdh/\': u\'政策新闻\',\n    }\n        \n    @every(minutes=1 * 60)\n    def on_start(self):\n        url_prefix = \'http://yz.chsi.com.cn/\'\n        for key in self.url_content:\n            url = url_prefix + key\n            self.crawl(url, save={\'key\': key}, callback=self.index_page)\n\n    @config(age=24 * 60)\n    def index_page(self, response):\n        key = response.save[\'key\']\n        for each in response.doc(\'.news_list li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'.spanTime\').text()\n            self.crawl(url, save = {\'key\': key, \'date\': date}, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'form a\').items():\n            \n            self.crawl(each.attr.href, save = {\'key\': key}, callback=self.index_page)\n        \'\'\'\n        \n\n    @config(priority=2)\n    def detail_page(self, response):\n        key = response.save[\'key\']\n        url = response.url\n        title = response.doc(\'title\').text()\n        dt = response.save[\'date\']\n        \'\'\'\n        content = \'\'\n        content_list = []\n        for item in response.doc(\'.left > * > p\'):\n            content_list.append(pyquery.PyQuery(item).html())\n        content = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        \'\'\'\n        content = response.doc(\'#article_dnull\').html()\n        if not content:\n            return None\n        bread = self.url_content[key]\n        subject = u\'考研\'\n        try:\n            source = response.doc(\'.sate_info span\').text().split(u\'来源：\')[-1]\n        except:\n            source = u\'研究生招生信息网\'\n        return {\n            \"url\": url,\n            \"title\": title,\n            \"date\": dt,\n            \"content\": content,\n            \"bread\": [bread,],\n            \"subject\": subject,\n            \"source\": source,\n            \"class\": 46,\n            \"data_weight\": 0,\n            \"cover\": \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n        }\n',NULL,1.0000,3.0000,1471332811.1229),('kaoyan_cnky_inc','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-11 14:19:24\n# Project: kaoyan_cnky_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    headers = {}\n    \n    page_dict = {\n\n        \'http://www.cnky.net/fuxi/zhengzhi/index.shtml\':[u\'公共课\',u\'政治\'],\n        \'http://www.cnky.net/fuxi/yingyu/index.shtml\':[u\'公共课\',u\'英语\'],\n        \'http://www.cnky.net/fuxi/shuxue/index.shtml\':[u\'公共课\',u\'数学\'],\n        \'http://www.cnky.net/fuxi/kaoyanzhuanyeke/index.shtml\':[u\'专业课\',u\'其他\'],\n        \'http://www.cnky.net/fuxi/zhuanyeke/index.shtml\':[u\'备考准备\',u\'备考经验\'],\n        \'http://www.cnky.net/fuxi/mingshi/index.shtml\':[u\'备考准备\',u\'备考经验\']\n    }\n    \n    @every(minutes=1*60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'div#listcontent li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'h2 > a\').text()\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'年\',\'-\').replace(\'月\',\'-\').replace(\'日\',\'\')\n            url = each.find(\'h2 > a\').attr.href\n            self.crawl(url, save = _dict ,callback=self.detail_page)\n       \n            \n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save\n       content_list = []\n       for each in response.doc(\'div.g-pct p\').items():\n            info = each.html()\n            if info:\n                content_list.append(info)\n       if not content_list:\n            return\n       res_dict[\'content\'] = \'\'.join(\'<p>%s</p>\'%k for k in content_list if k and k.strip())\n       res_dict[\'source\'] = \'cnky.net\'\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'subject\'] = u\'考研\'\n       res_dict[\'url\'] = response.url\n       res_dict[\'class\'] = 46\n       return res_dict\n',NULL,1.0000,3.0000,1471332836.1628),('kaoyan_eol_inc','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-11 19:54:16\n# Project: kaoyan_kaoyan\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        _dict = {\n            \'zhinan\': u\'备考经验\',           \n            \'zhengzhi\': u\'考研政治\',\n            \'yingyu\': u\'考研英语\',\n            \'shuxue\': u\'考研数学\',\n            \'zhuan_ye_ke\': u\'专业课\',\n        }\n        #for i in range(2):\n        for k, v in _dict.iteritems():\n            self.crawl(\'http://kaoyan.eol.cn/fu_xi/%s/\'%(k),  save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://kaoyan.eol.cn/nnews/\', save={\'bread\': u\'研招动态\'}, callback=self.index_page)\n        self.crawl(\'http://kaoyan.eol.cn/bao_kao/zheng_ce_bian_hua/\', save={\'bread\': u\'政策新闻\'}, callback=self.index_page)\n        self.crawl(\'http://kaoyan.eol.cn/bao_kao/re_men/\', save={\'bread\': u\'高校动态\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.page_left li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'span\').text()\n            self.crawl(url,save={\'bread\': bread, \'date\': date}, callback=self.detail_page)\n        #for each in response.doc(\'.tPage > a\').items():\n        #    self.crawl(each.attr.href,save={\'bread\': bread}, callback=self.detail_page)\n\n    def strip(self, _str):\n        if not _str:\n            return _str\n        return _str.replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\').strip(\' \').replace(u\'考研帮\',\'\').replace(u\'帮学堂刷视频\',\'\').replace(u\'帮学堂\',\'\')\n    \n    @config(priority=2)\n    def detail_page(self, response):\n        bread = response.save[\'bread\']\n        date = response.save[\'date\']\n        if not date:\n            date = response.doc(\'.articleInfo\').text()[:10]\n        content = self.strip(response.doc(\'.TRS_Editor\').remove(\'img\').remove(\'a\').html())\n        if not content:\n            return None\n        try:\n            source = response.doc(\'.page_time\').text().split()[1].strip()\n        except:\n            source = \'eol\'\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.page_title\').text(),\n            \"date\": date,\n            \"bread\": [bread,],\n            \"content\": content,\n            \"subject\": u\'考研\',\n            \"source\": source,\n            \"cover\": \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471332814.6296),('kaoyan_kaoshidian','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-31 09:48:14\n# Project: kaoyan_kaoshidian\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    dic = {\n        \'zixun/yxxx/\': u\'院校资讯\',\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in self.dic.iteritems():\n            self.crawl(\'http://bbs.kaoshidian.com/%s\'%k, save={\'bread\': v}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.newsList li\').items():\n            bread = response.save[\'bread\']\n            url = each.find(\'a\').attr.href.replace(\'zixun/yxxx/\',\'\')\n            img = each.find(\'img\').attr.src.replace(\'zixun/yxxx/\',\'\')\n            if img.startswith(\'data\'):\n                img = \'http://bbs.kaoshidian.com/\' + img\n            self.crawl(url, save={\'bread\': bread, \'cover\': img}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        sources = response.doc(\'.article-resource\').text().split()\n        try:\n            source = sources[0].split(u\'：\')[-1]\n        except:\n            source = u\'考试点\'\n        try:\n            date = sources[1].split(u\'：\')[-1].split(\'-\')\n            date = \'%s-%.2d-%.2d\'%(date[0], int(date[1]), int(date[2]))\n        except:\n            date = \'2016-04-01\'\n        content_list = []\n        for each in response.doc(\'.articleCon p\').items():\n            if each.text().startswith(u\'【考试点编辑\'):\n                break\n            try:\n                tmp = each.remove(\'a\').html().replace(u\'考试点\', \'\')\n                content_list.append((tmp))\n            except:\n                continue\n        if not content_list:\n            return None\n        try:\n            cover = response.save[\'cover\']\n        except:\n            cover = \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(1, 20))\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.articleTitle\').text(),\n            \"source\": source,\n            \"date\": date,\n            \"subject\": u\'考研\',\n            \"class\": 46,\n            \"data_weight\": 0,\n            \"bread\": [response.save[\'bread\'],],\n            \"content\": \'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n            \"cover\": cover,\n        }\n',NULL,1.0000,3.0000,1471332838.6387),('kaoyan_kaoyan_inc','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-11 19:54:16\n# Project: kaoyan_kaoyan\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        _dict = {\n            \'baokao/zhinan\': u\'报考指南\',\n            \'baokao/jingyan\': u\'报考经验\',\n            \'xinwen/zhengce/\': u\'政策新闻\',\n            \'zhaosheng\': u\'招生信息\',\n            \'baokao/zexiao/\': u\'择选院校\',\n            \'beikao/jingyan/\': u\'备考经验\',\n            \'fushi/jingyan/\': u\'复试攻略\',\n            \'yingyu/zhenti/\': u\'考研英语\',\n            \'zhengzhi/zhenti/\': u\'考研政治\',\n            \'zhengzhi/dagang/\': u\'考研政治\',\n            \'zhengzhi/jingyan/\': u\'考研政治\',\n            \'yingyu/dagang/\': u\'考研英语\',\n            \'yingyu/jingyan/\': u\'考研英语\',\n            \'shuxue/jingyan/\': u\'考研数学\',\n            \'shuxue/dagang/\': u\'考研数学\',\n            \'shuxue/zhenti/\': u\'考研数学\',\n            \'zhuanyeke/zhenti/\': u\'专业课\',\n            \'zhuanyeke/jingyan/\': u\'专业课\',\n            \'zhuanyeke/dagang/\': u\'专业课\',\n        }\n        for k, v in _dict.iteritems():\n            self.crawl(\'http://www.kaoyan.com/%s/\'%k, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://tiaoji.kaoyan.com/xinxi/\', save={\'bread\': u\'调剂指南\'}, callback=self.index_page)\n        self.crawl(\'http://mba.kaoyan.com/beikao/\', save={\'bread\': u\'MBA\'}, callback=self.index_page)\n        self.crawl(\'http://mba.kaoyan.com/zixun/\', save={\'bread\': u\'MBA\'}, callback=self.index_page)\n        self.crawl(\'http://mba.kaoyan.com/baokao/\', save={\'bread\': u\'MBA\'}, callback=self.index_page)\n        self.crawl(\'http://mpacc.kaoyan.com/tiaoji/\', save={\'bread\': u\'MPAcc\'}, callback=self.index_page)\n        self.crawl(\'http://mpacc.kaoyan.com/baokao/\', save={\'bread\': u\'MPAcc\'}, callback=self.index_page)\n        self.crawl(\'http://mpacc.kaoyan.com/beikao/\', save={\'bread\': u\'MPAcc\'}, callback=self.index_page)\n        self.crawl(\'http://mpacc.kaoyan.com/fushi/\', save={\'bread\': u\'MPAcc\'}, callback=self.index_page)\n        self.crawl(\'http://www.kaoyan.com/jianzhang/\', save={\'bread\': u\'招生简章\'}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.areaZslist > li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'.fr\').text()\n            self.crawl(url,save={\'bread\': bread, \'date\': date}, callback=self.detail_page)\n        #for each in response.doc(\'.tPage > a\').items():\n        #    self.crawl(each.attr.href,save={\'bread\': bread}, callback=self.index_page)\n\n    def strip(self, _str):\n        if not _str:\n            return _str\n        return _str.replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\').strip(\' \').replace(u\'考研帮\',\'\').replace(u\'帮学堂刷视频\',\'\').replace(u\'帮学堂\',\'\')\n\n    @config(priority=2)\n    def detail_page(self, response):\n        bread = response.save[\'bread\']\n        date = response.save[\'date\']\n        if not date:\n            date = response.doc(\'.articleInfo\').text()[:10]\n        content = self.strip(response.doc(\'.articleCon\').html())\n        if not content:\n            return None\n        title = response.doc(\'.articleTitle\').text()\n        if u\'汇总\' in title:\n            return None\n        try:\n            source = response.doc(\'.ml30\').text().split()[0].strip()\n        except:\n            source = u\'考研帮\'\n        if source == u\'本站原创\':\n            source = u\'考研帮\'\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"date\": date,\n            \"bread\": [bread,],\n            \"content\": content,\n            \"subject\": u\'考研\',\n            \"source\": source,\n            \"cover\": \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471332818.5245),('kaoyan_kuakao','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-11 19:54:16\n# Project: kaoyan_kaoyan\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        _dict = {\n            \'politics\': u\'考研政治\',\n            \'english\': u\'考研英语\',\n            \'maths\': u\'考研数学\',\n            \'zyk\': u\'专业课\',\n        }\n        #for i in range(2):\n        for k, v in _dict.iteritems():\n            self.crawl(\'http://www.kuakao.com/%s/\'%(k),  save={\'bread\': v}, callback=self.index_page)\n        \n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.examBar li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'span\').text()\n            self.crawl(url,save={\'bread\': bread, \'date\': date}, callback=self.detail_page)\n        #for each in response.doc(\'.tPage > a\').items():\n        #    self.crawl(each.attr.href,save={\'bread\': bread}, callback=self.detail_page)\n\n    def strip(self, _str):\n        if not _str:\n            return _str\n        return _str.replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\').strip(\' \').replace(u\'考研帮\',\'\').replace(u\'帮学堂刷视频\',\'\').replace(u\'帮学堂\',\'\')\n    \n    @config(priority=2)\n    def detail_page(self, response):\n        bread = response.save[\'bread\']\n        date = response.save[\'date\']\n        if not date:\n            date = response.doc(\'.articleInfo\').text()[:10]\n        content_list = []\n        for each in response.doc(\'.artTxt > p\').items():\n            content_list.append((each.text()))\n             \n        content_list = content_list[2:-4]\n        if not content_list:\n            return None\n        try:\n            source = response.doc(\'.green\').text().split(u\'：\')[-1].strip()\n        except:\n            source = u\'跨考网\'\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.artTit\').text(),\n            \"date\": date,\n            \"bread\": [bread,],\n            \"content\": \'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n            \"subject\": u\'考研\',\n            \"source\": source,\n            \"cover\": \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n            \"class\": 46,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471332822.5563),('kaoyan_yuloo','kaoyan','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-24 16:11:21\n# Project: kaoyan_yuloo\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    dic = {\n        \'political/zhidao/\': u\'考研政治\',\n        \'political/jyfd/\': u\'考研政治\',\n        #\'political/zhenti/\': u\'政治\',\n        \'english/zhidao/\': u\'考研英语\',\n        \'english/jyfd/\': u\'考研英语\',\n        #\'english/zhenti/\': u\'英语\',\n        \'math/zhidao/\': u\'考研数学\',\n        \'math/jyfd/\': u\'考研数学\',\n        #\'math/zhenti/\': u\'数学\',\n        \'zyss/\': u\'专业课\',\n        \'dagang/zhuanyeke/\': u\'专业课\',\n        \'kyjy\': u\'备考经验\',\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k, v in self.dic.iteritems():\n            self.crawl(\'http://www.yuloo.com/kaoyan/%s\'%k, save={\'key\': v}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.left .clearfix li\').items():\n            key = response.save[\'key\']\n            url = each.find(\'a\').attr.href\n            date = each.find(\'span\').text()\n            if date[:4] >= \'2015\':\n                self.crawl(url, save={\'key\': key, \'date\': date}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        bread = response.save[\'key\']\n        content = response.doc(\'.jiathis_streak\').html().replace(\'\\n\',\'\')\n        if not content:\n            return None\n        source = response.doc(\'.top_h2 > p\').text().split(u\'发布时间\')[0].split(\':\')[-1].lstrip().rstrip()\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h1\').text(),\n            \"date\": response.save[\'date\'],\n            \"bread\": [bread,],\n            \"source\": source if source else u\'育路考研网\',\n            \"subject\": u\'考研\',\n            \"class\": 46,\n            \"data_weight\": 0,\n            \"content\": content,\n            \'cover\': \'http://file.gsxservice.com/zhanqun/static/images/list/%d.jpg\'%(randrange(20)),\n        }\n',NULL,1.0000,3.0000,1471332826.2625),('kaozhengtikt_qnr','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-28 09:43:57\n# Project: kaozhengtiku_qingnianrenwang\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ndalei = [\n    u\'建筑工程\',\n    u\'财会经济\',\n    u\'医学卫生\',\n    u\'外语考试\',\n    u\'职业资格\',\n    u\'学历教育\',\n    u\'计算机考试\'\n]\nxiaolei = {\n            u\'一级建造师\':u\'一级建造师\',\n            u\'二级建造师\':u\'二级建造师\',\n            u\'监理工程师\':u\'监理工程师\',\n            u\'咨询工程师\':u\'咨询工程师\',\n            u\'造价工程师\':u\'造价工程师\',\n            u\'结构工程师\':u\'结构工程师\',\n            u\'电气工程师\':u\'电气工程师\',\n            u\'物业管理师\':u\'物业管理师\',\n            u\'经济师\':u\'经济师\',\n            u\'设计师\':u\'设计师\',\n            u\'初级会计\':u\'初级会计师\',\n            u\'中级会计师\':u\'中级会计师\',\n            u\'注册会计师\':u\'注册会计师\',\n            u\'统计师\':u\'统计师\',\n            u\'审计师\':u\'审计师\',\n            u\'注册税务师\':u\'注册税务师\',\n            u\'执业药师\':u\'执业药师\',\n            u\'执业药师\':u\'执业药师\',\n            u\'执业护士\':u\'执业护士\',\n            u\'临床执业\':u\'临床执业\',\n            u\'中西医执业\':u\'中西医执业\',\n            u\'中医执业\':u\'中医执业\',\n            u\'主治\':u\'主治\',\n            u\'检验\':u\'检验\',\n            u\'英语四级\':u\'英语四六级\',\n            u\'英语六级\':u\'英语四六级\',\n            u\'雅思\':u\'雅思\',\n            u\'托福\':u\'托福\',\n            u\'GRE考试\':u\'GRE考试\',\n            u\'职称英语\':u\'职称英语\',\n            u\'公共英语\':u\'公共英语\',\n            u\'商务英语\':u\'商务英语\',\n            u\'日语\':u\'日语\',\n            u\'人力资源\':u\'人力资源\',\n            u\'心理咨询师\':u\'心理咨询师\',\n            u\'物流师\':u\'物流师\',\n            u\'公共营养师\':u\'公共营养师\',\n            u\'秘书资格\':u\'秘书资格\',\n            u\'证券经纪人\':u\'证券经纪人\',\n            u\'电子商务\':u\'电子商务\',\n            u\'国家司法\':u\'国家司法\',\n            u\'成人高考\':u\'成人高考\',\n            u\'自考\':u\'自考\',\n            u\'MBA\':u\'MBA\',\n            u\'法律硕士\':u\'法律硕士\',\n            u\'会计硕士\':u\'会计硕士\',\n            u\'工程硕士\':u\'工程硕士\',\n            u\'MPA\':u\'公共硕士\',\n            u\'考研\':u\'考研\',\n            u\'计算机等级\':u\'计算机等级\',\n            u\'软件水平\':u\'软件水平\',\n            u\'微软认证\':u\'微软认证\',\n            u\'Cisco认证\':u\'Cisco认证\',\n            u\'Oracle认证\':u\'Oracle认证\',\n            u\'职称计算机\':u\'职称计算机\',\n            u\'Java认证\':u\'Java认证\',\n            u\'华为认证\':u\'华为认证\',\n    }\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.qnr.cn/zhenti/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'table\').items():\n            for each1 in each.find(\'a\').items():\n                _dict = {}\n                _dict[\'bread\'] = [each.find(\'td\').eq(0).text().split(\'(\')[0].strip(),\'\']\n                _dict[\'bread\'][1] = each1.text().strip()\n                self.crawl(each1.attr.href, save = _dict, callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        _dict = {}\n        for each in response.doc(\'.Right_last_lst\').items():\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'.R_date\').text().replace(\'/\',\'-\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n            \n        for each in response.doc(\'.P_Con\').items():\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'.time\').text().replace(\'/\',\'-\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []\n        for each in response.doc(\'.mar10 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'#manadona > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n        if len(list) == 0 and response.doc(\'#manadona\'):     \n            list.append(response.doc(\'#manadona\').remove(\'a\').remove(\'p\').html())\n            \n        for each in response.doc(\'#xx20 > div > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n                       \n            \n        for each in response.doc(\'#xx23 > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.remove(\'a\').html())\n            \n        for each in response.doc(\'#xx27 > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.remove(\'a\').html())\n\n        for each in response.doc(\'.hao > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'.mini > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n        if len(list) == 0 and response.doc(\'.mini\'):     \n            list.append(response.doc(\'.mini\').remove(\'a\').remove(\'p\').html())\n                       \n        \n        for each in response.doc(\'#shtdxlnews_4 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n         \n        for each in response.doc(\'#tb42 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'#gtsadfas > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'.nali\').items():\n            list.append(response.doc(\'.nali\').remove(\'a\').remove(\'p\').html())\n            \n        content = \'\'.join(\'<p>%s<p/>\' % s for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            bread = []\n            for val in dalei:\n                if val[0:2] in response.save.get(\'bread\')[0]:\n                    bread.append(val)\n                    for k, v in xiaolei.iteritems():\n                        if k in response.save.get(\'bread\')[1]:\n                            bread.append(v)\n            if len(bread) < 2:\n                bread = response.save.get(\'bread\')\n            return {\n                \"url\": response.url,\n                \"title\": response.doc(\'#tb41 > span\').text(),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'青年人网讯\',\'\').replace(\'青年人\',\'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": bread, \n                \"date\": response.save.get(\'date\'),\n                \"tdk_description\":response.doc(\'meta\').eq(1).attr.content.replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"tdk_keywords\":response.doc(\'meta\').eq(0).attr.content.replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"tdk_title\":response.doc(\'title\').eq(0).text().replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"青年人网\",\n\n            }\n',NULL,1.0000,3.0000,1472624383.4057),('keyword_monitor',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-05 10:17:49\n# Project: baidu_keyword_2\n\nfrom pyspider.libs.base_handler import *\nfrom urllib import quote, unquote\nimport redis\nimport urlparse\nimport json\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nclass Handler(BaseHandler):\n\n    crawl_config = {\n        \'itag\':\'0.1\',\n        \"headers\": {\n        \'Host\': \'www.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.84 Safari/537.36\'\n        }\n    }\n\n    def __init__(self):\n        self.r = redis.StrictRedis(host=\'localhost\', port=6379, db=13,charset=\'utf－8\')\n        \n        self.words = {}\n        for line in open(\'/apps/home/worker/xuzhihao/keyword2\'):\n            (word, info) = line.strip(\'\\n\').split(\'\\t\')\n            data = json.loads(info)\n            #self.words[word] = {k: {\'title\': v, \'find\': 0} for k, v in data.iteritems()}\n            self.words[u\'%s\'%word] = {}\n            for k, v in data.iteritems():\n                self.words[u\'%s\'%word][k] = {\'title\': v, \'find\': 0}\n        \'\'\'\n        self.words = {u\'成都MPA培训\':\n                          {\'http://www.genshuixue.com/bj/st--878_1116.html\':\n                               {\'title\': u\'【成都MPA培训|成都MPA辅导机构|成都MPA培训班费用】-跟谁学成都站\',\n                                \'find\': 0\n                                }\n                          },\n                    }\n        \'\'\'\n\n    @every(minutes=24*60)\n    def on_start(self):\n        for word, word_info in self.words.iteritems():\n            self.crawl(\'https://www.baidu.com/s?wd=%s&rsv_spt=1&rsv_iqid=0xffd5385a0000af37&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=5&rsv_sug1=3&rsv_sug7=100\'%(word), save={\"word\": word, \"word_info\": word_info,},  callback=self.index_page)\n\n    def judge(self, data):\n        for k, v in data.iteritems():\n            if v[\'find\'] == 0:\n                return False\n        return True\n\n    @config(priority=2)\n    def index_page(self, response):\n        try:\n            word = response.save[\'word\']\n            word_info = response.save[\'word_info\']\n        except Exception, e:\n            return\n\n        for each in response.doc(\'#page a\').items():\n            if self.r.get(word):\n                #print \'redis in\'\n                return\n            if \'pn\' in each.attr.href:\n                _parse = urlparse.urlparse(each.attr.href)\n                keys = urlparse.parse_qs(_parse.query)\n                try:\n                    pn = int(keys[\'pn\'][0])\n                except Exception, e:\n                    pn = -1\n                r_key = \'%s_%d\'%(word, pn)\n                #print r_key, self.r.get(r_key)\n                if not self.r.get(r_key):\n                    self.r.set(r_key, 1)\n                    self.r.expire(r_key, 10*60*60)\n                    self.crawl(each.attr.href, save={\"pn\": pn, \"word\": word, \"word_info\": word_info}, callback=self.index_page)\n\n        for index, each in enumerate(response.doc(\'.result\').items()):\n            target_url = each.find(\'.f13 a\').text()\n            #print target_url\n            res ={\n                    \"pn\": None,\n                    \"index\": index,\n                    \"word\": word,\n                    \"word_info\": None,\n                    \"target_title\": each.text(),\n                    \"url\":None\n                }\n\n            if \'genshuixue\' in target_url:\n                try:\n                    pn = response.save[\'pn\']\n                except Exception, e:\n                    pn = 0\n                bd_title = each.find(\'.t\').text().replace(\' \',\'\')\n                if bd_title[-3:] == \'...\':\n                    bd_title = bd_title[:-3]\n                for k, v in word_info.iteritems():\n                    if u\'%s\'%bd_title in v[\'title\']:\n                        #print bd_title\n                        res[\'pn\'] = pn\n                        res[\'word_info\'] = {k: v}\n                        #print self.words\n                        self.words[word][k][\'find\'] = 1\n                        res[\'url\'] = k\n                        if self.judge(self.words[word]):\n                            #print word\n                            self.r.set(word, 1)\n                            self.r.expire(word, 60)\n                        return res\n',NULL,10.0000,10.0000,1467701059.6946),('keyword_monitor_accurate',NULL,'STOP','# -*- encoding: utf-8 -*-\n# Created on 2016-07-04 15:44:10\n# Project: keyword_monitor_accurate\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport urlparse\nimport time\nimport json\nimport MySQLdb\n\nclass ConnectionUtil(object):\n\n    def __init__(self,connection):\n        self.connection = connection\n\n    def cursor(self):\n        if self.connection:\n            self.cursor  = self.connection.cursor()\n            return self.cursor\n        else:\n            return  None\n\n    def __enter__(self):\n        return self.cursor()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n            #print \'ok\'\n            if self.connection:\n                self.connection.commit()\n            if self.cursor:\n                self.cursor.close()\n            if self.connection:\n                self.connection.close()\n            if exc_type is not None:\n                #print \'errror\'\n                print exc_val\n                #traceback.print_exc()\n                return True\n\ndef qs(url):\n    query = urlparse.urlparse(url).query\n    return dict([(k,v[0]) for k,v in urlparse.parse_qs(query).items()])\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.1\',\n        \"headers\": {\n        \'Host\': \'www.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n\n    index_dict = {}\n    \n    @every(minutes=3 * 24 * 60)\n    def on_start(self):\n        with open(\'/apps/home/rd/hexing/data/keyword\',\'r\') as f:\n            for line in f:\n                if line :\n                    line = line.strip()\n                    arr = line.split(\'\\t\')\n                    for index in range(10):\n                        self.crawl(\'https://www.baidu.com/s?wd=%s&pn=%s&rsv_spt=1&rsv_iqid=0xa0eaa7930001b982&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=1&rsv_sug2=0&inputT=995&rsv_sug4=995\'%(arr[0],index*10),save = {\'query\':arr[0],\'info\':json.loads(arr[1])},priority = 100 - index * 10,callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n      if response.save.get(\'query\') not in self.index_dict:\n            #time.sleep(1)\n            for index , each in enumerate(response.doc(\'.result\').items()):\n                _dict = {}\n                _dict[\'title\'] = each.find(\'h3 a\').text()\n                _dict[\'query\'] = response.save.get(\'query\')\n                _dict[\'url\'] = each.find(\'.f13 a\').text()\n                _dict[\'info\'] = response.save.get(\'info\')\n                page_dict = qs(response.url)\n                if \'pn\' not in page_dict:\n                    pn = 0\n                else:\n                    pn = page_dict[\'pn\']\n                for k,v in response.save.get(\'info\').items():\n                    if each.find(\'h3 a\').text().replace(\'...\',\'\').replace(\' \',\'\') in v:\n\n                        rank = index + 1 + int(pn) \n                        self.index_dict[response.save.get(\'query\')] = rank\n                        _dict[\'rank\'] = rank\n                        _dict[\'accurate_url\'] =  k\n                        return _dict\n               \n\n    @config(priority=2)\n    def detail_page(self, response):\n        #res_dict = response.save\n        #res_dict[\'index\'] = self.index_dict[response.save.get(\'query\')]\n        #return res_dict\n        pass',NULL,5.0000,5.0000,1469408683.4581),('keyword_monitor_fuzzy',NULL,'RUNNING','# -*- encoding: utf-8 -*-\n# Created on 2016-07-04 15:44:10\n# Project: keyword_monitor_m\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport urlparse\nimport time\nimport json\nfrom urllib import quote \n\n\ndef qs(url):\n    query = urlparse.urlparse(url).query\n    return dict([(k,v[0]) for k,v in urlparse.parse_qs(query).items()])\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'v0.1\',\n        \"headers\": {\n        \'Host\': \'www.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n\n    index_dict = {}\n    \n    @every(minutes=3*24*60)\n    def on_start(self):\n       with open(\'/apps/home/rd/hexing/data/keyword.txt\',\'r\') as f:\n            for line in f:\n                if line :\n                    line = line.strip()\n                    #arr = line.split(\'\\t\')\n                    for index in range(3):\n                        self.crawl(\'https://www.baidu.com/s?wd=%s&pn=%s&rsv_spt=1&rsv_iqid=0xa0eaa7930001b982&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=1&rsv_sug2=0&inputT=995&rsv_sug4=995\'%(line,index*10),save = {\'query\':line },priority = 100 - index * 10,callback=self.index_page)\n                      \n\n    @config(age=1*1)\n    def index_page(self, response):\n      if response.save.get(\'query\') not in self.index_dict:\n            for index , each in enumerate(response.doc(\'.result\').items()):\n                _dict = {}\n                _dict[\'title\'] = each.find(\'h3 a\').text()\n                _dict[\'query\'] = response.save.get(\'query\')\n                _dict[\'url\'] = each.find(\'.f13 a\').text()\n                page_dict = qs(response.url)\n                if \'pn\' not in page_dict:\n                    pn = 0\n                else:\n                    pn = page_dict[\'pn\']\n                if \'genshuixue\' in _dict[\'url\']:\n                    self.index_dict[response.save.get(\'query\')] = index + 1 + int(pn)      \n                    _dict[\'rank\'] = index + 1 +int(pn)                   \n                    return _dict\n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        pass',NULL,5.0000,5.0000,1472609600.2853),('kuaiji_dongaokuaiji','kztk','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-30 14:19:18\n# Project: kuaiji_dongaokuaiji\n\nfrom pyspider.libs.base_handler import *\n\n\nkey_url =[\n    #\'http://chuji.dongao.com/zchjsdh/kstk/mryl/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/lnzt/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/tblx/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/mnst/\',\n    \'http://zhongji.dongao.com/ghzcdh/kstk/lnzt/\',\n    #\'http://zhongji.dongao.com/ghzcdh/kstk/mryl/\',\n    \'http://zhongji.dongao.com/ghzcdh/kstk/tblx/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/lnzt/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/mryl/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/tblx/\',\n    \'http://zhukuai.dongao.com/zchjsdh/kstk/lnzt/\',\n    #\'http://zhukuai.dongao.com/zchjsdh/kstk/mryl/\',\n    \'http://zhukuai.dongao.com/zchjsdh/kstk/tblx/\',\n    \'http://gaoji.dongao.com/ghzcdh/kstk/lnzt/\',\n    #\'http://gaoji.dongao.com/ghzcdh/kstk/mryl/\',\n    \'http://congye.dongao.com/qg/kstk/lnzt/\',\n    #\'http://congye.dongao.com/qg/kstk/mryl/\',\n    \'http://congye.dongao.com/qg/kstk/mnks/\',\n]\n\ntype_dict = {\n            \'chuji\':[u\'财会经济\',u\'初级会计师\'],\n            \'zhongji\':[u\'财会经济\',u\'中级会计师\'],\n            \'shiwushi\':[u\'财会经济\',u\'注册税务师\'],\n            \'zhukuai\':[u\'财会经济\',u\'注册会计师\'],\n            \'gaoji\':[u\'财会经济\',u\'高级会计师\'],\n            \'congye\':[u\'财会经济\',u\'会计从业\'],\n            u\'注册会计师\':[u\'财会经济\',u\'注册会计师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'初级会计职称\':[u\'财会经济\',u\'初级会计师\'],\n            u\'中级会计职称\':[u\'财会经济\',u\'中级会计师\'],\n            u\'税务师\':[u\'财会经济\',u\'注册税务师\'],\n            u\'统计师\':[u\'财会经济\',u\'统计师\'],\n            u\'审计师\':[u\'财会经济\',u\'审计师\'],\n            u\'高级经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'高级会计\':[u\'财会经济\',u\'高级会计师\'],\n            u\'理财规划师\':[u\'财会经济\',u\'理财规划师\'],\n\n\n            u\'英语四级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'英语六级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'雅思\':[u\'外语考试\',u\'雅思\'],\n            u\'托福\':[u\'外语考试\',u\'托福\'],\n            u\'职称英语\':[u\'外语考试\',u\'职称英语\'],\n            u\'商务英语\':[u\'外语考试\',u\'商务英语\'],\n            u\'公共英语\':[u\'外语考试\',u\'公共英语\'],\n            u\'日语\':[u\'外语考试\',u\'日语\'],\n            u\'GRE考试\':[u\'外语考试\',u\'GRE考试\'],\n            u\'专四专八\':[u\'外语考试\',u\'专四专八\'],\n            u\'口译笔译\':[u\'外语考试\',u\'口译笔译\'],\n\n            u\'一级建造师\':[u\'建筑工程\',u\'一级建造师\'],\n            u\'二级建造师\':[u\'建筑工程\',u\'二级建造师\'],\n            u\'咨询工程师\':[u\'建筑工程\',u\'咨询工程师\'],\n            u\'造价工程师\':[u\'建筑工程\',\'造价工程师\'],\n            u\'结构工程师\':[u\'建筑工程\',\'结构工程师\'],\n            u\'物业管理\':[u\'建筑工程\',u\'物业管理师\'],\n            u\'城市规划\':[u\'建筑工程\',u\'城市规划师\'],\n            u\'给排水工程\':[u\'建筑工程\',u\'给排水工程\'],\n            u\'电气工程\':[u\'建筑工程\',u\'电气工程师\'],\n            u\'公路监理师\':[u\'建筑工程\',u\'公路监理师\'],\n            u\'消防工程师\':[u\'建筑工程\',u\'消防工程师\'],\n\n            u\'物流师\':[u\'职业资格\',u\'物流师\'],\n            u\'人力资源\':[u\'职业资格\',u\'人力资源\'],\n            u\'心理咨询师\':[u\'职业资格\',u\'心理咨询师\'],\n            u\'公共营养师\':[u\'职业资格\',u\'公共营养师\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'证券从业资格\':[u\'职业资格\',u\'证券经纪人\'],\n            u\'电子商务师\':[u\'职业资格\',u\'电子商务\'],\n            u\'期货从业\':[u\'职业资格\',u\'期货从业\'],\n            u\'教师资格\':[u\'职业资格\',u\'教师资格\'],\n            u\'管理咨询师\':[u\'职业资格\',u\'管理咨询师\'],\n            u\'导游证\':[u\'职业资格\',u\'导游证\'],\n            \n            u\'英语\':[u\'学历教育\',u\'考研\'],\n            u\'数学\':[u\'学历教育\',u\'考研\'],\n            u\'政治\':[u\'学历教育\',u\'考研\'],\n            u\'专业课\':[u\'学历教育\',u\'考研\'],\n\n            u\'执业医师\':[u\'医学卫生\',u\'执业医师\'],\n            u\'执业药师\':[u\'医学卫生\',u\'执业药师\'],\n            u\'临床医师\':[u\'医学卫生\',u\'临床执业\'],\n            u\'中医医师\':[u\'医学卫生\',u\'中医执业\'],\n            u\'中西医医师\':[u\'医学卫生\',u\'中西医执业\'],\n            u\'中医助理\':[u\'医学卫生\',u\'中医助理\'],\n            u\'中西医助理\':[u\'医学卫生\',u\'中西医助理\'],\n            u\'主治\':[u\'医学卫生\',u\'主治\'],\n            u\'检验\':[u\'医学卫生\',u\'检验\'],\n            u\'执业护士资格\':[u\'医学卫生\',u\'执业护士\'],\n\n\n            u\'成人高考\':[u\'学历教育\',u\'成人高考\'],\n            u\'自学考试\':[u\'学历教育\',u\'自考\'],\n            u\'MBA考试\':[u\'学历教育\',u\'MBA\'],\n            u\'法律硕士\':[u\'学历教育\',u\'法律硕士\'],\n            u\'专升本\':[u\'学历教育\',u\'专升本\'],\n            #u\'\':[u\'学历教育\',u\'工程硕士\'],\n            u\'MPA考试\':[u\'学历教育\',u\'公共硕士\'],\n            #u\'\':[u\'学历教育\',u\'考研\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\': \'0.1\',\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for href in key_url:\n            _dict = {}\n            if href.split(\'//\')[1].split(\'.\')[0] in type_dict:\n                _dict[\'bread\'] = type_dict[href.split(\'//\')[1].split(\'.\')[0]]\n                self.crawl(href, save = _dict, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.column_list li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'/\', \'-\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        for each in response.doc(\'.page_number a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n        for each in response.doc(\'.showpage a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'.pabt-30\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'相关推荐\' in each1.text():\n                    continue\n                if each1.find(\'b\'):\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())  \n                    \n        for each in response.doc(\'.article\').items():\n            for each1 in each.find(\'.content > p\').items():\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'相关推荐\' in each1.text():\n                    continue\n                if each1.find(\'b\'):\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())   \n     \n                    \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'东奥\',\'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'), \n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',\'\').replace(\'None\',\'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',\'\').replace(\'None\',\'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'None\',\'\') + str(response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'None\',\'\'),\n                \"tdk_title\":response.doc(\'head > title\').eq(0).text().replace(u\'东奥会计在线\',\'\') + u\'跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"东奥会计在线\",\n\n            }\n',NULL,1.0000,3.0000,1471334630.9780),('kuaiji_dongaokuaiji_inc','kuaiji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 16:06:51\n# Project: kuaiji_dongaokuaiji_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nkey_url =[\n    #\'http://chuji.dongao.com/zchjsdh/kstk/mryl/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/lnzt/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/tblx/\',\n    \'http://chuji.dongao.com/zchjsdh/kstk/mnst/\',\n    \'http://zhongji.dongao.com/ghzcdh/kstk/lnzt/\',\n    #\'http://zhongji.dongao.com/ghzcdh/kstk/mryl/\',\n    \'http://zhongji.dongao.com/ghzcdh/kstk/tblx/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/lnzt/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/mryl/\',\n    \'http://shuiwushi.dongao.com/taxdh/kstk/tblx/\',\n    \'http://zhukuai.dongao.com/zchjsdh/kstk/lnzt/\',\n    #\'http://zhukuai.dongao.com/zchjsdh/kstk/mryl/\',\n    \'http://zhukuai.dongao.com/zchjsdh/kstk/tblx/\',\n    \'http://gaoji.dongao.com/ghzcdh/kstk/lnzt/\',\n    #\'http://gaoji.dongao.com/ghzcdh/kstk/mryl/\',\n    \'http://congye.dongao.com/qg/kstk/lnzt/\',\n    #\'http://congye.dongao.com/qg/kstk/mryl/\',\n    \'http://congye.dongao.com/qg/kstk/mnks/\',\n]\n\ntype_dict = {\n            \'chuji\':[u\'财会经济\',u\'初级会计师\'],\n            \'zhongji\':[u\'财会经济\',u\'中级会计师\'],\n            \'shiwushi\':[u\'财会经济\',u\'注册税务师\'],\n            \'zhukuai\':[u\'财会经济\',u\'注册会计师\'],\n            \'gaoji\':[u\'财会经济\',u\'高级会计师\'],\n            \'congye\':[u\'财会经济\',u\'会计从业\'],\n            u\'注册会计师\':[u\'财会经济\',u\'注册会计师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'初级会计职称\':[u\'财会经济\',u\'初级会计师\'],\n            u\'中级会计职称\':[u\'财会经济\',u\'中级会计师\'],\n            u\'税务师\':[u\'财会经济\',u\'注册税务师\'],\n            u\'统计师\':[u\'财会经济\',u\'统计师\'],\n            u\'审计师\':[u\'财会经济\',u\'审计师\'],\n            u\'高级经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'高级会计\':[u\'财会经济\',u\'高级会计师\'],\n            u\'理财规划师\':[u\'财会经济\',u\'理财规划师\'],\n\n\n            u\'英语四级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'英语六级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'雅思\':[u\'外语考试\',u\'雅思\'],\n            u\'托福\':[u\'外语考试\',u\'托福\'],\n            u\'职称英语\':[u\'外语考试\',u\'职称英语\'],\n            u\'商务英语\':[u\'外语考试\',u\'商务英语\'],\n            u\'公共英语\':[u\'外语考试\',u\'公共英语\'],\n            u\'日语\':[u\'外语考试\',u\'日语\'],\n            u\'GRE考试\':[u\'外语考试\',u\'GRE考试\'],\n            u\'专四专八\':[u\'外语考试\',u\'专四专八\'],\n            u\'口译笔译\':[u\'外语考试\',u\'口译笔译\'],\n\n            u\'一级建造师\':[u\'建筑工程\',u\'一级建造师\'],\n            u\'二级建造师\':[u\'建筑工程\',u\'二级建造师\'],\n            u\'咨询工程师\':[u\'建筑工程\',u\'咨询工程师\'],\n            u\'造价工程师\':[u\'建筑工程\',\'造价工程师\'],\n            u\'结构工程师\':[u\'建筑工程\',\'结构工程师\'],\n            u\'物业管理\':[u\'建筑工程\',u\'物业管理师\'],\n            u\'城市规划\':[u\'建筑工程\',u\'城市规划师\'],\n            u\'给排水工程\':[u\'建筑工程\',u\'给排水工程\'],\n            u\'电气工程\':[u\'建筑工程\',u\'电气工程师\'],\n            u\'公路监理师\':[u\'建筑工程\',u\'公路监理师\'],\n            u\'消防工程师\':[u\'建筑工程\',u\'消防工程师\'],\n\n            u\'物流师\':[u\'职业资格\',u\'物流师\'],\n            u\'人力资源\':[u\'职业资格\',u\'人力资源\'],\n            u\'心理咨询师\':[u\'职业资格\',u\'心理咨询师\'],\n            u\'公共营养师\':[u\'职业资格\',u\'公共营养师\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'证券从业资格\':[u\'职业资格\',u\'证券经纪人\'],\n            u\'电子商务师\':[u\'职业资格\',u\'电子商务\'],\n            u\'期货从业\':[u\'职业资格\',u\'期货从业\'],\n            u\'教师资格\':[u\'职业资格\',u\'教师资格\'],\n            u\'管理咨询师\':[u\'职业资格\',u\'管理咨询师\'],\n            u\'导游证\':[u\'职业资格\',u\'导游证\'],\n            \n            u\'英语\':[u\'学历教育\',u\'考研\'],\n            u\'数学\':[u\'学历教育\',u\'考研\'],\n            u\'政治\':[u\'学历教育\',u\'考研\'],\n            u\'专业课\':[u\'学历教育\',u\'考研\'],\n\n            u\'执业医师\':[u\'医学卫生\',u\'执业医师\'],\n            u\'执业药师\':[u\'医学卫生\',u\'执业药师\'],\n            u\'临床医师\':[u\'医学卫生\',u\'临床执业\'],\n            u\'中医医师\':[u\'医学卫生\',u\'中医执业\'],\n            u\'中西医医师\':[u\'医学卫生\',u\'中西医执业\'],\n            u\'中医助理\':[u\'医学卫生\',u\'中医助理\'],\n            u\'中西医助理\':[u\'医学卫生\',u\'中西医助理\'],\n            u\'主治\':[u\'医学卫生\',u\'主治\'],\n            u\'检验\':[u\'医学卫生\',u\'检验\'],\n            u\'执业护士资格\':[u\'医学卫生\',u\'执业护士\'],\n\n\n            u\'成人高考\':[u\'学历教育\',u\'成人高考\'],\n            u\'自学考试\':[u\'学历教育\',u\'自考\'],\n            u\'MBA考试\':[u\'学历教育\',u\'MBA\'],\n            u\'法律硕士\':[u\'学历教育\',u\'法律硕士\'],\n            u\'专升本\':[u\'学历教育\',u\'专升本\'],\n            #u\'\':[u\'学历教育\',u\'工程硕士\'],\n            u\'MPA考试\':[u\'学历教育\',u\'公共硕士\'],\n            #u\'\':[u\'学历教育\',u\'考研\'],\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\': \'0.1\',\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for href in key_url:\n            _dict = {}\n            if href.split(\'//\')[1].split(\'.\')[0] in type_dict:\n                _dict[\'bread\'] = type_dict[href.split(\'//\')[1].split(\'.\')[0]]\n                self.crawl(href, save = _dict, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.column_list li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'span\').text().replace(\'/\', \'-\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n        #for each in response.doc(\'.page_number a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n        #for each in response.doc(\'.showpage a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'.pabt-30\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'相关推荐\' in each1.text():\n                    continue\n                if each1.find(\'b\'):\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())  \n                    \n        for each in response.doc(\'.article\').items():\n            for each1 in each.find(\'.content > p\').items():\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'相关推荐\' in each1.text():\n                    continue\n                if each1.find(\'b\'):\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())   \n     \n                    \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'东奥\',\'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'), \n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',\'\').replace(\'None\',\'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',\'\').replace(\'None\',\'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'None\',\'\') + str(response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(u\'东奥会计在线\',u\'跟谁学\').replace(\'None\',\'\'),\n                \"tdk_title\":response.doc(\'head > title\').eq(0).text().replace(u\'东奥会计在线\',\'\') + u\'跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"东奥会计在线\",\n\n            }\n',NULL,1.0000,3.0000,1472546254.4227),('kuaiji_zhengbao','kztk','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-30 18:18:41\n# Project: kuaiji_zhengbao\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ntype_dict = {\n            \'chuji\':[u\'财会经济\',u\'初级会计师\'],\n            \'chujizhicheng\':[u\'财会经济\',u\'初级会计师\'],\n            \'zhongji\':[u\'财会经济\',u\'中级会计师\'],\n            \'zhongjizhicheng\':[u\'财会经济\',u\'中级会计师\'],\n            \'shiwushi\':[u\'财会经济\',u\'注册税务师\'],\n            \'zhukuai\':[u\'财会经济\',u\'注册会计师\'],\n            \'gaoji\':[u\'财会经济\',u\'高级会计师\'],\n            \'congye\':[u\'财会经济\',u\'会计从业\'],\n            u\'注册会计师\':[u\'财会经济\',u\'注册会计师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'初级会计职称\':[u\'财会经济\',u\'初级会计师\'],\n            u\'中级会计职称\':[u\'财会经济\',u\'中级会计师\'],\n            u\'税务师\':[u\'财会经济\',u\'注册税务师\'],\n            u\'统计师\':[u\'财会经济\',u\'统计师\'],\n            u\'审计师\':[u\'财会经济\',u\'审计师\'],\n            u\'高级经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'高级会计\':[u\'财会经济\',u\'高级会计师\'],\n            u\'理财规划师\':[u\'财会经济\',u\'理财规划师\'],\n\n\n            u\'英语四级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'英语六级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'雅思\':[u\'外语考试\',u\'雅思\'],\n            u\'托福\':[u\'外语考试\',u\'托福\'],\n            u\'职称英语\':[u\'外语考试\',u\'职称英语\'],\n            u\'商务英语\':[u\'外语考试\',u\'商务英语\'],\n            u\'公共英语\':[u\'外语考试\',u\'公共英语\'],\n            u\'日语\':[u\'外语考试\',u\'日语\'],\n            u\'GRE考试\':[u\'外语考试\',u\'GRE考试\'],\n            u\'专四专八\':[u\'外语考试\',u\'专四专八\'],\n            u\'口译笔译\':[u\'外语考试\',u\'口译笔译\'],\n\n            \'zaojia\':[u\'建筑工程\',\'造价工程师\'],\n            u\'一级建造师\':[u\'建筑工程\',u\'一级建造师\'],\n            u\'二级建造师\':[u\'建筑工程\',u\'二级建造师\'],\n            u\'咨询工程师\':[u\'建筑工程\',u\'咨询工程师\'],\n            u\'造价工程师\':[u\'建筑工程\',\'造价工程师\'],\n            u\'结构工程师\':[u\'建筑工程\',\'结构工程师\'],\n            u\'物业管理\':[u\'建筑工程\',u\'物业管理师\'],\n            u\'城市规划\':[u\'建筑工程\',u\'城市规划师\'],\n            u\'给排水工程\':[u\'建筑工程\',u\'给排水工程\'],\n            u\'电气工程\':[u\'建筑工程\',u\'电气工程师\'],\n            u\'公路监理师\':[u\'建筑工程\',u\'公路监理师\'],\n            u\'消防工程师\':[u\'建筑工程\',u\'消防工程师\'],\n            u\'消防\':[u\'建筑工程\',u\'消防工程师\'],\n\n            u\'物流师\':[u\'职业资格\',u\'物流师\'],\n            u\'人力资源\':[u\'职业资格\',u\'人力资源\'],\n            u\'心理咨询师\':[u\'职业资格\',u\'心理咨询师\'],\n            u\'公共营养师\':[u\'职业资格\',u\'公共营养师\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'证券从业资格\':[u\'职业资格\',u\'证券经纪人\'],\n            u\'电子商务师\':[u\'职业资格\',u\'电子商务\'],\n            u\'期货从业\':[u\'职业资格\',u\'期货从业\'],\n            u\'教师资格\':[u\'职业资格\',u\'教师资格\'],\n            u\'管理咨询师\':[u\'职业资格\',u\'管理咨询师\'],\n            u\'导游证\':[u\'职业资格\',u\'导游证\'],\n            \n            u\'英语\':[u\'学历教育\',u\'考研\'],\n            u\'数学\':[u\'学历教育\',u\'考研\'],\n            u\'政治\':[u\'学历教育\',u\'考研\'],\n            u\'专业课\':[u\'学历教育\',u\'考研\'],\n\n            u\'执业医师\':[u\'医学卫生\',u\'执业医师\'],\n            u\'执业药师\':[u\'医学卫生\',u\'执业药师\'],\n            u\'临床医师\':[u\'医学卫生\',u\'临床执业\'],\n            u\'中医医师\':[u\'医学卫生\',u\'中医执业\'],\n            u\'中西医医师\':[u\'医学卫生\',u\'中西医执业\'],\n            u\'中医助理\':[u\'医学卫生\',u\'中医助理\'],\n            u\'中西医助理\':[u\'医学卫生\',u\'中西医助理\'],\n            u\'主治\':[u\'医学卫生\',u\'主治\'],\n            u\'检验\':[u\'医学卫生\',u\'检验\'],\n            u\'执业护士资格\':[u\'医学卫生\',u\'执业护士\'],\n\n\n            u\'成人高考\':[u\'学历教育\',u\'成人高考\'],\n            u\'自学考试\':[u\'学历教育\',u\'自考\'],\n            u\'MBA考试\':[u\'学历教育\',u\'MBA\'],\n            u\'法律硕士\':[u\'学历教育\',u\'法律硕士\'],\n            u\'专升本\':[u\'学历教育\',u\'专升本\'],\n            #u\'\':[u\'学历教育\',u\'工程硕士\'],\n            u\'MPA考试\':[u\'学历教育\',u\'公共硕士\'],\n            #u\'\':[u\'学历教育\',u\'考研\'],\n}\n\n\nsome_url =[\n    \'http://www.chinaacc.com/chujizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/chujizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/\',\n    \'http://www.chinaacc.com/zaojia/zt/\',\n    \'http://www.chinaacc.com/zaojia/mnst/\',    \n]\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url in some_url:\n            _dict = {}\n            _dict[\'bread\'] = type_dict[url.split(\'/\')[3]]\n            self.crawl(url, save = _dict, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.xinxi li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.fl > a\').text()\n            if \'报名\' in _dict[\'title\'] or \'名师\' in _dict[\'title\'] or \'汇总\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'.fr\').text().replace(\'[\',\'\').replace(\']\',\'\')\n            self.crawl(each.find(\'.fl > a\').attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页 \n        for each in response.doc(\'divpagestrcjzc a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'#fontzoom\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'全网首发\' in each1.text():\n                    continue\n                if u\'估分更准确\' in each1.text():\n                    continue\n                if u\'独家\' in each1.text():\n                    continue\n                if u\'点击查看\' in each1.text():\n                    continue\n                if u\'点击参与\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text() or u\'推荐阅读\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'以上\' in each1.text() and u\'是中华会计网\' in each1.text():\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            \n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\', \'\').replace(\'\\n\', \'\').replace(\'\\t\', \'\').replace(u\'中华会计网校\', \'\').replace(\'【 】\', \'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'),\n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'中华会计网校\', \'\').replace(\n                    \'None\', \'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'中华会计网校\', \'\').replace(\n                    \'None\', \'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'中华会计网校\', u\'\').replace(\n                    \'建设网校\', \'\').replace(\'None\', \'\').replace(\'建设工程教育网\', \'\') + str(response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(\n                    u\'中华会计网校\', \'\').replace(\'建设网校\', \'\').replace(\'None\', \'\').replace(\'建设工程教育网\', \'\'),\n                \"tdk_title\": response.doc(\'head > title\').eq(0).text().replace(u\'中华会计网校\', \'\') + u\' 跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"中华会计网校\",\n            }\n\n',NULL,1.0000,3.0000,1471334642.4074),('kuaiji_zhengbao_inc','kuaiji','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 16:11:21\n# Project: kuaiji_zhengbao_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ntype_dict = {\n            \'chuji\':[u\'财会经济\',u\'初级会计师\'],\n            \'chujizhicheng\':[u\'财会经济\',u\'初级会计师\'],\n            \'zhongji\':[u\'财会经济\',u\'中级会计师\'],\n            \'zhongjizhicheng\':[u\'财会经济\',u\'中级会计师\'],\n            \'shiwushi\':[u\'财会经济\',u\'注册税务师\'],\n            \'zhukuai\':[u\'财会经济\',u\'注册会计师\'],\n            \'gaoji\':[u\'财会经济\',u\'高级会计师\'],\n            \'congye\':[u\'财会经济\',u\'会计从业\'],\n            u\'注册会计师\':[u\'财会经济\',u\'注册会计师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'初级会计职称\':[u\'财会经济\',u\'初级会计师\'],\n            u\'中级会计职称\':[u\'财会经济\',u\'中级会计师\'],\n            u\'税务师\':[u\'财会经济\',u\'注册税务师\'],\n            u\'统计师\':[u\'财会经济\',u\'统计师\'],\n            u\'审计师\':[u\'财会经济\',u\'审计师\'],\n            u\'高级经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'经济师\':[u\'财会经济\',u\'经济师\'],\n            u\'高级会计\':[u\'财会经济\',u\'高级会计师\'],\n            u\'理财规划师\':[u\'财会经济\',u\'理财规划师\'],\n\n\n            u\'英语四级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'英语六级\':[u\'外语考试\',u\'英语四六级\'],\n            u\'雅思\':[u\'外语考试\',u\'雅思\'],\n            u\'托福\':[u\'外语考试\',u\'托福\'],\n            u\'职称英语\':[u\'外语考试\',u\'职称英语\'],\n            u\'商务英语\':[u\'外语考试\',u\'商务英语\'],\n            u\'公共英语\':[u\'外语考试\',u\'公共英语\'],\n            u\'日语\':[u\'外语考试\',u\'日语\'],\n            u\'GRE考试\':[u\'外语考试\',u\'GRE考试\'],\n            u\'专四专八\':[u\'外语考试\',u\'专四专八\'],\n            u\'口译笔译\':[u\'外语考试\',u\'口译笔译\'],\n\n            \'zaojia\':[u\'建筑工程\',\'造价工程师\'],\n            u\'一级建造师\':[u\'建筑工程\',u\'一级建造师\'],\n            u\'二级建造师\':[u\'建筑工程\',u\'二级建造师\'],\n            u\'咨询工程师\':[u\'建筑工程\',u\'咨询工程师\'],\n            u\'造价工程师\':[u\'建筑工程\',\'造价工程师\'],\n            u\'结构工程师\':[u\'建筑工程\',\'结构工程师\'],\n            u\'物业管理\':[u\'建筑工程\',u\'物业管理师\'],\n            u\'城市规划\':[u\'建筑工程\',u\'城市规划师\'],\n            u\'给排水工程\':[u\'建筑工程\',u\'给排水工程\'],\n            u\'电气工程\':[u\'建筑工程\',u\'电气工程师\'],\n            u\'公路监理师\':[u\'建筑工程\',u\'公路监理师\'],\n            u\'消防工程师\':[u\'建筑工程\',u\'消防工程师\'],\n            u\'消防\':[u\'建筑工程\',u\'消防工程师\'],\n\n            u\'物流师\':[u\'职业资格\',u\'物流师\'],\n            u\'人力资源\':[u\'职业资格\',u\'人力资源\'],\n            u\'心理咨询师\':[u\'职业资格\',u\'心理咨询师\'],\n            u\'公共营养师\':[u\'职业资格\',u\'公共营养师\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'秘书资格\':[u\'职业资格\',u\'秘书资格\'],\n            u\'证券从业资格\':[u\'职业资格\',u\'证券经纪人\'],\n            u\'电子商务师\':[u\'职业资格\',u\'电子商务\'],\n            u\'期货从业\':[u\'职业资格\',u\'期货从业\'],\n            u\'教师资格\':[u\'职业资格\',u\'教师资格\'],\n            u\'管理咨询师\':[u\'职业资格\',u\'管理咨询师\'],\n            u\'导游证\':[u\'职业资格\',u\'导游证\'],\n            \n            u\'英语\':[u\'学历教育\',u\'考研\'],\n            u\'数学\':[u\'学历教育\',u\'考研\'],\n            u\'政治\':[u\'学历教育\',u\'考研\'],\n            u\'专业课\':[u\'学历教育\',u\'考研\'],\n\n            u\'执业医师\':[u\'医学卫生\',u\'执业医师\'],\n            u\'执业药师\':[u\'医学卫生\',u\'执业药师\'],\n            u\'临床医师\':[u\'医学卫生\',u\'临床执业\'],\n            u\'中医医师\':[u\'医学卫生\',u\'中医执业\'],\n            u\'中西医医师\':[u\'医学卫生\',u\'中西医执业\'],\n            u\'中医助理\':[u\'医学卫生\',u\'中医助理\'],\n            u\'中西医助理\':[u\'医学卫生\',u\'中西医助理\'],\n            u\'主治\':[u\'医学卫生\',u\'主治\'],\n            u\'检验\':[u\'医学卫生\',u\'检验\'],\n            u\'执业护士资格\':[u\'医学卫生\',u\'执业护士\'],\n\n\n            u\'成人高考\':[u\'学历教育\',u\'成人高考\'],\n            u\'自学考试\':[u\'学历教育\',u\'自考\'],\n            u\'MBA考试\':[u\'学历教育\',u\'MBA\'],\n            u\'法律硕士\':[u\'学历教育\',u\'法律硕士\'],\n            u\'专升本\':[u\'学历教育\',u\'专升本\'],\n            #u\'\':[u\'学历教育\',u\'工程硕士\'],\n            u\'MPA考试\':[u\'学历教育\',u\'公共硕士\'],\n            #u\'\':[u\'学历教育\',u\'考研\'],\n}\n\n\nsome_url =[\n    \'http://www.chinaacc.com/chujizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/chujizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/\',\n    \'http://www.chinaacc.com/zaojia/zt/\',\n    \'http://www.chinaacc.com/zaojia/mnst/\',    \n]\n\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for url in some_url:\n            _dict = {}\n            _dict[\'bread\'] = type_dict[url.split(\'/\')[3]]\n            self.crawl(url, save = _dict, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.xinxi li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.fl > a\').text()\n            if \'报名\' in _dict[\'title\'] or \'名师\' in _dict[\'title\'] or \'汇总\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'.fr\').text().replace(\'[\',\'\').replace(\']\',\'\')\n            self.crawl(each.find(\'.fl > a\').attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页 \n        #for each in response.doc(\'divpagestrcjzc a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href, save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'#fontzoom\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'全网首发\' in each1.text():\n                    continue\n                if u\'估分更准确\' in each1.text():\n                    continue\n                if u\'独家\' in each1.text():\n                    continue\n                if u\'点击查看\' in each1.text():\n                    continue\n                if u\'点击参与\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text() or u\'推荐阅读\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'以上\' in each1.text() and u\'是中华会计网\' in each1.text():\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            \n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\', \'\').replace(\'\\n\', \'\').replace(\'\\t\', \'\').replace(u\'中华会计网校\', \'\').replace(\'【 】\', \'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'),\n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'中华会计网校\', \'\').replace(\n                    \'None\', \'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'中华会计网校\', \'\').replace(\n                    \'None\', \'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'中华会计网校\', u\'\').replace(\n                    \'建设网校\', \'\').replace(\'None\', \'\').replace(\'建设工程教育网\', \'\') + str(response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(\n                    u\'中华会计网校\', \'\').replace(\'建设网校\', \'\').replace(\'None\', \'\').replace(\'建设工程教育网\', \'\'),\n                \"tdk_title\": response.doc(\'head > title\').eq(0).text().replace(u\'中华会计网校\', \'\') + u\' 跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"中华会计网校\",\n            }\n\n',NULL,1.0000,3.0000,1472546258.5260),('kztk_haoxue','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-29 16:24:42\n# Project: kztk_haoxue\n\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ndalei = [\n    u\'建筑工程\',\n    u\'财会经济\',\n    u\'医学卫生\',\n    u\'外语考试\',\n    u\'职业资格\',\n    u\'学历教育\',\n    u\'计算机考试\'\n]\nxiaolei = {\n    u\'一级建造师\': u\'一级建造师\',\n    u\'二级建造师\': u\'二级建造师\',\n    u\'监理工程师\': u\'监理工程师\',\n    u\'咨询工程师\': u\'咨询工程师\',\n    u\'造价工程师\': u\'造价工程师\',\n    u\'结构工程师\': u\'结构工程师\',\n    u\'电气工程师\': u\'电气工程师\',\n    u\'物业管理师\': u\'物业管理师\',\n    u\'经济师\': u\'经济师\',\n    u\'设计师\': u\'设计师\',\n    u\'初级会计职称\': u\'初级会计师\',\n    u\'会计从业证\': u\'初级会计师\',\n    u\'中级会计师\': u\'中级会计师\',\n    u\'注册会计师\': u\'注册会计师\',\n    u\'统计师\': u\'统计师\',\n    u\'审计师\': u\'审计师\',\n    u\'注册税务师\': u\'注册税务师\',\n    u\'执业药师\': u\'执业药师\',\n    u\'护士资格\': u\'执业护士\',\n    u\'临床执业医师\': u\'临床执业\',\n    u\'中西医执业医师\': u\'中西医执业\',\n    u\'中医执业医师\': u\'中医执业\',\n    u\'主治\': u\'主治\',\n    u\'检验\': u\'检验\',\n    u\'英语四级\': u\'英语四六级\',\n    u\'英语六级\': u\'英语四六级\',\n    u\'雅思\': u\'雅思\',\n    u\'托福\': u\'托福\',\n    u\'GRE考试\': u\'GRE考试\',\n    u\'职称英语\': u\'职称英语\',\n    u\'公共英语\': u\'公共英语\',\n    u\'商务英语\': u\'商务英语\',\n    u\'日语\': u\'日语\',\n    u\'人力资源\': u\'人力资源\',\n    u\'心理咨询师\': u\'心理咨询师\',\n    u\'物流师\': u\'物流师\',\n    u\'公共营养师\': u\'公共营养师\',\n    u\'秘书资格\': u\'秘书资格\',\n    u\'证券经纪人\': u\'证券经纪人\',\n    u\'电子商务\': u\'电子商务\',\n    u\'国家司法\': u\'国家司法\',\n    u\'成人高考\': u\'成人高考\',\n    u\'自考\': u\'自考\',\n    u\'MBA\': u\'MBA\',\n    u\'法律硕士\': u\'法律硕士\',\n    u\'会计硕士\': u\'会计硕士\',\n    u\'工程硕士\': u\'工程硕士\',\n    u\'MPA\': u\'公共硕士\',\n    u\'考研\': u\'考研\',\n    u\'计算机等级\': u\'计算机等级\',\n    u\'软件水平\': u\'软件水平\',\n    u\'微软认证\': u\'微软认证\',\n    u\'Cisco认证\': u\'Cisco认证\',\n    u\'Oracle认证\': u\'Oracle认证\',\n    u\'职称计算机\': u\'职称计算机\',\n    u\'Java认证\': u\'Java认证\',\n    u\'华为认证\': u\'华为认证\',\n}\n\nadd_url =[\n    \'lnzt/\',\n    \'mnst/\',\n    \'mryl/\',\n    \'kddq/\',\n    \'zjlx/\'\n]\n\nda_xiao_lei = {\n    u\'一级建造师\':u\'建筑工程\',\n    u\'二级建造师\':u\'建筑工程\',\n    u\'消防工程师\':u\'建筑工程\',\n    u\'护士资格\':u\'医学卫生\',\n    u\'初级护师\':u\'医学卫生\',\n    u\'主管护师\':u\'医学卫生\',\n    u\'临床执业医师\':u\'医学卫生\',\n    u\'临床助理医师\':u\'医学卫生\',\n    u\'中医执业医师\':u\'医学卫生\',\n    u\'中医助理医师\':u\'医学卫生\',\n    u\'中西医执业医师\':u\'医学卫生\',\n    u\'中西医助理医师\':u\'医学卫生\',\n    u\'执业药师\':u\'医学卫生\',\n    u\'会计从业证\':u\'财会经济\',\n    u\'初级会计职称\':u\'财会经济\',\n    u\'银行从业\':u\'财会经济\',\n    u\'证券从业\':u\'财会经济\',\n    u\'基金从业\':u\'财会经济\',\n\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.5haoxue.net/sitemap/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.tit a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [\'\', \'\']\n            _dict[\'bread\'][1] = each.text()\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for val in add_url:\n            self.crawl(response.url + val, save = response.save, callback=self.list_page1)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page1(self, response):      \n        for each in response.doc(\'.m-post-col li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'span\').text()\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'.f-clear > .f-f14\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'小编推荐\' in each1.text():\n                    continue\n                if u\'编辑推荐\' in each1.text():\n                    continue\n                if u\'点击查看\' in each1.text():\n                    continue\n                if u\'直接点击\' in each1.text():\n                    continue\n                if u\'我们特别开通了\' in each1.text():\n                    continue\n                if u\'希望对您有所帮助\' in each1.text():\n                    continue\n                if u\'编辑第一时间为您搜集整理了\' in each1.text():\n                    continue\n                if each1.find(\'a\') and each1.find(\'img\'):\n                    continue\n                else:\n                    list.append(each1.remove(\'a\').html())\n            for each2 in each.find(\'center\').items():\n                list.append(each2.html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            bread = []\n            bread.append(da_xiao_lei[response.save.get(\'bread\')[1]])\n            if response.save.get(\'bread\')[1] in xiaolei.keys():\n                bread.append(xiaolei[response.save.get(\'bread\')[1]])\n            if len(bread) < 2:\n                bread.append(response.save.get(\'bread\')[1])\n            return {\n                \"url\": response.url,\n                \"title\": response.doc(\'.g-news h1\').text().strip(),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'好学教育\',u\'跟谁学\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": bread, \n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content.replace(u\'好学教育\',\'\'),\n                \"tdk_desc\": response.doc(\'meta[name=\"description\"]\').eq(0).attr.content.replace(u\'好学教育\',u\'跟谁学\'),\n                \"tdk_title\":response.doc(\'head > title\').eq(0).text().replace(u\'好学教育\',\'\') + u\'跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"好学教育\",\n\n            }\n\n',NULL,1.0000,3.0000,1472624374.6165),('kztk_haoxuejiaoyu','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-29 15:42:04\n# Project: kztk_haoxuejiaoyu\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ndalei = [\n    u\'建筑工程\',\n    u\'财会经济\',\n    u\'医学卫生\',\n    u\'外语考试\',\n    u\'职业资格\',\n    u\'学历教育\',\n    u\'计算机考试\'\n]\nxiaolei = {\n    u\'一级建造师\': u\'一级建造师\',\n    u\'二级建造师\': u\'二级建造师\',\n    u\'监理工程师\': u\'监理工程师\',\n    u\'咨询工程师\': u\'咨询工程师\',\n    u\'造价工程师\': u\'造价工程师\',\n    u\'结构工程师\': u\'结构工程师\',\n    u\'电气工程师\': u\'电气工程师\',\n    u\'物业管理师\': u\'物业管理师\',\n    u\'经济师\': u\'经济师\',\n    u\'设计师\': u\'设计师\',\n    u\'初级会计职称\': u\'初级会计师\',\n    u\'会计从业证\': u\'初级会计师\',\n    u\'中级会计师\': u\'中级会计师\',\n    u\'注册会计师\': u\'注册会计师\',\n    u\'统计师\': u\'统计师\',\n    u\'审计师\': u\'审计师\',\n    u\'注册税务师\': u\'注册税务师\',\n    u\'执业药师\': u\'执业药师\',\n    u\'护士资格\': u\'执业护士\',\n    u\'临床执业医师\': u\'临床执业\',\n    u\'中西医执业医师\': u\'中西医执业\',\n    u\'中医执业医师\': u\'中医执业\',\n    u\'主治\': u\'主治\',\n    u\'检验\': u\'检验\',\n    u\'英语四级\': u\'英语四六级\',\n    u\'英语六级\': u\'英语四六级\',\n    u\'雅思\': u\'雅思\',\n    u\'托福\': u\'托福\',\n    u\'GRE考试\': u\'GRE考试\',\n    u\'职称英语\': u\'职称英语\',\n    u\'公共英语\': u\'公共英语\',\n    u\'商务英语\': u\'商务英语\',\n    u\'日语\': u\'日语\',\n    u\'人力资源\': u\'人力资源\',\n    u\'心理咨询师\': u\'心理咨询师\',\n    u\'物流师\': u\'物流师\',\n    u\'公共营养师\': u\'公共营养师\',\n    u\'秘书资格\': u\'秘书资格\',\n    u\'证券经纪人\': u\'证券经纪人\',\n    u\'电子商务\': u\'电子商务\',\n    u\'国家司法\': u\'国家司法\',\n    u\'成人高考\': u\'成人高考\',\n    u\'自考\': u\'自考\',\n    u\'MBA\': u\'MBA\',\n    u\'法律硕士\': u\'法律硕士\',\n    u\'会计硕士\': u\'会计硕士\',\n    u\'工程硕士\': u\'工程硕士\',\n    u\'MPA\': u\'公共硕士\',\n    u\'考研\': u\'考研\',\n    u\'计算机等级\': u\'计算机等级\',\n    u\'软件水平\': u\'软件水平\',\n    u\'微软认证\': u\'微软认证\',\n    u\'Cisco认证\': u\'Cisco认证\',\n    u\'Oracle认证\': u\'Oracle认证\',\n    u\'职称计算机\': u\'职称计算机\',\n    u\'Java认证\': u\'Java认证\',\n    u\'华为认证\': u\'华为认证\',\n}\n\nadd_url =[\n    \'lnzt/\',\n    \'mnst/\',\n    \'mryl/\',\n    \'kddq/\',\n    \'zjlx/\'\n]\n\nda_xiao_lei = {\n    u\'一级建造师\':u\'建筑工程\',\n    u\'二级建造师\':u\'建筑工程\',\n    u\'消防工程师\':u\'建筑工程\',\n    u\'护士资格\':u\'医学卫生\',\n    u\'初级护师\':u\'医学卫生\',\n    u\'主管护师\':u\'医学卫生\',\n    u\'临床执业医师\':u\'医学卫生\',\n    u\'临床助理医师\':u\'医学卫生\',\n    u\'中医执业医师\':u\'医学卫生\',\n    u\'中医助理医师\':u\'医学卫生\',\n    u\'中西医执业医师\':u\'医学卫生\',\n    u\'中西医助理医师\':u\'医学卫生\',\n    u\'执业医师\':u\'医学卫生\',\n    u\'会计从业证\':u\'财会经济\',\n    u\'初级会计职称\':u\'财会经济\',\n    u\'银行从业\':u\'财会经济\',\n    u\'证券从业\':u\'财会经济\',\n    u\'基金从业\':u\'财会经济\',\n\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.5haoxue.net/sitemap/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.tit a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [\'\', \'\']\n            _dict[\'bread\'][1] = each.text()\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for val in add_url:\n            self.crawl(response.url + val, save = response.save, callback=self.list_page1)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page1(self, response):      \n        for each in response.doc(\'.m-post-col li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'span\').text()\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'.f-clear > .f-f14\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'小编推荐\' in each1.text():\n                    continue\n                if u\'编辑推荐\' in each1.text():\n                    continue\n                if u\'点击查看\' in each1.text():\n                    continue\n                if u\'直接点击\' in each1.text():\n                    continue\n                if u\'我们特别开通了\' in each1.text():\n                    continue\n                if u\'希望对您有所帮助\' in each1.text():\n                    continue\n                if u\'编辑第一时间为您搜集整理了\' in each1.text():\n                    continue\n                if each1.find(\'a\') and each1.find(\'img\'):\n                    continue\n                else:\n                    list.append(each1.remove(\'a\').html())\n            for each2 in each.find(\'center\').items():\n                list.append(each2.html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            bread = []\n            bread.append(da_xiao_lei[response.save.get(\'bread\')[1]])\n            if response.save.get(\'bread\')[1] in xiaolei.keys():\n                bread.append(xiaolei[response.save.get(\'bread\')[1]])\n            if len(bread) < 2:\n                bread = response.append(response.save.get(\'bread\')[1])\n            return {\n                \"url\": response.url,\n                \"title\": response.doc(\'.g-news h1\').text().strip(),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'好学教育\',u\'跟谁学\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": bread, \n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content.replace(u\'好学教育\',\'\'),\n                \"tdk_desc\": response.doc(\'meta[name=\"description\"]\').eq(0).attr.content.replace(u\'好学教育\',u\'跟谁学\'),\n                \"tdk_title\":response.doc(\'head > title\').eq(0).text().replace(u\'好学教育\',\'\') + u\'跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"好学教育\",\n\n            }\n',NULL,1.0000,3.0000,1472624377.4321),('kztk_qnr_inc','kztk','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 15:42:14\n# Project: kztk_qnr_inc\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-28 09:43:57\n# Project: kaozhengtiku_qingnianrenwang\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ndalei = [\n    u\'建筑工程\',\n    u\'财会经济\',\n    u\'医学卫生\',\n    u\'外语考试\',\n    u\'职业资格\',\n    u\'学历教育\',\n    u\'计算机考试\'\n]\nxiaolei = {\n            u\'一级建造师\':u\'一级建造师\',\n            u\'二级建造师\':u\'二级建造师\',\n            u\'监理工程师\':u\'监理工程师\',\n            u\'咨询工程师\':u\'咨询工程师\',\n            u\'造价工程师\':u\'造价工程师\',\n            u\'结构工程师\':u\'结构工程师\',\n            u\'电气工程师\':u\'电气工程师\',\n            u\'物业管理师\':u\'物业管理师\',\n            u\'经济师\':u\'经济师\',\n            u\'设计师\':u\'设计师\',\n            u\'初级会计\':u\'初级会计师\',\n            u\'中级会计师\':u\'中级会计师\',\n            u\'注册会计师\':u\'注册会计师\',\n            u\'统计师\':u\'统计师\',\n            u\'审计师\':u\'审计师\',\n            u\'注册税务师\':u\'注册税务师\',\n            u\'执业药师\':u\'执业药师\',\n            u\'执业药师\':u\'执业药师\',\n            u\'执业护士\':u\'执业护士\',\n            u\'临床执业\':u\'临床执业\',\n            u\'中西医执业\':u\'中西医执业\',\n            u\'中医执业\':u\'中医执业\',\n            u\'主治\':u\'主治\',\n            u\'检验\':u\'检验\',\n            u\'英语四级\':u\'英语四六级\',\n            u\'英语六级\':u\'英语四六级\',\n            u\'雅思\':u\'雅思\',\n            u\'托福\':u\'托福\',\n            u\'GRE考试\':u\'GRE考试\',\n            u\'职称英语\':u\'职称英语\',\n            u\'公共英语\':u\'公共英语\',\n            u\'商务英语\':u\'商务英语\',\n            u\'日语\':u\'日语\',\n            u\'人力资源\':u\'人力资源\',\n            u\'心理咨询师\':u\'心理咨询师\',\n            u\'物流师\':u\'物流师\',\n            u\'公共营养师\':u\'公共营养师\',\n            u\'秘书资格\':u\'秘书资格\',\n            u\'证券经纪人\':u\'证券经纪人\',\n            u\'电子商务\':u\'电子商务\',\n            u\'国家司法\':u\'国家司法\',\n            u\'成人高考\':u\'成人高考\',\n            u\'自考\':u\'自考\',\n            u\'MBA\':u\'MBA\',\n            u\'法律硕士\':u\'法律硕士\',\n            u\'会计硕士\':u\'会计硕士\',\n            u\'工程硕士\':u\'工程硕士\',\n            u\'MPA\':u\'公共硕士\',\n            u\'考研\':u\'考研\',\n            u\'计算机等级\':u\'计算机等级\',\n            u\'软件水平\':u\'软件水平\',\n            u\'微软认证\':u\'微软认证\',\n            u\'Cisco认证\':u\'Cisco认证\',\n            u\'Oracle认证\':u\'Oracle认证\',\n            u\'职称计算机\':u\'职称计算机\',\n            u\'Java认证\':u\'Java认证\',\n            u\'华为认证\':u\'华为认证\',\n    }\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.qnr.cn/zhenti/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'table\').items():\n            for each1 in each.find(\'a\').items():\n                _dict = {}\n                _dict[\'bread\'] = [each.find(\'td\').eq(0).text().split(\'(\')[0].strip(),\'\']\n                _dict[\'bread\'][1] = each1.text().strip()\n                self.crawl(each1.attr.href, save = _dict, callback=self.list_page)\n            \n    @config(age=1 * 1)\n    def list_page(self, response):\n        _dict = {}\n        for each in response.doc(\'.Right_last_lst\').items():\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'.R_date\').text().replace(\'/\',\'-\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n            \n        for each in response.doc(\'.P_Con\').items():\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'date\'] = each.find(\'.time\').text().replace(\'/\',\'-\')\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []\n        for each in response.doc(\'.mar10 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'#manadona > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n        if len(list) == 0 and response.doc(\'#manadona\'):     \n            list.append(response.doc(\'#manadona\').remove(\'a\').remove(\'p\').html())\n            \n        for each in response.doc(\'#xx20 > div > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n                       \n            \n        for each in response.doc(\'#xx23 > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.remove(\'a\').html())\n            \n        for each in response.doc(\'#xx27 > p\').items():\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.remove(\'a\').html())\n\n        for each in response.doc(\'.hao > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'.mini > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n        if len(list) == 0 and response.doc(\'.mini\'):     \n            list.append(response.doc(\'.mini\').remove(\'a\').remove(\'p\').html())\n                       \n        \n        for each in response.doc(\'#shtdxlnews_4 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n         \n        for each in response.doc(\'#tb42 > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'#gtsadfas > p\').items():\n            if each.find(\'a\'):\n                continue\n            if each.attr.align == \'left\':\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            elif each.attr.align:\n                if each.find(\'img\') and each.text().strip() == \'\':\n                    pass\n                else:\n                    continue\n            list.append(each.html())\n            \n        for each in response.doc(\'.nali\').items():\n            list.append(response.doc(\'.nali\').remove(\'a\').remove(\'p\').html())\n            \n        content = \'\'.join(\'<p>%s<p/>\' % s for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            bread = []\n            for val in dalei:\n                if val[0:2] in response.save.get(\'bread\')[0]:\n                    bread.append(val)\n                    for k, v in xiaolei.iteritems():\n                        if k in response.save.get(\'bread\')[1]:\n                            bread.append(v)\n            if len(bread) < 2:\n                bread = response.save.get(\'bread\')\n            return {\n                \"url\": response.url,\n                \"title\": response.doc(\'#tb41 > span\').text(),\n                \"content\": content.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'青年人网讯\',\'\').replace(\'青年人\',\'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": bread, \n                \"date\": response.save.get(\'date\'),\n                \"tdk_description\":response.doc(\'meta\').eq(1).attr.content.replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"tdk_keywords\":response.doc(\'meta\').eq(0).attr.content.replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"tdk_title\":response.doc(\'title\').eq(0).text().replace(\'青年人\',\'\').replace(\'-\',\' \'),\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"青年人网\",\n\n            }\n',NULL,1.0000,3.0000,1472546284.3822),('parenting','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-17 17:03:44\n# Project: parenting\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as pq\nimport urllib\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for index in xrange(87):\n            self.crawl(\'http://www.parenting.com.tw/subcategory/10-健康與營養/?page=%s\'%(index+1), callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'div.articleList ul li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.txtWrap h1 a\').text()\n            url = each.find(\'.txtWrap h1 a\').attr.href\n            _dict[\'cover\'] = each.find(\'.coverPic\').attr.style or \'\'\n            _dict[\'cover\'] = \'http:\' + _dict[\'cover\'].split(\'url(\')[-1].replace(\');\',\'\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        \n            \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        content_list = []\n        \n        for each in response.doc(\'div.articleContent > p\').items():\n            info = each.html()\n            if info and u\'延伸閱讀\' in info :\n                break\n            if info and u\'請勿轉載\' in info :\n                continue\n            elif info:\n                content_list.append(info)\n           \n                \n        count = 0\n        for each in response.doc(\'ul.pagination li\').items():\n            page = each.text()\n            if page and page.isdigit():\n                count = count + 1\n                \n        baseUrl = response.url\n        if count > 1:\n            for index in range(1,count):\n               try:\n                 next_info = pq(urllib.urlopen(baseUrl+\'?page=\'+str(index+1)).read().decode(\'utf-8\'))\n                 for each in next_info.find(\'div.articleContent > p\').items():\n                    info = each.html()\n                    if info and u\'延伸閱讀\' in info:\n                        break\n                    if info and u\'請勿轉載\' in info :\n                        continue\n                    elif info:\n                        content_list.append(info)\n               except:\n                    continue                  \n                    \n        if not content_list:\n            return\n        res_dict[\'content\'] = \'\'.join([\'<p class=\"newClass\">%s</p>\'%s for s in content_list if s and s.strip()])\n        res_dict[\'date\'] = response.doc(\'div.info\').text().split()[0]\n        res_dict[\'subject\'] = u\'亲子\'\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'跟谁学\'\n        res_dict[\'url\'] = response.url\n        res_dict[\'bread\'] = [u\'营养健康\']\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1472624369.2227),('proxy_monitor',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-19 17:37:58\n# Project: proxy_monitor\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        url = \'http://10.252.40.85:5000/tasks?project=gaokao_school_point_v2\'\n        self.crawl(url, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1459164249.8550),('qa_360',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-05 10:49:43\n# Project: qa_360\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        \'\'\'\n        with open(\'/apps/home/worker/100zhan.txt\') as f:\n            for line in f:\n                subject = line.strip(\'\\n\')\n                self.crawl(\'http://wenda.so.com/search/?q=\' + subject + \'&filt=20\', save={\'subject\': subject}, callback=self.index_page)\n        \'\'\'\n        word = u\'考证\'\n        self.crawl(\'http://wenda.so.com/search/?q=\' + word + \'&filt=20\', save={\'subject\': word}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        subject = response.save[\'subject\']\n        for each in response.doc(\'.qa-i-hd a\').items():\n            title = each.text()\n            self.crawl(each.attr.href,save={\'title\': title, \'subject\': subject}, callback=self.detail_page)\n        for each in response.doc(\'.pagination a\').items():\n            self.crawl(each.attr.href, save={\'subject\': subject}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for info in response.doc(\'.mod-resolved-ans > .bd\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.text > div a\').text()\n            _dic[\'date\'] = info.find(\'.text > span\').text().split()[-1].replace(\'.\',\'-\')\n            _dic[\'content\'] = info.find(\'.resolved-cnt\').html()\n            _dic[\'avatar\'] = info.find(\'.info img\').attr.src\n            content_list.append((_dic))\n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'考证题库\',\n            \"answers\": content_list,\n            \"source\": \'360\',\n            \"question_detail\": response.doc(\'.q-cnt\').text(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471335493.6510),('qa_sougou',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-02 14:39:04\n# Project: sogou_wenwen\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport pyquery\nimport json\n\nclass Handler(BaseHandler):\n   \n    crawl_config = {\n\n#\'proxy\': \'111.56.13.152:80\',\n            \"headers\":{\n\'Accept\':\'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\',\n\'Accept-Encoding\':\'gzip, deflate, sdch\',\n\'Accept-Language\':\'zh-CN,zh;q=0.8,en;q=0.6\',\n\'Cache-Control\':\'max-age=0\',\n\'Connection\':\'keep-alive\',\n\'Cookie\':\'ww_search_tips=nulln; ww_sTitle=%u6570%u5B66%u77E5%u8BC6%u624B%u5DE5%u5236%u4F5C*GMAT*%u4E2D%u8003*%u6258%u798F; ww_filter=1; CXID=E52AD3C93ACC10138699410B1397001C; IPLOC=CN4201; SUID=E105133A5EC90D0A0000000056FE42CD; ssuid=2208855100; ww_orig_ref=\"http%3A%2F%2Fwenwen.sogou.com%2Fs%2F%3Fw%3D%25E6%2595%25B0%25E5%25AD%25A6%25E7%259F%25A5%25E8%25AF%2586%25E6%2589%258B%25E5%25B7%25A5%25E5%2588%25B6%25E4%25BD%259C%26st%3D4\"; ld=blllllllll2glh9nlllllVtRwyclllllnPGVwZllll9lllllxylll5@@@@@@@@@@; MAIN_SESSIONID=n111i0ehc1fgbna386m03gin89ti.n11\',\n\'Host\':\'wenwen.sogou.com\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.76 Mobile Safari/537.36\',\n    }\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        \'\'\'\n        with open(\'/apps/home/worker/100zhan.txt\') as f:\n            for line in f:\n                self.crawl(\'http://wenwen.sogou.com/s/?w=\' + line.strip(\'\\n\') + \'&search=%E6%90%9C%E7%8B%97%E6%90%9C%E7%B4%A2&st=4&ch=11\', save={\'subject\': line.strip(\'\\n\')}, callback=self.index_page)\n        \'\'\'\n        self.crawl(\'http://wenwen.sogou.com/s/?w=%E6%89%98%E7%A6%8F&st=4&ch=11\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        subject = \'\' #response.save.get(\'subject\',\'\')\n        for each in response.doc(\'a[href^=\"http://wenwen.sogou.com/z/\"]\').items():\n            title = each.text()\n            self.crawl(each.attr.href, save={\'title\': title, \'subject\': subject}, callback=self.detail_page)\n        for each in response.doc(\'a[href^=\"http://wenwen.sogou.com/s/\"]\').items():\n            if \'&pg=\' in each.attr.href:\n                self.crawl(each.attr.href, save={\'subject\': subject}, callback=self.index_page)\n            \n        \n    @config(priority=2)\n    def detail_page(self, response):\n        title = response.save[\'title\'] #response.doc(\'#questionTitle\').text()        \n        \n        content_list = []\n        for info in response.doc(\'.satisfaction-answer\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.question-info > div\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.question-info span\').text()\n            _dic[\'content\'] = info.find(\'.answer-con\').html()\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic))\n        for info in response.doc(\'.default-answer\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.info-wrap\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.time\').text()\n            _dic[\'content\'] = info.find(\'.answer-con\').html()\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic))\n        try:\n            subject = response.save[\'subject\']\n        except:\n            subject = None\n\n        if not content_list:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"subject\": subject,\n            \"answers\": content_list,\n            \"question_detail\": response.doc(\'.question-con\').text(),\n        }',NULL,1.0000,3.0000,1460186026.7081),('qiuzhi_jianliwang','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-07 11:01:22\n# Project: qiuzhi_kuaiji\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/dianzijianlimoban/\', callback=self.index_page)\n\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/gerenjianlimoban/\', callback=self.index_page)\n\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/yingwenjianlimoban/\', callback=self.index_page)\n\n        self.crawl(\'http://jianli.yjbys.com/jianlimoban/qiuzhijianlimoban/\', callback=self.index_page)\n\n\n    @config(age=1* 60)\n    def index_page(self, response):\n        for each in response.doc(\'dt > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.pagelist a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n        \'\'\'\n        \n\n    @config(priority=2)\n    def detail_page(self, response):\n        try:\n            table = \'<table align=\"center\">\' +  response.doc(\'table\').html() + \'</table>\'\n        except:\n            table = \'\'\n        #intro = response.doc(\'.content > p\').text().split()[0]\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.title\').text(),\n            \"content\": response.doc(\'.content\').remove(\'script\').remove(\'div\').remove(\'a\').html().replace(\'jianli.yjbys.com\', \'\').replace(\'yjbys\', \'\').replace(\'\\n\',\'\'), #intro + table, \n            \"data_weight\": 0,\n            \"subject\": u\'求职\',\n            \"bread\": [u\'简历\',],\n            \"class\": 46,\n            \"source\": u\'简历网\',\n        }\n',NULL,1.0000,1.0000,1471329820.5476),('qiuzhi_shixisheng','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-16 10:50:19\n# Project: qiuzhi_shixisheng\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.yingjiesheng.com/commend-parttime-1.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.bg_0\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'title\'] = each.find(\'a\').text()\n            _save[\'date\'] = each.find(\'.date\').text()\n            #print _save[\'title\'], _save[\'date\']\n            self.crawl(url,save=_save, callback=self.detail_page)\n\n        for each in response.doc(\'.bg_1\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'title\'] = each.find(\'a\').text()\n            _save[\'date\'] = each.find(\'.date\').text()\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.page > a\').items():\n            url = each.attr.href\n            self.crawl(url, callback=self.index_page)\n        \'\'\'\n    \n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        res[\"content\"] = response.doc(\'.jobIntro\').html()\n        res[\"subject\"] = u\'求职\'\n        res[\"source\"] = u\'yingjiesheng\'\n        res[\'bread\'] = [u\'实习生\',]\n        if not res[\'content\']: \n            res[\'content\'] = response.doc(\'.job_list\').html()\n        res[\'class\'] = 46 \n        res[\'data_weight\'] = 0\n        return res\n',NULL,1.0000,3.0000,1471329837.9661),('qiuzhi_shixisheng_shixisheng','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-11 19:09:16\n# Project: qiuzhi_shixi_shixisheng\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    dic = {\n    \"opj_p0wwm34iuy3y\": [\n        {\n            \"name\": \"教育\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ydckmvemhhj9\",\n                    \"name\": \"教务\"\n                },\n                {\n                    \"uuid\": \"opj_i2atsrq7meb2\",\n                    \"name\": \"教师\"\n                },\n                {\n                    \"uuid\": \"opj_gsfxm6khk1mw\",\n                    \"name\": \"幼教\"\n                },\n                {\n                    \"uuid\": \"opj_anu5gkjal5cq\",\n                    \"name\": \"培训\"\n                },\n                {\n                    \"uuid\": \"opj_ovpebzlsbjbs\",\n                    \"name\": \"课程\"\n                }\n            ]\n        },\n        {\n            \"name\": \"咨询\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_v7lu487vtqcj\",\n                    \"name\": \"咨询/顾问\"\n                }\n            ]\n        }\n    ],\n    \"opj_4mun5l8wqssz\": [\n        {\n            \"name\": \"软件\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_twuuznrzmbdu\",\n                    \"name\": \"IOS\"\n                },\n                {\n                    \"uuid\": \"opj_frm2gpvkcpua\",\n                    \"name\": \"数据库\"\n                },\n                {\n                    \"uuid\": \"opj_worxhycw0gym\",\n                    \"name\": \"C#/.NET\"\n                },\n                {\n                    \"uuid\": \"opj_fwnc2trdffiw\",\n                    \"name\": \"Hadoop\"\n                },\n                {\n                    \"uuid\": \"opj_edurcbhia2n3\",\n                    \"name\": \"Android\"\n                },\n                {\n                    \"uuid\": \"opj_qc9afdwzavw9\",\n                    \"name\": \"算法\"\n                },\n                {\n                    \"uuid\": \"opj_dzsdsfwm0ntt\",\n                    \"name\": \"IT运维\"\n                },\n                {\n                    \"uuid\": \"opj_bgq6sdxqx1i8\",\n                    \"name\": \"Python\"\n                },\n                {\n                    \"uuid\": \"opj_rjdqlspswhli\",\n                    \"name\": \"云计算/大数据\"\n                },\n                {\n                    \"uuid\": \"opj_lsvexg8ehsjx\",\n                    \"name\": \"Node.js\"\n                },\n                {\n                    \"uuid\": \"opj_wbbl3ezi6ecv\",\n                    \"name\": \"数据挖掘\"\n                },\n                {\n                    \"uuid\": \"opj_bkbverpen0w3\",\n                    \"name\": \"PHP\"\n                },\n                {\n                    \"uuid\": \"opj_qpgp7xcerkex\",\n                    \"name\": \"Ruby/Perl\"\n                },\n                {\n                    \"uuid\": \"opj_ykka3ldcn21u\",\n                    \"name\": \"测试\"\n                },\n                {\n                    \"uuid\": \"opj_dxnurenognxo\",\n                    \"name\": \"Java\"\n                },\n                {\n                    \"uuid\": \"opj_jlm8stx4zz26\",\n                    \"name\": \"C/C++\"\n                },\n                {\n                    \"uuid\": \"opj_snjlgjshvfwn\",\n                    \"name\": \"前端\"\n                }\n            ]\n        },\n        {\n            \"name\": \"运营\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ielyo96oix78\",\n                    \"name\": \"新媒体\"\n                },\n                {\n                    \"uuid\": \"opj_okd5xorkjaks\",\n                    \"name\": \"内容运营\"\n                },\n                {\n                    \"uuid\": \"opj_uu6f6lasdshg\",\n                    \"name\": \"编辑\"\n                },\n                {\n                    \"uuid\": \"opj_ysojgsemw0fe\",\n                    \"name\": \"SEO\"\n                },\n                {\n                    \"uuid\": \"opj_cmp8uy9f6v2m\",\n                    \"name\": \"产品运营\"\n                }\n            ]\n        },\n        {\n            \"name\": \"硬件\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_rnbmq9wmvb54\",\n                    \"name\": \"嵌入式\"\n                },\n                {\n                    \"uuid\": \"opj_an2cblxq5b7w\",\n                    \"name\": \"集成电路\"\n                }\n            ]\n        },\n        {\n            \"name\": \"设计\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_mrxvzqkechw2\",\n                    \"name\": \"Flash\"\n                },\n                {\n                    \"uuid\": \"opj_0ic5vg9cusry\",\n                    \"name\": \"UI/UE\"\n                },\n                {\n                    \"uuid\": \"opj_uvbdobspakmu\",\n                    \"name\": \"特效\"\n                },\n                {\n                    \"uuid\": \"opj_5upcwefmaqhj\",\n                    \"name\": \"网页/美工\"\n                },\n                {\n                    \"uuid\": \"opj_uvhgcoyjkeyl\",\n                    \"name\": \"2D/3D\"\n                }\n            ]\n        },\n        {\n            \"name\": \"通信\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ppqp36lwiw1i\",\n                    \"name\": \"物联网\"\n                },\n                {\n                    \"uuid\": \"opj_gy6kzykxrpiq\",\n                    \"name\": \"射频\"\n                },\n                {\n                    \"uuid\": \"opj_unqfld9abm0b\",\n                    \"name\": \"通信\"\n                }\n            ]\n        },\n        {\n            \"name\": \"产品\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_zbmvshsqlpco\",\n                    \"name\": \"用户研究\"\n                },\n                {\n                    \"uuid\": \"opj_zs7hmkfz698q\",\n                    \"name\": \"产品助理\"\n                }\n            ]\n        }\n    ],\n    \"opj_32vfxjgfgkad\": [\n        {\n            \"name\": \"金融\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_tp0s0i4mg3jc\",\n                    \"name\": \"基金\"\n                },\n                {\n                    \"uuid\": \"opj_nrvscepwebo6\",\n                    \"name\": \"证券\"\n                },\n                {\n                    \"uuid\": \"opj_scgzknrfc9o8\",\n                    \"name\": \"风控\"\n                },\n                {\n                    \"uuid\": \"opj_pr9lpdarivfz\",\n                    \"name\": \"金融\"\n                }\n            ]\n        },\n        {\n            \"name\": \"投资\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_e7omu0xnhmi1\",\n                    \"name\": \"分析师\"\n                },\n                {\n                    \"uuid\": \"opj_swxgid8t2ylp\",\n                    \"name\": \"投资\"\n                }\n            ]\n        },\n        {\n            \"name\": \"法务\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_g3ufnzwf3ic7\",\n                    \"name\": \"合规\"\n                },\n                {\n                    \"uuid\": \"opj_5pv0h0gmmks4\",\n                    \"name\": \"律师\"\n                },\n                {\n                    \"uuid\": \"opj_d00vwoe9xk2f\",\n                    \"name\": \"法务\"\n                }\n            ]\n        },\n        {\n            \"name\": \"银行\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_yiffuqvioqk4\",\n                    \"name\": \"客户经理\"\n                },\n                {\n                    \"uuid\": \"opj_v8dzgkcdej8i\",\n                    \"name\": \"部门经理\"\n                },\n                {\n                    \"uuid\": \"opj_dfhiwt0i19fe\",\n                    \"name\": \"贷款\"\n                },\n                {\n                    \"uuid\": \"opj_a9ayetvpyzdw\",\n                    \"name\": \"大堂经理\"\n                }\n            ]\n        },\n        {\n            \"name\": \"保险\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_p2ifplcmars6\",\n                    \"name\": \"业务\"\n                },\n                {\n                    \"uuid\": \"opj_qww5tuazg0zs\",\n                    \"name\": \"保单\"\n                }\n            ]\n        },\n        {\n            \"name\": \"财会\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_a3hiwtqufpe2\",\n                    \"name\": \"审计\"\n                },\n                {\n                    \"uuid\": \"opj_hxlreh5cirnh\",\n                    \"name\": \"税务\"\n                },\n                {\n                    \"uuid\": \"opj_ydfam0kaxoa7\",\n                    \"name\": \"财务\"\n                },\n                {\n                    \"uuid\": \"opj_b38dn1wq26td\",\n                    \"name\": \"会计/出纳\"\n                }\n            ]\n        }\n    ],\n    \"opj_v7ygyfozpgy2\": [\n        {\n            \"name\": \"商务\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_jlsgwadhibpr\",\n                    \"name\": \"商务\"\n                },\n                {\n                    \"uuid\": \"opj_afjnklufvsfh\",\n                    \"name\": \"招投标\"\n                }\n            ]\n        },\n        {\n            \"name\": \"销售\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_qwrmbcrtzaud\",\n                    \"name\": \"推广\"\n                },\n                {\n                    \"uuid\": \"opj_dkhdshammijx\",\n                    \"name\": \"销售\"\n                }\n            ]\n        },\n        {\n            \"name\": \"公关\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_uuhho9meehxo\",\n                    \"name\": \"媒介\"\n                },\n                {\n                    \"uuid\": \"opj_fli2uh3axvam\",\n                    \"name\": \"公关\"\n                }\n            ]\n        },\n        {\n            \"name\": \"客服\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_eiy2odktulsm\",\n                    \"name\": \"客户服务\"\n                },\n                {\n                    \"uuid\": \"opj_jcvrwy2p6up9\",\n                    \"name\": \"销售支持\"\n                }\n            ]\n        },\n        {\n            \"name\": \"市场\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_xvmkicckmgan\",\n                    \"name\": \"渠道\"\n                },\n                {\n                    \"uuid\": \"opj_xtkcoq6vmcdr\",\n                    \"name\": \"分析/调研\"\n                },\n                {\n                    \"uuid\": \"opj_jf4xyp7jigdd\",\n                    \"name\": \"策划\"\n                },\n                {\n                    \"uuid\": \"opj_icis7su6fxju\",\n                    \"name\": \"品牌\"\n                },\n                {\n                    \"uuid\": \"opj_egxi3f7yog0h\",\n                    \"name\": \"市场\"\n                }\n            ]\n        }\n    ],\n    \"opj_g68s2gm09jdv\": [\n        {\n            \"name\": \"人力资源\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_acrvakvqyzik\",\n                    \"name\": \"人事/HR\"\n                },\n                {\n                    \"uuid\": \"opj_qtc4so3cydmo\",\n                    \"name\": \"招聘\"\n                },\n                {\n                    \"uuid\": \"opj_awun8u4ogl3m\",\n                    \"name\": \"企业文化\"\n                }\n            ]\n        },\n        {\n            \"name\": \"猎头\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ytjvc22kmoob\",\n                    \"name\": \"猎头\"\n                }\n            ]\n        },\n        {\n            \"name\": \"行政\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_y7p3orvwtr2v\",\n                    \"name\": \"行政\"\n                },\n                {\n                    \"uuid\": \"opj_m1mxy4ydijfc\",\n                    \"name\": \"前台\"\n                },\n                {\n                    \"uuid\": \"opj_89efrspgjjcn\",\n                    \"name\": \"助理\"\n                }\n            ]\n        }\n    ],\n    \"opj_cn9j3euwuvtl\": [\n        {\n            \"name\": \"外语\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_gs8yoyp0nefd\",\n                    \"name\": \"英语\"\n                },\n                {\n                    \"uuid\": \"opj_0hdaz9wznwv2\",\n                    \"name\": \"日语\"\n                },\n                {\n                    \"uuid\": \"opj_ys0eyzn5kkbt\",\n                    \"name\": \"翻译\"\n                }\n            ]\n        },\n        {\n            \"name\": \"外贸\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_b0ktbhlgvfnb\",\n                    \"name\": \"报关员\"\n                },\n                {\n                    \"uuid\": \"opj_z2vez1xgsf0x\",\n                    \"name\": \"外贸专员\"\n                }\n            ]\n        }\n    ],\n    \"opj_jxvy9fuotcvj\": [\n        {\n            \"name\": \"广告\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_l7utfoe88ylr\",\n                    \"name\": \"创意\"\n                },\n                {\n                    \"uuid\": \"opj_gojfv3twwx5r\",\n                    \"name\": \"策划\"\n                },\n                {\n                    \"uuid\": \"opj_x3tgzqznj1dj\",\n                    \"name\": \"AE\"\n                }\n            ]\n        },\n        {\n            \"name\": \"编辑\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_mspx4safdbwp\",\n                    \"name\": \"编辑/采编\"\n                },\n                {\n                    \"uuid\": \"opj_2672dgvnktgm\",\n                    \"name\": \"校对/排版\"\n                }\n            ]\n        },\n        {\n            \"name\": \"设计\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_fxel0nldueqw\",\n                    \"name\": \"美术设计\"\n                },\n                {\n                    \"uuid\": \"opj_zrgyhehcendi\",\n                    \"name\": \"工业设计\"\n                },\n                {\n                    \"uuid\": \"opj_izptkxcdrwdb\",\n                    \"name\": \"平面设计\"\n                },\n                {\n                    \"uuid\": \"opj_2xaf5axvltwq\",\n                    \"name\": \"视觉设计\"\n                }\n            ]\n        },\n        {\n            \"name\": \"媒体\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_mgnrscpzykac\",\n                    \"name\": \"记者\"\n                },\n                {\n                    \"uuid\": \"opj_rc5eetcarzu5\",\n                    \"name\": \"主持/播音\"\n                },\n                {\n                    \"uuid\": \"opj_vu4ra0k6bmiz\",\n                    \"name\": \"编导\"\n                }\n            ]\n        },\n        {\n            \"name\": \"艺术\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_bd9fqvuufgg6\",\n                    \"name\": \"演艺\"\n                },\n                {\n                    \"uuid\": \"opj_xziotarow0eq\",\n                    \"name\": \"摄影\"\n                }\n            ]\n        }\n    ],\n    \"opj_e4tvvgcavs2j\": [\n        {\n            \"name\": \"体育快消\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ilurdnsopa6g\",\n                    \"name\": \"快消\"\n                },\n                {\n                    \"uuid\": \"opj_z8dhzwpigjlk\",\n                    \"name\": \"体育\"\n                }\n            ]\n        },\n        {\n            \"name\": \"机械制造\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_vbauw4zsmsnn\",\n                    \"name\": \"质量\"\n                },\n                {\n                    \"uuid\": \"opj_oheplpt1dlow\",\n                    \"name\": \"机械设计\"\n                },\n                {\n                    \"uuid\": \"opj_urjflgxfv4dd\",\n                    \"name\": \"生产\"\n                },\n                {\n                    \"uuid\": \"opj_hf93dmd6ombh\",\n                    \"name\": \"安全\"\n                },\n                {\n                    \"uuid\": \"opj_327srkkqritm\",\n                    \"name\": \"设备\"\n                },\n                {\n                    \"uuid\": \"opj_un7mpmwbhwhs\",\n                    \"name\": \"自动化\"\n                }\n            ]\n        },\n        {\n            \"name\": \"物流采购\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_bcwafqijkvfc\",\n                    \"name\": \"采购\"\n                },\n                {\n                    \"uuid\": \"opj_bqhq5es6a7mk\",\n                    \"name\": \"供应链\"\n                },\n                {\n                    \"uuid\": \"opj_4zrljtoab9qg\",\n                    \"name\": \"物流\"\n                }\n            ]\n        },\n        {\n            \"name\": \"建筑房产\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_fvmfqadehva7\",\n                    \"name\": \"城规/市政\"\n                },\n                {\n                    \"uuid\": \"opj_itkittci96ka\",\n                    \"name\": \"工程造价\"\n                },\n                {\n                    \"uuid\": \"opj_kxy7uvay18ez\",\n                    \"name\": \"建筑\"\n                },\n                {\n                    \"uuid\": \"opj_waxyjpf3jy45\",\n                    \"name\": \"土木\"\n                },\n                {\n                    \"uuid\": \"opj_prf03bdanvzr\",\n                    \"name\": \"园林\"\n                },\n                {\n                    \"uuid\": \"opj_svfrapw9izjb\",\n                    \"name\": \"地产开发/策划\"\n                },\n                {\n                    \"uuid\": \"opj_0ajhcvn62omh\",\n                    \"name\": \"房产销售\"\n                },\n                {\n                    \"uuid\": \"opj_4bu0spgnss1c\",\n                    \"name\": \"给排水\"\n                },\n                {\n                    \"uuid\": \"opj_kj9bj8nck8n2\",\n                    \"name\": \"物业管理\"\n                }\n            ]\n        },\n        {\n            \"name\": \"生物医疗\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_a6wuiaia7zz7\",\n                    \"name\": \"医生\"\n                },\n                {\n                    \"uuid\": \"opj_fntj9r5yanty\",\n                    \"name\": \"医药\"\n                },\n                {\n                    \"uuid\": \"opj_cvbkzomriwym\",\n                    \"name\": \"生物\"\n                },\n                {\n                    \"uuid\": \"opj_jqoi0qrflscd\",\n                    \"name\": \"护理\"\n                }\n            ]\n        },\n        {\n            \"name\": \"能源环保\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_odmfoqvehqd0\",\n                    \"name\": \"矿产\"\n                },\n                {\n                    \"uuid\": \"opj_y5xs9hzcw8rw\",\n                    \"name\": \"能源\"\n                },\n                {\n                    \"uuid\": \"opj_ctqjqjhzzftu\",\n                    \"name\": \"环保\"\n                }\n            ]\n        },\n        {\n            \"name\": \"食品材料\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_grb2ruhukb8i\",\n                    \"name\": \"材料\"\n                },\n                {\n                    \"uuid\": \"opj_8mj8jkeniaev\",\n                    \"name\": \"食品\"\n                }\n            ]\n        },\n        {\n            \"name\": \"NGO公益\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_ijdiapsz9edf\",\n                    \"name\": \"志愿者\"\n                }\n            ]\n        }\n    ],\n    \"opj_texrs8przurn\": [\n        {\n            \"name\": \"电子\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_mpcknopmcbsn\",\n                    \"name\": \"光电\"\n                },\n                {\n                    \"uuid\": \"opj_gdsqvqa3xrjf\",\n                    \"name\": \"半导体/芯片\"\n                },\n                {\n                    \"uuid\": \"opj_4ydypxlridqr\",\n                    \"name\": \"电子工程\"\n                }\n            ]\n        },\n        {\n            \"name\": \"电气\",\n            \"child\": [\n                {\n                    \"uuid\": \"opj_3zamopr7jxkd\",\n                    \"name\": \"电气设计\"\n                },\n                {\n                    \"uuid\": \"opj_ufpnyxgzxeyj\",\n                    \"name\": \"电气工程\"\n                }\n            ]\n        }\n    ]\n}\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k in self.dic:\n            for childs in self.dic[k]:\n                for child in childs[\'child\']:\n                    name = child[\'name\']\n                    for i in range(1, 5):\n                        self.crawl(\'http://www.shixiseng.com/interns?k=\' + name + \'&p=%d\'%i, callback=self.index_page)\n\n    @config(age=1 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.job_inf_inf\').items():\n            title = \'【%s】%s招聘%s实习生\'%(each.find(\'.addr_box > span\').text(), each.find(\'.company_name\').text() , each.find(\'a > h3\').text())\n            self.crawl(each.find(\'a\').attr.href, save={\'title\': title}, callback=self.detail_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.parent_job > li\').items():\n            self.crawl(each.attr.href, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        title = response.save[\'title\']\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"content\": response.doc(\'.dec_content\').html(),\n            \"date\": response.doc(\'.update_time\').text()[:10],\n            \"data_weight\": 0,\n            \"subject\": u\'求职\',\n            \"bread\": [u\'实习生\',],\n            \"class\": 46,\n            \"source\": u\'shixisheng\',\n        }\n',NULL,1.0000,3.0000,1471329830.4341),('qiuzhi_xuanjianghui_inc','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-15 17:25:49\n# Project: qiuzhi_yingjiesheng\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://my.yingjiesheng.com/xuanjianghui.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.bg0\').items():\n            tr = [info for info in  each.find(\'td\').items()]\n            _dict = {}\n            _dict[\'city\'] =  tr[0].text()\n            _dict[\'date\'] = tr[1].text()\n            _dict[\'hour\'] = tr[2].find(\'img\').attr.src\n            _dict[\'company\'] = tr[3].text()\n            url = tr[3].find(\'a\').attr.href\n            _dict[\'school\'] = tr[4].text()\n            _dict[\'address\'] = tr[5].text()\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        for each in response.doc(\'.bg1\').items():\n            tr = [info for info in  each.find(\'td\').items()]\n            _dict = {}\n            _dict[\'city\'] =  tr[0].text()\n            _dict[\'date\'] = tr[1].text()\n            _dict[\'hour\'] = tr[2].find(\'img\').attr.src\n            _dict[\'company\'] = tr[3].text()\n            url = tr[3].find(\'a\').attr.href\n            _dict[\'school\'] = tr[4].text()\n            _dict[\'address\'] = tr[5].text()\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        #for each in response.doc(\'.page > a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        res[\"title\"] = response.doc(\'.xjh_infos_h1\').text()\n        res[\"content\"] = response.doc(\'.colspan2\').html()\n        res[\'bread\'] = [u\'宣讲会\']\n        res[\'subject\'] = u\'求职\'\n        res[\'source\'] = u\'yingjiesheng\'\n        return res\n',NULL,1.0000,3.0000,1471329841.7512),('qiuzhi_zhaopin','qiuzhi','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-16 10:50:19\n# Project: qiuzhi_shixi\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.yingjiesheng.com/commend-fulltime-1.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.bg_0\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'title\'] = each.find(\'a\').text()\n            _save[\'date\'] = each.find(\'.date\').text()\n            #print _save[\'title\'], _save[\'date\']\n            self.crawl(url,save=_save, callback=self.detail_page)\n\n        for each in response.doc(\'.bg_1\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'title\'] = each.find(\'a\').text()\n            _save[\'date\'] = each.find(\'.date\').text()\n            self.crawl(url, save=_save, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.page > a\').items():\n            url = each.attr.href\n            self.crawl(url, callback=self.index_page)\n        \'\'\'\n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        res[\"content\"] = response.doc(\'.jobIntro\').html()\n        res[\"subject\"] = u\'求职\'\n        res[\"source\"] = u\'yingjiesheng\'\n        res[\'bread\'] = [u\'招聘信息\',]\n        if not res[\'content\']: \n            res[\'content\'] = response.doc(\'.job_list\').html()\n        res[\'class\'] = 46 \n        res[\'data_weight\'] = 0\n        return res\n',NULL,1.0000,3.0000,1471329846.4821),('qq_wenda','wenda','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-09 15:09:15\n# Project: qq_wenda\n\nfrom pyspider.libs.base_handler import *\nimport time\nimport datetime\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\n\ndef get_Date(d):\n    if not d or u\'分钟\' in d or u\'小时\' in d:\n        return time.strftime(\'%Y-%m-%d\',time.localtime())\n    elif u\'1天前\' in d:\n         now_time = datetime.datetime.now()\n         yes_time = now_time + datetime.timedelta(days=-1)\n         return yes_time.strftime(\'%Y-%m-%d\')\n    elif u\'2天前\' in d:\n         now_time = datetime.datetime.now()\n         yes_time = now_time + datetime.timedelta(days=-2)\n         return yes_time.strftime(\'%Y-%m-%d\')\n    else:\n        return d\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.2\',\n        \'headers\':{\n            \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://wenwen.qq.com/cate/?cid=0&tp=6&pg=\', callback=self.list_page)\n\n    \n    @config(age=24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'ul.category-list li a\').items():\n            _dict = {}\n            _dict[\'bread\'] = each.text()\n            #_dict[\'bread\'].append(each.text())\n            self.crawl(each.attr.href, save = _dict,callback=self.type_page)\n            \n    \n    @config(age=24 * 60 * 60)\n    def type_page(self, response):\n        for each in response.doc(\'ul.category-list li a\').items():\n            _dict = {}\n            _dict[\'bread\'] = [ response.save.get(\'bread\') , each.text() ]\n            \n            #_dict[\'bread\'].append(each.text())\n            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\n        \n    @config(age=24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'div.lib-list li\').items():\n            url = each.find(\'.qa-title a\').eq(0).attr.href\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.qa-title a\').eq(0).text()\n            #create_time = each.find(\'div.qa-info .t\').text()\n            #_dict[\'create_time\'] = get_Date(create_time)\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'.pagination a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            #_dict[\'title\'] = each.find(\'.qa-title a\').eq(0).text()\n            #create_time = each.find(\'div.qa-info .t\').text()\n            #_dict[\'create_time\'] = get_Date(create_time)\n            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\n            \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'question_detail\'] = response.doc(\'h3#questionTitle\').html()\n        res_dict[\'create_user\'] = response.doc(\'.question-head a\').eq(0).text()\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = \'搜狗问问\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'create_time\'] = response.doc(\'.question-head .time\').text().split()[0]\n        answers_list = []\n        for each in response.doc(\'.satisfaction-answer\').items():\n                _dict = {}\n                #_dict[\'img\'] = each.find(\'.user-conent img\').eq(0).attr.src\n                _dict[\'user_name\'] = each.find(\'.user-conent .question-info\').text().split()[0]\n                _dict[\'create_time\'] = each.find(\'.user-conent .question-info\').text().split()[-1]\n                _dict[\'content\'] = each.find(\'.answer-con\').html()\n                answers_list.append(_dict)\n        for each in response.doc(\'.answer-wrap .default-answer\').items():\n                _dict = {}\n                #_dict[\'img\'] = each.find(\'.user-conent img\').eq(0).attr.src\n                _dict[\'user_name\'] = each.find(\'.user-conent .question-info\').text().split()[0]\n                _dict[\'create_time\'] = each.find(\'.user-conent .question-info\').text().split()[-1]\n                _dict[\'content\'] = each.find(\'.answer-con\').html()\n                answers_list.append(_dict)\n        if not answers_list:\n            return\n        res_dict[\'answers\'] = answers_list\n        return res_dict\n',NULL,1.0000,3.0000,1469236598.0451),('question_360',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-05 10:49:43\n# Project: qa_360\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    \'proxy\': \'111.13.136.46:80\',\n \"headers\":{\n         \n\'Cache-Control\': \'no-cache\',\n\'Connection\': \'keep-alive\',\n\'Host\': \'wenda.so.com\',\n\'Pragma\': \'no-cache\',\n\'Upgrade-Insecure-Requests\': \'1\',\n\'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        #with open(\'/apps/home/worker/xuzhihao/school_list\') as f:\n        #    for line in f:\n        #        word = line.strip(\'\\t\')\n        word = u\'高考\'\n        self.crawl(\'http://wenda.so.com/search/?q=\' + word + \'&filt=20\', headers= self.crawl_config[\'headers\'], save={\'school\': word}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        school = response.save[\'school\']\n        for each in response.doc(\'.qa-i-hd a\').items():\n            title = each.text()\n            self.crawl(each.attr.href,save={\'title\': title, \'school\': response.save[\'school\']}, callback=self.detail_page)\n        for each in response.doc(\'.pagination a\').items():\n            self.crawl(each.attr.href, save={\'school\': school}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for info in response.doc(\'.mod-resolved-ans > .bd\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.text > div a\').text()\n            _dic[\'date\'] = info.find(\'.text > span\').text().split()[-1].replace(\'.\',\'-\')\n            _dic[\'content\'] = info.find(\'.resolved-cnt\').html()\n            _dic[\'avatar\'] = info.find(\'.info img\').attr.src\n            content_list.append((_dic))\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.js-ask-title\').text(),\n            \"school\": response.save[\'school\'],\n            \"answers\": content_list,\n            \"question_detail\": response.doc(\'.q-cnt\').text(),\n        }\n',NULL,10.0000,10.0000,1469000269.4831),('question_baidu',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-26 19:40:36\n# Project: question_baidu\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'headers\': {\n            \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\',\n            \'Host\':\'zhidao.baidu.com\',\n            \'Referer\': \'http://zhidao.baidu.com/question/295410048.html?fr=iks&word=%C8%FD%D1%C7%B3%C7%CA%D0%D6%B0%D2%B5%D1%A7%D4%BA&ie=gbk\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for line in open(\'/apps/home/worker/xuzhihao/school_list\'):\n            data = line.strip(\'\\n\')\n            self.crawl(\'http://zhidao.baidu.com/search?ct=17&pn=0&tn=ikaslist&rn=10&word=\' + data + \'&fr=wwwt\', save={\'ques\': data}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        ques = response.save[\'ques\']\n        for each in response.doc(\'.dl > .line > a\').items():\n            url = each.attr.href\n            if url.startswith(\'http://zhidao.baidu.com/\'):\n                self.crawl(each.attr.href,save={\'ques\': ques}, fetch_type=\'js\', callback=self.detail_page)\n               \n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for info in response.doc(\'.answer\').items(): \n            _dic = {}\n            _dic[\'name\'] = info.find(\'.question-info > div\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.question-info span\').text()\n            _dic[\'content\'] = info.find(\'.content\').html()\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic))\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.ask-title\').text(),\n            \"content\": content_list,\n            \"ques\": response.save[\'ques\'],\n        }\n',NULL,1.0000,3.0000,1462354837.4214),('question_baidu_2','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-26 19:40:36\n# Project: question_baidu\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'headers\': {\n            \'Cookie\': \'ipInfoCity=undefined; BIDUPSID=E86D6D1C5EB675F2CB9CF17431C6D0F8; PSTM=1456297492; IK_CID_84=1; IK_CID_85=1; IK_CID_74=12; IK_CID_83=6; BDUSS=mhPREFSNVN5QmJhWWlTQU05OHlrTFJ1RXNncU8wTjJ4ZzlDYXhFV3JafklRRXBYQVFBQUFBJCQAAAAAAAAAAAEAAACa-JFPsNm80rulwaq4-sut0acAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMizIlfIsyJXa; plus_cv=0::m:1-nav:6290bd16-hotword:40d8db89; BAIDUID=12455DCFF0A2EEB24A1668FB3AC77E9E:FG=1; IK_CID_1031=1; IK_CID_78=1; IK_CID_82=2; IK_CID_80=3; BDRCVFR[HE-BYYE3Dh3]=I67x6TjHwwYf0; pgv_pvi=2805833728; pgv_si=s8603317248; SFSSID=ts1e5cc3g3392p34livqhqn532; MCITY=-%3A; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; H_PS_PSSID=18880_18286_1453_7477_19671_18280_19805_19899_19559_19808_19843_19902_19860_15142_11478_19910; IK_USERVIEW=1; Hm_lvt_6859ce5aaf00fb00387e6434e4fcc925=1461937476,1461937578,1461938068,1462354380; Hm_lpvt_6859ce5aaf00fb00387e6434e4fcc925=1462354429; IK_12455DCFF0A2EEB24A1668FB3AC77E9E=8; IK_CID_1=5; Hm_lvt_16bc67e4f6394c05d03992ea0a0e9123=1461671023,1462354505; Hm_lpvt_16bc67e4f6394c05d03992ea0a0e9123=1462354547\',\n        \'Host\':\'zhidao.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\',\n        \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\',\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        #for line in open(\'/apps/home/worker/xuzhihao/school_list\'):\n        #    data = line.strip(\'\\n\')\n        data = \'高考\'\n        self.crawl(\'http://zhidao.baidu.com/search?ct=17&pn=0&tn=ikaslist&rn=10&word=\' + data + \'&fr=wwwt\', save={\'school\': data}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.dl > .line > a\').items():\n            self.crawl(each.attr.href, save={\'school\': response.save[\'school\']}, callback=self.detail_page)\n        for each in response.doc(\'.list-inner div > a\').items():\n            self.crawl(each.attr.href, save={\'school\': response.save[\'school\']}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        _dic = {}\n        _dic[\'content\'] = response.doc(\'.quality-content-detail\').html()\n        content_list.append(_dic)\n        for info in response.doc(\'.answer\').items(): \n            _dic = {}\n            _dic[\'name\'] = info.find(\'.question-info > div\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.pos-time\').text()\n\n            content = info.find(\'.best-text\').html()\n            if not content:\n                content = info.find(\'.answer-text\').html()\n            if not content:\n                content = info.html()\n            _dic[\'content\'] = content\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic))\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.ask-title\').text(),\n            \"content\": content_list,\n            \"school\": response.save[\'school\'],\n        }',NULL,10.0000,10.0000,1472624360.0199),('question_sogou','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-02 14:39:04\n# Project: sogou_wenwen\n\nfrom pyspider.libs.base_handler import *\nimport re\nimport pyquery\nimport json\n\nclass Handler(BaseHandler):\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for line in open(\'/apps/home/worker/xuzhihao/question\'):\n            word = line.split()[0]\n            subject = line.split()[-1]\n            self.crawl(\'http://wenwen.sogou.com/s/?w=\' + word + \'&st=4&ch=11\', save={\'ques\': line.split()}, callback=self.index_page)\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        ques = response.save[\'ques\']\n        for each in response.doc(\'a[href^=\"http://wenwen.sogou.com/z/\"]\').items():\n            title = each.text()\n            self.crawl(each.attr.href, save={\'title\': title, \'ques\': ques}, callback=self.detail_page)\n            break\n        \'\'\'\n        for each in response.doc(\'a[href^=\"http://wenwen.sogou.com/s/\"]\').items():\n            if \'&pg=\' in each.attr.href:\n                self.crawl(each.attr.href, save={\'bread\': bread},  callback=self.index_page)\n        \'\'\'\n            \n        \n    @config(priority=2)\n    def detail_page(self, response):\n        title = response.save[\'title\'] #response.doc(\'#questionTitle\').text()        \n        \n        content_list = []\n        for info in response.doc(\'.satisfaction-answer\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.question-info > div\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.question-info span\').text()\n            _dic[\'content\'] = info.find(\'.answer-con\').html()\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic))\n        for info in response.doc(\'.default-answer\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.info-wrap\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.time\').text()\n            _dic[\'content\'] = info.find(\'.answer-con\').html()\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic))\n        for info in response.doc(\'.resolved > div\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.info-wrap\').text().split(\' \')[0]\n            _dic[\'date\'] = info.find(\'.time\').text()\n            _dic[\'content\'] = info.find(\'.answer-con\').html()\n            _dic[\'avatar\'] = info.find(\'.user-pic img\').attr.src\n            content_list.append((_dic)) \n        try:\n            subject = response.save[\'bread\']\n        except:\n            subject = None\n\n        if not content_list:\n            return\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"subject\": subject,\n            \"answers\": content_list,\n            \"ques\": response.save[\'ques\'],\n            \"question_detail\": response.doc(\'.question-con\').text(),\n        }',NULL,1.0000,3.0000,1472624354.2127),('question_wenda_gaokao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-05 14:55:40\n# Project: question_wenda_gaokao\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://gaokao.chsi.com.cn/zxdy/zxs--method-listDefault,forumid-6267733,year-2007,start-0.dhtml\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n\n        for each in response.doc(\'.ulPage a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n        _list = []    \n        for each in response.doc(\'.question_cnt_tr > td > div\').items():\n            answer = each.remove(\'strong\').find(\'.question_a\').html()\n            question = each.remove(\'div\').text()\n            if \'<a\' in answer:\n                self.crawl(each.find(\'a\').attr.href, callback=self.detail_page)\n            else:\n                _list.append({\n                    \"url\": response.url,\n                    \"question\": question,\n                    \"answer\": answer,\n                })\n        return _list\n          \n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n            \"answer\": \'\'.join([\'<p>%s</p>\'%each.text() for each in response.doc(\'p\').items()]),\n            \"question\": response.doc(\'.question_cnt_tr > td > div\').remove(\'div\').text(),\n        }\n',NULL,5.0000,5.0000,1472624351.5562),('sat_zhan_inc','sat','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 18:19:22\n# Project: sat_xiaozhan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    type_dict = {\n        \'yufa\': u\'SAT语法\',\n        \'yuedu\': u\'SAT阅读\',\n        \'xiezuo\': u\'SAT写作\',\n        \'shuxue\': u\'SAT数学\',\n        \'zonghe\': u\'SAT其他\',\n        \'jihua\': u\'备考计划\',\n        \'tifen/beikao\': u\'备考计划\',\n        \'gaofen\': u\'高分心得\',\n        \'fuxi\': u\'复习攻略\',\n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for type, v in self.type_dict.iteritems():\n            self.crawl(\'http://sat.zhan.com/%s/\'%type, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/sat/kaoqian/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/sat/kaohou/\', save={\'bread\': u\'包括指南\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/sat/fukao/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/sat/beikao/\', save={\'bread\': u\'备考计划\'}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.experience-item\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.index-middle-info-3 > .pull-left\').text().split()[0]\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.index-middle-info-2\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        for each in response.doc(\'.things_list > .pull-right\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.padding_right_10\').text()\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.text\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n\n        #for each in response.doc(\'nav a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        content_list = []\n        for each in  response.doc(\'.article-content > p\').items():\n            content_list.append((each.html().replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(u\'小站\',\'\')))\n        content_list = content_list[:-1]\n        if not content_list:\n            return None\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'% v for v in content_list])\n        res_dict[\'subject\'] = u\'SAT\'\n        res_dict[\'source\'] = u\'小站\'\n        res_dict[\'tag\'] = response.doc(\'.tag > a\').text().split(\' \')\n        bread = [res_dict[\'bread\'], ]\n        bread.extend(response.doc(\'.head-crumbs-a-active\').text().split(\' \'))\n        if res_dict[\'date\'][:3] != \'201\':\n            res_dict[\'date\'] = response.doc(\'.pull-left > span\').text().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        res_dict[\'bread\'] = list(set(bread))\n        res_dict[\'class\'] = 31\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471334978.0441),('sdgwy_zhonggong','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-08 18:32:43\n# Project: sdgwy_zhonggong\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.offcn.com/sdgwy/ziliao/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.zg_lm_list > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.zg_lm_2\').text()\n            _dict[\'date\'] = each.find(\'font\').text()\n            self.crawl(each.find(\'.zg_lm_2\').attr.href, save = _dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.a1\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):        \n        return {\n            \"url\": response.url,\n            \'bread\': [u\'公务员\'],\n            \'title\': response.save.get(\'title\'),\n            \'date\': response.save.get(\'date\'),\n            \"html\": response.doc(\'.zg_show_word\').html(),\n            \'source\': u\'中公教育\',\n            \'class\': 36,\n            \'subject\': u\'经验\',\n            \'data_weight\': 0,\n        }',NULL,1.0000,3.0000,1472624744.7933),('SegementFault_wenda','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-05 10:05:14\n# Project: SegementFault_wenda\n\nfrom pyspider.libs.base_handler import *\nimport datetime\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\',\n        \'headers\':{\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'https://segmentfault.com/questions?page=1\', callback=self.index_page)\n\n    def parse_time(self, _str):\n        if u\'小时\' in _str or u\'分钟\' in _str:\n            return datetime.datetime.now().strftime(\'%Y-%m-%d\')\n        if u\'天前\' in _str:\n            try:\n                gap = int(_str.split(u\'天前\')[0].strip(\' \'))\n            except:\n                gap = 0\n            return (datetime.datetime.now() - datetime.timedelta(days=gap)).strftime(\'%Y-%m-%d\')\n        try:\n            if u\'年\' not in _str:\n                year = \'2016\'\n            else:\n                year = _str.split(u\'年\')[0].strip(\' \')\n            \n            month = int(_str.split(u\'年\')[1].split(u\'月\')[0].strip(\' \'))\n            day = int(_str.split(u\'月\')[1].split(u\'日\')[0].strip(\' \'))\n            return \'%s-%.2d-%.2d\'%(year, month, day)\n        except Exception, e:\n            print e\n            return \'\'\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.stream-list__item\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.summary .title\').text()\n            url = each.find(\'.summary .title\').find(\'a\').attr.href\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'.pagination a\').items():\n            self.crawl(each.attr.href,callback=self.index_page)\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'question_detail\'] = response.doc(\'.question\').text()\n        content_list = []\n        for each in response.doc(\'.widget-answers article\').items():\n             info = each.find(\'.answer\').html().replace(\'\\n\', \'\')\n             if info:\n                    _dict = {}\n                    _dict[\'content\'] = info\n                    _dict[\'img\'] = each.find(\'.mr10 img\').attr.src\n                    _dict[\'user_name\'] = each.find(\'.answer__info--author-name\').text()\n                    #print each.find(\'.list-inline\').text()\n                    _dict[\'create_time\'] = self.parse_time(each.find(\'.list-inline\').text())\n                    content_list.append(_dict)\n        if not content_list:\n            return\n        res_dict[\'create_user\'] = response.doc(\'.question__author a\').text().split()[0]\n\n        res_dict[\'create_time\'] = self.parse_time(response.doc(\'.question__author\').remove(\'a\').text())\n        res_dict[\'url\'] = response.url\n        res_dict[\'answers\'] = content_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'source\'] = \'segmentfault\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        return res_dict\n',NULL,1.0000,3.0000,1472624462.2436),('sheying_fsbus','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-11 12:14:31\n# Project: sheying_fsbus\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport cPickle\nimport time\n\nmax_pageno = 5\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".post_box\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\".post_img\").find(\"a\"):\n                ret.append(P(url_node))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        page_node = P(response.doc(\"#page .wp-pagenavi\"))\n        next_page = page_node.find(\".current\").next()\n        if not next_page:\n            return False\n        pageno = re.findall(r\"/page/(\\d+)/\", next_page.attr.href)\n        if not pageno:\n            return False\n        try:\n            pageno = int(pageno[0])\n            if pageno > max_pageno:\n                return False\n        except:\n            return False\n        \n        if next_page:\n            return P(next_page)\n        else:\n            return False\n        \n        return False\n    \n    @staticmethod\n    def parse_detail(response):\n        ret = {}\n        detail_node = P(response.doc(\".post-content\"))\n        if not detail_node:\n            return ret\n        title = detail_node.find(\"h1\").text()\n        detail_node = P(detail_node.find(\"h1\").next().html())\n        detail_node.find(\"center\").remove()\n        \n        detail = detail_node.html()\n        \n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n        \n        return {\"title\": title,\n                \"url\": response.url,\n                \"content\": detail,\n                \"bread\": response.save.get(\"bread\"),\n                \"source\": u\"www.fsbus.com\",\n                \"data_weight\": 0,\n                \"class\": 33,\n                \"subject\": u\"摄影\",\n                \"date\": time.strftime(\"%Y-%m-%d\", time.localtime(time.time())),\n                \"image_list\": image_list,\n                }\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\n            \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.fsbus.com/sheyingjiqiao/\",\n            \"bread\": [\"摄影技巧\"]\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.fsbus.com/sheyingjiaocheng/\",\n            \"bread\": [\"摄影教程\"]\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.fsbus.com/renxiangsheying/\",\n            \"bread\": [\"人像摄影\"]\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.fsbus.com/hunshasheying/\",\n            \"bread\": [\"婚纱摄影\"]\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.fsbus.com/ertongsheying/\",\n            \"bread\": [\"儿童摄影\"]\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.fsbus.com/sheyingqicai/\",\n            \"bread\": [\"器材新闻\"]\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.fsbus.com/sheyinghouqi/\",\n            \"bread\": [\"后期处理\"]\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.fsbus.com/danfanrumen/\",\n            \"bread\": [\"单反入门\"]\n        },\n    ]\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                       },\n                       callback = self.index_page)\n            \n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n',NULL,1.0000,3.0000,1472624560.3058),('sheying_heiguangwang','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-11 12:14:31\n# Project: sheying_fsbus\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport cPickle\nimport time\n\nmax_pageno = 10\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".item\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\".img\").find(\"a\"):\n                ret.append(P(url_node))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        page_node = P(response.doc(\".pager\"))\n        try:\n            next_page = page_node.find(\".a1\")\n            if next_page:\n                current_pageno = page_node.find(\"span\").text()\n                next_pageno = P(next_page[-1]).attr.href.strip(\")\").split(\",\")[-1]\n                if int(next_pageno) > int(current_pageno):\n                    # 限制只抓前20页\n                    if int(next_pageno) > max_pageno:\n                        return False\n                    return P(\'\'\'<a href=\"%s&p=%s\">next</a>\'\'\' % (response.save.get(\"base_url\"), next_pageno))\n                return False\n\n            else:\n                return False\n        except Exception as info:\n            return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        if not re.search(r\"heiguang.com\", response.url):\n            return False\n        ret = {}\n        if response.url.find(\"http://bbs.heiguang.com/\") != -1:\n            title = response.doc(\"#thread_subject\").text()\n            detail = response.doc(\".pcb \").html()\n            date_string = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n            bread = response.save.get(\"bread\")\n        else:\n            title = response.doc(\".artTitle\").text()\n            detail_node = response.doc(\".artContent .article\")\n            detail_node.find(\"hr\").next().remove()\n            detail = detail_node.html()\n            \n            date_string = response.doc(\".artInfo\").find(\"span\").text().strip()\n            if not re.match(r\"\\d{4}-\\d{2}-\\d{2}$\", date_string):\n                date_string = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n\n            bread = response.save.get(\"bread\")\n            if response.url.find(\"photography/syjc/\") != -1:\n                bread = [u\"摄影教程\"]\n                \n        url = response.url\n        if not detail.strip():\n            return False\n        \n        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n        return {\"title\": title,\n                \"url\": url,\n                \"content\": detail,\n                \"bread\": bread,\n                \"source\": u\"www.heiguang.com\",\n                \"data_weight\": 0,\n                \"class\": 33,\n                \"subject\": u\"摄影\",\n                \"date\": date_string,\n                \"image_list\": image_list,\n                }\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, save = response.save, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, callback = spider_handle.index_page)\n        \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(spider_handle, parser, response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.heiguang.com/api.php?op=news_jiekou&a=get_list&cat=32,36,145\",\n            \"bread\": [\"视觉盛宴\"],\n        },\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://www.heiguang.com/api.php?op=news_jiekou&a=get_list&cat=22,24\",\n            \"bread\": [\"摄影资讯\"],\n        },\n    ]\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                           \'base_url\': crawler[\"url\"],\n                       },\n                       callback = self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n',NULL,1.0000,3.0000,1472624567.3600),('sheying_xingshe','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-09 19:50:08\n# Project: sheying_xingshe\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\n\nmax_pageno = 40\n\nclass HrefType(object):\n    FENGNIAO_TRAVEL_XINGSHE = 1\n    FENGNIAO_ACADEMY = 2\n    FENGNIAO_IMAGE = 3\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_list = [\n        # 旅游摄影\n        {\n            \"url\": \"http://travel.fengniao.com/list_1345.html\",\n            \"bread\": [\"行摄资讯\",],\n            \"type\": HrefType.FENGNIAO_TRAVEL_XINGSHE,\n        },\n        {\n            \"url\": \"http://travel.fengniao.com/list_1463.html\",\n            \"bread\": [\"国内旅游\",],\n            \"type\": HrefType.FENGNIAO_TRAVEL_XINGSHE,\n        },\n        {\n            \"url\": \"http://travel.fengniao.com/list_1464.html\",\n            \"bread\": [\"国外旅游\",],\n            \"type\": HrefType.FENGNIAO_TRAVEL_XINGSHE,\n        },\n        {\n            \"url\": \"http://travel.fengniao.com/list_1467.html\",\n            \"bread\": [\"美食摄影\",],\n            \"type\": HrefType.FENGNIAO_TRAVEL_XINGSHE,\n        },\n        {\n            \"url\": \"http://travel.fengniao.com/list_1466.html\",\n            \"bread\": [\"行摄装备\",],\n            \"type\": HrefType.FENGNIAO_TRAVEL_XINGSHE,\n        },\n        {\n            \"url\": \"http://travel.fengniao.com/list_1607.html\",\n            \"bread\": [\"行摄视觉\",],\n            \"type\": HrefType.FENGNIAO_TRAVEL_XINGSHE,\n        },\n        {\n            \"url\": \"http://travel.fengniao.com/list_1343.html\",\n            \"bread\": [\"行摄攻略\",],\n            \"type\": HrefType.FENGNIAO_TRAVEL_XINGSHE,\n        },\n\n\n        # 摄影学院\n        {\n            \"url\": \"http://academy.fengniao.com/list_968.html\",\n            \"bread\": [\"摄影技巧\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://academy.fengniao.com/list_969.html\",\n            \"bread\": [\"后期处理\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://academy.fengniao.com/list_967.html\",\n            \"bread\": [\"单反摄影技巧\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://academy.fengniao.com/list_970.html\",\n            \"bread\": [\"单反入门\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://academy.fengniao.com/list_1335.html\",\n            \"bread\": [\"摄影书籍\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://academy.fengniao.com/list_1501.html\",\n            \"bread\": [\"摄影入门教程\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n\n        # 大师作品\n        {\n            \"url\": \"http://image.fengniao.com/list_1422.html\",\n            \"bread\": [\"大师作品资讯\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://image.fengniao.com/list_1586.html\",\n            \"bread\": [\"图说天下\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://image.fengniao.com/list_1425.html\",\n            \"bread\": [\"影史\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://image.fengniao.com/list_1421.html\",\n            \"bread\": [\"书屋电影\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://image.fengniao.com/list_1587.html\",\n            \"bread\": [\"视觉盛宴\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://image.fengniao.com/list_1556.html\",\n            \"bread\": [\"访谈\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n\n        {\n            \"url\": \"http://auto.fengniao.com/list_1642.html\",\n            \"bread\": [\"行摄自驾\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://auto.fengniao.com/list_1643.html\",\n            \"bread\": [\"行摄自驾\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://auto.fengniao.com/list_1644.html\",\n            \"bread\": [\"行摄自驾图赏\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://auto.fengniao.com/list_1646.html\",\n            \"bread\": [\"汽车文化\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n\n        {\n            \"url\": \"http://qicai.fengniao.com/list_1441.html\",\n            \"bread\": [\"摄影器材故事\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n\n        {\n            \"url\": \"http://qicai.fengniao.com/list_1584.html\",\n            \"bread\": [\"摄影器材美图\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n\n        {\n            \"url\": \"http://qicai.fengniao.com/list_1437.html\",\n            \"bread\": [\"评测试用\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n        {\n            \"url\": \"http://qicai.fengniao.com/list_1436.html\",\n            \"bread\": [\"器材新闻\",],\n            \"type\": HrefType.FENGNIAO_ACADEMY,\n        },\n\n    ]    \n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\'bread\': crawler.get(\"bread\", []), \'href_type\': crawler.get(\"type\", HrefType.FENGNIAO_TRAVEL_XINGSHE)},\n                       callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        bread = response.save.get(\"bread\")\n        href_type = response.save.get(\"href_type\", HrefType.FENGNIAO_TRAVEL_XINGSHE)\n        if href_type in (HrefType.FENGNIAO_TRAVEL_XINGSHE,\n                         HrefType.FENGNIAO_ACADEMY,):\n            try:\n                list_news = P(response.doc(\".list_news\"))\n                if not list_news:\n                    list_news = P(response.doc(\".main .section .gallery\"))\n                    \n                for url_node in list_news.find(\"a\"):\n                    _dict = {\"bread\": bread}\n                    url = P(url_node).attr.href\n                    if re.match(r\"http://[^/]+?fengniao.com/(?:slide/)?\\d+/\\d+(?:_\\d{1,3})?.html\", url):\n                        self.crawl(url, save = response.save, callback=self.detail_page)\n                        \n                page_num = P(response.doc(\".page_num\"))\n                if not page_num:\n                    return False\n                next_page = page_num.find(\".next\")\n                if not next_page:\n                    return False\n                pageno = re.findall(r\"list_\\d+_(\\d+).html\", next_page.attr.href)\n                if not pageno:\n                    return False\n                pageno = int(pageno[0])\n                if pageno > max_pageno:\n                    return False\n                if next_page:\n                    self.crawl(next_page.attr.href, save = response.save, callback=self.index_page)\n                    \n            except Exception as info:\n                print info\n                return False\n            \n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        href_type = response.save.get(\"href_type\", HrefType.FENGNIAO_TRAVEL_XINGSHE)\n        if href_type in (HrefType.FENGNIAO_TRAVEL_XINGSHE,\n                         HrefType.FENGNIAO_ACADEMY):\n            view_more = response.doc(\".mod-btn2\")\n            if view_more:\n                self.crawl(view_more.attr.href, save = response.save, callback=self.detail_page)\n            else:\n                content = P(response.doc(\".cont\"))\n                if content:\n                    try:\n                        title = content.find(\".h2\").text()\n                        if not title:\n                            return False\n                        detail = content.find(\".txt-wrap\").html().strip()\n                        if not detail:\n                            return detail\n                        date_node = content.find(\"span.txt\")\n                        date_string = None\n                        if date_node:\n                            date_list = re.findall(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\", date_node.text())\n                            if len(date_list):\n                                date_string = date_list[0]\n                            else:\n                                return False\n                        else:\n                            return False\n                        detail = re.sub(r\"<a[^>]+?/>\", \"\", detail)\n                        detail = re.sub(r\'\'\'class=\"[^\"]+?\"\'\'\', \"\", detail)\n                        image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n                        save = response.save\n                        ret = {\n                            \"bread\": save.get(\"bread\"),\n                            \"url\": response.url,\n                            \"content\": detail,\n                            \"source\": u\"travel.fengniao.com\",\n                            \"data_weight\": 0,\n                            \"class\": 33,\n                            \"subject\": u\"摄影\",\n                            \"date\": date_string[:10],\n                            \"title\": title,\n                            \"image_list\": image_list,\n                            #\"cover\": image_list[0] if len(image_list) else \"\"\n                        }\n                        return ret\n                    except Exception as info:\n                        return False\n                else:\n                    content = P(response.doc(\".main .section\"))\n                    if content:\n                        try:\n                            title = content.find(\"h1\").text()\n                            if not title:\n                                return False\n                            detail = content.find(\".article\").html().strip()\n                            if not detail:\n                                return detail\n                            date_node = content.find(\"#pubtime_baidu\")\n                            date_string = None\n                            if date_node:\n                                date_list = re.findall(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\", date_node.text())\n                                if len(date_list):\n                                    date_string = date_list[0][:10]\n                                else:\n                                    return False\n                            else:\n                                return False\n                            detail = re.sub(r\"<a[^>]+?/>\", \"\", detail)\n                            detail = re.sub(r\'\'\'class=\"[^\"]+?\"\'\'\', \"\", detail)\n                            image_list = re.findall(r\'\'\'<img[^>]+?src=\"([^\"]+?)\"[^>]+?>\'\'\', detail, re.S)\n                            save = response.save\n                            ret = {\n                                \"bread\": save.get(\"bread\"),\n                                \"url\": response.url,\n                                \"content\": detail,\n                                \"source\": u\"travel.fengniao.com\",\n                                \"data_weight\": 0,\n                                \"class\": 33,\n                                \"subject\": u\"摄影\",\n                                \"date\": date_string[:10],\n                                \"title\": title,\n                                \"image_list\": image_list,\n                                #\"cover\": image_list[0] if len(image_list) else \"\"\n                            }\n                            return ret\n                        except Exception as info:\n                            return False\n                    else:\n                        pigInfoJson = re.findall(r\'\'\'var picInfosJson = \'([^=]+?)\\\'\'\'\', response.doc.html())\n                        if not len(pigInfoJson):\n                            return False\n                        pigInfoJson = json.JSONDecoder().decode(pigInfoJson[0])\n                        image_list = []\n                        image_html = u\"<div><br />\"\n                        for item in pigInfoJson:\n                            if \"pic_url_1920_b\" in item:\n                                image_list.append(item[\"pic_url_1920_b\"])\n                                image_html += u\'\'\'<img src=\"%s\" /><br />\'\'\' % item[\"pic_url_1920_b\"]\n                        image_html += u\"</div>\"\n                                \n                        detail = P(response.doc(\".description .temporary\"))\n                        detail_string = image_html\n                        date_string = None\n                        for elem in detail.find(\"span\"):\n                            if P(elem).attr(\"style\") == \"display:none;\":\n                                html = P(elem).html()\n                                detail_string = html.strip() + detail_string\n                                date_string = re.findall(r\"\\d{4}-\\d{2}-\\d{2}\", html)\n                                if len(date_string):\n                                    date_string = date_string[0]\n                                else:\n                                    return False\n                                break\n                        title = P(response.doc(\"h1.title\"))\n                        if title:\n                            title = re.sub(r\"<span[^>]+?>.*?</span>\", \"\", title.html())\n                        else:\n                            return False\n                            \n                        ret = {\n                                \"bread\": response.save.get(\"bread\"),\n                                \"url\": response.url,\n                                \"content\": detail_string.strip(),\n                                \"source\": u\"travel.fengniao.com\",\n                                \"data_weight\": 0,\n                                \"class\": 33,\n                                \"subject\": u\"摄影\",\n                                \"date\": date_string[:10],\n                                \"title\": title,\n                                \"image_list\": image_list\n                            }\n                        return ret\n                        \n                        \n        else:\n            # TODO\n            return False\n',NULL,1.0000,3.0000,1472624563.3305),('shici2_liuxue86','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 16:05:48\n# Project: shici2_liuxue86\n\nfrom pyspider.libs.base_handler import *\nimport re,json\n\nshangxi_dict = {\n    u\'译文及注释\':\'translation_note\',\n    u\'鉴赏\':\'parse_appreciation\',\n    u\'艺术特色\':\'art_appreciation\',\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'headers\':{\n\'Host\':\'tool.liuxue86.com\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n}\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://tool.liuxue86.com/shici/\',headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.top20 .main_c2 > ul\').eq(0).find(\'a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.shici_wai li > a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pagination_Q a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n        \n    \n    @config(priority=2)\n    def detail_page(self, response):\n        tags = \'\'\n        shici_type = \'\' \n        if u\'类型\' in response.doc(\'.main_content_shiren_left_div > span\').eq(-1).text():\n            shici_type = response.doc(\'.main_content_shiren_left_div > span\').eq(-1).text().split(\':\')[-1].strip()\n        if u\'诗词的标签\' in response.doc(\'.main_conter_right_1 > .top20 > div\').eq(0).find(\'p\').text():\n            tags = response.doc(\'.main_conter_right_1 > .top20 > div\').eq(0).remove(\'p\').text().replace(\' \',\',\')\n        _dict = {\n            \'translation_note\':\'\',\n            \'parse_appreciation\':\'\',\n            \'art_appreciation\':\'\',\n        }\n        for each in response.doc(\'.content_shiren_wenben div.wenben_div\').items():\n            if each.text() not in shangxi_dict.keys():\n                continue\n            each1 = each.next().next()\n            con = \'\'\n            #print each.html()\n            while each1.html() and \'<h2\' not in each1.html():\n                con += each1.html().strip()\n                each1 = each1.next()\n            _dict[shangxi_dict[each.text()]] = con\n            \n        res = {\n            \n            \"url\": response.url,\n            \"author_url\":response.doc(\'span > a\').eq(1).attr.href,\n            \"name\":response.doc(\'h1 > span\').text().strip(),\n            \"type\":shici_type,\n            \"tags\":tags,    \n            \"author\":response.doc(\'.main_content_shiren_left_div > span > a\').eq(1).text().strip(),\n            \"dynasty\":response.doc(\'.main_content_shiren_left_div > span > a\').eq(0).text().strip(),\n            \"content\":response.doc(\'.main_content_shiren_left_div2\').remove(\'span\').html().strip(),\n            \"translation_note\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'translation_note\']).replace(\'</a>\',\'\'),\n            \"parse_appreciation\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'parse_appreciation\']).replace(\'</a>\',\'\'),\n            \"art_appreciation\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'art_appreciation\']).replace(\'</a>\',\'\'),\n        }\n        return res\n',NULL,1.0000,3.0000,1472624739.5914),('shici_liuxue86','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 15:48:07\n# Project: shici_liuxue86\n\nfrom pyspider.libs.base_handler import *\nimport re,json\n\nshangxi_dict = {\n    u\'译文及注释\':\'translation_note\',\n    u\'鉴赏\':\'parse_appreciation\',\n    u\'艺术特色\':\'art_appreciation\',\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'headers\':{\n\'Host\':\'tool.liuxue86.com\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n}\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://tool.liuxue86.com/shici/\',headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.top20 .main_c2 > ul\').eq(0).find(\'a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.shici_wai li > a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pagination_Q a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n        \n    \n    @config(priority=2)\n    def detail_page(self, response):\n        tags = \'\'\n        shici_type = \'\' \n        if u\'类型\' in response.doc(\'.main_content_shiren_left_div > span\').eq(-1).text():\n            shici_type = response.doc(\'.main_content_shiren_left_div > span\').eq(-1).text().split(\':\')[-1].strip()\n        if u\'诗词的标签\' in response.doc(\'.main_conter_right_1 > .top20 > div\').eq(0).find(\'p\').text():\n            tags = response.doc(\'.main_conter_right_1 > .top20 > div\').eq(0).remove(\'p\').text().replace(\' \',\',\')\n        _dict = {\n            \'translation_note\':\'\',\n            \'parse_appreciation\':\'\',\n            \'art_appreciation\':\'\',\n        }\n        for each in response.doc(\'.content_shiren_wenben div.wenben_div\').items():\n            if each.text() not in shangxi_dict.keys():\n                continue\n            each1 = each.next().next()\n            con = \'\'\n            #print each.html()\n            while each1.html() and \'<h2\' not in each1.html():\n                con += each1.html().strip()\n                each1 = each1.next()\n            _dict[shangxi_dict[each.text()]] = con\n            \n        res = {\n            \n            \"url\": response.url,\n            \"author_url\":response.doc(\'span > a\').eq(1).attr.href,\n            \"name\":response.doc(\'h1 > span\').text().strip(),\n            \"type\":shici_type,\n            \"tags\":tags,    \n            \"author\":response.doc(\'.main_content_shiren_left_div > span > a\').eq(1).text().strip(),\n            \"dynasty\":response.doc(\'.main_content_shiren_left_div > span > a\').eq(0).text().strip(),\n            \"content\":response.doc(\'.main_content_shiren_left_div2 > p\').html().strip(),\n            \"translation_note\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'translation_note\']).replace(\'</a>\',\'\'),\n            \"parse_appreciation\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'parse_appreciation\']).replace(\'</a>\',\'\'),\n            \"art_appreciation\":re.sub(r\'<a[^>]+>\',\'\',_dict[\'art_appreciation\']).replace(\'</a>\',\'\'),\n        }\n        return res\n',NULL,1.0000,3.0000,1472624343.0640),('shiren2_liuxue86','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 09:43:29\n# Project: shiren2_liuxue86\n\n\nfrom pyspider.libs.base_handler import *\nimport re,json\nfrom pyquery import PyQuery as pq \n\nshangxi_dict = {\n    u\'家庭成员\':\'family\',\n    u\'主要成就\':\'achievement\',\n    u\'文学成就\':\'achievement\',\n    u\'为官生涯\':\'achievement\',\n    u\'为政举措\':\'achievement\',\n    u\'生平\':\'life_story\',\n    u\'评价\':\'evaluation\',\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'headers\':{\n\'Host\':\'tool.liuxue86.com\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n}\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://tool.liuxue86.com/shici/\',headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.main_c2\').eq(0).find(\'a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.clearfix.top20 li > a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pagination_Q a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n        \n    \n    @config(priority=2)\n    def detail_page(self, response):\n        _dict = {\n            \'life_story\':\'\',\n            \'achievement\':\'\',\n            \'family\':\'\',\n            \'evaluation\':\'\',\n        }\n        \n        \n        \n        if pq(url=\'http://baike.baidu.com/search/word?word=\'+response.doc(\'h1 > span\').text().strip(),encoding=\'utf8\'):\n            baike = pq(url=\'http://baike.baidu.com/search/word?word=\'+response.doc(\'h1 > span\').text().strip(),encoding=\'utf8\')\n            for each in baike.find(\'.para-title\').remove(\'span\').items():\n                #print each.find(\'h2\').text()\n                if u\'评价\' in each.find(\'h2\').text():\n                    each1 = each.next()\n                    con = \'\'\n                    #print each.html()\n                    while each1.text() and \'anchor-list\' not in each1.outerHtml():\n                        con += each1.remove(\'img\').remove(\'.description\').html().strip()\n                        each1 = each1.next()\n                        _dict[shangxi_dict[u\'评价\']] += \'<p>\'+re.sub(\'<a[^>]+>\',\'\',con).replace(\'</a>\',\'\')+\'</p>\'\n        \n        \n        \n        \n        author_introduction = \'\'\n        if u\'来自百科\' in response.doc(\'.main_content_shiren_left > p\').eq(1).text():\n            author_introduction = response.doc(\'.main_content_shiren_left > p\').eq(1).text().split(u\'来自百科：\')[1].replace(u\'无\',\'\').strip()\n        \n        \n        \n        \n        for each in response.doc(\'.content_shiren_wenben h2\').items():\n            if each.text() not in shangxi_dict.keys():\n                continue\n            if each.next().text():\n                each1 = each.next()\n            else:\n                each1 = each.next().next()\n            con = \'\'\n            #print each.html()\n            while each1.html() and \'<h2\' not in each1.outerHtml():\n                con += each1.html().strip()\n                each1 = each1.next()\n            _dict[shangxi_dict[each.text()]] += \'<p>\'+re.sub(\'<a[^>]+>\',\'\',con).replace(\'</a>\',\'\')+\'</p>\'\n            \n            \n            \n            \n            \n        res = {\n            \"url\": response.url,\n            \"author\":response.doc(\'h1 > span\').text().strip(),\n            \"author_introduction\":re.sub(r\'<a[^>]+>\',\'\',author_introduction,re.S).replace(\'</a>\',\'\'),\n            \"dynasty\":response.doc(\'span > a\').text().strip(),\n            \"dynasty_introduction\":pq(url=response.doc(\'span > a\').attr.href,encoding=\'utf8\').find(\'.main_conter_Art\').html(),\n            \"desc\":response.doc(\'.main_content_shiren_left_div > span\').eq(1).text().split(u\'描述：\')[1].strip().replace(u\'无\',\'\'),\n            \"abstract\":response.doc(\'.main_content_shiren_left > p\').eq(0).text().split(u\'简介：\')[1].strip().replace(u\'无\',\'\'),\n            \"life_story\":_dict[\'life_story\'],\n            \"achievement\":_dict[\'achievement\'],\n            \"family\":_dict[\'family\'],\n            \"evaluation\":_dict[\'evaluation\'],\n        }\n        #return json.dumps(res)\n        return res\n',NULL,2.0000,3.0000,1472624727.7873),('shiren_img','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 17:45:30\n# Project: shiren_img\n\nfrom pyspider.libs.base_handler import *\nimport re,json\nfrom pyquery import PyQuery as pq \n\nshangxi_dict = {\n    u\'家庭成员\':\'family\',\n    u\'主要成就\':\'achievement\',\n    u\'文学成就\':\'achievement\',\n    #u\'书法成就\':\'achievement\',\n    u\'生平\':\'life_story\',\n    u\'人物评价\':\'evaluation\',\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'headers\':{\n\'Host\':\'tool.liuxue86.com\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n}\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://tool.liuxue86.com/shici/\',headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.main_c2\').eq(0).find(\'a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.clearfix.top20 li > a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pagination_Q a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n        \n    \n    @config(priority=2)\n    def detail_page(self, response):\n        author_img = \'\'\n        if \'shiren.jpg\' not in response.doc(\'.main_content_shiren_right > img\').attr[\'src\']:\n                author_img = response.doc(\'.main_content_shiren_right > img\').attr[\'src\']\n        return {\n            \"url\": response.url,\n            \"name\": response.doc(\'h1 > span\').text().strip(),\n            \"author_img\":author_img\n        }',NULL,1.0000,3.0000,1472624725.1802),('shiren_liuxue86','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 15:52:22\n# Project: shiren_liuxue86\n\n\nfrom pyspider.libs.base_handler import *\nimport re,json\nfrom pyquery import PyQuery as pq \n\nshangxi_dict = {\n    u\'家庭成员\':\'family\',\n    u\'主要成就\':\'achievement\',\n    u\'文学成就\':\'achievement\',\n    u\'为官生涯\':\'achievement\',\n    u\'生平\':\'life_story\',\n    u\'人物评价\':\'evaluation\',\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'headers\':{\n\'Host\':\'tool.liuxue86.com\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'\n}\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://tool.liuxue86.com/shici/\',headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.main_c2\').eq(0).find(\'a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.clearfix.top20 li > a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.pagination_Q a\').items(): \n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n        \n    \n    @config(priority=2)\n    def detail_page(self, response):\n        _dict = {\n            \'life_story\':\'\',\n            \'achievement\':\'\',\n            \'family\':\'\',\n            \'evaluation\':\'\',\n        }\n        \n        \n        \n        if pq(url=\'http://baike.baidu.com/search/word?word=\'+response.doc(\'h1 > span\').text().strip(),encoding=\'utf8\'):\n            baike = pq(url=\'http://baike.baidu.com/search/word?word=\'+response.doc(\'h1 > span\').text().strip(),encoding=\'utf8\')\n            for each in baike.find(\'.para-title\').remove(\'span\').items():\n                #print each.find(\'h2\').text()\n                if u\'人物评价\' == each.find(\'h2\').text():\n                    each1 = each.next()\n                    con = \'\'\n                    #print each.html()\n                    while each1.text() and \'anchor-list\' not in each1.outerHtml():\n                        con += each1.remove(\'img\').remove(\'.description\').html().strip()\n                        each1 = each1.next()\n                        _dict[shangxi_dict[each.find(\'h2\').remove(\'span\').text()]] = re.sub(\'<a[^>]+>\',\'\',con).replace(\'</a>\',\'\')\n        \n        \n        \n        \n        author_introduction = \'\'\n        if u\'来自百科\' in response.doc(\'.main_content_shiren_left > p\').eq(1).text():\n            author_introduction = response.doc(\'.main_content_shiren_left > p\').eq(1).text().split(u\'来自百科：\')[1].replace(u\'无\',\'\').strip()\n        \n        \n        \n        \n        for each in response.doc(\'.content_shiren_wenben h2\').items():\n            if each.text() not in shangxi_dict.keys():\n                continue\n            if each.next().text():\n                each1 = each.next()\n            else:\n                each1 = each.next().next()\n            con = \'\'\n            #print each.html()\n            while each1.html() and \'<h2\' not in each1.outerHtml():\n                con += each1.html().strip()\n                each1 = each1.next()\n            _dict[shangxi_dict[each.text()]] = re.sub(\'<a[^>]+>\',\'\',con).replace(\'</a>\',\'\')\n            \n            \n            \n            \n            \n        res = {\n            \"url\": response.url,\n            \"author\":response.doc(\'h1 > span\').text().strip(),\n            \"author_introduction\":re.sub(r\'<a[^>]+>\',\'\',author_introduction,re.S).replace(\'</a>\',\'\'),\n            \"dynasty\":response.doc(\'span > a\').text().strip(),\n            \"dynasty_introduction\":pq(url=response.doc(\'span > a\').attr.href,encoding=\'utf8\').find(\'.main_conter_Art\').html(),\n            \"desc\":response.doc(\'.main_content_shiren_left_div > span\').eq(1).text().split(u\'描述：\')[1].strip().replace(u\'无\',\'\'),\n            \"abstract\":response.doc(\'.main_content_shiren_left > p\').eq(0).text().split(u\'简介：\')[1].strip().replace(u\'无\',\'\'),\n            \"life_story\":_dict[\'life_story\'],\n            \"achievement\":_dict[\'achievement\'],\n            \"family\":_dict[\'family\'],\n            \"evaluation\":_dict[\'evaluation\'],\n        }\n        #return json.dumps(res)\n        return res\n',NULL,1.0000,3.0000,1472624730.4839),('shufa_inc','shufa','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 10:22:48\n# Project: shufa_wzxx\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n    \n    \n    page_list = {\n\n        \'http://www.wzxx.org/shufalilun/shufashi/\':[u\'书法历史\'],\n        \'http://www.wzxx.org/shufalilun/gudaishujia/\':[u\'书法杂谈\'],\n        \'http://www.wzxx.org/shufalilun/lilunzhishi/\':[u\'书法知识\'],\n        \'http://www.wzxx.org/shufalilun/jibenjifa/\':[u\'基本技法\'],\n        \'http://www.wzxx.org/shufalilun/zhuanke/\':[u\'篆刻知识\'],\n        \'http://www.wzxx.org/shufalilun/yingbi/\':[u\'硬笔书法\'],\n        \'http://www.wzxx.org/shufalilun/diandi/\':[u\'书海点滴\']\n\n    }\n    \n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_list.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'div.vlist li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text()\n            _dict[\'date\'] = each.find(\'span\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            url = each.find(\'a\').attr.href\n            self.crawl(url, save = _dict,callback=self.detail_page)\n      \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'div#text\').remove(\'ins\').remove(\'script\').html()\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = u\'书法\'\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'wzxx.org\'\n        res_dict[\'data_weight\'] = 0\n        #res_dict[\'bread\'] = [u\'文章资讯\']\n        return res_dict\n',NULL,1.0000,3.0000,1472546336.4362),('shufa_wzxx','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 10:22:48\n# Project: shufa_wzxx\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n    \n    \n    page_list = {\n\n        \'http://www.wzxx.org/shufalilun/shufashi/\':[u\'书法历史\'],\n        \'http://www.wzxx.org/shufalilun/gudaishujia/\':[u\'书法杂谈\'],\n        \'http://www.wzxx.org/shufalilun/lilunzhishi/\':[u\'书法知识\'],\n        \'http://www.wzxx.org/shufalilun/jibenjifa/\':[u\'基本技法\'],\n        \'http://www.wzxx.org/shufalilun/zhuanke/\':[u\'篆刻知识\'],\n        \'http://www.wzxx.org/shufalilun/yingbi/\':[u\'硬笔书法\'],\n        \'http://www.wzxx.org/shufalilun/diandi/\':[u\'书海点滴\']\n\n    }\n    \n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_list.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'div.vlist li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'a\').text()\n            _dict[\'date\'] = each.find(\'span\').text()\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            url = each.find(\'a\').attr.href\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'div.listpage a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'div#text\').remove(\'ins\').remove(\'script\').html()\n        res_dict[\'url\'] = response.url\n        res_dict[\'subject\'] = u\'书法\'\n        res_dict[\'class\'] = 33\n        res_dict[\'source\'] = \'wzxx.org\'\n        res_dict[\'data_weight\'] = 0\n        #res_dict[\'bread\'] = [u\'文章资讯\']\n        return res_dict\n',NULL,1.0000,3.0000,1472624801.0568),('sina_wenda_2','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-28 18:23:33\n# Project: sina_wenda\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nimport datetime\nimport MySQLdb\n\nclass ConnectionUtil(object):\n\n    def __init__(self,connection):\n        self.connection = connection\n\n    def cursor(self):\n        if self.connection:\n            self.cursor  = self.connection.cursor()\n            return self.cursor\n        else:\n            return  None\n\n    def __enter__(self):\n        return self.cursor()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n            #print \'ok\'\n            if self.connection:\n                self.connection.commit()\n            if self.cursor:\n                self.cursor.close()\n            if self.connection:\n                self.connection.close()\n            if exc_type is not None:\n                #print \'errror\'\n                print exc_val\n                #traceback.print_exc()\n                return True\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'v0.2\'\n        #\'headers\':self.headers\n    }\n    \n    headers = {\n     \'Accept\':\'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\',\n\'Accept-Encoding\':\'gzip, deflate, sdch\',\n\'Accept-Language\':\'zh-CN,zh;q=0.8,en;q=0.6\',\n\'Cache-Control\':\'max-age=0\',\n\'Connection\':\'keep-alive\',\n\'Host\':\'iask.sina.com.cn\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\',\n\n    }\n\n    @every(minutes=1*30)\n    def on_start(self):\n       sql = \'select * from tb_query where sina_flag = 0 limit 5000\'\n       con = MySQLdb.connect(db=\'querydb\',host=\'127.0.0.1\',user=\'root\',passwd=\'root\',charset=\'utf8\')\n       with ConnectionUtil(con) as cursor:\n           cursor.execute(sql)\n           result =  cursor.fetchall()\n       for each in result:\n           _dict = {}\n           _dict[\'query\'] = each[2]\n           _dict[\'id\'] = each[0]\n           line = each[2]\n           for index in range(3):\n               self.crawl(\'http://iask.sina.com.cn/search?searchWord=%s&page=%s\'%(line,index),save = _dict ,headers=self.headers,callback=self.index_page)\n                            \n                       \n    @config(age=10*24*60*60)\n    def index_page(self, response):\n        for each in response.doc(\'.search_list .search_item\').items():\n            _dict = {}\n            #_dict[\'category_id\'] = response.save.get(\'category_id\')\n            _dict[\'title\'] = each.find(\'h2 a\').text()\n            _dict[\'id\'] = response.save.get(\'id\')\n            url = each.find(\'h2 a\').attr.href\n            question_other = each.find(\'.answer_det a\').eq(0).text()\n            _dict[\'create_time\'] = get_date(question_other)\n            self.crawl(url, save = _dict,headers=self.headers,callback=self.detail_page)\n        \n            \n    @config(priority=2)\n    def detail_page(self, response):\n       sql = \'update tb_query  set sina_flag = %s  where id = %s\'\n       con = MySQLdb.connect(db=\'querydb\',host=\'127.0.0.1\',user=\'root\',passwd=\'root\',charset=\'utf8\')\n       with ConnectionUtil(con) as cursor:\n           cursor.execute(sql,(1,int(response.save.get(\'id\'))))\n       res_dict = response.save\n       question_other = response.doc(\'span.ask-time\').text().strip()\n       res_dict[\'create_time\'] = get_date(question_other)\n       res_dict[\'category_id\'] = response.doc(\'div.dib span a\').text().split()\n       res_dict[\'title\'] = response.doc(\'h3.title-f22\').text()\n       if u\'title\' not in res_dict or not res_dict[\'title\']:\n                res_dict[\'title\'] = response.doc(\'div.question_text > .pre_img\').remove(\'div.link_layer\').text()\n       res_dict[\'question_detail\'] = response.doc(\'.question_text\').remove(\'.link_layer\').text()\n       answers_list = []\n                \n       for each in response.doc(\'li.clearfix\').items():\n            info = each.find(\'.answer_txt\').text()\n            if info:\n                 _dict = {}\n                 _dict[\'content\'] = info\n                 _dict[\'user_name\'] = each.find(\'.user_wrap > a\').text()\n                 question_time = each.find(\'.answer_t\').text().strip()\n                 _dict[\'create_time\'] = get_date(question_time)\n                 answers_list.append(_dict)\n                    \n       for each in response.doc(\'.good_answer\').items():\n            info = each.find(\'.answer_text\').text()\n            if info:\n                 _dict = {}\n                 _dict[\'content\'] = info\n                 _dict[\'user_name\'] = each.find(\'.answer_tip > a\').text()\n                 question_time = each.find(\'.answer_tip .time\').text().replace(\'|\',\'\').strip()\n                 _dict[\'create_time\'] = get_date(question_time)\n                 answers_list.append(_dict)\n                    \n       if not answers_list:\n                 return \n       res_dict[\'answers\'] = answers_list\n       res_dict[\'url\'] = response.url\n       res_dict[\'source\'] = \'sina\'\n       res_dict[\'subject\'] = u\'主站问答\'\n       res_dict[\'class\'] = 34\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'create_user\'] = response.doc(\'.ask_autho span.user_wrap\').children(\'a\').text() \n       del res_dict[\'id\']\n       return res_dict\n',NULL,1.0000,3.0000,1472624321.1399),('sj33_cn','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 17:10:20\n# Project: sj33_cn\n\nfrom pyspider.libs.base_handler import *\nimport re\ndef getDate(date):\n    if date.startswith(\'201\'):\n        return date\n    else:\n        return \'2016-\' + date\n\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    contents = \'\'\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n        content = content.replace(\'</a>\',\'\')\n    contents += content\n    return  contents\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.sj33.cn/news/sjxw/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'ul.list li\').items():\n            _dict = {}\n            _dict[\'title\'] =  each.find(\'a\').text()\n            _dict[\'date\'] = getDate(each.find(\'span\').text().replace(\'[\',\'\').replace(\']\',\'\'))\n            url =  each.find(\'a\').attr.href\n            self.crawl(url, save =  _dict,callback=self.detail_page)\n        for each in response.doc(\'div.showpage a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        content_list = []\n        for each in response.doc(\'div.artcon > p\').items():\n            info = each.html()\n            if info:\n                content_list.append(removeLink(info))\n        if not content_list:\n            return\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        res_dict[\'subject\'] = \'设计\'\n        res_dict[\'class\'] = 46\n        res_dict[\'source\'] = \'33sj.cn\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'bread\'] = [\'文章资讯\']\n        return res_dict\n',NULL,2.0000,3.0000,1472624835.1578),('taoerge','erge','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-17 14:34:58\n# Project: taoerge\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'v0.5\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.taoerge.com/ertongequ/geci/\' ,callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):       \n        for each in response.doc(\'div.txt ul > li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'h2 b\').text()\n            url = each.find(\'h2 b a\').attr.href\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'ul.pagelist a\').items():\n            self.crawl(each.attr.href,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'div.content\').remove(\'p\').html().strip()\n        res_dict[\'date\'] =  response.doc(\'div.info\').eq(0).text().split()[1]\n        res_dict[\'source\'] = \'taoerge.com\'\n        res_dict[\'class\'] = 46\n        res_dict[\'subject\'] = \'儿歌\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'url\'] = response.url\n        res_dict[\'bread\'] = [ u\'文章资讯\']\n        if \'content\' in res_dict and res_dict[\'content\']:\n            return res_dict\n        else:\n            return \n',NULL,1.0000,3.0000,1472106047.4393),('test_js','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-04 18:45:04\n# Project: test_js\n\nfrom pyspider.libs.base_handler import *\nimport sys\nimport MySQLdb\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport urllib\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    headers = {\n\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://121.42.178.220:8080/\',fetch_type=\'js\',save = {\'school_name\':u\'石林\'}, callback=self.index_page)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        print response.doc(\'pre.best-text\').html()\n        #url =  response.doc(\'div.summary-pic img\').attr.src\n        #if url:\n            #urllib.urlretrieve(url, filename=\'/Users/bjhl/Documents/imgs/\'+response.save.get(\'school_name\')+\'.jpg\')\n        if not response.doc(\'.main-content\'):\n            return None\n        img_list = []\n        for each in response.doc(\'div.para img\').items():\n            img_list.append(each.attr.src)\n        _dict = {}\n        if not img_list:\n            return None\n        _dict[\'photoes\'] = img_list\n        _dict[\'school_name\'] = response.save.get(\'school_name\')\n        return _dict\n        \'\'\'_dict = {}\n        for each in response.doc(\'div[label-module=\"para-title\"]\').items():\n            key =  each.children(\'h2\').remove(\'span\').text()\n            content_list = list()\n            nx = each.next()        \n            while nx:\n                if nx.attr[\'label-module\'] == \'para-title\' or nx.attr[\'id\'] ==\'open-tag\':\n                    break\n                if nx.attr[\'class\'] == \'para\':  \n                    content_list.append(\'<p>\'+nx.remove(\'sup\').text()+\'</p>\')            \n                nx = nx.next()\n            if content_list and key:\n                  _dict[key] = \'\'.join(content_list)\n        for each in response.doc(\'dt.basicInfo-item\').items():\n            key = \'\'.join(v for v in each.text().split())\n            nx = each.next()\n            if \'basicInfo-item\' in nx.attr[\'class\'] and key:\n                _dict[key] = nx.text()\n        if not _dict:\n            return\n        return _dict\'\'\'\n\n            \n            \n            \n            \n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1472624310.5856),('tiku_chazidian_k12','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-09 10:06:34\n# Project: tiku_chazidian_k12\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://tiku.chazidian.com/gz/?book=&grade=&page=1\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.list_sj_xx > a\').items():\n            for name in [u\'语文\', u\'数学\', u\'英语\', u\'物理\', u\'化学\', u\'生物\', u\'政治\', u\'历史\', u\'地理\']:\n                if name in each.text():\n                    subject = u\'高中\' + name\n                    break\n            self.crawl(each.attr.href, save={\'subject\': subject}, callback=self.list_page)\n        for each in response.doc(\'.page_link\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'p > a\').items():\n            self.crawl(each.attr.href, save=response.save, callback=self.detail_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content = [v.html() for v in response.doc(\'.select_c > dl\').items()]\n        _dic = {\n        u\'高中生物\': \'P_548362\',\n        u\'高中化学\': \'P_548361\',\n        u\'高中物理\': \'P_548360\',\n        u\'高中地理\': \'P_548359\',\n        u\'高中历史\': \'P_548358\',\n        u\'高中政治\': \'P_548357\',\n        u\'高中英语\': \'P_548356\',\n        u\'高中数学\': \'P_548355\',\n        u\'高中语文\': \'P_548354\',\n        }\n        subject = response.save[\'subject\']\n        return {\n            \"url\": response.url,\n            \"title\": content[0],\n            \"answer\": content[1].replace(\'\\r\',\'\').replace(\'\\n\',\'\'),\n            \"analyse\": content[2].replace(\'\\r\',\'\').replace(\'\\n\',\'\'),\n            \"degree\": randrange(1, 5),\n            \"source\": \"chazidian\",\n            \"data_weight\": 0,\n            \"subject\": subject,\n            \"class\": \"35\",\n            \"category_id\": [_dic[subject], ],\n        }\n',NULL,1.0000,3.0000,1472624314.6577),('tiku_cooco','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-09 10:06:34\n# Project: tiku_chazidian_k12\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\nimport urllib2\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\n         \n    }\n    \n    header_base = {\n\'Cookie\':\'bdshare_firstime=1468894646778; AJSTAT_ok_pages=5; AJSTAT_ok_times=2; Hm_lvt_c8ad39b9579b4816dc8f6f805c190308=1468291884,1468894647; Hm_lpvt_c8ad39b9579b4816dc8f6f805c190308=1468895300\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\',\n\'X-Prototype-Version\':\'1.5.0_rc0\'\n\n    }\n    tag_dic = {\n        u\'人物传记类\': \'p_565\',\n        u\'作家作品\': \'p_1501\',\n        u\'修改应用文\': \'p_662\',\n        u\'修辞格\': \'p_32\',\n        u\'先秦诸子百家\': \'p_1541\',\n        u\'公文类\': \'p_1456\',\n        u\'其他\': \'p_204\',\n        u\'历史事件类\': \'p_1551\',\n        u\'历史记载类(二十四史、地方志、野史等)\': \'p_2954\',\n        u\'古代散文、兵法农医、天文术数、艺术杂学等\': \'p_1517\',\n        u\'句子衔接\': \'p_157\',\n        u\'命题作文\': \'p_717\',\n        u\'图文转换\': \'p_23\',\n        u\'外国文学\': \'p_1504\',\n        u\'字形\': \'p_19\',\n        u\'字音\': \'p_31\',\n        u\'实用类文本阅读\': \'p_1160\',\n        u\'小作文\': \'p_1472\',\n        u\'小说\': \'p_1502\',\n        u\'成语(熟语)\': \'p_1457\',\n        u\'散文类\': \'p_658\',\n        u\'曲\': \'p_1518\',\n        u\'材料作文\': \'p_710\',\n        u\'标点符号\': \'p_33\',\n        u\'楚辞汉赋骈文(歌行、乐府)\': \'p_1983\',\n        u\'现代文学类文本阅读\': \'p_1161\',\n        u\'病句辨析\': \'p_5\',\n        u\'论述类文本阅读\': \'p_653\',\n        u\'词\': \'p_1091\',\n        u\'词语\': \'p_30\',\n        u\'诗\': \'p_22\',\n        u\'话题作文\': \'p_714\',\n        u\'语法\': \'p_100\',\n        u\'语言的简明连贯得体\': \'p_4\',\n        u\'近代文学\': \'p_1515\',\n        u\'选用、仿用、变换句式\': \'p_15\',\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://gzyw.cooco.net.cn/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        _list = []\n        for each in response.doc(\'.lftTxt > a\').items():\n            tag = each.text()\n            self.crawl(each.attr.href, save={\'tag\': tag}, callback=self.list_page)\n        \n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        tag = response.save[\'tag\']\n        max_page = 1\n        print response.url\n        for each in response.doc(\'.page-numbers\').items():\n            if each.text().isdigit() and int(each.text())>max_page:\n                max_page = int(each.text())\n        url = response.url\n        if url[-1] == \'/\':\n            url = url[:-1]\n        for i in range(1, max_page):\n            _url = \'/\'.join(url.split(\'/\')[:-1]) + \'/\' + str(i)\n            print _url\n            _dict = {\'X-Requested-With\':\'XMLHttpRequest\',\'Referer\':response.url,\'Origin\':\'http://gzyw.cooco.net.cn\'}\n            self.header_base.update(_dict)\n            self.crawl(_url, save=response.save, headers=self.header_base,callback=self.list_page2)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def list_page2(self, response):       \n        for each in response.doc(\'span > a\').items():\n            self.crawl(each.attr.href, save=response.save, callback=self.detail_page)\n        for each in response.doc(\'.bottom  a\').items():\n            self.crawl(each.attr.href, save=response.save, callback=self.detail_page)\n        \n        \n    @config(age=10 * 24 * 60 * 60)\n    def detail_page(self, response):\n        tag = response.save[\'tag\']\n\n        title = \'\'.join([\'<p>%s</p>\'%v.html() for v in response.doc(\'.txt p\').items()])\n        subject = u\'高中语文\'\n            \n        answer = \'\'.join([\'<p>%s</p>\'%v.text() for v in response.doc(\'.daan p\').items() if \'co\' not in v.text() and \'oo\' not in v.text()] )\n        dic = {\n                \"url\": response.url,\n                \"title\": title,\n                \"answer\": answer,\n                \"analyse\": \'\',\n                \"degree\": randrange(1, 5),\n                \"source\": \"chazidian\",\n                \"data_weight\": 0,\n                \"subject\": subject,\n                \"class\": \"35\",\n                \"category_id\": [self.tag_dic.get(tag, \'p_548354\'), ],\n        }\n            \n        return dic\n',NULL,1.0000,3.0000,1472624717.2643),('tiku_gzywtk','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-09 15:53:57\n# Project: tiku_gzywtk\n\nfrom pyspider.libs.base_handler import *\nfrom random import randrange\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    dic = {\n        \'zy\': \'p_31\',\n        \'bd\': \'p_33\',\n        \'cy\': \'p_1091\',\n        \'bj\': \'p_5\',\n        \'wyw\': \'p_18117\',\n        \'sg\': \'p_18337\',\n        \'mj\': \'p_478\',\n        \'yy\': \'p_1163\',\n        \'xdw\': \'p_482\',\n        \'zw\': \'p_709\',\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k, v in self.dic.iteritems():\n            self.crawl(\'http://www.gzywtk.com/kaodian/%s-list.aspx\'%k, save={\'cid\': v}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'#typelist > li\').items():\n            dic = {\n            \'title\': each.find(\'.limid\').html(),\n            \'category_id\': [response.save[\'cid\'],],\n            }\n            self.crawl(each.find(\'a\').attr.href, save=dic, callback=self.detail_page)\n        for each in response.doc(\'.datapager > a\').items():\n            self.crawl(each.attr.href, save={\'cid\': response.save[\'cid\']}, callback=self.index_page)\n\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        res[\"url\"] = response.url\n        res[\'answer\'] = response.doc(\'.content\').eq(1).html()\n        res[\'degree\'] = randrange(1, 5)\n        res[\"source\"] = \"gzywtk\"\n        res[\"data_weight\"] = 0\n        res[\"subject\"] = u\'高中语文\'\n        res[\"class\"] = 35\n        return res',NULL,1.0000,3.0000,1472624307.1408),('toefl_taisha_inc','toefl','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 16:14:58\n# Project: toefl_taisha\n\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    type_dict = {\n        \'news\': u\'快讯动态\', \'guidance\': u\'复习攻略\', \'experience\': u\'复习攻略\', \'download\': u\'快讯动态\', \'guid\': u\'高分心得\', \'machine\': u\'托福机经\',\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in self.type_dict.iteritems(): \n            self.crawl(\'http://www.taisha.org/toefl/%s/\'%k, save={\'bread\': v}, callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.html_content dd\').items():\n            _dict = {}\n            url = each.find(\'.title > a\').attr.href\n            #_dict[\'url\'] = url\n            _dict[\'bread\'] = each.find(\'.bot > a\').text().split(\' \')\n            _dict[\'bread\'].append((bread))\n            _dict[\'brief\'] = self.replace(each.find(\'p\').text().replace(u\'[详情]\', \'\'))\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        #for each in response.doc(\'span > a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n    def replace(self, _str):\n        img1 = \'http://www.taisha.org/uploadfile/2015/1215/20151215035909345.jpg\'\n        if _str:\n            return _str.replace(u\'太傻留学\', u\'\').replace(u\'太傻\', \'\').replace(img1, \'\')\n        return _str\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        try:\n            date = response.doc(\'.txt_title span\').text()[-19:]\n        except:\n            date = \'\'\n        #content = response.doc(\'.txt_content\').html()\n        content_info = response.doc(\'.txt_content > p\')\n        content_list = []\n        for info in content_info:\n            answer = pyquery.PyQuery(info)\n            items = answer.html()\n            if \'20151215035909345.jpg\' not in items:\n                content_list.append((self.replace(items)))\n        content_list = content_list[:-7]\n        if not content_list:\n            return None\n        title = response.doc(\'.txt_title > h2\').text()\n        if u\'回忆版汇总\' in title:\n            return None\n        res_dict[\'title\'] = title\n        bread = res_dict[\'bread\']\n        if u\'托福机经\' in response.save[\'bread\']:\n            if u\'汇总\' in title:\n                return None\n            if u\'听力\' in title:\n                bread.append((u\'听力机经\'))\n            if u\'阅读\' in title:\n                bread.append((u\'阅读机经\'))\n            if u\'口语\' in title:\n                bread.append((u\'口语机经\'))\n            if u\'写作\' in title:\n                bread.append((u\'写作机经\'))\n        res_dict[\'bread\'] = list(set(bread)) \n        res_dict[\'url\'] = response.url\n        res_dict[\'date\'] = date[:10]\n        res_dict[\'source\'] = u\'太傻留学\'\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'subject\'] = u\'托福\'\n        res_dict[\'tag\'] = response.doc(\'#nav_location a\').text().split(\' \')[1:]\n        res_dict[\'class\'] = 27\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471329875.8495),('toefl_xdf_inc','toefl','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-26 14:19:35\n# Project: toefl_xdf_inc\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://toefl.xdf.cn/list_9069_1.html\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.txt_lists01 > li\').items():\n            url = each.find(\'a\').attr.href\n            date = each.find(\'.time\').text()\n            if u\'汇总\' not in each.find(\'a\').text():\n                self.crawl(url, save={\'date\': date}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for each in response.doc(\'.air_con > p\').items():\n            if u\'编辑推荐\' in each.text():\n                break\n            content_list.append((each.html().replace(\'\\n\',\'\').replace(\'\\r\',\'\').replace(\'\\t\',\'\')))\n        content_list = content_list[1:]\n        if not content_list:\n            return None\n        bread = [u\'托福机经\',]\n        title = response.doc(\'.title1\').text().replace(u\'新东方名师：\',\'\')\n        if u\'听力\' in title:\n            bread.append((u\'听力机经\'))\n        if u\'阅读\' in title:\n            bread.append((u\'阅读机经\'))\n        if u\'口语\' in title:\n            bread.append((u\'口语机经\'))\n        if u\'写作\' in title:\n            bread.append((u\'写作机经\'))\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"date\": response.save[\'date\'],\n            \"subject\": u\'托福\',\n            \"source\": \'xdf\',\n            \"class\": 27,\n            \"data_weight\":0,\n            \"bread\": bread,\n            \"content\": \'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n        }\n',NULL,1.0000,3.0000,1471329882.3647),('toefl_xiaozhan_inc','toefl','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-29 18:19:22\n# Project: toefl_xiaozhan\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    type_dict = {\n        \'fuxi\': u\'复习攻略\', \'tpo\': u\'托福TPO\', \'tingli\': u\'托福听力\', \'kouyu\': u\'托福口语\', \n        \'yuedu\': \'托福阅读\', \'zonghe\': u\'冲刺宝典\', \'jihua\': u\'备考计划\', \'tifen/beikao\': u\'备考计划\',\n        \'xiezuo\': u\'托福写作\', \'tfziliao\': u\'托福综合\', \'gaofen\': u\'高分心得\', \n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for type, v in self.type_dict.iteritems():\n            self.crawl(\'http://toefl.zhan.com/%s/\'%type, save={\'bread\': v}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/toefl/kaoqian/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/toefl/fukao/\', save={\'bread\': u\'冲刺宝典\'}, callback=self.index_page)\n        self.crawl(\'http://zt.zhan.com/toefl/beikao/\', save={\'bread\': u\'备考计划\'}, callback=self.index_page)\n\n    @config(age=1 * 60)\n    def index_page(self, response):\n        bread = response.save[\'bread\']\n        for each in response.doc(\'.things_list > .pull-right\').items():\n            url = each.find(\'a\').attr.href\n            _save = {}\n            _save[\'date\'] = each.find(\'.padding_right_10\').text()\n            #_save[\'tag\'] = each.find(\'dl > .pull-left > a\').text().split()\n            _save[\'brief\'] = each.find(\'.text\').text()\n            _save[\'bread\'] = bread\n            self.crawl(url, save=_save, callback=self.detail_page)\n        #for each in response.doc(\'nav a\').items():\n        #    self.crawl(each.attr.href, save={\'bread\': bread}, callback=self.index_page)\n\n    def resplace(self, _str):\n        if _str:\n            return _str.replace(u\'小站\',\'\').replace(u\'小站教育\',\'\')\n        return _str\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\"url\"] = response.url\n        brief = response.save[\'brief\']\n        res_dict[\'brief\'] = self.resplace(brief)\n        res_dict[\"title\"] = self.resplace(response.doc(\'h1\').text())\n        content_list = []\n        for each in  response.doc(\'.article-content > p\').items():\n            content_list.append((each.html().replace(\'\\r\',\'\').replace(\'\\t\',\'\').replace(\'\\n\',\'\')))\n        if not content_list:\n            return None\n        res_dict[\'content\'] = self.resplace(\'\'.join([\'<p>%s</p>\'% v for v in content_list]))\n        res_dict[\'subject\'] = u\'托福\'\n        res_dict[\'source\'] = u\'小站\'\n        res_dict[\'tag\'] = response.doc(\'.tag > a\').text().split(\' \')\n        bread = [res_dict[\'bread\'], ]\n        bread.extend(response.doc(\'.head-crumbs-a-active\').text().split(\' \'))\n        if res_dict[\'date\'][:3] != \'201\':\n            res_dict[\'date\'] = response.doc(\'.pull-left > span\').text().split()[0].replace(u\'年\',\'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n        res_dict[\'bread\'] = list(set(bread))\n        res_dict[\'class\'] = 27\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471329885.7682),('tuijian_haici','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-14 15:22:58\n# Project: tuijian_haici\n\nfrom pyspider.libs.base_handler import *\n\nimport sys\nimport MySQLdb\nimport traceback\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"cidiandb\", charset = \"utf8\")\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\': \'0.2\',\n    }\n\n    @every(minutes=1 * 30)\n    def on_start(self):\n        sql = \'select id, word from tb_word where tuijian_flag = 0 limit 10000\'\n        try:\n            cursor.execute(sql)\n            for (ci_id, word,) in cursor.fetchall():\n                #word = \'good\'\n                self.crawl(\'http://dict.cn/\'+word, save = {\'ci\': word, \'id\': ci_id}, callback=self.detail_page)\n        except:\n            traceback.print_exc()\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        word = response.save.get(\'ci\').strip()\n        ci_id = response.save.get(\'id\')\n        sql = \"update tb_word set tuijian_flag= 1 where id= %s\" % (ci_id)\n        try:           \n            cursor.execute(sql)\n            conn.commit()\n        except Exception, e:\n            print e\n            \n        recommend = {}\n        \n        for each in response.doc(\'.rel > h3\').items():\n            if u\'缩略词\' in each.text():\n                #for t in each.next().children().items():\n                 #   print t.html()\n                content = \'\'.join([\'<p>\'+v.text()+\'</p>\' for v in each.next().find(\'li\').items()])\n                recommend[each.text().strip()] = content\n            elif u\'临近单词\' in each.text(): \n                recommend[each.text().strip()] = each.next().remove(\'.sound\').text().replace(\'\\t\',\'\').replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n            \n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text().replace(u\'海词\',u\'跟谁学\'),\n            \"word\": response.save.get(\'ci\'),\n            \"recommend\": recommend,\n        }',NULL,5.0000,5.0000,1472624713.6245),('weibo_sina','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-17 18:59:47\n# Project: weibo_sina\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://weibo.com/p/1005053532679345\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1472624295.1862),('weixin_sougou',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-11 12:14:31\n# Project: sheying_fsbus\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as P\nimport re\nimport json\nimport cPickle\nimport time\n\ndetail_headers = {\n      \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n      \"Upgrade-Insecure-Requests\": \"1\",\n      \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"\n    }\n\nindex_headers = {\n      \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n      \"Cookie\": \"CXID=61CC362F8DB5AF26DB926FB349837DF7; SUV=00FA5F9E246E3FDC56C2B5484910E349; ssuid=2375315090; ad=zkllllllll2QZVNXq73CWONoohxQZrqfJpc@Tlllll9lllllVqxlw@@@@@@@@@@@; SUID=DC3F6E24516C860A56C0576700097B44; ABTEST=4|1466480404|v1; weixinIndexVisited=1; SNUID=4BA8F8B39693A2570288F98E972AD230; JSESSIONID=aaafIPdrNUcCPb_Tbeovv; ppinf=5|1466497966|1467707566|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZToyNzolRTUlQjAlOEYlRTUlQUUlODclRTUlQUUlOTl8Y3J0OjEwOjE0NjY0OTc5NjZ8cmVmbmljazoyNzolRTUlQjAlOEYlRTUlQUUlODclRTUlQUUlOTl8dXNlcmlkOjQ0OkM3NDBBQUFCMkM3RERCM0Y0Njg0NzFFRTUzMTYzQUJGQHFxLnNvaHUuY29tfA; pprdig=PMcHhapmHPZtX7WFM_2PIYC2cnXBsOAT_EIIh_UC9Z4kAqswRfsbRA3Mp2aFG1t9adYaCkjRQWP-dKMiNAxGJkHS9J6XJqRKV6s9h2iRMEFxkLGI05EGVIfa-bEO4KjhNrA3xzjoIDGqSwfmZizeN97d0f1wWxmM5nEzg-0nm5w; PHPSESSID=l3fu330sueq4uogqnavs6s5ot3; SUIR=4BA8F8B39693A2570288F98E972AD230; sct=14; IPLOC=CN3300; ppmdig=1466500977000000d7f8ecd6a79fbd98c17e6d636b5074b4\",\n      \"Host\": \"weixin.sogou.com\",\n      \"Upgrade-Insecure-Requests\": \"1\",\n      \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"\n}\n\nmax_pageno = 20\nclass Parser(object):\n    @staticmethod\n    def parse_list(response):\n        list_ret = P(response.doc(\".results\"))\n        ret = []\n        if list_ret:\n            for url_node in list_ret.find(\".txt-box h4\").find(\"a\"):\n                ret.append(P(url_node))\n        return ret\n\n    @staticmethod\n    def parse_nextpage(response):\n        page_node = P(response.doc(\"#pagebar_container\"))\n        try:\n            next_page = page_node.find(\"span\").next()\n            if next_page:\n                current_pageno = page_node.find(\"span\").text()\n                next_pageno = next_page.text()\n                if int(next_pageno) > int(current_pageno):\n                    # 限制只抓前20页\n                    if int(next_pageno) > max_pageno:\n                        return False\n                    return P(\'\'\'<a href=\"%s&page=%s\">next</a>\'\'\' % (response.save.get(\"base_url\"), next_pageno))\n                return False\n\n            else:\n                return False\n        except Exception as info:\n            return False\n    \n    @staticmethod\n    def parse_detail(spider_handle, parser, response):\n        ret = {}\n        detail_node = response.doc(\"#img-content\")\n        title = P(detail_node).find(\".rich_media_title\").text()\n        ret[\"title\"] = title\n        ret[\"date\"] = P(detail_node).find(\"#post-date\").text()\n        ret[\"nickname\"] = P(detail_node).find(\"a.rich_media_meta_nickname\").text()\n        ret[\"author\"] = P(detail_node).find(\"#post-date\").next().text()\n        ret[\"content\"] = P(detail_node).find(\"#js_content\").html().strip()\n        ret[\"readnum\"] = P(detail_node).find(\"#sg_readNum3\").text()\n        ret[\"likenum\"] = P(detail_node).find(\"#sg_likeNum3\").text()\n        ret[\"isorg\"] = P(detail_node).find(\"#copyright_logo\").text()\n        return ret\n\nclass Processor(object):\n    @staticmethod\n    def list_processor(spider_handle, parser, response):\n        # 解析列表页\n        list_result = parser.parse_list(response)\n        if list_result:\n            for item in list_result:\n                spider_handle.crawl(item.attr.href, fetch_type = \"js\", save = response.save, headers = detail_headers, callback = spider_handle.detail_page)\n\n        # 是否有翻页\n        next_page = parser.parse_nextpage(response)\n        if next_page:\n            spider_handle.crawl(next_page.attr.href, save = response.save, headers = index_headers, callback = spider_handle.index_page)\n        \n    @staticmethod\n    def detail_processor(spider_handle, parser, response):\n        return parser.parse_detail(spider_handle, parser, response)\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    crawler_parser = {\n        \"1\": {\n            \"processor\": Processor,\n            \"parser\": Parser,\n        }\n    }\n    crawler_list = [\n        {   \n            \"key\": \"1\",\n            \"url\": \"http://weixin.sogou.com/weixin?type=2&query=%E9%9B%85%E6%80%9D\",\n            \"bread\": [\"微信搜狗\"],\n        },\n    ]\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        for crawler in self.crawler_list:\n            self.crawl(crawler[\"url\"],\n                       save = {\n                           \'bread\': crawler.get(\"bread\", []),\n                           \'__parser_key__\': crawler.get(\"key\"),\n                           \'base_url\': crawler[\"url\"],\n                       },\n                       headers = index_headers,\n                       callback = self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        crawler_parser_dict.get(\"processor\").list_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n    #@config(priority=2)\n    @config(age=1)\n    def detail_page(self, response):\n        crawler_parser_dict = self.crawler_parser.get(response.save.get(\"__parser_key__\"))\n        return crawler_parser_dict.get(\"processor\").detail_processor(self, crawler_parser_dict.get(\"parser\"), response)\n\n\n',NULL,1.0000,3.0000,1466571067.6703),('wenda2_ntce','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-26 17:43:42\n# Project: wenda2_ntce\n\nfrom pyspider.libs.base_handler import *\nimport time,datetime\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天之前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u\'天之前\', \'\')))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n        \"headers\": {\n        \'Host\':\'www.ntce.com\',\n        \'Referer\':\'http://www.ntce.com/ask\',\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.ntce.com/ask/\', headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.searchcontent a\').items():\n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        if response.doc(\'.list13 li\'):\n            for each in response.doc(\'.list13 li\').items():\n                if \'ivl\' in each.outerHtml():\n                    continue\n                _dict = {}\n                _dict[\'category_id\'] = each.find(\'.role\').text()\n                _dict[\'title\'] = each.find(\'.con > a\').text()\n                _dict[\'create_user\'] = \'\'\n                self.crawl(each.find(\'.con > a\').attr.href, headers=self.crawl_config[\'headers\'], save=_dict, callback=self.detail_page)\n            #翻页\n            for each in response.doc(\'.pages a\').items():\n                self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'create_time\'] = response.doc(\'.p_w > .time\').text().split()[0]\n        res_dict[\'create_user\'] = response.doc(\'.con > div\').eq(0).text().split(u\'：\')[1]\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = u\'教师资格网\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'question_detail\'] = response.doc(\'.hist > div\').text() if response.doc(\'.hist > div\') else \'\'\n\n        answers_list = []\n\n        for each in response.doc(\'.r_box > .con\').items():\n            if  each.find(\'.ss > .time\'):\n                create_time = each.find(\'.ss > .time\').text().split()[0]\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            if not each.find(\'.details > span\'):\n                continue\n            #print each.html()\n            answers_list.append({\n                \"content\":  each.find(\'.details > span\').html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": response.doc(\'.r_box > .ttl > div\').text().replace(u\'回复\',\'\'),\n                })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,1.0000,3.0000,1472624280.9034),('wenda2_xdf','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-26 10:58:07\n# Project: wenda2_xdf\n\n\nfrom pyspider.libs.base_handler import *\nimport time,datetime\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天之前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u\'天之前\', \'\')))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n        \"headers\": {\n        \'Host\': \'ask.koolearn.com \',\n        \'Referer\':\'http://ask.koolearn.com/new/list/1/1/0\',\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://ask.koolearn.com/\', headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.whole-nav .popup-title > a\').items():\n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'], save={\'category_id\':[each.text()]}, callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.j-mouse-ev\').items():\n            _dict = {}\n            _dict[\'category_id\'] = response.save.get(\'category_id\')\n            _dict[\'title\'] = each.find(\'.list-item-name > a\').text()\n            _dict[\'create_time\'] = get_date(each.find(\'.item-time\').text())\n            _dict[\'create_user\'] = each.find(\'.item-user-name\').text()\n            self.crawl(each.find(\'.list-item-name > a\').attr.href, headers=self.crawl_config[\'headers\'], save=_dict, callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.list_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = u\'新东方\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'question_detail\'] = response.doc(\'.q-content\').text() if response.doc(\'.q-content\') else \'\'\n        \n        answers_list = []\n        \n        for each in response.doc(\'.answer-info-box\').items():\n            if  each.find(\'.answer-time\'):\n                create_time = get_date(each.find(\'.answer-time\').text())\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            if not each.find(\'.teacher-answer-content > p\'):\n                continue\n            answers_list.append({\n                \"content\":  each.find(\'.teacher-answer-content > p\').html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'.answer-teach-info > a\').text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,5.0000,5.0000,1472624278.1592),('wenda3_baidu','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-23 14:09:42\n# Project: wenda3_baidu\nfrom pyspider.libs.base_handler import *\nimport MySQLdb\nimport traceback\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nfrom urllib import quote\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"querydb\", charset = \"utf8\")\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n\n        \"headers\": {\n        \'Host\': \'zhidao.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\', \n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n    \n\n    @every(minutes=1 * 30)\n    def on_start(self):\n        sql = \'select id, query from tb_query where flag = 0 limit 300\'\n        try:\n            cursor.execute(sql)\n            for (query_id, query,) in cursor.fetchall():\n                #print type(query) \n                #query = query.encode(\"UTF-8\")\n                #print type(query)\n                #print query\n                #query = unicode(query,\'utf-8\')\n                #print type(query)\n                #print query\n                #word = \'good\'\n                \n                for i in range(0,51,10):\n                    \n                    self.crawl(\'http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s\'%(i,query),save = {\'query\':query, \'id\':query_id}, proxy=\"122.96.59.106:80\", headers=self.crawl_config[\'headers\'], callback=self.index_page)\n        except Exception,e:\n            print e\n            \n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.dl\').items():\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.line > a\').text()\n            url = each.find(\'.line > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'],callback=self.detail_page)\n        for each in response.doc(\'.mb-20 > dl\').items():\n            if u\'百度百科\' in each.text():\n                #print \'ok\'\n                continue\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.mb-8 > a\').text()\n            \n            url = each.find(\'.mb-8 > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pager-next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        query_id = response.save.get(\'id\')\n        sql = \"update tb_query set flag=1 where id=%s\" % (query_id)\n        try:           \n            cursor.execute(sql)\n            #print query_id\n            #print \'ok\'\n            conn.commit()\n        except Exception, e:\n            print e\n        #if response.doc(\'.word-replace\'):\n        #    return\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = \'baidu\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'category_id\'] = []\n        res_dict[\'class\'] = 34\n        #print response.doc(\'.q-content\').html()\n        res_dict[\'question_detail\'] = response.doc(\'.q-content\').text() if not response.doc(\'.q-content .word-replace\') else \'\'\n        \n        answers_list = []\n        if response.doc(\'.content > .mb-10\'):\n            if not response.doc(\'.content > .mb-10 .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'div.mb-15 > .pos-time\'):\n                    create_time = response.doc(\'div.mb-15 > .pos-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.content > .mb-10\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'div.mt-10\').text(),\n                })\n        if response.doc(\'.quality-content > .quality-content-detail\'):\n            if not response.doc(\'.quality-content > .quality-content-detail .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if response.doc(\'.quality-info > .reply-time\'):\n                    create_time = response.doc(\'.quality-info > .reply-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.quality-content > .quality-content-detail\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'.q-name > a\').text(),\n                })\n        if response.doc(\'.ec-answer\'):\n            #print u\'企业回答\'\n            if not response.doc(\'.ec-answer .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'.ec-time\'):\n                    create_time = response.doc(\'.ec-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.ec-answer\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": \'\',\n                })\n        if response.doc(\'.best-related dd > span\'):\n            if not response.doc(\'.best-related dd > span .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n                if  response.doc(\'.best-related i > span\'):\n                    create_time = response.doc(\'.best-related i\').eq(-1).text().split()[0]\n                    user_name = response.doc(\'.best-related i\').eq(0).text()\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                    user_name = \'\'\n                answers_list.append({\n                    \"content\":  response.doc(\'.best-related dd > span\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": user_name,\n                })\n        for each in response.doc(\'div.answer-text\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            if  each.parent().prev().find(\'.pos-time\'):\n                create_time = each.parent().prev().find(\'.pos-time\').text().split()[0]\n                if not each.parent().prev().find(\'.pos-time\').next():\n                    user_name = each.parent().prev().remove(\'.pos-time\').text()\n                else:\n                    user_name = each.parent().prev().find(\'.pos-time\').next().text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                user_name = \'\'\n            answers_list.append({\n                \"content\":  each.html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": user_name,\n            })\n        for each in response.doc(\'dl.other-answer > .answer\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n            if  each.find(\'.ext-info > i\'):\n                create_time = each.find(\'.ext-info > i\').eq(-1).text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  each.children().eq(0).html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'.ext-info > i\').eq(0).text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'create_user\'] = response.doc(\'.ask-info > span\').eq(1).text()\n        return res_dict\n',NULL,2.0000,3.0000,1472624633.8624),('wenda4_baidu','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-01 10:09:00\n# Project: wenda4_baidu\n\nfrom pyspider.libs.base_handler import *\nimport MySQLdb\nimport traceback\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nfrom urllib import quote\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"querydb\", charset = \"utf8\")\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n\n        \"headers\": {\n        \'Host\': \'zhidao.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\', \n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n    \n\n    @every(minutes=1 * 30)\n    def on_start(self):\n        sql = \'select id, query from tb_query where flag = 0 limit 3000\'\n        try:\n            cursor.execute(sql)\n            for (query_id, query,) in cursor.fetchall():\n                #print type(query) \n                #query = query.encode(\"UTF-8\")\n                #print type(query)\n                #print query\n                #query = unicode(query,\'utf-8\')\n                #print type(query)\n                #print query\n                #word = \'good\'\n                query = query.replace(\' \',\'%20\')\n\n                for i in range(0,51,10):\n                    \n                    self.crawl(\'http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s\'%(i,query),save = {\'query\':query, \'id\':query_id}, proxy=\"122.96.59.106:80\", headers=self.crawl_config[\'headers\'], callback=self.index_page)\n        except Exception,e:\n            print e\n            \n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.dl\').items():\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.line > a\').text()\n            url = each.find(\'.line > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'],callback=self.detail_page)\n        for each in response.doc(\'.mb-20 > dl\').items():\n            if u\'百度百科\' in each.text():\n                #print \'ok\'\n                continue\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.mb-8 > a\').text()\n            \n            url = each.find(\'.mb-8 > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pager-next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        query_id = response.save.get(\'id\')\n        sql = \"update tb_query set flag=1 where id=%s\" % (query_id)\n        try:           \n            cursor.execute(sql)\n            #print query_id\n            #print \'ok\'\n            conn.commit()\n        except Exception, e:\n            print e\n        #if response.doc(\'.word-replace\'):\n        #    return\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = \'baidu\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'category_id\'] = []\n        res_dict[\'class\'] = 34\n        #print response.doc(\'.q-content\').html()\n        res_dict[\'question_detail\'] = response.doc(\'.q-content\').text() if not response.doc(\'.q-content .word-replace\') else \'\'\n        \n        answers_list = []\n        if response.doc(\'.content > .mb-10\'):\n            if not response.doc(\'.content > .mb-10 .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'div.mb-15 > .pos-time\'):\n                    create_time = response.doc(\'div.mb-15 > .pos-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.content > .mb-10\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'div.mt-10\').text(),\n                })\n        if response.doc(\'.quality-content > .quality-content-detail\'):\n            if not response.doc(\'.quality-content > .quality-content-detail .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if response.doc(\'.quality-info > .reply-time\'):\n                    create_time = response.doc(\'.quality-info > .reply-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.quality-content > .quality-content-detail\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'.q-name > a\').text(),\n                })\n        if response.doc(\'.ec-answer\'):\n            #print u\'企业回答\'\n            if not response.doc(\'.ec-answer .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'.ec-time\'):\n                    create_time = response.doc(\'.ec-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.ec-answer\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": \'\',\n                })\n        if response.doc(\'.best-related dd > span\'):\n            if not response.doc(\'.best-related dd > span .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n                if  response.doc(\'.best-related i > span\'):\n                    create_time = response.doc(\'.best-related i\').eq(-1).text().split()[0]\n                    user_name = response.doc(\'.best-related i\').eq(0).text()\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                    user_name = \'\'\n                answers_list.append({\n                    \"content\":  response.doc(\'.best-related dd > span\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": user_name,\n                })\n        for each in response.doc(\'div.answer-text\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            if  each.parent().prev().find(\'.pos-time\'):\n                create_time = each.parent().prev().find(\'.pos-time\').text().split()[0]\n                if not each.parent().prev().find(\'.pos-time\').next():\n                    user_name = each.parent().prev().remove(\'.pos-time\').text()\n                else:\n                    user_name = each.parent().prev().find(\'.pos-time\').next().text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                user_name = \'\'\n            answers_list.append({\n                \"content\":  each.html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": user_name,\n            })\n        for each in response.doc(\'dl.other-answer > .answer\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n            if  each.find(\'.ext-info > i\'):\n                create_time = each.find(\'.ext-info > i\').eq(-1).text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  each.children().eq(0).html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'.ext-info > i\').eq(0).text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'create_user\'] = response.doc(\'.ask-info > span\').eq(1).text()\n        return res_dict',NULL,2.0000,3.0000,1472624274.5596),('wenda5_baidu','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-06 17:33:45\n# Project: wenda5_baidu\n\nfrom pyspider.libs.base_handler import *\nimport MySQLdb\nimport traceback\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nfrom urllib import quote\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"querydb\", charset = \"utf8\")\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n\n        \"headers\": {\n        \'Host\': \'zhidao.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\', \n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n    \n\n    @every(minutes=1 * 30)\n    def on_start(self):\n        sql = \'select id, query from tb_query where flag = 0 limit 3000\'\n        try:\n            cursor.execute(sql)\n            for (query_id, query,) in cursor.fetchall():\n                #print type(query) \n                #query = query.encode(\"UTF-8\")\n                #print type(query)\n                #print query\n                #query = unicode(query,\'utf-8\')\n                #print type(query)\n                #print query\n                #word = \'good\'\n                query = query.replace(\' \',\'%20\')\n\n                for i in range(0,51,10):\n                    \n                    self.crawl(\'http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s\'%(i,query),save = {\'query\':query, \'id\':query_id}, proxy=\"122.96.59.106:80\", headers=self.crawl_config[\'headers\'], callback=self.index_page)\n        except Exception,e:\n            print e\n            \n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.dl\').items():\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.line > a\').text()\n            url = each.find(\'.line > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'],callback=self.detail_page)\n        for each in response.doc(\'.mb-20 > dl\').items():\n            if u\'百度百科\' in each.text():\n                #print \'ok\'\n                continue\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.mb-8 > a\').text()\n            \n            url = each.find(\'.mb-8 > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pager-next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        query_id = response.save.get(\'id\')\n        sql = \"update tb_query set flag=1 where id=%s\" % (query_id)\n        try:           \n            cursor.execute(sql)\n            #print query_id\n            #print \'ok\'\n            conn.commit()\n        except Exception, e:\n            print e\n        #if response.doc(\'.word-replace\'):\n        #    return\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = \'baidu\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'category_id\'] = []\n        res_dict[\'class\'] = 34\n        #print response.doc(\'.q-content\').html()\n        res_dict[\'question_detail\'] = response.doc(\'.q-content\').text() if not response.doc(\'.q-content .word-replace\') else \'\'\n        \n        answers_list = []\n        if response.doc(\'.content > .mb-10\'):\n            if not response.doc(\'.content > .mb-10 .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'div.mb-15 > .pos-time\'):\n                    create_time = response.doc(\'div.mb-15 > .pos-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.content > .mb-10\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'div.mt-10\').text(),\n                })\n        if response.doc(\'.quality-content > .quality-content-detail\'):\n            if not response.doc(\'.quality-content > .quality-content-detail .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if response.doc(\'.quality-info > .reply-time\'):\n                    create_time = response.doc(\'.quality-info > .reply-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.quality-content > .quality-content-detail\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'.q-name > a\').text(),\n                })\n        if response.doc(\'.ec-answer\'):\n            #print u\'企业回答\'\n            if not response.doc(\'.ec-answer .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'.ec-time\'):\n                    create_time = response.doc(\'.ec-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.ec-answer\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": \'\',\n                })\n        if response.doc(\'.best-related dd > span\'):\n            if not response.doc(\'.best-related dd > span .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n                if  response.doc(\'.best-related i > span\'):\n                    create_time = response.doc(\'.best-related i\').eq(-1).text().split()[0]\n                    user_name = response.doc(\'.best-related i\').eq(0).text()\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                    user_name = \'\'\n                answers_list.append({\n                    \"content\":  response.doc(\'.best-related dd > span\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": user_name,\n                })\n        for each in response.doc(\'div.answer-text\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            if  each.parent().prev().find(\'.pos-time\'):\n                create_time = each.parent().prev().find(\'.pos-time\').text().split()[0]\n                if not each.parent().prev().find(\'.pos-time\').next():\n                    user_name = each.parent().prev().remove(\'.pos-time\').text()\n                else:\n                    user_name = each.parent().prev().find(\'.pos-time\').next().text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                user_name = \'\'\n            answers_list.append({\n                \"content\":  each.html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": user_name,\n            })\n        for each in response.doc(\'dl.other-answer > .answer\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n            if  each.find(\'.ext-info > i\'):\n                create_time = each.find(\'.ext-info > i\').eq(-1).text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  each.children().eq(0).html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'.ext-info > i\').eq(0).text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'create_user\'] = response.doc(\'.ask-info > span\').eq(1).text()\n        return res_dict',NULL,1.0000,3.0000,1472624270.7981),('wenda6_baidu','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-11 09:57:18\n# Project: wenda6_baidu\n\nfrom pyspider.libs.base_handler import *\nimport MySQLdb\nimport traceback\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nfrom urllib import quote\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"querydb\", charset = \"utf8\")\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n\n        \"headers\": {\n        \'Host\': \'zhidao.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\', \n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n    \n\n    @every(minutes=1 * 30)\n    def on_start(self):\n        sql = \'select id, query from tb_query where flag = 0 limit 3000\'\n        try:\n            cursor.execute(sql)\n            for (query_id, query,) in cursor.fetchall():\n                #print type(query) \n                #query = query.encode(\"UTF-8\")\n                #print type(query)\n                #print query\n                #query = unicode(query,\'utf-8\')\n                #print type(query)\n                #print query\n                #word = \'good\'\n                query = query.replace(\' \',\'%20\')\n\n                for i in range(0,51,10):\n                    \n                    self.crawl(\'http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s\'%(i,query),save = {\'query\':query, \'id\':query_id}, proxy=\"122.96.59.106:80\", headers=self.crawl_config[\'headers\'], callback=self.index_page)\n        except Exception,e:\n            print e\n            \n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.dl\').items():\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.line > a\').text()\n            url = each.find(\'.line > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'],callback=self.detail_page)\n        for each in response.doc(\'.mb-20 > dl\').items():\n            if u\'百度百科\' in each.text():\n                #print \'ok\'\n                continue\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.mb-8 > a\').text()\n            \n            url = each.find(\'.mb-8 > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pager-next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        query_id = response.save.get(\'id\')\n        sql = \"update tb_query set flag=1 where id=%s\" % (query_id)\n        try:           \n            cursor.execute(sql)\n            #print query_id\n            #print \'ok\'\n            conn.commit()\n        except Exception, e:\n            print e\n        #if response.doc(\'.word-replace\'):\n        #    return\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = \'baidu\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'category_id\'] = []\n        res_dict[\'class\'] = 34\n        #print response.doc(\'.q-content\').html()\n        res_dict[\'question_detail\'] = response.doc(\'.q-content\').text() if not response.doc(\'.q-content .word-replace\') else \'\'\n        \n        answers_list = []\n        if response.doc(\'.content > .mb-10\'):\n            if not response.doc(\'.content > .mb-10 .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'div.mb-15 > .pos-time\'):\n                    create_time = response.doc(\'div.mb-15 > .pos-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.content > .mb-10\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'div.mt-10\').text(),\n                })\n        if response.doc(\'.quality-content > .quality-content-detail\'):\n            if not response.doc(\'.quality-content > .quality-content-detail .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if response.doc(\'.quality-info > .reply-time\'):\n                    create_time = response.doc(\'.quality-info > .reply-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.quality-content > .quality-content-detail\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'.q-name > a\').text(),\n                })\n        if response.doc(\'.ec-answer\'):\n            #print u\'企业回答\'\n            if not response.doc(\'.ec-answer .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'.ec-time\'):\n                    create_time = response.doc(\'.ec-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.ec-answer\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": \'\',\n                })\n        if response.doc(\'.best-related dd > span\'):\n            if not response.doc(\'.best-related dd > span .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n                if  response.doc(\'.best-related i > span\'):\n                    create_time = response.doc(\'.best-related i\').eq(-1).text().split()[0]\n                    user_name = response.doc(\'.best-related i\').eq(0).text()\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                    user_name = \'\'\n                answers_list.append({\n                    \"content\":  response.doc(\'.best-related dd > span\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": user_name,\n                })\n        for each in response.doc(\'div.answer-text\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            if  each.parent().prev().find(\'.pos-time\'):\n                create_time = each.parent().prev().find(\'.pos-time\').text().split()[0]\n                if not each.parent().prev().find(\'.pos-time\').next():\n                    user_name = each.parent().prev().remove(\'.pos-time\').text()\n                else:\n                    user_name = each.parent().prev().find(\'.pos-time\').next().text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                user_name = \'\'\n            answers_list.append({\n                \"content\":  each.html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": user_name,\n            })\n        for each in response.doc(\'dl.other-answer > .answer\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n            if  each.find(\'.ext-info > i\'):\n                create_time = each.find(\'.ext-info > i\').eq(-1).text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  each.children().eq(0).html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'.ext-info > i\').eq(0).text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'create_user\'] = response.doc(\'.ask-info > span\').eq(1).text()\n        return res_dict',NULL,2.0000,3.0000,1472624267.6545),('wenda7_baidu','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-13 12:59:36\n# Project: wenda7_baidu\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-11 09:57:18\n# Project: wenda6_baidu\n\nfrom pyspider.libs.base_handler import *\nimport MySQLdb\nimport traceback\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\nfrom urllib import quote\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"querydb\", charset = \"utf8\")\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n\n        \"headers\": {\n        \'Host\': \'zhidao.baidu.com\',\n        \'Pragma\': \'no-cache\',\n        \'Upgrade-Insecure-Requests\': \'1\', \n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n        }\n    }\n    \n\n    @every(minutes=1 * 30)\n    def on_start(self):\n        sql = \'select id, query from tb_query where flag = 0 limit 3000\'\n        try:\n            cursor.execute(sql)\n            for (query_id, query,) in cursor.fetchall():\n                #print type(query) \n                #query = query.encode(\"UTF-8\")\n                #print type(query)\n                #print query\n                #query = unicode(query,\'utf-8\')\n                #print type(query)\n                #print query\n                #word = \'good\'\n                query = query.replace(\' \',\'%20\')\n\n                for i in range(0,51,10):\n                    \n                    self.crawl(\'http://zhidao.baidu.com/search?lm=0&rn=10&pn=%s&fr=search&ie=utf8&word=%s\'%(i,query),save = {\'query\':query, \'id\':query_id}, proxy=\"122.96.59.106:80\", headers=self.crawl_config[\'headers\'], callback=self.index_page)\n        except Exception,e:\n            print e\n            \n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.dl\').items():\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.line > a\').text()\n            url = each.find(\'.line > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'],callback=self.detail_page)\n        for each in response.doc(\'.mb-20 > dl\').items():\n            if u\'百度百科\' in each.text():\n                #print \'ok\'\n                continue\n            _dict = {}\n            _dict[\'id\'] = response.save.get(\'id\') \n            _dict[\'query\'] = response.save.get(\'query\') \n            _dict[\'title\'] = each.find(\'.mb-8 > a\').text()\n            \n            url = each.find(\'.mb-8 > a\').attr.href\n            _dict[\'create_time\'] = each.find(\'.mr-8\').text().split()[0]\n            self.crawl(url, save = _dict,headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'.pager-next\').items():\n         #   self.crawl(each.attr.href, save = response.save, callback=self.index_page)   \n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        query_id = response.save.get(\'id\')\n        sql = \"update tb_query set flag=1 where id=%s\" % (query_id)\n        try:           \n            cursor.execute(sql)\n            #print query_id\n            #print \'ok\'\n            conn.commit()\n        except Exception, e:\n            print e\n        #if response.doc(\'.word-replace\'):\n        #    return\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = \'baidu\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'category_id\'] = []\n        res_dict[\'class\'] = 34\n        #print response.doc(\'.q-content\').html()\n        res_dict[\'question_detail\'] = response.doc(\'.q-content\').text() if not response.doc(\'.q-content .word-replace\') else \'\'\n        \n        answers_list = []\n        if response.doc(\'.content > .mb-10\'):\n            if not response.doc(\'.content > .mb-10 .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'div.mb-15 > .pos-time\'):\n                    create_time = response.doc(\'div.mb-15 > .pos-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.content > .mb-10\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'div.mt-10\').text(),\n                })\n        if response.doc(\'.quality-content > .quality-content-detail\'):\n            if not response.doc(\'.quality-content > .quality-content-detail .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if response.doc(\'.quality-info > .reply-time\'):\n                    create_time = response.doc(\'.quality-info > .reply-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.quality-content > .quality-content-detail\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": response.doc(\'.q-name > a\').text(),\n                })\n        if response.doc(\'.ec-answer\'):\n            #print u\'企业回答\'\n            if not response.doc(\'.ec-answer .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                if  response.doc(\'.ec-time\'):\n                    create_time = response.doc(\'.ec-time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                answers_list.append({\n                    \"content\":  response.doc(\'.ec-answer\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": \'\',\n                })\n        if response.doc(\'.best-related dd > span\'):\n            if not response.doc(\'.best-related dd > span .word-replace\'):\n                if response.doc(\'.question-list-item-tag > a\'):\n                    res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n                res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n                if  response.doc(\'.best-related i > span\'):\n                    create_time = response.doc(\'.best-related i\').eq(-1).text().split()[0]\n                    user_name = response.doc(\'.best-related i\').eq(0).text()\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                    user_name = \'\'\n                answers_list.append({\n                    \"content\":  response.doc(\'.best-related dd > span\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": user_name,\n                })\n        for each in response.doc(\'div.answer-text\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            if  each.parent().prev().find(\'.pos-time\'):\n                create_time = each.parent().prev().find(\'.pos-time\').text().split()[0]\n                if not each.parent().prev().find(\'.pos-time\').next():\n                    user_name = each.parent().prev().remove(\'.pos-time\').text()\n                else:\n                    user_name = each.parent().prev().find(\'.pos-time\').next().text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                user_name = \'\'\n            answers_list.append({\n                \"content\":  each.html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": user_name,\n            })\n        for each in response.doc(\'dl.other-answer > .answer\').items():\n            if each.find(\'.word-replace\'):\n                continue\n            if response.doc(\'.question-list-item-tag > a\'):\n                res_dict[\'category_id\'] = [v.text() for v in response.doc(\'.question-list-item-tag > a\').items() if v]\n            res_dict[\'question_detail\'] = response.doc(\'.qb-content\').html()\n            if  each.find(\'.ext-info > i\'):\n                create_time = each.find(\'.ext-info > i\').eq(-1).text()\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  each.children().eq(0).html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'.ext-info > i\').eq(0).text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'create_user\'] = response.doc(\'.ask-info > span\').eq(1).text()\n        return res_dict',NULL,1.0000,3.0000,1472624264.5529),('wenda_360','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-04 14:47:05\n# Project: wenda_360\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nimport time\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    \n    header = {\n\'Cache-Control\': \'no-cache\',\n\'Connection\': \'keep-alive\',\n\'Host\': \'wenda.so.com\',\n\'Pragma\': \'no-cache\',\n\'Upgrade-Insecure-Requests\': \'1\',\n\'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\',\n\'Cookie\': \'test_cookie_enable=null; __guid=9114931.2491033014933137000.1459824554175.862; WDTKID=414ac95d77915818; count=1; search_last_sid=33e636c8a753b0bad387d3695c798a7b; search_last_kw=%u513F%u6B4C; erules=p4-3%7Cp2-1\'\n   }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        #with open(\'/apps/home/rd/hexing/data/query.txt\',\'r\') as f:\n        #    for line in f:\n        #        if line:\n        #            line = line.strip()\n        line = \'儿歌\'\n        for index in range(20):\n            self.crawl(\'http://wenda.so.com/search/?q=%s&pn=%s\'%(line,index),save = {\'query\':line} ,headers = self.header,callback=self.index_page)\n            \n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for  each  in response.doc(\'.qa-list .item\').items():\n                _dict = {}\n                _dict[\'query\'] = response.save.get(\'query\') \n                _dict[\'title\'] = each.find(\'.qa-i-hd a\').text()\n                url = each.find(\'.qa-i-hd a\').attr.href\n                _dict[\'create_time\'] = each.find(\'.qa-i-ft\').text().split()[-1].replace(\'.\', \'-\')\n                self.crawl(url, save = _dict,headers = self.header,callback=self.detail_page)\n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = \'360\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'question_detail\'] = response.doc(\'.q-cnt\').text()\n        if  response.doc(\'.bd .text > span\').text():\n            create_time = response.doc(\'.bd .text > span\').text().split()[-1].replace(\'.\',\'-\')\n        else:\n            create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n        res_dict[\'answers\'] = {\n            \"content\":  response.doc(\'.resolved-cnt\').html(),\n            \"create_time\": create_time,\n            \"user_name\": response.doc(\'.text > div a\').text(),\n        }\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'create_user\'] = response.doc(\'.text > span > .ask-author\').text()\n        return res_dict\n',NULL,1.0000,3.0000,1472624630.5586),('wenda_360_jiaoyu','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-04-05 10:49:43\n# Project: qa_360\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    dic = {\'http://wenda.so.com/c/2763?pn=0&filt=20\': u\'\\u6821\\u56ed\\u8bdd\\u9898\', \'http://wenda.so.com/c/578?pn=0&filt=20\': u\'\\u5347\\u5b66\\u5165\\u5b66\', \'http://wenda.so.com/c/2790?pn=0&filt=20\': u\'\\u751f\\u7269\', \'http://wenda.so.com/c/2787?pn=0&filt=20\': u\'\\u5b66\\u4e60\\u65b9\\u6cd5\', \'http://wenda.so.com/c/96?pn=0&filt=20\': u\'\\u7406\\u5de5\\u5b66\\u79d1\', \'http://wenda.so.com/c/2794?pn=0&filt=20\': u\'\\u5316\\u5b66\', \'http://wenda.so.com/c/2793?pn=0&filt=20\': u\'\\u6570\\u5b66\', \'http://wenda.so.com/c/2789?pn=0&filt=20\': u\'\\u7269\\u7406\', \'http://wenda.so.com/c/2792?pn=0&filt=20\': u\'\\u519c\\u4e1a\', \'http://wenda.so.com/c/100?pn=0&filt=20\': u\'\\u516c\\u52a1\\u5458\', \'http://wenda.so.com/c/101?pn=0&filt=20\': u\'\\u7559\\u5b66\\u51fa\\u56fd\', \'http://wenda.so.com/c/138?pn=0&filt=20\': u\'\\u5bb6\\u5ead\\u6559\\u80b2\', \'http://wenda.so.com/c/99?pn=0&filt=20\': u\'\\u8d44\\u683c\\u8003\\u8bd5\', \'http://wenda.so.com/c/98?pn=0&filt=20\': u\'\\u4e2d\\u5c0f\\u5b66\\u4f5c\\u4e1a\', \'http://wenda.so.com/c/95?pn=0&filt=20\': u\'\\u4eba\\u6587\\u793e\\u79d1\', \'http://wenda.so.com/c/2788?pn=0&filt=20\': u\'\\u8bed\\u6587\', \'http://wenda.so.com/c/94?pn=0&filt=20\': u\'\\u9ad8\\u7b49\\u9662\\u6821\', \'http://wenda.so.com/c/97?pn=0&filt=20\': u\'\\u5916\\u8bed\', \'http://wenda.so.com/c/2791?pn=0&filt=20\': u\'\\u5de5\\u7a0b\\u5b66\'}\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k, v in self.dic.iteritems():\n            self.crawl(k, save={\'type\': v}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.question-wrap a\').items():\n            \n            self.crawl(each.attr.href,save={\'type\': response.save[\'type\'],}, callback=self.detail_page)\n        for each in response.doc(\'.pagination a\').items():\n            self.crawl(each.attr.href, save={\'type\': response.save[\'type\']}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for info in response.doc(\'.mod-resolved-ans > .bd\').items():\n            _dic = {}\n            _dic[\'name\'] = info.find(\'.text > div a\').text()\n            try:\n                _dic[\'date\'] = info.find(\'.text > span\').text().split()[-1].replace(\'.\',\'-\')\n            except:\n                pass\n            _dic[\'content\'] = info.find(\'.resolved-cnt\').html()\n            _dic[\'avatar\'] = info.find(\'.info img\').attr.src\n            content_list.append((_dic))\n        for info in response.doc(\'.mod-other-ans li\').items():\n            try:\n                _dic = {}\n                _dic[\'name\'] = info.find(\'.text > div a\').text()\n                _dic[\'date\'] = info.find(\'.text > span\').text().split()[-1].replace(\'.\',\'-\')\n                _dic[\'content\'] = info.find(\'.other-ans-cnt\').html()\n                _dic[\'avatar\'] = info.find(\'.info img\').attr.src\n                content_list.append((_dic))\n            except:\n                continue\n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.js-ask-title\').text(),\n            \"type\": response.save[\'type\'],\n            \"answers\": content_list,\n            \"question_detail\": response.doc(\'.q-cnt\').text(),\n            \"create_date\": response.doc(\'.hd .text > span\').text().split()[-1],\n        }\n',NULL,5.0000,5.0000,1472624285.4987),('wenda_51offer','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-27 17:54:11\n# Project: wenda_51offer\n\nfrom pyspider.libs.base_handler import *\nimport re\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for i in range(48):\n            self.crawl(\'http://www.51offer.com/wenda/index.html?problemType=1&currentPage=%d\'%i, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.s_question\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)           \n        # 翻页\n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n          \n    @config(priority=2)\n    def detail_page(self, response):\n        r = re.compile(\"[^0-9:\\s]\")\n        t_str = response.doc(\'.detail .timeset\').eq(0).text()\n        t_arr = r.split(t_str)\n        answer = \'\'\n        flag = 1\n        for each in response.doc(\'.editBox  p\').items():\n            if u\'社区推荐\' in each.text() or u\'热门内容推荐\' in each.text() :\n                flag = 0\n                break\n            else:\n                if each.html():\n                    answer +=  each.html()\n        res_answer = response.doc(\'.answerContent\').eq(0).text() + response.doc(\'.detail > div > div\').eq(0).text() if flag else answer\n        if not res_answer:\n            return None\n        return {\n            \"url\": response.url,\n            \"question\": response.doc(\'.askTitle\').text(),\n            \"types\": response.doc(\'.askLabel > a\').text(),\n            \"question_detail\": response.doc(\'.ask > div\').eq(1).text(),\n            \"create_date\": t_arr[0]+\'-\'+t_arr[1]+\'-\'+t_arr[2],\n            \"answer\": res_answer,\n            \"answer_date\": t_arr[0]+\'-\'+t_arr[1]+\'-\'+t_arr[2],\n    }',NULL,1.0000,3.0000,1472624260.8111),('wenda_cxy_bky','cxy','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 10:54:58\n# Project: wenda_cxy_bky\n\n\nfrom pyspider.libs.base_handler import *\n\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'周前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'个月前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'年前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'https://q.cnblogs.com/tag/list?pageindex=1\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'tr td li a\').items():\n            self.crawl(each.attr.href+\'/solved\', callback=self.list_page)\n        #翻页\n        for each in response.doc(\'#pager a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n            \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.news_item\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.news_entry > a\').text()\n            self.crawl(each.find(\'.news_entry > a\').attr.href, save=_dict,  callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'#pager a\').items():\n            self.crawl(each.attr.href, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for info in response.doc(\'.qitem_best_answer_inner\').items():\n            _dic = {}\n            #print info.find(\'.answer_author\').text()\n            _dic[\'name\'] = info.find(\'.answer_author > .bluelink\').eq(0).text()\n            _dic[\'date\'] = info.find(\'.answer_author\').text().split()[-2]\n            _dic[\'content\'] = info.find(\'.q_content\').html().strip()\n            content_list.append((_dic))\n        for info in response.doc(\'.qitem_all_answer_inner > .q_answeritem \').items():\n            _dic = {}\n            #print info.find(\'.answer_author\').text()\n            _dic[\'name\'] = info.find(\'.answer_author > .bluelink\').eq(0).text()\n            _dic[\'date\'] = info.find(\'.answer_author\').text().split()[-2]\n            _dic[\'content\'] = info.find(\'.q_content\').html().strip()\n            content_list.append((_dic))\n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'程序员\',\n            \"answers\": content_list,\n            \"source\": u\'博客园\',\n            \"question_detail\": response.doc(\'#qes_content\').html().strip(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471655369.7138),('wenda_cxy_bky_inc','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-22 18:08:37\n# Project: wenda_cxy_bky_inc\n\n\n\nfrom pyspider.libs.base_handler import *\n\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'周前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'个月前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'年前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'https://q.cnblogs.com/tag/list?pageindex=1\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'tr td li a\').items():\n            self.crawl(each.attr.href+\'/solved\', callback=self.list_page)\n        #翻页\n        for each in response.doc(\'#pager a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n            \n    @config(age=1 * 1)\n    def list_page(self, response):\n        for each in response.doc(\'.news_item\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.news_entry > a\').text()\n            self.crawl(each.find(\'.news_entry > a\').attr.href, save=_dict,  callback=self.detail_page)\n        #翻页\n        #for each in response.doc(\'#pager a\').items():\n         #   self.crawl(each.attr.href, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for info in response.doc(\'.qitem_best_answer_inner\').items():\n            _dic = {}\n            #print info.find(\'.answer_author\').text()\n            _dic[\'name\'] = info.find(\'.answer_author > .bluelink\').eq(0).text()\n            _dic[\'date\'] = info.find(\'.answer_author\').text().split()[-2]\n            _dic[\'content\'] = info.find(\'.q_content\').html().strip()\n            content_list.append((_dic))\n        for info in response.doc(\'.qitem_all_answer_inner > .q_answeritem \').items():\n            _dic = {}\n            #print info.find(\'.answer_author\').text()\n            _dic[\'name\'] = info.find(\'.answer_author > .bluelink\').eq(0).text()\n            _dic[\'date\'] = info.find(\'.answer_author\').text().split()[-2]\n            _dic[\'content\'] = info.find(\'.q_content\').html().strip()\n            content_list.append((_dic))\n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'程序员\',\n            \"answers\": content_list,\n            \"source\": u\'博客园\',\n            \"question_detail\": response.doc(\'#qes_content\').html().strip(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1472624256.5877),('wenda_cxy_csdn','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-19 18:22:17\n# Project: wenda_cxy_csdn\n\n\n\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 10:14:04\n# Project: csdn_proxy_change\n\n\n\nfrom pyspider.libs.base_handler import *\n\nimport json,re\nmenu = \'\'\'{\n    \"forumNodes\": [{\"name\":\"\\u79fb\\u52a8\\u5f00\\u53d1\",\"url\":\"/forums/Mobile\",\"children\":[{\"name\":\"iOS\",\"url\":\"/forums/ios\"},{\"name\":\"Android\",\"url\":\"/forums/Android\"},{\"name\":\"Swift\",\"url\":\"/forums/swift\"},{\"name\":\"Windows\\u5ba2\\u6237\\u7aef\\u5f00\\u53d1\",\"url\":\"/forums/WindowsMobile\"},{\"name\":\"Symbian\",\"url\":\"/forums/Symbian\"},{\"name\":\"BlackBerry\",\"url\":\"/forums/BlackBerry\"},{\"name\":\"Qt\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u79fb\\u52a8\\u652f\\u4ed8\",\"url\":\"/forums/PaypalCommunity\"},{\"name\":\"\\u79fb\\u52a8\\u5e7f\\u544a\",\"url\":\"/forums/MobileAD\"},{\"name\":\"\\u5fae\\u4fe1\\u5f00\\u53d1\",\"url\":\"/forums/weixin\"},{\"name\":\"\\u79fb\\u52a8\\u5f00\\u53d1\\u5176\\u4ed6\\u95ee\\u9898\",\"url\":\"/forums/Mobile_Other\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/MobileNonTechnical\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"},{\"name\":\"\\u8054\\u901aWO+\\u5f00\\u653e\\u5e73\\u53f0\",\"url\":\"/forums/chinaunicom\"}]},{\"name\":\"\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/CloudComputing\",\"children\":[{\"name\":\"IaaS\",\"children\":[{\"name\":\"OpenStack\",\"url\":\"/forums/OpenStack\"}]},{\"name\":\"PaaS/SaaS\",\"children\":[{\"name\":\"Cloud Foundry\",\"url\":\"/forums/CloudFoundry\"},{\"name\":\"GAE\",\"url\":\"/forums/GAE\"}]},{\"name\":\"\\u6570\\u636e\\u4e2d\\u5fc3\\u8fd0\\u7ef4\",\"children\":[{\"name\":\"\\u670d\\u52a1\\u5668\",\"url\":\"/forums/server\"},{\"name\":\"\\u7f51\\u7edc\",\"url\":\"/forums/network\"},{\"name\":\"\\u865a\\u62df\\u5316\",\"url\":\"/forums/virtual\"}]},{\"name\":\"AWS\",\"url\":\"/forums/AWS\"},{\"name\":\"\\u534e\\u4e3a\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/fusioncloud\"},{\"name\":\"\\u5f00\\u653e\\u5e73\\u53f0\",\"url\":\"/forums/OpenAPI\"},{\"name\":\"\\u4e91\\u5b89\\u5168\",\"url\":\"/forums/ST_Security\"},{\"name\":\"\\u5206\\u5e03\\u5f0f\\u8ba1\\u7b97/Hadoop\",\"url\":\"/forums/hadoop\"},{\"name\":\"\\u4e91\\u5b58\\u50a8\",\"url\":\"/forums/CloudStorage\"},{\"name\":\"Docker\",\"url\":\"/forums/docker\"},{\"name\":\"Spark\",\"url\":\"/forums/spark\"},{\"name\":\"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\",\"url\":\"/forums/DE\"}]},{\"name\":\"\\u4f01\\u4e1aIT\",\"url\":\"/forums/Enterprise\",\"children\":[{\"name\":\"\\u4e2d\\u95f4\\u4ef6\",\"children\":[{\"name\":\"\\u4e2d\\u95f4\\u4ef6\",\"url\":\"/forums/Middleware\"},{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"JBoss\",\"url\":\"/forums/JBoss\"}]},{\"name\":\"\\u4f01\\u4e1a\\u7ba1\\u7406\\u8f6f\\u4ef6\",\"children\":[{\"name\":\"\\u6d88\\u606f\\u534f\\u4f5c\",\"url\":\"/forums/ExchangeServer\"},{\"name\":\"SharePoint\",\"url\":\"/forums/SharePoint\"}]},{\"name\":\"Atlassian\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/atlassian\"},{\"name\":\"JetBrains\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/JetBrains\"},{\"name\":\"\\u5730\\u7406\\u4fe1\\u606f\\u7cfb\\u7edf\",\"url\":\"/forums/GIS\"},{\"name\":\"\\u4f01\\u4e1a\\u4fe1\\u606f\\u5316\",\"url\":\"/forums/Enterprise_Information\"},{\"name\":\"ERP/CRM\",\"url\":\"/forums/ERP\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/Enterprise_Other\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"},{\"name\":\"Enterprise Architect\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/EA\"}]},{\"name\":\".NET\\u6280\\u672f\",\"url\":\"/forums/DotNET\",\"children\":[{\"name\":\"C#\",\"url\":\"/forums/CSharp\"},{\"name\":\"ASP.NET\",\"url\":\"/forums/ASPDotNET\"},{\"name\":\".NET Framework\",\"url\":\"/forums/DotNETFramework\"},{\"name\":\"Web Services\",\"url\":\"/forums/DotNETWebServices\"},{\"name\":\"VB.NET\",\"url\":\"/forums/VBDotNET\"},{\"name\":\"VC.NET\",\"url\":\"/forums/VCDotNet\"},{\"name\":\"\\u56fe\\u8868\\u533a\",\"url\":\"/forums/DotNETReport\"},{\"name\":\".NET\\u6280\\u672f\\u524d\\u77bb\",\"url\":\"/forums/DotNET_NewTech\"},{\"name\":\".NET\\u5206\\u6790\\u4e0e\\u8bbe\\u8ba1\",\"url\":\"/forums/DotNETAnalysisAndDesign\"},{\"name\":\"\\u7ec4\\u4ef6/\\u63a7\\u4ef6\\u5f00\\u53d1\",\"url\":\"/forums/DotNET_Controls\"},{\"name\":\"SharePoint\",\"url\":\"/forums/SharePoint\"},{\"name\":\"WPF/Silverlight\",\"url\":\"/forums/Silverlight\"},{\"name\":\"LINQ\",\"url\":\"/forums/LINQ\"},{\"name\":\"VSTS\",\"url\":\"/forums/VSTS\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/DotNET_Other\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DotNETNonTechnical\"},{\"name\":\"Xamarin\\u6280\\u672f\",\"url\":\"/forums/Xamarin\"}]},{\"name\":\"Java \\u6280\\u672f\",\"url\":\"/forums/Java\",\"children\":[{\"name\":\"Java SE\",\"url\":\"/forums/J2SE\"},{\"name\":\"J2ME\",\"url\":\"/forums/J2ME\"},{\"name\":\"Java Web \\u5f00\\u53d1\",\"url\":\"/forums/Java_WebDevelop\"},{\"name\":\"Java EE\",\"url\":\"/forums/J2EE\"},{\"name\":\"Eclipse\",\"url\":\"/forums/Eclipse\"},{\"name\":\"Java\\u5176\\u4ed6\\u76f8\\u5173\",\"url\":\"/forums/JavaOther\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/JavaNonTechnical\"},{\"name\":\"JBoss\",\"url\":\"/forums/JBoss\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"}]},{\"name\":\"Web \\u5f00\\u53d1\",\"url\":\"/forums/WebDevelop\",\"children\":[{\"name\":\"ASP\",\"url\":\"/forums/ASP\"},{\"name\":\"ASP.NET\",\"url\":\"/forums/ASPDotNET\"},{\"name\":\"JSP\",\"url\":\"/forums/Java_WebDevelop\"},{\"name\":\"PHP\",\"url\":\"/forums/PHP\",\"children\":[{\"name\":\"\\u5f00\\u6e90\\u8d44\\u6e90\",\"url\":\"/forums/PHPOpenSource\"},{\"name\":\"\\u57fa\\u7840\\u7f16\\u7a0b\",\"url\":\"/forums/PHPBase\"},{\"name\":\"Framework\",\"url\":\"/forums/PHPFramework\"}]},{\"name\":\"JavaScript\",\"url\":\"/forums/JavaScript\"},{\"name\":\"\\u641c\\u7d22\\u5f15\\u64ce\\u6280\\u672f\",\"url\":\"/forums/SearchEngine\"},{\"name\":\"Ajax \\u6280\\u672f\",\"url\":\"/forums/Ajax\"},{\"name\":\"VBScript\",\"url\":\"/forums/vbScript\"},{\"name\":\"CGI\",\"url\":\"/forums/CGI\"},{\"name\":\"XML/XSL\",\"url\":\"/forums/XMLSOAP\"},{\"name\":\"IIS\",\"url\":\"/forums/IIS\"},{\"name\":\"Apache\",\"url\":\"/forums/Apache\"},{\"name\":\"HTML(CSS)\",\"url\":\"/forums/HTMLCSS\"},{\"name\":\"ColdFusion\",\"url\":\"/forums/ColdFusion\"},{\"name\":\"Ruby/Rails\",\"url\":\"/forums/ROR\"},{\"name\":\"\\u8de8\\u6d4f\\u89c8\\u5668\\u5f00\\u53d1\",\"url\":\"/forums/CrossBrowser\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/WebDevelop_Other\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/WebNonTechnical\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"HTML5\",\"url\":\"/forums/HTML5\"}]},{\"name\":\"\\u5f00\\u53d1\\u8bed\\u8a00/\\u6846\\u67b6\",\"children\":[{\"name\":\"VC/MFC\",\"url\":\"/forums/VC\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/VC_Basic\"},{\"name\":\"\\u754c\\u9762\",\"url\":\"/forums/VC_UI\"},{\"name\":\"\\u7f51\\u7edc\\u7f16\\u7a0b\",\"url\":\"/forums/VC_Network\"},{\"name\":\"\\u8fdb\\u7a0b/\\u7ebf\\u7a0b/DLL\",\"url\":\"/forums/VC_Process\"},{\"name\":\"ATL/ActiveX/COM\",\"url\":\"/forums/VC_ActiveX\"},{\"name\":\"\\u6570\\u636e\\u5e93\",\"url\":\"/forums/VC_Database\"},{\"name\":\"\\u786c\\u4ef6/\\u7cfb\\u7edf\",\"url\":\"/forums/VC_Hardware\"},{\"name\":\"HTML/XML\",\"url\":\"/forums/VC_HTML\"},{\"name\":\"\\u56fe\\u5f62\\u5904\\u7406/\\u7b97\\u6cd5\",\"url\":\"/forums/VC_ImageProcessing\"},{\"name\":\"\\u8d44\\u6e90\",\"url\":\"/forums/VCResources\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7c7b\",\"url\":\"/forums/VC_NonTechnical\"}]},{\"name\":\"VB\",\"url\":\"/forums/VB\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/VB_Basic\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7c7b\",\"url\":\"/forums/VB_NonTechnical\"},{\"name\":\"\\u63a7\\u4ef6\",\"url\":\"/forums/VB_Controls\"},{\"name\":\"API\",\"url\":\"/forums/VB_API\"},{\"name\":\"\\u6570\\u636e\\u5e93(\\u5305\\u542b\\u6253\\u5370\\uff0c\\u5b89\\u88c5\\uff0c\\u62a5\\u8868)\",\"url\":\"/forums/VB_Database\"},{\"name\":\"\\u591a\\u5a92\\u4f53\",\"url\":\"/forums/VB_Multimedia\"},{\"name\":\"\\u7f51\\u7edc\\u7f16\\u7a0b\",\"url\":\"/forums/VB_Network\"},{\"name\":\"VBA\",\"url\":\"/forums/VBA\"},{\"name\":\"COM/DCOM/COM+\",\"url\":\"/forums/VB_COM\"},{\"name\":\"\\u8d44\\u6e90\",\"url\":\"/forums/VBResources\"}]},{\"name\":\"Delphi\",\"url\":\"/forums/Delphi\",\"children\":[{\"name\":\"VCL\\u7ec4\\u4ef6\\u5f00\\u53d1\\u53ca\\u5e94\\u7528\",\"url\":\"/forums/DelphiVCL\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u76f8\\u5173\",\"url\":\"/forums/DelphiDB\"},{\"name\":\"Windows SDK/API\",\"url\":\"/forums/DelphiAPI\"},{\"name\":\"\\u7f51\\u7edc\\u901a\\u4fe1/\\u5206\\u5e03\\u5f0f\\u5f00\\u53d1\",\"url\":\"/forums/DelphiNetwork\"},{\"name\":\"\\u8bed\\u8a00\\u57fa\\u7840/\\u7b97\\u6cd5/\\u7cfb\\u7edf\\u8bbe\\u8ba1\",\"url\":\"/forums/DelphiBase\"},{\"name\":\"GAME\\uff0c\\u56fe\\u5f62\\u5904\\u7406/\\u591a\\u5a92\\u4f53\",\"url\":\"/forums/DelphiMultimedia\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DelphiNonTechnical\"}]},{\"name\":\"C++ Builder\",\"url\":\"/forums/BCB\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/BCBBase\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u53ca\\u76f8\\u5173\\u6280\\u672f\",\"url\":\"/forums/BCBDB\"},{\"name\":\"VCL\\u7ec4\\u4ef6\\u4f7f\\u7528\\u548c\\u5f00\\u53d1\",\"url\":\"/forums/BCBVCL\"},{\"name\":\"Windows SDK/API\",\"url\":\"/forums/BCBAPI\"},{\"name\":\"\\u7f51\\u7edc\\u53ca\\u901a\\u8baf\\u5f00\\u53d1\",\"url\":\"/forums/BCBNetwork\"},{\"name\":\"ActiveX/COM/DCOM\",\"url\":\"/forums/BCBCOM\"},{\"name\":\"\\u8336\\u9986\",\"url\":\"/forums/BCBTeaHouses\"}]},{\"name\":\"C/C++\",\"url\":\"/forums/Cpp\",\"children\":[{\"name\":\"\\u65b0\\u624b\\u4e50\\u56ed\",\"url\":\"/forums/Cpp_Freshman\"},{\"name\":\"C\\u8bed\\u8a00\",\"url\":\"/forums/C\"},{\"name\":\"C++ \\u8bed\\u8a00\",\"url\":\"/forums/CPPLanguage\"},{\"name\":\"\\u5de5\\u5177\\u5e73\\u53f0\\u548c\\u7a0b\\u5e8f\\u5e93\",\"url\":\"/forums/Cpp_ToolsPlatform\"},{\"name\":\"\\u6a21\\u5f0f\\u53ca\\u5b9e\\u73b0\",\"url\":\"/forums/Cpp_Model\"},{\"name\":\"\\u5176\\u4ed6\\u6280\\u672f\\u95ee\\u9898\",\"url\":\"/forums/Cpp_Other\"},{\"name\":\"Qt\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Cpp_NonTechnical\"}]},{\"name\":\"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00\",\"url\":\"/forums/OtherLanguage\",\"open\":true,\"children\":[{\"name\":\"OpenCL\\u548c\\u5f02\\u6784\\u7f16\\u7a0b\",\"url\":\"/forums/Heterogeneous\"},{\"name\":\"Go\\u8bed\\u8a00\",\"url\":\"/forums/golang\"},{\"name\":\"JBoss\\u6280\\u672f\\u4ea4\\u6d41\",\"url\":\"/forums/JBoss\"},{\"name\":\"\\u6c47\\u7f16\\u8bed\\u8a00\",\"url\":\"/forums/ASM\"},{\"name\":\"\\u811a\\u672c\\u8bed\\u8a00\\uff08Perl/Python\\uff09\",\"url\":\"/forums/OL_Script\"},{\"name\":\"Office\\u5f00\\u53d1/ VBA\",\"url\":\"/forums/OfficeDevelopment\"},{\"name\":\"VFP\",\"url\":\"/forums/VFP\"},{\"name\":\"\\u5176\\u4ed6\\u5f00\\u53d1\\u8bed\\u8a00\",\"url\":\"/forums/OtherLanguage_Other\"}]}]},{\"name\":\"\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":null,\"children\":[{\"name\":\"\\u5927\\u6570\\u636e\",\"children\":[{\"name\":\"Hadoop\",\"url\":\"/forums/hadoop\"}]},{\"name\":\"MS-SQL Server\",\"url\":\"/forums/MSSQL\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/MSSQL_Basic\"},{\"name\":\"\\u5e94\\u7528\\u5b9e\\u4f8b\",\"url\":\"/forums/MSSQL_Cases\"},{\"name\":\"\\u7591\\u96be\\u95ee\\u9898\",\"url\":\"/forums/MSSQL_DifficultProblems\"},{\"name\":\"\\u65b0\\u6280\\u672f\\u524d\\u6cbf\",\"url\":\"/forums/MSSQL_NewTech\"},{\"name\":\"SQL Server BI\",\"url\":\"/forums/SQLSERVERBI\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7248\",\"url\":\"/forums/MSSQL_NonTechnical\"}]},{\"name\":\"PowerBuilder\",\"url\":\"/forums/PowerBuilder\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u7c7b\",\"url\":\"/forums/PB_Basic\"},{\"name\":\"Pb\\u811a\\u672c\\u8bed\\u8a00\",\"url\":\"/forums/PBScript\"},{\"name\":\"DataWindow\",\"url\":\"/forums/PB_DataWindow\"},{\"name\":\"API \\u8c03\\u7528\",\"url\":\"/forums/PB_API\"},{\"name\":\"\\u63a7\\u4ef6\\u4e0e\\u754c\\u9762\",\"url\":\"/forums/PB_Controls\"},{\"name\":\"Pb Web \\u5e94\\u7528\",\"url\":\"/forums/PB_WEB\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u76f8\\u5173\",\"url\":\"/forums/PB_Database\"},{\"name\":\"\\u9879\\u76ee\\u7ba1\\u7406\",\"url\":\"/forums/PB_ProjectManagement\"},{\"name\":\"\\u975e\\u6280\\u672f\\u7248\",\"url\":\"/forums/PB_NonTechnical\"}]},{\"name\":\"Oracle\",\"url\":\"/forums/Oracle\",\"children\":[{\"name\":\"\\u57fa\\u7840\\u548c\\u7ba1\\u7406\",\"url\":\"/forums/Oracle_Management\"},{\"name\":\"\\u5f00\\u53d1\",\"url\":\"/forums/Oracle_Develop\"},{\"name\":\"\\u9ad8\\u7ea7\\u6280\\u672f\",\"url\":\"/forums/Oracle_Technology\"},{\"name\":\"\\u8ba4\\u8bc1\\u4e0e\\u8003\\u8bd5\",\"url\":\"/forums/Oracle_Certificate\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Oracle_NonTechnical\"}]},{\"name\":\"Informatica\",\"url\":\"/forums/Informatica\"},{\"name\":\"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/OtherDatabase\",\"open\":true,\"children\":[{\"name\":\"IBM DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"MongoDB\",\"url\":\"/forums/MongoDB\"},{\"name\":\"\\u6570\\u636e\\u4ed3\\u5e93\",\"url\":\"/forums/DataWarehouse\"},{\"name\":\"VFP\",\"url\":\"/forums/VFP\"},{\"name\":\"Access\",\"url\":\"/forums/Access\"},{\"name\":\"Sybase\",\"url\":\"/forums/Sybase\"},{\"name\":\"Informix\",\"url\":\"/forums/Informix\"},{\"name\":\"MySQL\",\"url\":\"/forums/MySQL\"},{\"name\":\"PostgreSQL\",\"url\":\"/forums/PostgreSQL\"},{\"name\":\"\\u6570\\u636e\\u5e93\\u62a5\\u8868\",\"url\":\"/forums/DatabaseReport\"},{\"name\":\"\\u5176\\u4ed6\\u6570\\u636e\\u5e93\",\"url\":\"/forums/OtherDatabase_Other\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/HPDatabase\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/DatabaseNonTechnical\"}]}]},{\"name\":\"Linux/Unix\\u793e\\u533a\",\"url\":\"/forums/Linux\",\"children\":[{\"name\":\"\\u7cfb\\u7edf\\u7ef4\\u62a4\\u4e0e\\u4f7f\\u7528\\u533a\",\"url\":\"/forums/Linux_System\"},{\"name\":\"\\u5e94\\u7528\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a\",\"url\":\"/forums/Linux_Development\"},{\"name\":\"\\u5185\\u6838\\u6e90\\u4ee3\\u7801\\u7814\\u7a76\\u533a\",\"url\":\"/forums/Linux_Kernel\"},{\"name\":\"\\u9a71\\u52a8\\u7a0b\\u5e8f\\u5f00\\u53d1\\u533a\",\"url\":\"/forums/Linux_Driver\"},{\"name\":\"CPU\\u548c\\u786c\\u4ef6\\u533a\",\"url\":\"/forums/Linux_Hardware\"},{\"name\":\"\\u4e13\\u9898\\u6280\\u672f\\u8ba8\\u8bba\\u533a\",\"url\":\"/forums/Linux_SpecialTopic\"},{\"name\":\"\\u5b9e\\u7528\\u8d44\\u6599\\u53d1\\u5e03\\u533a\",\"url\":\"/forums/Linux_Information\"},{\"name\":\"UNIX\\u6587\\u5316\",\"url\":\"/forums/Unix_Culture\"},{\"name\":\"Solaris\",\"url\":\"/forums/Solaris\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/LinuxNonTechnical\"}]},{\"name\":\"Windows\\u4e13\\u533a\",\"url\":\"/forums/Windows\",\"children\":[{\"name\":\"Windows\\u5ba2\\u6237\\u7aef\\u4f7f\\u7528\",\"url\":\"/forums/Windows7\"},{\"name\":\"Windows Server\",\"url\":\"/forums/WinNT2000XP2003\"},{\"name\":\"\\u7f51\\u7edc\\u7ba1\\u7406\\u4e0e\\u914d\\u7f6e\",\"url\":\"/forums/NetworkConfiguration\"},{\"name\":\"\\u5b89\\u5168\\u6280\\u672f/\\u75c5\\u6bd2\",\"url\":\"/forums/WindowsSecurity\"},{\"name\":\"\\u4e00\\u822c\\u8f6f\\u4ef6\\u4f7f\\u7528\",\"url\":\"/forums/WindowsBase\"},{\"name\":\"Microsoft Office\\u5e94\\u7528\",\"url\":\"/forums/OfficeBase\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/WindowsNonTechnical\"}]},{\"name\":\"\\u786c\\u4ef6/\\u5d4c\\u5165\\u5f00\\u53d1\",\"url\":\"/forums/Embedded\",\"children\":[{\"name\":\"\\u5d4c\\u5165\\u5f00\\u53d1(WinCE)\",\"url\":\"/forums/WinCE\"},{\"name\":\"\\u6c47\\u7f16\\u8bed\\u8a00\",\"url\":\"/forums/ASM\"},{\"name\":\"\\u786c\\u4ef6\\u8bbe\\u8ba1\",\"url\":\"/forums/Embedded_hardware\"},{\"name\":\"\\u9a71\\u52a8\\u5f00\\u53d1/\\u6838\\u5fc3\\u5f00\\u53d1\",\"url\":\"/forums/Embedded_driver\"},{\"name\":\"\\u5355\\u7247\\u673a/\\u5de5\\u63a7\",\"url\":\"/forums/Embedded_SCM\"},{\"name\":\"\\u65e0\\u7ebf\",\"url\":\"/forums/Embedded_wireless\"},{\"name\":\"\\u5176\\u4ed6\\u786c\\u4ef6\\u5f00\\u53d1\",\"url\":\"/forums/Embedded_Other\"},{\"name\":\"VxWorks\\u5f00\\u53d1\",\"url\":\"/forums/VxWorks\"},{\"name\":\"Qt\\u5f00\\u53d1\",\"url\":\"/forums/Qt\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/EmbeddedNonTechnical\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\",\"url\":\"/forums/HPC\"},{\"name\":\"\\u667a\\u80fd\\u786c\\u4ef6\",\"url\":\"/forums/SmartHardware\"}]},{\"name\":\"\\u6e38\\u620f\\u5f00\\u53d1\",\"url\":\"/forums/GameDevelop\",\"children\":[{\"name\":\"Cocos2d-x\",\"url\":\"/forums/GD_Cocos2d-x\"},{\"name\":\"Unity3D\",\"url\":\"/forums/GD_Unity3D\"},{\"name\":\"\\u5176\\u4ed6\\u6e38\\u620f\\u5f15\\u64ce\",\"url\":\"/forums/Othergameengines\"},{\"name\":\"\\u6e38\\u620f\\u7b56\\u5212\\u4e0e\\u8fd0\\u8425\",\"url\":\"/forums/Gdesignoperation\"}]},{\"name\":\"\\u7f51\\u7edc\\u4e0e\\u901a\\u4fe1\",\"url\":\"/forums/network_communication\",\"children\":[{\"name\":\"\\u7f51\\u7edc\\u534f\\u8bae\\u4e0e\\u914d\\u7f6e\",\"url\":\"/forums/IP_Protocolconfiguration\"},{\"name\":\"\\u7f51\\u7edc\\u7ef4\\u62a4\\u4e0e\\u7ba1\\u7406\",\"url\":\"/forums/maintainmanage\"},{\"name\":\"\\u4ea4\\u6362\\u53ca\\u8def\\u7531\\u6280\\u672f\",\"url\":\"/forums/Hardware_SwitchRouter\"},{\"name\":\"CDN\",\"url\":\"/forums/NetworkC_CDN\"},{\"name\":\"\\u901a\\u4fe1\\u6280\\u672f\",\"url\":\"/forums/ST_Network\"},{\"name\":\"VOIP\\u6280\\u672f\\u63a2\\u8ba8\",\"url\":\"/forums/voip\"}]},{\"name\":\"\\u6269\\u5145\\u8bdd\\u9898\",\"url\":\"/forums/Other\",\"children\":[{\"name\":\"\\u704c\\u6c34\\u4e50\\u56ed\",\"url\":\"/forums/FreeZone\"},{\"name\":\"\\u7a0b\\u5e8f\\u4eba\\u751f\",\"url\":\"/forums/ProgrammerStory\"},{\"name\":\"\\u7a0b\\u5e8f\\u5a9b\\u4e16\\u754c\",\"url\":\"/forums/ProgramGirls\"},{\"name\":\"\\u7a0b\\u5e8f\\u5458\\u4ea4\\u53cb\",\"url\":\"/forums/ProgramFriends\"},{\"name\":\"\\u4e09\\u5341\\u800c\\u7acb\",\"url\":\"/forums/30Plus\"},{\"name\":\"\\u6e38\\u620f\\u4e13\\u533a\",\"url\":\"/forums/Game\"},{\"name\":\"\\u4e1a\\u754c\\u65b0\\u95fb\",\"url\":\"/forums/ITnews\"},{\"name\":\"\\u7a0b\\u5e8f\\u5458\\u82f1\\u8bed\",\"url\":\"/forums/English\"},{\"name\":\"\\u6c42\\u804c\\u4e0e\\u62db\\u8058\",\"url\":\"/forums/CAREER\"},{\"name\":\"\\u8ba1\\u7b97\\u673a\\u56fe\\u4e66\",\"url\":\"/forums/Book\"},{\"name\":\"\\u5927\\u5b66\\u65f6\\u4ee3\",\"url\":\"/forums/CollegeTime\"},{\"name\":\"\\u8df3\\u86a4\\u5e02\\u573a\",\"url\":\"/forums/Trade\"},{\"name\":\"\\u8f6f\\u4ef6\\u6c42\\u52a9\",\"url\":\"/forums/Shareware\"}]},{\"name\":\"\\u6328\\u8e22\\u804c\\u6daf\",\"url\":\"/forums/CAREER\",\"children\":[{\"name\":\"\\u6c42\\u804c\\u9762\\u8bd5\",\"url\":\"/forums/WorkplaceCommunication\"},{\"name\":\"\\u4f01\\u4e1a\\u70b9\\u8bc4\",\"url\":\"/forums/TECHHUNT\"},{\"name\":\"\\u804c\\u573a\\u8bdd\\u9898\",\"url\":\"/forums/OFFICELIFE\"},{\"name\":\"JOB \\u9a7f\\u7ad9\",\"url\":\"/forums/jobservice\"}]},{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u793e\\u533a\",\"url\":\"/forums/eSDK\",\"children\":[{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b\",\"url\":\"/forums/DevChallenge2016\"},{\"name\":\"\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/hwfsdeveloper\"},{\"name\":\"\\u4f01\\u4e1a\\u901a\\u4fe1\",\"url\":\"/forums/hwucdeveloper\"},{\"name\":\"BYOD\",\"url\":\"/forums/hwbyoddeveloper\"},{\"name\":\"\\u5927\\u6570\\u636e\",\"children\":[{\"name\":\"FusionInsight HD\",\"url\":\"/forums/fusioninsightdeveloper\"},{\"name\":\"FusionInsight Universe\",\"url\":\"/forums/hwuniversedeveloper\"}]},{\"name\":\"Digital inCloud\",\"url\":\"/forums/hwswdeveloper\"},{\"name\":\"CaaS\",\"url\":\"/forums/hwcndeveloper\"},{\"name\":\"SDN\",\"url\":\"/forums/hwsdndeveloper\"},{\"name\":\"\\u4f01\\u4e1a\\u7f51\\u7edc\\u5f00\\u53d1\",\"url\":\"/forums/hwendeveloper\"},{\"name\":\"\\u654f\\u6377\\u7f51\\u7edc\",\"url\":\"/forums/hwesightdeveloper\"},{\"name\":\"eLTE\",\"url\":\"/forums/hwbbtdeveloper\"},{\"name\":\"\\u7269\\u8054\\u7f51\\u5f00\\u53d1\",\"url\":\"/forums/hwiotdeveloper\"},{\"name\":\"\\u79fb\\u52a8\\u5f00\\u653e\\u5de5\\u573a\",\"url\":\"/forums/hwwldeveloper\"},{\"name\":\"OpenLife\\u667a\\u6167\\u5bb6\\u5ead\",\"url\":\"/forums/OpenLife\"},{\"name\":\"HUAWEI Code Craft\",\"url\":\"/forums/hwcodecraft\"}]},{\"name\":\"IBM \\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"Rational\",\"url\":\"/forums/Rational\"},{\"name\":\"Lotus\",\"url\":\"/forums/Lotus\"},{\"name\":\"IBM\\u4e91\\u8ba1\\u7b97\",\"url\":\"/forums/ibmcloud\"},{\"name\":\"IBM \\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"Tivoli\",\"url\":\"/forums/Tivoli\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"}]},{\"name\":\"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a\",\"children\":[{\"name\":\"\\u82f1\\u7279\\u5c14\\u6280\\u672f\",\"url\":\"/forums/intel\"}]},{\"name\":\"Qualcomm\\u5f00\\u53d1\\u8bba\\u575b\",\"children\":[{\"name\":\"Qualcomm\\u5f00\\u53d1\",\"url\":\"/forums/qualcomm\"}]},{\"name\":\"\\u4f01\\u4e1a\\u6280\\u672f\",\"children\":[{\"name\":\"IBM \\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"WebSphere\",\"url\":\"/forums/WebSphere\"},{\"name\":\"DB2\",\"url\":\"/forums/DB2\"},{\"name\":\"Rational\",\"url\":\"/forums/Rational\"},{\"name\":\"Lotus\",\"url\":\"/forums/Lotus\"},{\"name\":\"IBM\\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"Tivoli\",\"url\":\"/forums/Tivoli\"},{\"name\":\"IBM AIX\",\"url\":\"/forums/AIX\"},{\"name\":\"Power Linux\",\"url\":\"/forums/PowerLinux\"}]},{\"name\":\"\\u82f1\\u7279\\u5c14\\u8f6f\\u4ef6\\u5f00\\u53d1\\u793e\\u533a\",\"children\":[{\"name\":\"\\u82f1\\u7279\\u5c14\\u6280\\u672f\",\"url\":\"/forums/intel\"}]},{\"name\":\"T\\u5ba2\\u8bba\\u575b\",\"url\":\"/forums/tcl\"},{\"name\":\"Paypal\\u5f00\\u53d1\\u8005\\u793e\\u533a\",\"url\":\"/forums/PaypalCommunity\"},{\"name\":\"CUDA\",\"url\":\"/forums/CUDA\",\"children\":[{\"name\":\"CUDA\\u7f16\\u7a0b\",\"url\":\"/forums/CUDA_Dev\"},{\"name\":\"CUDA\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\\u8ba8\\u8bba\",\"url\":\"/forums/CUDA_Compute\"},{\"name\":\"CUDA on Linux\",\"url\":\"/forums/CUDA_Linux\"},{\"name\":\"CUDA on Windows XP\",\"url\":\"/forums/CUDA_WinXP\"}]},{\"name\":\"Google\\u6280\\u672f\\u793e\\u533a\",\"children\":[{\"name\":\"Google\\u6280\\u672f\\u793e\\u533a\",\"url\":\"/forums/GoogleCommunity\"},{\"name\":\"Android\",\"url\":\"/forums/Android\"}]},{\"name\":\"Microsoft Office \\u5e94\\u7528\\u4e8e\\u5f00\\u53d1\",\"children\":[{\"name\":\"Office\\u5f00\\u53d1\",\"url\":\"/forums/OfficeDevelopment\"},{\"name\":\"Office\\u4f7f\\u7528\",\"url\":\"/forums/OfficeBase\"}]}]},{\"name\":\"\\u5176\\u4ed6\\u6280\\u672f\\u8bba\\u575b\",\"children\":[{\"name\":\"\\u8f6f\\u4ef6\\u5de5\\u7a0b/\\u7ba1\\u7406\",\"url\":\"/forums/SE\",\"children\":[{\"name\":\"\\u8f6f\\u4ef6\\u6d4b\\u8bd5\",\"url\":\"/forums/SE_Quality\"},{\"name\":\"\\u7814\\u53d1\\u7ba1\\u7406\",\"url\":\"/forums/SE_Management\"},{\"name\":\"\\u654f\\u6377\\u5f00\\u53d1\",\"url\":\"/forums/Agile\"},{\"name\":\"\\u7248\\u672c\\u63a7\\u5236\",\"url\":\"/forums/CVS_SVN\"},{\"name\":\"\\u8bbe\\u8ba1\\u6a21\\u5f0f\",\"url\":\"/forums/DesignPatterns\"}]},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u5f00\\u53d1\",\"url\":\"/forums/HPDevelopment\",\"children\":[{\"name\":\"\\u9ad8\\u6027\\u80fd\\u8ba1\\u7b97\",\"url\":\"/forums/HPC\"},{\"name\":\"\\u9ad8\\u6027\\u80fdWEB\\u5f00\\u53d1\",\"url\":\"/forums/HPWebDevelop\"},{\"name\":\"\\u9ad8\\u6027\\u80fd\\u6570\\u636e\\u5e93\\u5f00\\u53d1\",\"url\":\"/forums/HPDatabase\"},{\"name\":\"\\u6d77\\u91cf\\u6570\\u636e\\u5904\\u7406/\\u641c\\u7d22\\u6280\\u672f\",\"url\":\"/forums/SearchEngine\"},{\"name\":\"\\u6570\\u636e\\u7ed3\\u6784\\u4e0e\\u7b97\\u6cd5\",\"url\":\"/forums/ST_Arithmetic\"}]},{\"name\":\"\\u4e13\\u9898\\u5f00\\u53d1/\\u6280\\u672f/\\u9879\\u76ee\",\"url\":\"/forums/SpecialTopic\",\"children\":[{\"name\":\"OpenAPI\",\"url\":\"/forums/OpenAPI\"},{\"name\":\"OpenStack\",\"url\":\"/forums/OpenStack\"},{\"name\":\"\\u673a\\u5668\\u89c6\\u89c9\",\"url\":\"/forums/ST_Image\"},{\"name\":\"OpenCV\",\"url\":\"/forums/OpenCV\"},{\"name\":\"\\u4fe1\\u606f/\\u7f51\\u7edc\\u5b89\\u5168\",\"url\":\"/forums/ST_Security\"},{\"name\":\"\\u4eba\\u5de5\\u667a\\u80fd\\u6280\\u672f\",\"url\":\"/forums/AI\"},{\"name\":\"\\u8d28\\u91cf\\u7ba1\\u7406/\\u8f6f\\u4ef6\\u6d4b\\u8bd5\",\"url\":\"/forums/SE_Quality\"}]},{\"name\":\"\\u591a\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/MediaAndFlash\",\"children\":[{\"name\":\"\\u591a\\u5a92\\u4f53/\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/Multimedia\"},{\"name\":\"\\u56fe\\u8c61\\u5de5\\u5177\\u4f7f\\u7528\",\"url\":\"/forums/ImageTools\"},{\"name\":\"Flash\\u6d41\\u5a92\\u4f53\\u5f00\\u53d1\",\"url\":\"/forums/FlashDevelop\"},{\"name\":\"\\u4ea4\\u4e92\\u5f0f\\u8bbe\\u8ba1\",\"url\":\"/forums/InteractiveDesign\"},{\"name\":\"WPF/Silverlight\",\"url\":\"/forums/Silverlight\"},{\"name\":\"Flex\",\"url\":\"/forums/Flex\"}]},{\"name\":\"\\u786c\\u4ef6\\u4f7f\\u7528\",\"url\":\"/forums/HardwareUse\",\"children\":[{\"name\":\"\\u6570\\u7801\\u8bbe\\u5907\",\"url\":\"/forums/Hardware_Digital\"},{\"name\":\"\\u7535\\u8111\\u6574\\u673a\\u53ca\\u914d\\u4ef6\",\"url\":\"/forums/Hardware_Computer\"},{\"name\":\"\\u5916\\u8bbe\\u53ca\\u529e\\u516c\\u8bbe\\u5907\",\"url\":\"/forums/Hardware_Peripheral\"},{\"name\":\"\\u88c5\\u673a\\u4e0e\\u5347\\u7ea7\\u53ca\\u5176\\u4ed6\",\"url\":\"/forums/Hardware_DIY\"},{\"name\":\"\\u975e\\u6280\\u672f\\u533a\",\"url\":\"/forums/Hardware_NonTechnical\"}]},{\"name\":\"\\u4ea7\\u54c1/\\u5382\\u5bb6\",\"url\":\"/forums/ADS\",\"children\":[{\"name\":\"IBM \\u5f00\\u53d1\\u8005\",\"url\":\"/forums/IBMDeveloper\"},{\"name\":\"\\u5fae\\u521b\\u8f6f\\u4ef6\\u5f00\\u53d1\\u7ba1\\u7406\",\"url\":\"/forums/WeiChuang\"},{\"name\":\"\\u5176\\u4ed6\",\"url\":\"/forums/ADSOther\"}]}]},{\"name\":\"\\u57f9\\u8bad\\u8ba4\\u8bc1\",\"url\":\"/forums/Trainning\",\"children\":[{\"name\":\"IT\\u57f9\\u8bad\",\"url\":\"/forums/ITCertificate\"}]},{\"name\":\"\\u7ad9\\u52a1\\u4e13\\u533a\",\"url\":\"/forums/Support\",\"children\":[{\"name\":\"\\u793e\\u533a\\u516c\\u544a\",\"url\":\"/forums/placard\"},{\"name\":\"\\u6d3b\\u52a8\\u4e13\\u533a\",\"url\":\"/forums/Activity\"},{\"name\":\"\\u5ba2\\u670d\\u4e13\\u533a\",\"url\":\"/forums/Service\"},{\"name\":\"\\u7248\\u4e3b\\u4e13\\u533a\",\"url\":\"/forums/Moderator\"},{\"name\":\"\\u300a\\u7a0b\\u5e8f\\u5458\\u300b\\u6742\\u5fd7\",\"url\":\"/forums/Programmer\"}]}],\n    \"isLogined\": \"false\",\n    \"isModerator\": \"false\",\n    \"favoriteForumUrls\": [],\n    \"lastForumNodes\": [{\"name\":\"\\u534e\\u4e3a\\u5f00\\u53d1\\u8005\\u5927\\u8d5b \",\"url\":\"/forums/DevChallenge2016\"},{\"name\":\"\\u6570\\u5b57\\u5316\\u4f01\\u4e1a\\u4e91\\u5e73\\u53f0\\u8bba\\u575b\",\"url\":\"/forums/DE\"},{\"name\":\"OpenCV\",\"url\":\"/forums/OpenCV\"},{\"name\":\"FusionInsight HD\",\"url\":\"/forums/fusioninsightdeveloper\"},{\"name\":\"HUAWEI Code Craft\",\"url\":\"/forums/hwcodecraft\"},{\"name\":\"JetBrains\\u6280\\u672f\\u8bba\\u575b\",\"url\":\"/forums/JetBrains\"},{\"name\":\"Enterprise Architect\",\"url\":\"/forums/EA\"}]\n  }\'\'\'\nmenu = json.loads(menu)\n\n\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'周前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-7*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'个月前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-30*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if u\'年前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(years=-1*int(d[0]))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n\nclass Handler(BaseHandler):\n    crawl_config = {\n\'headers\':{\n\'proxy\':\'222.219.130.190:8998\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'}\n\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for each in menu[\'forumNodes\']:\n            if each.has_key(\'children\'):\n                for ea in each[\'children\']:\n                    if ea.has_key(\'url\'):\n                            url = ea[\'url\']\n                            self.crawl(\'http://bbs.csdn.net\'+url+\'/closed\',headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.title > a\').items():\n            _dict = {}\n            _dict[\'title\'] = each.text()\n            self.crawl(each.attr.href, save = _dict,headers=self.crawl_config[\'headers\'], callback=self.detail_page)\n        #翻页\n        for each in response.doc(\'.next\').items():\n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        #print len(response.doc(\'table.post.topic\'))\n        content_list = []\n        for info in response.doc(\'.post\').items():\n            if \'topic\' in info.attr[\'class\']:\n                continue\n            if u\'被管理员删除\' in info.find(\'.post_body\').remove(\'fieldset\').html().strip():\n                continue\n            _dic = {}\n            #print info.find(\'.answer_author\').text()\n            _dic[\'name\'] = info.find(\'.username > a\').text()\n            _dic[\'date\'] = info.find(\'.time\').text().split()[-2]\n            _dic[\'content\'] = info.find(\'.post_body\').remove(\'fieldset\').html().strip()\n            content_list.append((_dic))\n        \n        if not content_list:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.save[\'title\'],\n            #\"subject\": response.save[\'subject\'],\n            \"subject\": u\'程序员\',\n            \"answers\": content_list,\n            \"source\": u\'csdn\',\n            \"question_detail\": re.sub(\'<!--.*\',\'\',response.doc(\'.topic > .post_body\').remove(\'*\').html()).strip(),\n            \"class\": 47,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1472624251.4656),('wenda_eask','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-27 11:18:41\n# Project: wenda_eask\n\nfrom pyspider.libs.base_handler import *\nimport time,datetime\nfrom pyquery import PyQuery as pq\nimport urllib2\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天之前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u\'天之前\', \'\')))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n        \"headers\": {\n        \'Referer\':\'http://wenda.eask.org/\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\',\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://wenda.eask.org/tiwen.html\', headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'td strong > font\').items():\n            self.crawl(each.parent().parent().attr.href, headers=self.crawl_config[\'headers\'], save={\'category_id\':[each.text().replace(u\'、\',\'\')]}, callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for i in range(1,4000):\n            if i == 1:\n                self.crawl(response.url+\'index.html\', headers=self.crawl_config[\'headers\'], save=response.save, callback=self.list1_page)\n            else:\n                self.crawl(response.url+\'index_\'+str(i)+\'.html\', headers=self.crawl_config[\'headers\'], save=response.save, callback=self.list1_page)\n                \n    @config(age=10 * 24 * 60 * 60)\n    def list1_page(self, response):\n        for each in response.doc(\'.news_list li\').items(): \n            _dict = {}\n            _dict[\'category_id\'] = response.save.get(\'category_id\')\n            _dict[\'title\'] = each.find(\'a\').text()\n            _dict[\'create_time\'] = each.find(\'span\').text()\n            _dict[\'create_user\'] = \'\'\n            self.crawl(each.find(\'a\').attr.href, headers=self.crawl_config[\'headers\'], save=_dict, callback=self.detail_page)\n        \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = u\'\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'question_detail\'] = response.doc(\'#text\').html().split(\'<br />\')[0].split(\':\')[1] if response.doc(\'#text\') else \'\'\n        #print res_dict[\'question_detail\']\n        answers_list = []\n        ask_more = response.doc(\'#text script\').eq(-1).attr.src\n        content = \'\'\n        if pq(ask_more).find(\'table tr\').eq(1):\n            content = pq(ask_more).find(\'table tr\').eq(1).find(\'td\').html()\n        if content == \'\' and response.doc(\'#text > p\'):\n            content = \'\'.join(v.html() for v in response.doc(\'#text > p\').items())\n        if content == \'\' or u\'上一个问题\' in content:\n            return\n        answers_list.append({\n            \"content\":  content,\n            \"create_time\": res_dict[\'create_time\'],\n            \"user_name\": \'\',\n        })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,1.0000,3.0000,1472624239.6506),('wenda_gongwuyuan','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-26 15:52:03\n# Project: wenda_gongwuyuan\n\n\nfrom pyspider.libs.base_handler import *\nimport time,datetime\n\nadd_words = {\n    #\'bj\',\n    #\'sh\',\n    \'sd\':u\'山东公务员\',#2\n    #\'js\',\n    \'zj\':u\'浙江公务员\',#2\n    #\'ah\',\n    #\'jl\',\n    #\'fj\',\n    \'gd\':u\'广东公务员\',\n    \'gx\':u\'广西公务员\',#3\n    \'hn\':u\'河南公务员\',\n    #\'tj\',\n    \'hb\':u\'河北公务员\',\n    #\'hlj\',\n    \'sx\':u\'山西公务员\',#3\n    \'gs\':u\'甘肃公务员\',\n    #\'hu\',\n    #\'he\',\n    \'sc\':u\'四川公务员\',\n    \'cq\':u\'重庆公务员\',\n    \'yn\':u\'云南公务员\',\n    #\'gz\',\n    #\'xz\',\n    \'nx\':u\'宁夏公务员\',\n    \'xj\':u\'新疆公务员\',\n    \'qh\':u\'青海公务员\',\n    \'ln\':u\'辽宁公务员\',\n    \'jx\':u\'江西公务员\',\n    #\'nm\',\n}\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天之前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u\'天之前\', \'\')))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \"itag\":\'0.2\',\n        \"headers\": {\n        \'Upgrade-Insecure-Requests\':\'1\',\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.3\',\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in add_words.iteritems():\n            self.crawl(\'http://www.\'+k+\'gwy.org/ask/\', save = {\'category_id\':v}, headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        if response.doc(\'.list13 li\'):\n            for each in response.doc(\'.list13 li\').items():\n                if \'ivl\' in each.outerHtml():\n                    continue\n                _dict = {}\n                _dict[\'category_id\'] = response.save.get(\'category_id\')\n                _dict[\'title\'] = each.find(\'.con > a\').text()\n                _dict[\'create_user\'] = each.find(\'.role\').text()\n                _dict[\'tag\'] = 1\n                self.crawl(each.find(\'.con > a\').attr.href, headers=self.crawl_config[\'headers\'], save=_dict, callback=self.detail_page)\n            #翻页\n            for each in response.doc(\'.pages > a\').items():\n                self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.index_page)\n        elif response.doc(\'.list02 li\'):\n            for each in response.doc(\'.list02 li\').items():\n                if \'ivl\' in each.outerHtml():\n                    continue\n                _dict = {}\n                _dict[\'category_id\'] = response.save.get(\'category_id\')\n                _dict[\'title\'] = each.find(\'.link02\').text()\n                _dict[\'create_user\'] = each.find(\'.cat\').text().replace(u\'【\',\'\').replace(u\'】\',\'\')\n                _dict[\'tag\'] = 2\n                self.crawl(each.find(\'.link02\').attr.href, headers=self.crawl_config[\'headers\'], save=_dict, callback=self.detail_page)\n        \n            #翻页\n            for each in response.doc(\'.next > a\').items():\n                self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.index_page)\n        else:\n            for each in response.doc(\'tr tr\').items():\n                _dict = {}\n                _dict[\'category_id\'] = response.save.get(\'category_id\')\n                _dict[\'title\'] = each.find(\'.hei13\').text()\n                _dict[\'create_user\'] = \'\'\n                _dict[\'create_time\'] = each.find(\'.style36 > div\').text()\n                _dict[\'tag\'] = 3\n                self.crawl(each.find(\'.hei13\').attr.href, headers=self.crawl_config[\'headers\'], save=_dict, callback=self.detail_page)\n        \n            #翻页\n            for each in response.doc(\'.page > a\').items():\n                self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.index_page) \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        if res_dict[\'tag\'] == 1:\n            res_dict[\'create_time\'] = response.doc(\'.con > .ss > .time\').text().split()[0]\n            res_dict[\'url\'] = response.url\n            res_dict[\'source\'] = u\'公务员\'\n            res_dict[\'subject\'] = u\'主站问答\'\n            res_dict[\'class\'] = 34\n            res_dict[\'question_detail\'] = response.doc(\'.details\').eq(0).text() if response.doc(\'.details\').eq(0) else \'\'\n\n            answers_list = []\n\n            for each in response.doc(\'.r_box\').items():\n                if  each.find(\'.btm .time\'):\n                    create_time = each.find(\'.btm .time\').text().split()[0]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if not each.find(\'.details\'):\n                    continue\n                answers_list.append({\n                    \"content\":  each.find(\'.details\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": each.find(\'.btm > .ss\').remove(\'.time\').text(),\n                })\n        elif res_dict[\'tag\'] == 2:\n            res_dict[\'create_time\'] = response.doc(\'.c_l_c_07_1 .q_foot\').text().split()[0].split(u\'：\')[1]\n            res_dict[\'url\'] = response.url\n            res_dict[\'source\'] = u\'公务员\'\n            res_dict[\'subject\'] = u\'主站问答\'\n            res_dict[\'class\'] = 34\n            res_dict[\'question_detail\'] = response.doc(\'.q_con > div\').text() if response.doc(\'.q_con > div\') else \'\'\n\n            answers_list = []\n\n            for each in response.doc(\'.c_l_c_07_2\').items():\n                if  each.find(\'.solve_date\'):\n                    create_time = each.find(\'.solve_date\').text().split()[0].split(u\'：\')[1]\n                else:\n                    create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n                if not each.find(\'.q_con > div\'):\n                    continue\n                answers_list.append({\n                    \"content\":  each.find(\'.q_con > div\').html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": each.find(\'.q_title > div\').remove(\'span\').text(),\n                })\n        else:\n            res_dict[\'create_user\'] = response.doc(\'.style8\').text().split(\':\')[2].split(u\'】\')[0]\n            res_dict[\'url\'] = response.url\n            res_dict[\'source\'] = u\'公务员\'\n            res_dict[\'subject\'] = u\'主站问答\'\n            res_dict[\'class\'] = 34\n            res_dict[\'question_detail\'] = response.doc(\'.ask\').text() if response.doc(\'.ask\') else \'\'\n\n            answers_list = []\n\n            for each in response.doc(\'.replay\').items():\n                if u\'待回复\' in each.text():\n                    continue\n                create_time = res_dict[\'create_time\']\n                answers_list.append({\n                    \"content\":  each.html().strip(),\n                    \"create_time\": create_time,\n                    \"user_name\": \'\',\n                })\n        res_dict.pop(\'tag\')\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,5.0000,5.0000,1472624243.2094),('wenda_liuyouwang','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-27 17:55:22\n# Project: wenda_liuyouwang\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://wenda.liuu.cn/?/home/explore/sort_type-new__day-0\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'h4\').items():\n            self.crawl(each.find(\'a\').eq(0).attr.href, callback=self.detail_page)           \n        # 翻页\n        for each in response.doc(\'.clearfix a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n          \n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"question\": response.doc(\'h1\').text(),\n            \"types\": response.doc(\'.aw-topic-editor span\').text(),\n            \"question_detail\": response.doc(\'.aw-mod-body > .markitup-box\').text(),\n            \"create_date\":response.doc(\'.aw-question-detail-meta > span\').text().split()[0],\n            \"answer\": response.doc(\'.aw-dynamic-topic-content .markitup-box\').text(),\n            \"answer_date\": response.doc(\'.aw-dynamic-topic-meta > span.aw-text-color-999\').text().split()[0],\n    }\n',NULL,1.0000,3.0000,1472624233.0703),('wenda_tgnet',NULL,'STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-30 15:59:47\n# Project: wenda_tgnet\n\n\nfrom pyspider.libs.base_handler import *\nimport time,datetime,re\n\ndef get_date(d):\n    if not d:\n        return  time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'分钟\' in d or u\'小时\' in d:\n            return time.strftime(\'%Y-%m-%d\',time.localtime())\n    if u\'天之前\' in d:\n             now_time = datetime.datetime.now()\n             yes_time = now_time + datetime.timedelta(days=-1*int(d.replace(u\'天之前\', \'\')))\n             return yes_time.strftime(\'%Y-%m-%d\')\n    if not d.startswith(u\'20\'):\n        return \'20\'+d\n    else:\n        return d\n    \nclass Handler(BaseHandler):\n    crawl_config = {\n        \"headers\": {\n        \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n        }\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://wd.tgnet.com/QuestionList/0/1/0/0/0/1/\', headers=self.crawl_config[\'headers\'], callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.tableTopic tbody tr\').items():\n            #print each.find(\'.title > a\').eq(0).text()\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.title > a\').eq(0).text().strip()\n            _dict[\'create_time\'] = get_date(each.find(\'td > .cLGray\').text())\n            _dict[\'create_user\'] = each.find(\'td > a\').text()\n            self.crawl(each.find(\'.title > a\').eq(0).attr.href, headers=self.crawl_config[\'headers\'],save=_dict, callback=self.detail_page)\n         \n        #翻页\n        for each in response.doc(\'.p > a\').items():\n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.index_page)\n            \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = u\'天工网\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'question_detail\'] = response.doc(\'#tblInfo .content > div\').eq(0).remove(\'.IsApp\').html().strip()\n        if \'strong\' in res_dict[\'question_detail\']:\n            res_dict[\'question_detail\'] = response.doc(\'#tblInfo .content > div\').eq(0).remove(\'.IsApp\').find(\'strong\').eq(0).html().strip()\n        res_dict[\'question_detail\'] = re.sub(r\'<a[^>]+>\',\'\',res_dict[\'question_detail\']).replace(\'</a>\',\'\')\n        answers_list = []\n        \n        for each in response.doc(\'table.tableReply\').items():\n            if \'tblInfo\' in each.outerHtml():\n                continue\n            if  each.find(\'.topInfo\'):\n                create_time = get_date(each.find(\'.topInfo\').remove(\'*\').text().split()[2].strip().replace(\'/\',\'-\'))\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            answers_list.append({\n                \"content\":  re.sub(r\'<a[^>]+>\',\'\',each.find(\'.subjectReply\').html().strip()).replace(\'</a>\',\'\'),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'strong > a\').text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'category_id\'] = [\'0\',]\n        return res_dict',NULL,1.0000,3.0000,1472607168.7625),('wenda_tiandao','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-06-27 14:40:57\n# Project: wenda_tiandao\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://ask.tiandaoedu.com/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.ptit > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n        for each in response.doc(\'.pages > a\').items():\n            self.crawl(each.attr.href, callback=self.index_page)\n\n\n    @config(priority=2)\n    def detail_page(self, response):\n        answer = response.doc(\'.answer_con > span\').text()\n        answer = answer.replace(u\'如果还有相关的留学问题需要了解，欢迎拨打天道留学的免费咨询热线400-019-0038进行咨询，或者点击页面的“在线咨询”与顾问老师直接对话。\', \'\')\n        if not answer:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'.q_con > .yh\').text(),\n            \"question_detail\": response.doc(\'.zhw_p\').text(),\n            \"create_date\": response.doc(\'.q_con > span\').text().split(u\'点击\')[0].split(u\'：\')[-1].strip(\' \'),\n            \"answer\": answer,\n        }\n',NULL,1.0000,3.0000,1472624227.2917),('wenda_tianxing','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-26 10:44:01\n# Project: wenda_tianxing\n\nfrom pyspider.libs.base_handler import *\nimport time\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \"headers\": {\n            \'Host\':\'www.tesoon.com\',\n            \'Referer\':\'http://www.tesoon.com/ask/get_class.php?fatherid=947&status=H\',\n            \'Upgrade-Insecure-Requests\':\'1\',\n            \'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\'\n}\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.tesoon.com/ask/catalog.htm\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.lh18 > a\').items():\n            if u\'更多\' not in each.text():\n                self.crawl(each.attr.href, save = {\'category_id\':each.text()}, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n        self.crawl(\'http://www.tesoon.com/ask/get_class.php?fatherid=10&classifyflag=0\', save = {\'category_id\':u\'考试答案交流区\'}, headers=self.crawl_config[\'headers\'], callback=self.list_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.lh25 tr\').items():\n            _dict = {}\n            _dict[\'category_id\'] = response.save.get(\'category_id\')\n            _dict[\'title\'] = each.find(\'a.f14\').text()\n            _dict[\'create_time\'] = each.find(\'td\').eq(4).text()\n            _dict[\'create_user\'] = each.find(\'.c6nul > font\').text()\n            self.crawl(each.find(\'a.f14\').attr.href, headers=self.crawl_config[\'headers\'], save=_dict, callback=self.detail_page)\n        \n        #翻页\n        for each in response.doc(\'.lh25 tr\').eq(-2).find(\'a\').items():\n            self.crawl(each.attr.href, headers=self.crawl_config[\'headers\'],  save=response.save, callback=self.list_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'url\'] = response.url\n        res_dict[\'source\'] = u\'天星教育\'\n        res_dict[\'subject\'] = u\'主站问答\'\n        res_dict[\'class\'] = 34\n        res_dict[\'question_detail\'] = response.doc(\'tr > .lh13\').text() if response.doc(\'tr > .lh13\') else \'\'\n        \n        answers_list = []\n        \n        for each in response.doc(\'div.new2_hd\').items():\n            #print each.html()\n            if  each.find(\'.hdsj\'):\n                create_time = each.find(\'.hdsj\').text().split()[0]\n            else:\n                create_time = time.strftime(\'%Y-%m-%d\',time.localtime())\n            if not each.find(\'tr > .lh15\'):\n                continue\n            answers_list.append({\n                \"content\":  each.find(\'tr > .lh15\').remove(\'a\').html().strip(),\n                \"create_time\": create_time,\n                \"user_name\": each.find(\'tr > .c6 > .a05\').text(),\n            })\n        if len(answers_list) == 0:\n            return\n        res_dict[\'answers\'] = answers_list\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1472624196.8032),('wenda_yygrammar','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-28 09:57:39\n# Project: wenda_yygrammar\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'proxy\':\'222.186.30.45:80\'\n    }\n\n    header = {\n\n\'Accept\':\'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\',\n\'Accept-Encoding\':\'gzip, deflate, sdch\',\n\'Accept-Language\':\'zh-CN,zh;q=0.8,en;q=0.6\',\n\'Cache-Control\':\'max-age=0\',\n\'Connection\':\'keep-alive\',\n\'Host\':\'ask.yygrammar.com\',\n\'Referer\':\'http://ask.yygrammar.com/c-all/2/1.html?WebShieldSessionVerify=gvLjMxMl5Tyr8APkYMki\',\n\'Upgrade-Insecure-Requests\':\'1\',\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://ask.yygrammar.com/c-all/2/1.html\', proxy=\'222.186.30.45:80\',headers = self.header,callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'table tr\').items():\n            if each.find(\'td\'):\n                _dict = {}\n                _dict[\'title\'] = each.find(\'.title .wrap > a\').text()\n                url = each.find(\'.title .wrap > a\').attr.href\n                _dict[\'create_time\'] = each.find(\'td\').eq(2).text().split()[0].replace(\'/\',\'-\')\n                self.crawl(url, save = _dict,callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n       res_dict = response.save        \n       res_dict[\'category_id\'] = [u\'英语问答\']\n       res_dict[\'question_detail\'] = response.doc(\'div.description\').html()\n       answers_list = []\n       info = response.doc(\'div.qa-content\').remove(\'.appendcontent\').html()\n       if info:\n                 _dict = {}\n                 _dict[\'content\'] = info\n                 _dict[\'user_name\'] = response.doc(\'div.text > p.name  > a\').text()\n                 _dict[\'create_time\'] = response.doc(\'div.text > div.user-info  > span\').eq(4).text().split()[1].replace(\'/\',\'-\')\n                 answers_list.append(_dict)\n       if not answers_list:\n                 return \n       res_dict[\'answers\'] = answers_list\n       res_dict[\'url\'] = response.url\n       res_dict[\'source\'] = \'yygrammar\'\n       res_dict[\'subject\'] = u\'主站问答\'\n       res_dict[\'class\'] = 34\n       res_dict[\'data_weight\'] = 0\n       res_dict[\'create_user\'] = \'\'         \n       return res_dict\n',NULL,1.0000,3.0000,1472624206.6477),('xiaoxue_51edu_news','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-15 14:41:21\n# Project: xiaoxue_shiti\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    bread_dic = {\'yinianji\': u\'一年级\',\n                 \'ernianji\': u\'二年级\',\n                 \'sannianji\': u\'三年级\',\n                 \'sinianji\': u\'四年级\',\n                 \'wunianji\': u\'五年级\',\n                 \'liunianji\': u\'六年级\',\n                 \'yuwen\': u\'语文\',\n                 \'shuxue\': u\'数学\',\n                 \'yingyu\': u\'英语\',\n                 }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.51edu.com/xiaoxue/yinianji/\', callback=self.index_page)\n        self.crawl(\'http://www.51edu.com/xiaoxue/ernianji/\', callback=self.index_page)\n        self.crawl(\'http://www.51edu.com/xiaoxue/sannianji/\', callback=self.index_page)\n        self.crawl(\'http://www.51edu.com/xiaoxue/sinianji/\', callback=self.index_page)\n        self.crawl(\'http://www.51edu.com/xiaoxue/wunianji/\', callback=self.index_page)\n        self.crawl(\'http://www.51edu.com/xiaoxue/liunianji/\', callback=self.index_page)\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.wdlbt\').items():\n            self.crawl(each.find(\'a\').attr.href, callback=self.index2_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index2_page(self, response):\n        for each in response.doc(\'.wdlbt\').items():\n            self.crawl(each.find(\'a\').attr.href, callback=self.index3_page)\n    \n    @config(age=10 * 24 * 60 * 60)\n    def index3_page(self, response):\n        for each in response.doc(\'li > p > a\').items():\n            self.crawl(each.attr.href,  callback=self.detail_page)\n        for each in response.doc(\'#pages > a\').items():\n            self.crawl(each.attr.href,  callback=self.index3_page)\n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = [v.text() for v in response.doc(\'#endText > p\').items()]\n        if not content_list:\n            return None\n        bread = []\n        for k, v in self.bread_dic.iteritems():\n            if k in response.url:\n                bread.append((v))\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'h2\').text(),\n            \"date\": response.doc(\'.wdnrbt > span\').text()[:10],\n            \"bread\": bread,\n            \"content\": \'\'.join([\'<p>%s</p>\'%v for v in content_list]),\n            \"subject\": u\'小学\',\n            \"class\": 33,\n            \"source\": \"51edu\",\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1472624218.1231),('xiaoxue_lianjia','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-05 17:33:31\n# Project: xiaoxu_lianjia_shanghai\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as pq\nimport urllib\nimport json\nimport sys\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\nxz_dict = {\nu\'公\':u\'公办\',\nu\'民\':u\'民办\',\nu\'私\':u\'私立\',\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://sz.lianjia.com/xuequfang/\', callback=self.index_page)\n        for i in range(2,16):\n            self.crawl(\'http://sz.lianjia.com/xuequfang/pg%d/\'%(i), callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.search-list .fl li\').items():\n            url = each.find(\'a\').attr.href\n            icon_img = each.find(\'a > img\').attr.src\n            self.crawl(url, save={\'icon_img\': icon_img}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res = {}\n        for each in response.doc(\'.list li\').items():\n            _each = each.text().split()\n            try:\n                res[_each[0]] = _each[1]\n            except:\n                pass\n         \n        _list = []\n        dic = {}\n        for each in response.doc(\'.li\').items():\n            dic = {\n                \'community\': each.find(\'.names > a\').text(),\n                \'building\': each.find(\'.building .tips\').text().replace(\'\\t\',\'\').replace(\'\\n\', \',\')\n            }\n            if dic[\'community\'] != \'\':\n                _list.append(dic)\n        res[\'community\'] = json.dumps(_list)\n        res[\'correspond_school \'] = json.dumps([v.text() for v in response.doc(\'p > a\').items() if v])\n        \n        #jz_list = []\n        \n        #for info in response.doc(\'.jianzhang a\').items():\n            #if u\'2016年\' in info.text():\n            #jz = pq(url=info.attr.href, encoding=\"utf-8\")\n            #jz_list.append(jz.find(\'.box\').html())\n        #res[\'student_guid\'] = list(set(jz_list))\n        res[\'name\'] = response.doc(\'h1 > b\').html()\n        \n        return res',NULL,1.0000,3.0000,1472624221.0417),('xiaoxue_ruyile','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-18 19:20:58\n# Project: xiaoxue\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    province_dict = {\n        \'http://www.ruyile.com/xxlb.aspx?id=2&t=2\': \'天津\', \n        \'http://www.ruyile.com/xxlb.aspx?id=3&t=2\': \'河北\', \n        \'http://www.ruyile.com/xxlb.aspx?id=4&t=2\': \'山西\', \n        \'http://www.ruyile.com/xxlb.aspx?id=5&t=2\': \'内蒙古\', \n        \'http://www.ruyile.com/xxlb.aspx?id=6&t=2\': \'辽宁\', \n        \'http://www.ruyile.com/xxlb.aspx?id=7&t=2\': \'吉林\', \n        \'http://www.ruyile.com/xxlb.aspx?id=8&t=2\': \'黑龙江\', \n        \'http://www.ruyile.com/xxlb.aspx?id=9&t=2\': \'上海\', \n        \'http://www.ruyile.com/xxlb.aspx?id=10&t=2\': \'江苏\', \n        \'http://www.ruyile.com/xxlb.aspx?id=11&t=2\': \'浙江\', \n        \'http://www.ruyile.com/xxlb.aspx?id=12&t=2\': \'安徽\', \n        \'http://www.ruyile.com/xxlb.aspx?id=13&t=2\': \'福建\', \n        \'http://www.ruyile.com/xxlb.aspx?id=14&t=2\': \'江西\', \n        \'http://www.ruyile.com/xxlb.aspx?id=15&t=2\': \'山东\', \n        \'http://www.ruyile.com/xxlb.aspx?id=16&t=2\': \'河南\', \n        \'http://www.ruyile.com/xxlb.aspx?id=17&t=2\': \'湖北\', \n        \'http://www.ruyile.com/xxlb.aspx?id=18&t=2\': \'湖南\', \n        \'http://www.ruyile.com/xxlb.aspx?id=19&t=2\': \'广东\', \n        \'http://www.ruyile.com/xxlb.aspx?id=20&t=2\': \'广西\', \n        \'http://www.ruyile.com/xxlb.aspx?id=21&t=2\': \'海南\', \n        \'http://www.ruyile.com/xxlb.aspx?id=22&t=2\': \'重庆\', \n        \'http://www.ruyile.com/xxlb.aspx?id=23&t=2\': \'四川\', \n        \'http://www.ruyile.com/xxlb.aspx?id=24&t=2\': \'贵州\', \n        \'http://www.ruyile.com/xxlb.aspx?id=25&t=2\': \'云南\', \n        \'http://www.ruyile.com/xxlb.aspx?id=26&t=2\': \'西藏\', \n        \'http://www.ruyile.com/xxlb.aspx?id=27&t=2\': \'陕西\', \n        \'http://www.ruyile.com/xxlb.aspx?id=28&t=2\': \'甘肃\', \n        \'http://www.ruyile.com/xxlb.aspx?id=29&t=2\': \'青海\', \n        \'http://www.ruyile.com/xxlb.aspx?id=30&t=2\': \'宁夏\', \n        \'http://www.ruyile.com/xxlb.aspx?id=31&t=2\': \'新疆\', \n        \'http://www.ruyile.com/xxlb.aspx?id=32&t=2\': \'台湾\', \n        \'http://www.ruyile.com/xxlb.aspx?id=33&t=2\': \'香港\', \n        \'http://www.ruyile.com/xxlb.aspx?id=34&t=2\': \'澳门\',\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for url, province in self.province_dict.iteritems():\n            \n            self.crawl(url, save={\'province\': province}, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        province = response.save[\'province\']\n        #for each in response.doc(\'.qylb > a\').items():\n        #    print \"\'%s\': \'%s\', \"% (each.attr.href, each.text())\n            \n        for each in response.doc(\'.sk\').items():\n            url = each.find(\'h4 > a\').attr.href\n            _save = {\n                \'name\': each.find(\'h4 > a\').text(),\n                \'logo\': each.find(\'img\').attr.src,\n            }\n            _save[\'province\'] = province\n            for info in each.remove(\'h4\').text().split():\n                try:\n                    (name, value) = info.split(u\'：\')\n                    _save[name] = value\n                except:\n                    continue\n            self.crawl(url, save=_save, callback=self.detail_page)\n        for each in response.doc(\'.fy > a\').items():\n            self.crawl(each.attr.href, save={\'province\': province}, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res = response.save\n        for each in response.doc(\'.z\').items():\n            try:\n                (name, value) = each.text().split(u\'：\')\n                res[name] = value\n            except:\n                continue\n        res[\'introduction\'] = response.doc(\'.jj\').html()\n        res[\'url\'] = response.url\n        \n        return res\n',NULL,1.0000,3.0000,1472624177.6699),('xiaoxu_lianjia_shanghai','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-05 17:33:31\n# Project: xiaoxu_lianjia_shanghai\n\nfrom pyspider.libs.base_handler import *\nfrom pyquery import PyQuery as pq\nimport urllib\nimport json\nimport sys\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\nxz_dict = {\nu\'公\':u\'公办\',\nu\'民\':u\'民办\',\nu\'私\':u\'私立\',\n}\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for i in range(1,9):\n            self.crawl(\'http://sh.lianjia.com/xuequ/d%s\'%(i), callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.list-wrap li\').items():\n            url = each.find(\'.info-panel > h2 a\').attr.href\n            icon_img = each.find(\'a > img\').attr.src\n            self.crawl(url, save={\'icon_img\': icon_img}, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res = {}\n        for each in response.doc(\'#introduction tr\').items():\n            try:\n                _each = each.find(\'td\').text().split()\n                res[_each[0]] = _each[1]\n            except:\n                pass\n        for each in response.doc(\'.aroundInfo td\').items():\n            try:\n                _each = each.text().split(u\'：\')\n                res[_each[0]] = _each[1]\n            except:\n                pass\n            \'\'\'\n            if u\'别名简称\' in each.text():\n                res[\'nick_name\'] = each.remove(\'.title\').text().replace(\'|\',\',\')\n            if u\'升学方式\' in each.text():\n                res[\'attend_way\'] = each.remove(\'.title\').text().replace(u\'对口\',\'\') \n            if u\'办学性质\' in each.text():\n                res[\'school_property\'] = \'\'\n                for k,v in xz_dict.iteritems():\n                    if k in each.remove(\'.title\').text():\n                        res[\'school_property\'] = v                       \n\n            if u\'名额限制\' in each.text():\n                res[\'numerus_clausus\'] = each.remove(\'.title\').text()\n            if u\'地址\' in each.text():\n                res[\'address\'] = each.find(\'.addrEllipsis \').text()\n            \'\'\'\n         \n        _list = []\n        dic = {}\n        for each in response.doc(\'#property tr\').items():\n            dic = {\n                \'community\': each.find(\'.propertyEllipsis\').text(),\n                \'building\': each.find(\'td\').eq(3).find(\'a\').attr[\'data-buildings\']\n            }\n            if dic[\'community\'] != \'\':\n                _list.append(dic)\n        res[\'community\'] = json.dumps(_list)\n        res[\'correspond_school \'] = json.dumps([v.text() for v in response.doc(\'td > p\').items() if v])\n        print res[\'community\']\n        print res[\'correspond_school \']\n        \n        #jz_list = []\n        \n        #for info in response.doc(\'.jianzhang a\').items():\n            #if u\'2016年\' in info.text():\n            #jz = pq(url=info.attr.href, encoding=\"utf-8\")\n            #jz_list.append(jz.find(\'.box\').html())\n        #res[\'student_guid\'] = list(set(jz_list))\n        res[\'name\'] = response.doc(\'.title-wrapper\').text()\n        return res',NULL,1.0000,3.0000,1472624215.3371),('xingzuo123','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 15:06:21\n# Project: xingzuo123\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n        \'http://www.xingzuo123.com/12xingzuo/\':[u\'星座\'],\n        \'http://www.xingzuo123.com/xingzuoyunshi/\':[u\'运势\'],\n        \'http://www.xingzuo123.com/htm/starlove/\':[u\'配对\'],\n        \'http://www.xingzuo123.com/12shengxiao/\':[u\'生肖\'],\n        \'http://www.xingzuo123.com/smdq/\':[u\'算命\'],\n        \'http://www.xingzuo123.com/mianxiang/\':[u\'面相\']\n\n\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.listbox li\').items():\n            _dict = {}\n            _dict[\'title\']  = each.find(\'a\').eq(0).text() or each.find(\'a\').eq(1).text() or \'\'\n            _dict[\'cover\'] = each.find(\'a\').eq(0).find(\'img\').attr.src or \'\'\n            url =  each.find(\'a\').eq(0).attr.href\n            _dict[\'date\'] = each.find(\'small\').text().split(\'：\')[-1].split()[0]\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'.pagelist a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'星座\'\n        res_dict[\'source\'] = \'xingzuo123.com\'\n        res_dict[\'url\'] = response.url\n        res_dict[\'data_weight\'] = 0\n        content_list = []\n        for each in response.doc(\'div.article_content.info_area  p\').items():\n            info = each.html()\n            if info:\n                content_list.append(info)\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        return res_dict\n',NULL,1.0000,3.0000,1472624183.0681),('xingzuo123_inc','xinzuo','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 15:06:21\n# Project: xingzuo123\n\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    page_dict = {\n        \'http://www.xingzuo123.com/12xingzuo/\':[u\'星座\'],\n        \'http://www.xingzuo123.com/xingzuoyunshi/\':[u\'运势\'],\n        \'http://www.xingzuo123.com/htm/starlove/\':[u\'配对\'],\n        \'http://www.xingzuo123.com/12shengxiao/\':[u\'生肖\'],\n        \'http://www.xingzuo123.com/smdq/\':[u\'算命\'],\n        \'http://www.xingzuo123.com/mianxiang/\':[u\'面相\']\n\n\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1)\n    def index_page(self, response):\n        for each in response.doc(\'.listbox li\').items():\n            _dict = {}\n            _dict[\'title\']  = each.find(\'a\').eq(0).text() or each.find(\'a\').eq(1).text() or \'\'\n            _dict[\'cover\'] = each.find(\'a\').eq(0).find(\'img\').attr.src or \'\'\n            url =  each.find(\'a\').eq(0).attr.href\n            _dict[\'date\'] = each.find(\'small\').text().split(\'：\')[-1].split()[0]\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'星座\'\n        res_dict[\'source\'] = \'xingzuo123.com\'\n        res_dict[\'url\'] = response.url\n        res_dict[\'data_weight\'] = 0\n        content_list = []\n        for each in response.doc(\'div.article_content.info_area  p\').items():\n            info = each.html()\n            if info:\n                content_list.append(info)\n        res_dict[\'content\'] = \'\'.join([\'<p>%s</p>\'%s for s in content_list if s and s.strip()])\n        return res_dict\n',NULL,1.0000,3.0000,1472546303.4206),('xmswim','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 09:56:55\n# Project: xmswim\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n    content = content.replace(\'</a>\',\'\')\n    return  content\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n\n    \n    page_dict = {\n\n        \'http://www.xmswim.com/new/\':[u\'游泳新闻\'],\n        \'http://www.xmswim.com/jishu/\':[u\'游泳技术\'],\n        \'http://www.xmswim.com/jiankang/\':[u\'游泳健康\'],\n        \'http://www.xmswim.com/rcswim/\':[u\'水上救生\'],\n        \'http://www.xmswim.com/resource/\':[u\'游泳资源\'],\n        \'http://www.xmswim.com/wenxian/\':[u\'游泳文献\']\n    }\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'div.deanartice li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.deanarticername a\').text()\n            _dict[\'cover\'] = each.find(\'.deanarticel img\').attr.src.replace(\'/new\',\'\').replace(\'/jishu\',\'\').replace(\'/jiankang\',\'\').replace(\'/rcswim\',\'\').replace(\'/resource\',\'\').replace(\'/wenxian\',\'\')\n            if \'template\'  in _dict[\'cover\']:\n                _dict[\'cover\'] = \'\'\n            url = each.find(\'.deanarticername a\').attr.href\n            _dict[\'date\'] = each.find(\'span.deanfabushijian\').text().split()[0]\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n        for each in response.doc(\'div.pg a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href,save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'td#article_content\').remove(\'script\').remove(\'embed\').html()\n        if res_dict[\'content\'] and \'</a>\' in res_dict[\'content\']:\n            res_dict[\'content\'] = removeLink(res_dict[\'content\'])\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'游泳\'\n        res_dict[\'source\'] = u\'xmswim.com\'\n        res_dict[\'url\'] = response.url\n        return res_dict\n',NULL,1.0000,3.0000,1472624187.8464),('xmswim_inc','youyong','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-23 09:56:55\n# Project: xmswim\n\nfrom pyspider.libs.base_handler import *\nimport re\npattern = re.compile(r\'<a.*?\">\',re.S)\ndef removeLink(content):\n    for link in pattern.findall(content):\n        content = content.replace(link,\'\')\n    content = content.replace(\'</a>\',\'\')\n    return  content\n\nclass Handler(BaseHandler):\n    crawl_config = {\n        \'itag\':\'0.1\'\n    }\n\n    \n    page_dict = {\n\n        \'http://www.xmswim.com/new/\':[u\'游泳新闻\'],\n        \'http://www.xmswim.com/jishu/\':[u\'游泳技术\'],\n        \'http://www.xmswim.com/jiankang/\':[u\'游泳健康\'],\n        \'http://www.xmswim.com/rcswim/\':[u\'水上救生\'],\n        \'http://www.xmswim.com/resource/\':[u\'游泳资源\'],\n        \'http://www.xmswim.com/wenxian/\':[u\'游泳文献\']\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_dict.items():\n            self.crawl(k, save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'div.deanartice li\').items():\n            _dict = {}\n            _dict[\'title\'] = each.find(\'.deanarticername a\').text()\n            _dict[\'cover\'] = each.find(\'.deanarticel img\').attr.src.replace(\'/new\',\'\').replace(\'/jishu\',\'\').replace(\'/jiankang\',\'\').replace(\'/rcswim\',\'\').replace(\'/resource\',\'\').replace(\'/wenxian\',\'\')\n            if \'template\'  in _dict[\'cover\']:\n                _dict[\'cover\'] = \'\'\n            url = each.find(\'.deanarticername a\').attr.href\n            _dict[\'date\'] = each.find(\'span.deanfabushijian\').text().split()[0]\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(url, save = _dict,callback=self.detail_page)\n       \n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        res_dict[\'content\'] = response.doc(\'td#article_content\').remove(\'script\').remove(\'embed\').html()\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'subject\'] = u\'游泳\'\n        res_dict[\'source\'] = u\'xmswim.com\'\n        res_dict[\'url\'] = response.url\n        return res_dict\n',NULL,1.0000,3.0000,1472546316.9926),('xuexiao_baike','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-01 09:37:52\n# Project: xuexiao_baike\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    \n    headers = {\n\n\'User-Agent\':\'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\'\n\n    }\n    \n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://baike.baidu.com/view/466443.htm\', headers = self.headers,callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        _dict = {}\n        for each in response.doc(\'div[label-module=\"para-title\"]\').items():\n            key =  each.children(\'h2\').remove(\'span\').text()\n            content_list = list()\n            nx = each.next()        \n            while nx:\n                if nx.attr[\'label-module\'] == \'para-title\' or nx.attr[\'id\'] ==\'open-tag\':\n                    break\n                if nx.attr[\'class\'] == \'para\':  \n                    content_list.append(\'<p>\'+nx.remove(\'sup\').text()+\'</p>\')            \n                nx = nx.next()\n            if content_list and key:\n                  _dict[key] = \'\'.join(content_list)\n        for each in response.doc(\'dt.basicInfo-item\').items():\n            key = \'\'.join(v for v in each.text().split())\n            nx = each.next()\n            if \'basicInfo-item\' in nx.attr[\'class\'] and key:\n                _dict[key] = nx.text()\n        if not _dict:\n            return\n        return _dict\n\n            \n            \n            \n            \n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1472624156.1716),('xuexiao_eol','delete','TODO','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-14 09:58:48\n# Project: xuexiao_eol\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\nfrom pyquery import PyQuery as pq\nimport requests\nimport json\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    set_dict = {\n\n            u\'电话\':\'phone\',\n            u\'校址\':\'address\',\n            u\'网址\':\'site\',\n            u\'学段\':\'period\',\n            u\'地区\':\'locate\',\n            u\'性质\':\'school_type\',\n\n    }\n    #intro.shtml 学校简介\n    #teachers.shtml 师资力量\n    #fest.shtml 办学特色\n    #facilities.shtml  学校风光\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://xuexiao.eol.cn/html4/1100/114000144/index.shtml\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        _dict = {}\n        for each in response.doc(\'table.line_22\').eq(0).find(\'td\').items():     \n              arr =  each.text().split(\'：\')\n              if len(arr) == 2 and arr[0] in self.set_dict:\n                    #print type(arr[1])\n                    _dict[self.set_dict[arr[0]]] = arr[1]\n        \n        \n        baseUrl = response.url.replace(\'index.shtml\',\'\')\n        resp = requests.get(baseUrl+\'intro.shtml\')  \n        #print len( pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.main\'))\n        #print pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.main\').eq(0).html()\n        _dict[\'school_summary\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.main\').eq(0).html()\n        resp = requests.get(baseUrl+\'teachers.html\')  \n        _dict[\'faculty\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.main\').eq(0).html()\n        resp = requests.get(baseUrl+\'fest.html\')  \n        _dict[\'feature_course\'] = pq(resp.text.encode(resp.encoding).decode(\'utf-8\')).find(\'div.main\').eq(0).html()\n        print _dict\n        \n               \n\n    @config(priority=2)\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text(),\n        }\n',NULL,1.0000,3.0000,1472624163.5729),('yasi_taisha_inc','ielts','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-25 17:44:23\n# Project: yasi_taisha\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    _dict = {\n    \'news\': \'快讯动态\', \'guidance\':\'分类资讯\', \'experience\':\'经验分享\', \'download\':\'分类资讯\', \'t-guide\':\'分类资讯\', \'machine\':\'雅思机经\'\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for page in self._dict: \n            self.crawl(\'http://www.taisha.org/ielts/%s/\'%page, save={\'type\': self._dict[page]}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        type = response.save[\'type\']\n        for each in response.doc(\'.html_content dd\').items():\n            _dict = {}\n            url = each.find(\'.title > a\').attr.href\n            #_dict[\'url\'] = url\n            _dict[\'tag\'] = each.find(\'.bot > a\').text().split(\' \')\n            _dict[\'brief\'] = each.find(\'p\').text().replace(u\'[详情]\', \'\')\n            _dict[\'type\'] = type\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        #for each in response.doc(\'span > a\').items():\n        #    self.crawl(each.attr.href, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save\n        try:\n            date = response.doc(\'.txt_title span\').text()[-19:]\n        except:\n            date = \'\'\n        #content = response.doc(\'.txt_content\').html()\n        content_info = response.doc(\'.txt_content > p\')\n        content_list = []\n        for info in content_info:\n            answer = pyquery.PyQuery(info)\n            items = answer.html()\n            if answer.text() == u\'太傻网雅思频道精品推荐：\':\n                break\n            content_list.append((items.replace(u\'<a href=\"http://www.taisha.org/ielts/\" target=\"_blank\" class=\"keylink\">雅思</a>\',u\'雅思\').replace(u\'太傻\', u\'跟谁学\')))\n        content_list = content_list[:-1]\n        if not content_list:\n            return None\n        title = response.doc(\'.txt_title > h2\').text()\n        if u\'回忆版\' in title:\n            return None\n        res_dict[\'title\'] = title \n        res_dict[\'url\'] = response.url\n        res_dict[\'date\'] = date[0:10]\n        res_dict[\'source\'] = u\'太傻留学\'\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'subject\'] = u\'雅思\'\n        res_dict[\'bread\'] = response.doc(\'#nav_location a\').text().split(\' \')[1:]\n        res_dict[\'bread\'].extend(res_dict[\'tag\'])\n        res_dict[\'bread\'].append((response.save[\'type\']))\n        res_dict[\'bread\'] = list(set(res_dict[\'bread\']))\n        res_dict[\'class\'] = 17\n        res_dict.pop(\'type\')\n        res_dict[\'data_weight\'] = 0\n        return res_dict',NULL,1.0000,3.0000,1471329896.5925),('yasi_xiaozhan_inc','ielts','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-08 10:48:14\n# Project: yasi_xiaozhan_inc\n\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for i in range(1,247):\n            url = \'http://ielts.zhan.com/\'\n            self.crawl(url, callback=self.index_page)\n            #self.crawl(url, fetch_type=\'js\', callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\'.pull-right > p > a\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        cnt = 0\n        for each in response.doc(\'.active > a\'):\n            cnt = cnt + 1\n        if cnt == 1:\n            return None\n        \n        url = response.url\n        brief = response.doc(\'.first-words\').text()\n        title = response.doc(\'title\').text()\n        title = title.replace(u\'小站\', u\'跟谁学\')\n        date = \'\'\n        for each in response.doc(\'.pull-left > span\').items():\n            #date =\'\'\n            date = each.text().replace(u\'年\', \'-\').replace(u\'月\',\'-\').replace(u\'日\',\'\')\n            break\n            \n        bread = []\n        for each in response.doc(\'.head-crumbs-a-active\').items():\n            bread.append(each.text())\n        \n        content_list = []\n        for info in response.doc(\'.article-content > p\'):\n            content_list.append(pyquery.PyQuery(info).html())\n        content = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        content = content.replace(\'img src=\\\"/uploadfile\', \'img src=\\\"http://ielts.zhan.com/uploadfile\')\n        content = content.replace(u\'小站\', u\'本站\')\n        brief = brief.replace(u\'小站\', u\'本站\')\n        if len(content) < 10:\n            return None\n        return {\n            \"url\": response.url,\n            \"title\": title,\n            \"brief\": brief,\n            \"content\": content,\n            \"date\": date,\n            \"source\": \'zhan.com\',\n            \"subject\": u\'雅思\',\n            \"bread\": bread,\n            \"class\": 17,\n            \"data_weight\": 0,\n        }\n',NULL,1.0000,3.0000,1471329900.4770),('yixueweisheng_zhengbao','kztk','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-01 16:03:05\n# Project: yixueweisheng_zhengbao\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ntype_dict = {\n    \'chuji\': [u\'财会经济\', u\'初级会计师\'],\n    \'chujizhicheng\': [u\'财会经济\', u\'初级会计师\'],\n    \'zhongji\': [u\'财会经济\', u\'中级会计师\'],\n    \'zhongjizhicheng\': [u\'财会经济\', u\'中级会计师\'],\n    \'shiwushi\': [u\'财会经济\', u\'注册税务师\'],\n    \'zhukuai\': [u\'财会经济\', u\'注册会计师\'],\n    \'gaoji\': [u\'财会经济\', u\'高级会计师\'],\n    \'congye\': [u\'财会经济\', u\'会计从业\'],\n    u\'注册会计师\': [u\'财会经济\', u\'注册会计师\'],\n    u\'经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'初级会计职称\': [u\'财会经济\', u\'初级会计师\'],\n    u\'中级会计职称\': [u\'财会经济\', u\'中级会计师\'],\n    u\'税务师\': [u\'财会经济\', u\'注册税务师\'],\n    u\'统计师\': [u\'财会经济\', u\'统计师\'],\n    u\'审计师\': [u\'财会经济\', u\'审计师\'],\n    u\'高级经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'高级会计\': [u\'财会经济\', u\'高级会计师\'],\n    u\'理财规划师\': [u\'财会经济\', u\'理财规划师\'],\n\n    u\'英语四级\': [u\'外语考试\', u\'英语四六级\'],\n    u\'英语六级\': [u\'外语考试\', u\'英语四六级\'],\n    u\'雅思\': [u\'外语考试\', u\'雅思\'],\n    u\'托福\': [u\'外语考试\', u\'托福\'],\n    u\'职称英语\': [u\'外语考试\', u\'职称英语\'],\n    u\'商务英语\': [u\'外语考试\', u\'商务英语\'],\n    u\'公共英语\': [u\'外语考试\', u\'公共英语\'],\n    u\'日语\': [u\'外语考试\', u\'日语\'],\n    u\'GRE考试\': [u\'外语考试\', u\'GRE考试\'],\n    u\'专四专八\': [u\'外语考试\', u\'专四专八\'],\n    u\'口译笔译\': [u\'外语考试\', u\'口译笔译\'],\n\n    \'zaojia\': [u\'建筑工程\', \'造价工程师\'],\n    u\'一级建造师\': [u\'建筑工程\', u\'一级建造师\'],\n    u\'二级建造师\': [u\'建筑工程\', u\'二级建造师\'],\n    u\'咨询工程师\': [u\'建筑工程\', u\'咨询工程师\'],\n    u\'造价工程师\': [u\'建筑工程\', \'造价工程师\'],\n    u\'结构工程师\': [u\'建筑工程\', \'结构工程师\'],\n    u\'物业管理\': [u\'建筑工程\', u\'物业管理师\'],\n    u\'城市规划\': [u\'建筑工程\', u\'城市规划师\'],\n    u\'给排水工程\': [u\'建筑工程\', u\'给排水工程\'],\n    u\'电气工程\': [u\'建筑工程\', u\'电气工程师\'],\n    u\'公路监理师\': [u\'建筑工程\', u\'公路监理师\'],\n    u\'消防工程师\': [u\'建筑工程\', u\'消防工程师\'],\n    u\'消防\': [u\'建筑工程\', u\'消防工程师\'],\n\n    u\'物流师\': [u\'职业资格\', u\'物流师\'],\n    u\'人力资源\': [u\'职业资格\', u\'人力资源\'],\n    u\'心理咨询师\': [u\'职业资格\', u\'心理咨询师\'],\n    u\'公共营养师\': [u\'职业资格\', u\'公共营养师\'],\n    u\'秘书资格\': [u\'职业资格\', u\'秘书资格\'],\n    u\'秘书资格\': [u\'职业资格\', u\'秘书资格\'],\n    u\'证券从业资格\': [u\'职业资格\', u\'证券经纪人\'],\n    u\'电子商务师\': [u\'职业资格\', u\'电子商务\'],\n    u\'期货从业\': [u\'职业资格\', u\'期货从业\'],\n    u\'教师资格\': [u\'职业资格\', u\'教师资格\'],\n    u\'管理咨询师\': [u\'职业资格\', u\'管理咨询师\'],\n    u\'导游证\': [u\'职业资格\', u\'导游证\'],\n\n    u\'英语\': [u\'学历教育\', u\'考研\'],\n    u\'数学\': [u\'学历教育\', u\'考研\'],\n    u\'政治\': [u\'学历教育\', u\'考研\'],\n    u\'专业课\': [u\'学历教育\', u\'考研\'],\n\n    u\'执业医师\': [u\'医学卫生\', u\'执业医师\'],\n    u\'初级药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'初级药士\': [u\'医学卫生\', u\'执业药师\'],\n    u\'主管药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'主管中药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'中药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'中药士\': [u\'医学卫生\', u\'执业药师\'],\n    u\'临床执业\': [u\'医学卫生\', u\'临床执业\'],\n    u\'临床医师\': [u\'医学卫生\', u\'临床执业\'],\n    u\'中医医师\': [u\'医学卫生\', u\'中医执业\'],\n    u\'中医执业\': [u\'医学卫生\', u\'中医执业\'],\n    u\'中西医医师\': [u\'医学卫生\', u\'中西医执业\'],\n    u\'中西执业\': [u\'医学卫生\', u\'中西医执业\'],\n    u\'中医助理\': [u\'医学卫生\', u\'中医助理\'],\n    u\'中西医助理\': [u\'医学卫生\', u\'中西医助理\'],\n    u\'主治\': [u\'医学卫生\', u\'主治\'],\n    u\'全科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'内科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'外科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'儿科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'妇产科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'检验\': [u\'医学卫生\', u\'检验\'],\n    u\'检验士\': [u\'医学卫生\', u\'检验\'],\n    u\'检验师\': [u\'医学卫生\', u\'检验\'],\n    u\'主管检验师\': [u\'医学卫生\', u\'检验\'],\n    u\'执业护士资格\': [u\'医学卫生\', u\'执业护士\'],\n    u\'护士资格\': [u\'医学卫生\', u\'执业护士\'],\n\n    u\'成人高考\': [u\'学历教育\', u\'成人高考\'],\n    u\'自学考试\': [u\'学历教育\', u\'自考\'],\n    u\'MBA考试\': [u\'学历教育\', u\'MBA\'],\n    u\'法律硕士\': [u\'学历教育\', u\'法律硕士\'],\n    u\'专升本\': [u\'学历教育\', u\'专升本\'],\n    # u\'\':[u\'学历教育\',u\'工程硕士\'],\n    u\'MPA考试\': [u\'学历教育\', u\'公共硕士\'],\n    # u\'\':[u\'学历教育\',u\'考研\'],\n}\n\nsome_url =[\n    \'http://www.chinaacc.com/chujizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/chujizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/\',\n    \'http://www.chinaacc.com/zaojia/zt/\',\n    \'http://www.chinaacc.com/zaojia/mnst/\',    \n]\n\nremove_name = [\n    u\'卫生网校\',\n    u\'医学书店\',\n    u\'定期考核\',\n    u\'医学会议\',\n    u\'继续教育\',\n    u\'学习卡\',\n    u\'首页\',\n    u\'论坛\',\n    u\'邮箱\',\n]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.med66.com/\', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'.navItem > div li\').items():\n            if each.text().replace(\' \', \'\') in remove_name:\n                continue\n            _dict = {}\n            if each.text().replace(\' \', \'\') in type_dict.keys():\n                _dict[\'bread\'] = type_dict[each.text().replace(\' \', \'\')]\n            else:\n                _dict[\'bread\'] = [\'医学卫生\', each.text().replace(\' \', \'\')]\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.list_page)\n         \n        \n    @config(age=10 * 24 * 60 * 60)\n    def list_page(self, response):\n        for each in response.doc(\'.snbody > .subnav\').items():\n            if \'复习指导\' not in each.text():\n                continue\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            for each1 in each.find(\'a\').items():\n                self.crawl(each1.attr.href, save = _dict, callback=self.list_page1)\n        \n    @config(age=10 * 24 * 60 * 60)\n    def list_page1(self, response):\n        for each in response.doc(\'.xinxi li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.fl > a\').text()\n            if \'报名\' in _dict[\'title\'] or \'名师\' in _dict[\'title\'] or \'汇总\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'.fr\').text().replace(\'[\',\'\').replace(\']\',\'\')\n            self.crawl(each.find(\'.fl > a\').attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页 \n        for each in response.doc(\'divpagestr2016 a\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'#fontzoom\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'全网首发\' in each1.text():\n                    continue\n                if u\'估分更准确\' in each1.text():\n                    continue\n                if u\'独家\' in each1.text():\n                    continue\n                if u\'点击\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text() or u\'推荐阅读\' in each1.text() or u\'相关推荐\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'以上\' in each1.text() and u\'是中华会计网\' in each1.text():\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            \n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\', \'\').replace(\'\\n\', \'\').replace(\'\\t\', \'\').replace(u\'医学教育网\', \'\').replace(\'【 】\', \'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'),\n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'医学教育网\', \'\').replace(\n                    \'None\', \'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'医学教育网\', \'\').replace(\n                    \'None\', \'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'医学教育网\', u\'\').replace(\n                    \'建设网校\', \'\').replace(\'None\', \'\') + str(\n                    response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(\n                    u\'医学教育网\', \'\').replace(\'建设网校\', \'\').replace(\'None\', \'\'),\n                \"tdk_title\": response.doc(\'head > title\').eq(0).text().replace(u\'医学教育网\', \'\') + u\' 跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"医学教育网\",\n            }\n\n',NULL,1.0000,3.0000,1471333122.6680),('yixue_zhengbao_inc','kztk','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-08-16 16:02:22\n# Project: yixue_zhengbao_inc\n\nfrom pyspider.libs.base_handler import *\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\ntype_dict = {\n    \'chuji\': [u\'财会经济\', u\'初级会计师\'],\n    \'chujizhicheng\': [u\'财会经济\', u\'初级会计师\'],\n    \'zhongji\': [u\'财会经济\', u\'中级会计师\'],\n    \'zhongjizhicheng\': [u\'财会经济\', u\'中级会计师\'],\n    \'shiwushi\': [u\'财会经济\', u\'注册税务师\'],\n    \'zhukuai\': [u\'财会经济\', u\'注册会计师\'],\n    \'gaoji\': [u\'财会经济\', u\'高级会计师\'],\n    \'congye\': [u\'财会经济\', u\'会计从业\'],\n    u\'注册会计师\': [u\'财会经济\', u\'注册会计师\'],\n    u\'经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'初级会计职称\': [u\'财会经济\', u\'初级会计师\'],\n    u\'中级会计职称\': [u\'财会经济\', u\'中级会计师\'],\n    u\'税务师\': [u\'财会经济\', u\'注册税务师\'],\n    u\'统计师\': [u\'财会经济\', u\'统计师\'],\n    u\'审计师\': [u\'财会经济\', u\'审计师\'],\n    u\'高级经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'经济师\': [u\'财会经济\', u\'经济师\'],\n    u\'高级会计\': [u\'财会经济\', u\'高级会计师\'],\n    u\'理财规划师\': [u\'财会经济\', u\'理财规划师\'],\n\n    u\'英语四级\': [u\'外语考试\', u\'英语四六级\'],\n    u\'英语六级\': [u\'外语考试\', u\'英语四六级\'],\n    u\'雅思\': [u\'外语考试\', u\'雅思\'],\n    u\'托福\': [u\'外语考试\', u\'托福\'],\n    u\'职称英语\': [u\'外语考试\', u\'职称英语\'],\n    u\'商务英语\': [u\'外语考试\', u\'商务英语\'],\n    u\'公共英语\': [u\'外语考试\', u\'公共英语\'],\n    u\'日语\': [u\'外语考试\', u\'日语\'],\n    u\'GRE考试\': [u\'外语考试\', u\'GRE考试\'],\n    u\'专四专八\': [u\'外语考试\', u\'专四专八\'],\n    u\'口译笔译\': [u\'外语考试\', u\'口译笔译\'],\n\n    \'zaojia\': [u\'建筑工程\', \'造价工程师\'],\n    u\'一级建造师\': [u\'建筑工程\', u\'一级建造师\'],\n    u\'二级建造师\': [u\'建筑工程\', u\'二级建造师\'],\n    u\'咨询工程师\': [u\'建筑工程\', u\'咨询工程师\'],\n    u\'造价工程师\': [u\'建筑工程\', \'造价工程师\'],\n    u\'结构工程师\': [u\'建筑工程\', \'结构工程师\'],\n    u\'物业管理\': [u\'建筑工程\', u\'物业管理师\'],\n    u\'城市规划\': [u\'建筑工程\', u\'城市规划师\'],\n    u\'给排水工程\': [u\'建筑工程\', u\'给排水工程\'],\n    u\'电气工程\': [u\'建筑工程\', u\'电气工程师\'],\n    u\'公路监理师\': [u\'建筑工程\', u\'公路监理师\'],\n    u\'消防工程师\': [u\'建筑工程\', u\'消防工程师\'],\n    u\'消防\': [u\'建筑工程\', u\'消防工程师\'],\n\n    u\'物流师\': [u\'职业资格\', u\'物流师\'],\n    u\'人力资源\': [u\'职业资格\', u\'人力资源\'],\n    u\'心理咨询师\': [u\'职业资格\', u\'心理咨询师\'],\n    u\'公共营养师\': [u\'职业资格\', u\'公共营养师\'],\n    u\'秘书资格\': [u\'职业资格\', u\'秘书资格\'],\n    u\'秘书资格\': [u\'职业资格\', u\'秘书资格\'],\n    u\'证券从业资格\': [u\'职业资格\', u\'证券经纪人\'],\n    u\'电子商务师\': [u\'职业资格\', u\'电子商务\'],\n    u\'期货从业\': [u\'职业资格\', u\'期货从业\'],\n    u\'教师资格\': [u\'职业资格\', u\'教师资格\'],\n    u\'管理咨询师\': [u\'职业资格\', u\'管理咨询师\'],\n    u\'导游证\': [u\'职业资格\', u\'导游证\'],\n\n    u\'英语\': [u\'学历教育\', u\'考研\'],\n    u\'数学\': [u\'学历教育\', u\'考研\'],\n    u\'政治\': [u\'学历教育\', u\'考研\'],\n    u\'专业课\': [u\'学历教育\', u\'考研\'],\n\n    u\'执业医师\': [u\'医学卫生\', u\'执业医师\'],\n    u\'初级药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'初级药士\': [u\'医学卫生\', u\'执业药师\'],\n    u\'主管药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'主管中药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'中药师\': [u\'医学卫生\', u\'执业药师\'],\n    u\'中药士\': [u\'医学卫生\', u\'执业药师\'],\n    u\'临床执业\': [u\'医学卫生\', u\'临床执业\'],\n    u\'临床医师\': [u\'医学卫生\', u\'临床执业\'],\n    u\'中医医师\': [u\'医学卫生\', u\'中医执业\'],\n    u\'中医执业\': [u\'医学卫生\', u\'中医执业\'],\n    u\'中西医医师\': [u\'医学卫生\', u\'中西医执业\'],\n    u\'中西执业\': [u\'医学卫生\', u\'中西医执业\'],\n    u\'中医助理\': [u\'医学卫生\', u\'中医助理\'],\n    u\'中西医助理\': [u\'医学卫生\', u\'中西医助理\'],\n    u\'主治\': [u\'医学卫生\', u\'主治\'],\n    u\'全科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'内科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'外科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'儿科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'妇产科主治\': [u\'医学卫生\', u\'主治\'],\n    u\'检验\': [u\'医学卫生\', u\'检验\'],\n    u\'检验士\': [u\'医学卫生\', u\'检验\'],\n    u\'检验师\': [u\'医学卫生\', u\'检验\'],\n    u\'主管检验师\': [u\'医学卫生\', u\'检验\'],\n    u\'执业护士资格\': [u\'医学卫生\', u\'执业护士\'],\n    u\'护士资格\': [u\'医学卫生\', u\'执业护士\'],\n\n    u\'成人高考\': [u\'学历教育\', u\'成人高考\'],\n    u\'自学考试\': [u\'学历教育\', u\'自考\'],\n    u\'MBA考试\': [u\'学历教育\', u\'MBA\'],\n    u\'法律硕士\': [u\'学历教育\', u\'法律硕士\'],\n    u\'专升本\': [u\'学历教育\', u\'专升本\'],\n    # u\'\':[u\'学历教育\',u\'工程硕士\'],\n    u\'MPA考试\': [u\'学历教育\', u\'公共硕士\'],\n    # u\'\':[u\'学历教育\',u\'考研\'],\n}\n\nsome_url =[\n    \'http://www.chinaacc.com/chujizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/chujizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/sw/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/jjf/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/qk/\',\n    \'http://www.chinaacc.com/zhongjizhicheng/stzx/cg/\',\n    \'http://www.chinaacc.com/zaojia/zt/\',\n    \'http://www.chinaacc.com/zaojia/mnst/\',    \n]\n\nremove_name = [\n    u\'卫生网校\',\n    u\'医学书店\',\n    u\'定期考核\',\n    u\'医学会议\',\n    u\'继续教育\',\n    u\'学习卡\',\n    u\'首页\',\n    u\'论坛\',\n    u\'邮箱\',\n]\n\nclass Handler(BaseHandler):\n    crawl_config = {\n            \'itag\':\'0.1\'\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        self.crawl(\'http://www.med66.com/\', callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        for each in response.doc(\'.navItem > div li\').items():\n            if each.text().replace(\' \', \'\') in remove_name:\n                continue\n            _dict = {}\n            if each.text().replace(\' \', \'\') in type_dict.keys():\n                _dict[\'bread\'] = type_dict[each.text().replace(\' \', \'\')]\n            else:\n                _dict[\'bread\'] = [\'医学卫生\', each.text().replace(\' \', \'\')]\n            self.crawl(each.find(\'a\').attr.href, save = _dict, callback=self.list_page)\n         \n        \n    @config(age=1*1)\n    def list_page(self, response):\n        for each in response.doc(\'.snbody > .subnav\').items():\n            if \'复习指导\' not in each.text():\n                continue\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            for each1 in each.find(\'a\').items():\n                self.crawl(each1.attr.href, save = _dict, callback=self.list_page1)\n        \n    @config(age=1*1)\n    def list_page1(self, response):\n        for each in response.doc(\'.xinxi li\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\')\n            _dict[\'title\'] = each.find(\'.fl > a\').text()\n            if \'报名\' in _dict[\'title\'] or \'名师\' in _dict[\'title\'] or \'汇总\' in _dict[\'title\']:\n                continue\n            _dict[\'date\'] = each.find(\'.fr\').text().replace(\'[\',\'\').replace(\']\',\'\')\n            self.crawl(each.find(\'.fl > a\').attr.href, save = _dict, callback=self.detail_page)\n        \n        #翻页 \n        #for each in response.doc(\'divpagestr2016 a\').items():\n         #   _dict = {}\n          #  _dict[\'bread\'] = response.save.get(\'bread\')\n           # self.crawl(each.attr.href, save = _dict, callback=self.list_page1)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        list = []                             \n        for each in response.doc(\'#fontzoom\').items():\n            for each1 in each.find(\'p\').items():\n                if u\'全网首发\' in each1.text():\n                    continue\n                if u\'估分更准确\' in each1.text():\n                    continue\n                if u\'独家\' in each1.text():\n                    continue\n                if u\'点击\' in each1.text():\n                    continue\n                if u\'到建设工程教育网论坛\' in each1.text():\n                    continue\n                if u\'特色班\' in each1.text():\n                    break\n                if u\'近几年\' in each1.text():\n                    break\n                if u\'考友咨询\' in each1.text():\n                    break\n                if u\'更多\' in each1.text() and u\'资讯\' in each1.text():\n                    break\n                if u\'推荐信息\' in each1.text() or u\'更多推荐\' in each1.text() or u\'推荐阅读\' in each1.text() or u\'相关推荐\' in each1.text():\n                    break\n                if u\'免费在线测试\' in each1.text():\n                    continue\n                if u\'在线测试系统\' in each1.text():\n                    continue\n                if u\'转载请注明出处\' in each1.text():\n                    continue\n                if u\'责任编辑\' in each1.text():\n                    break\n                if u\'相关链接\' in each1.text():\n                    break\n                if u\'以上\' in each1.text() and u\'是中华会计网\' in each1.text():\n                    break\n                else:\n                    list.append(each1.remove(\'a\').html())\n            \n        content = \'\'.join(\'<p>%s</p>\' % (s) for s in list if s)\n        if len(content.strip()) < 20:\n            pass\n        else:\n            \n            return {\n                \"url\": response.url,\n                \"title\": response.save.get(\'title\'),\n                \"content\": content.replace(\'\\r\', \'\').replace(\'\\n\', \'\').replace(\'\\t\', \'\').replace(u\'医学教育网\', \'\').replace(\'【 】\', \'\'),\n                \"subject\": u\"考证题库\",\n                \"bread\": response.save.get(\'bread\'),\n                \"date\": response.save.get(\'date\'),\n                \"tdk_keywords\": str(response.doc(\'meta[http-equiv=\"keywords\"]\').eq(0).attr.content).replace(u\'医学教育网\', \'\').replace(\n                    \'None\', \'\') + str(response.doc(\'meta[name=\"keywords\"]\').eq(0).attr.content).replace(u\'医学教育网\', \'\').replace(\n                    \'None\', \'\'),\n                \"tdk_desc\": str(response.doc(\'meta[http-equiv=\"description\"]\').eq(0).attr.content).replace(u\'医学教育网\', u\'\').replace(\n                    \'建设网校\', \'\').replace(\'None\', \'\') + str(\n                    response.doc(\'meta[name=\"description\"]\').eq(0).attr.content).replace(\n                    u\'医学教育网\', \'\').replace(\'建设网校\', \'\').replace(\'None\', \'\'),\n                \"tdk_title\": response.doc(\'head > title\').eq(0).text().replace(u\'医学教育网\', \'\') + u\' 跟谁学\',\n                \"class\": 33,\n                \"data_weight\": 0,\n                \"source\": u\"医学教育网\",\n            }\n\n',NULL,1.0000,3.0000,1471334954.4903),('youshengxiao_ysxiao_zhongdianxiaoxue','youshengxiao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-30 10:41:03\n# Project: youshengxiao_jiaoliu\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    page_dict = {\n    \'http://www.ysxiao.cn/xiaoxue-haidianqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-xichengqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-chaoyangqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-dongchengqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-fengtaiqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/xiaoxue-qitaqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/zhongdianxiaoxue/xiaoxue-shijingshanqu/\': u\'重点小学\',\n    \'http://www.ysxiao.cn/beijingyoushengxiao/youshengxiaoxianjie/\': u\'经验交流\'\n    }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k, v in self.page_dict.iteritems():\n            self.crawl(k, save={\'label\': v}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        label = response.save[\'label\']\n        for each in response.doc(\'.desc\').items():\n            _dict = {}\n            url = each.find(\'a\').attr.href\n            _dict[\'label\'] = label\n            _dict[\'date\'] = each.find(\'p\').text()\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        \n        #for each in response.doc(\'.pages a\').items():\n        #    self.crawl(each.attr.href, save={\'label\': label}, callback=self.index_page)\n            \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for each in response.doc(\'p\'):\n            _cont = pyquery.PyQuery(each).html().replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\')\n            if _cont:\n                content_list.append((_cont))\n        \n        res_dict = response.save\n        #if res_dict[\'label\'] == u\'北京幼升小招生简章\':\n        content_list = content_list[:-3]\n        if not content_list:\n            return None\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        res_dict[\"content\"] = \'\'.join([\'<p>%s</p>\'%v for v in content_list])\n        #res_dict[\"tags\"] = response.doc(\'#tags > a\').text().split()\n        res_dict[\'subject\'] = u\'幼升小\'\n        res_dict[\'bread\'] = response.doc(\'#position a\').text().split()\n        res_dict[\'bread\'][0] = res_dict[\'label\']\n        res_dict[\'bread\'] = list(set(res_dict[\'bread\']))\n        res_dict.pop(\'label\')\n        res_dict[\'brief\'] = response.doc(\'#desc\').text()\n        res_dict[\'class\'] = 23\n        res_dict[\'source\'] = \'ysxiao.com\'\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,1.0000,3.0000,1471332766.8603),('youshengxiao_ysxiao_zixun_inc','youshengxiao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-03-17 15:15:26\n# Project: youshengxiao_ysxiao_zixun_inc\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    zhengce_map = {\'haidianqu-youshengxiao\': u\'幼升小海淀区政策\', \n                   \'xichengqu-youshengxiao\': u\'幼升小西城区政策\', \n                   \'chaoyangqu-youshengxiao\': u\'幼升小朝阳区政策\', \n                   \'dongchengqu-youshengxiao\': u\'幼升小东城区政策\', \n                   \'qita\': u\'幼升小其他城区政策\', \n                   \'youshengxiao-huapian\': u\'北京幼升小招生划片\',\n                   \'zhaoshengjianzhang\': u\'北京幼升小招生简章\',\n                   \'youshengxiao-xuequfang\': u\'北京幼升小学区房\',\n                   \'youshengxiao-zexiao\': u\'北京幼升小择校技巧\',\n                   \'ruxuekaoshizhenti\': u\'北京幼升小名校试题\',\n                   \'youshengxiao-mianshi\': u\'北京幼升小面试辅导\',\n                   \'youshengxiao-jianli\': u\'北京幼升小简历\',\n                   \'beijing-youeryuan/youeryuanruyuanjiaoliu\': u\'北京幼升小经验交流\',\n                   \'zhaoshengjianzhang\': u\'招生简章\',\n                   }\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for page in self.zhengce_map:\n            self.crawl(\'http://www.ysxiao.cn/%s/\'%page, save={\'label\': self.zhengce_map[page]}, callback=self.index_page)\n\n    @config(age=1 * 1)\n    def index_page(self, response):\n        label = response.save[\'label\']\n        for each in response.doc(\'.desc\').items():\n            _dict = {}\n            url = each.find(\'a\').attr.href\n            img = each.find(\'.hasimg img\').attr.src\n            if img:\n                _dict[\'img\'] = \'http://www.ysxiao.cn\' + img\n            _dict[\'date\'] = each.find(\'p\').text()\n            _dict[\'label\'] = label\n            self.crawl(url, save=_dict, callback=self.detail_page)\n        \'\'\'\n        for each in response.doc(\'.pages a\').items():\n            self.crawl(each.attr.href, save={\'label\': label}, callback=self.index_page)\n        \'\'\' \n            \n    @config(priority=2)\n    def detail_page(self, response):\n        content_list = []\n        for each in response.doc(\'p\'):\n            _cont =pyquery.PyQuery(each).html()\n            if _cont:\n                _cont = _cont.replace(\'\\r\',\'\').replace(\'\\n\',\'\').replace(\'\\t\',\'\').replace(\'src=\"/fup/\',\'src=\"http://www.ysxiao.cn/fup/\')\n                content_list.append((_cont))\n        \n        res_dict = response.save\n        if res_dict[\'label\'] in (u\'北京幼升小招生简章\', u\'幼升小海淀区政策\', u\'幼升小西城区政策\', u\'幼升小朝阳区政策\', u\'幼升小东城区政策\', u\'幼升小其他城区政策\', u\'北京幼升小经验交流\'):\n            content_list = content_list[:-2]\n        if not content_list:\n            return None\n        res_dict[\"url\"] = response.url\n        res_dict[\"title\"] = response.doc(\'h1\').text()\n        \n        res_dict[\"content\"] = \'\'.join([\'<p>%s</p>\'%v for v in content_list])\n        res_dict[\"tag\"] = response.doc(\'#tags > a\').text().split()\n        res_dict[\'subject\'] = u\'幼升小\'\n        res_dict[\'bread\'] = response.doc(\'#position a\').text().split()\n        res_dict[\'bread\'][0] = res_dict[\'label\']\n        res_dict.pop(\'label\')\n        res_dict[\'bread\'] = list(set(res_dict[\'bread\']))\n        res_dict[\'source\'] = \'ysxiao\'\n        res_dict[\'class\'] = 23\n        res_dict[\'data_weight\'] = 0\n        return res_dict\n',NULL,1.0000,3.0000,1472441331.6511),('zhenti3_iciba','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-07-12 16:53:22\n# Project: zhenti3_iciba\n\nfrom pyspider.libs.base_handler import *\n\nimport sys\nimport MySQLdb\nimport traceback\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\n\nconn = MySQLdb.connect(host = \"127.0.0.1\", user = \"root\", passwd = \"123\", db = \"cidiandb\", charset = \"utf8\")\ncursor = conn.cursor()\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        sql = \'select id, word from tb_word where flag = 0 limit 10000\'\n        try:\n            cursor.execute(sql)\n            for (ci_id, word,) in cursor.fetchall():\n                self.crawl(\'http://www.iciba.com/\'+word, save = {\'ci\': word, \'id\': ci_id}, callback=self.detail_page)\n        except:\n            traceback.print_exc()\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\'a[href^=\"http\"]\').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        word = response.save.get(\'ci\').strip()\n        ci_id = response.save.get(\'id\')\n        sql = \"update tb_word set flag= 1 where id= %s\" % (ci_id)\n        try:           \n            cursor.execute(sql)\n            conn.commit()\n        except Exception, e:\n            print e\n        question = {}\n        for each in response.doc(\'.cet-tab\').items():\n            i = 0\n            for each_li in each.find(\'.article-list li\').items():\n                if u\'收起\' not in each_li.text() and each_li.text().strip() != \'\': \n                    question[each_li.text()] = each.find(\'.article\').find(\'.article-section\').eq(i).html().replace(\'\\n\',\'\').replace(\'\\r\',\'\')\n                    i += 1\n                    \n        \n        return {\n            \"url\": response.url,\n            \"title\": response.doc(\'title\').text().replace(u\'爱词霸\',u\'跟谁学\'),\n            \"word\": word,\n            \"question\": question,\n        }',NULL,5.0000,5.0000,1472624733.6077),('zhongkaocom','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-03 18:29:37\n# Project: zhongkaocom\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n       \n    }\n    \n    page_config = {\n       \n         \'http://www.zhongkao.com/baokao/zkzx/\':[\'政策资讯\',\'中考资讯\'],\n         \'http://www.zhongkao.com/baokao/zkzc/\':[\'政策资讯\',\'中考政策\'],\n\n         \'http://www.zhongkao.com/beikao/jyjl/\':[\'复习备考\',\'高分经验\'],\n         \'http://www.zhongkao.com/beikao/zy/zyjy/\':[\'复习备考\',\'高分经验\'],\n\n         \'http://www.zhongkao.com/beikao/ys/kqys/\':[\'考前调整\',\'中考饮食\'],\n         \'http://www.zhongkao.com/beikao/jz/jzbd/\':[\'考前调整\',\'中考家长\'],\n         \'http://www.zhongkao.com/beikao/xlzd/zkxl/\':[\'考前调整\',\'心理辅导\'],\n\n         \'http://www.zhongkao.com/beikao/zkfx/ywfx/\':[\'科目指导\',\'中考语文\'],\n         \'http://www.zhongkao.com/beikao/zkfx/sxfx/\':[\'科目指导\',\'中考数学\'],\n         \'http://www.zhongkao.com/beikao/zkfx/yyfx/\':[\'科目指导\',\'中考英语\'],\n         \'http://www.zhongkao.com/beikao/zkfx/wlfx/\':[\'科目指导\',\'中考物理\'],\n         \'http://www.zhongkao.com/beikao/zkfx/hxfx/\':[\'科目指导\',\'中考化学\'],\n         \'http://www.zhongkao.com/beikao/zkfx/zzfx/\':[\'科目指导\',\'中考政治\'],\n\n         \'http://www.zhongkao.com/beikao/zkzw/zkmf/\':[\'真题作文\',\'满分作文\'],\n         \'http://www.zhongkao.com/beikao/zkzw/zwdp/\':[\'真题作文\',\'作文指导\'],\n         \'http://www.zhongkao.com/beikao/zkzw/zkyy/\':[\'真题作文\',\'英语作文\'],\n         \'http://www.zhongkao.com/beikao/zkzw/15zkzw/\':[\'真题作文\',\'作文题目\'],\n         \'http://www.zhongkao.com/beikao/zkzw/14zkzw/\':[\'真题作文\',\'作文题目\']\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        for k,v in self.page_config.items():\n            self.crawl(k,save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n       for each in response.doc(\'.bk-item\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            url = each.find(\'h3\').find(\'a\').attr.href\n            _dict[\'title\'] = each.find(\'h3\').find(\'a\').html()\n            _dict[\'date\'] =  each.find(\'.c-b9\').text().split()[0]\n            self.crawl(url,save = _dict ,callback=self.detail_page)         \n       for each in response.doc(\'.pages > a[href]\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            self.crawl(each.attr.href,save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'.ft14 > p\').items():\n               content = pyquery.PyQuery(info)           \n               items = content.remove(\'a\').html()\n               if items and items.find(\'推荐\')>=0 :\n                    break\n               if items and items.strip():\n                    content_list.append(items.replace(\'中考网\',\'\'))        \n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'source\'] = \'zhongkao.com\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'title\'] = res_dict[\'title\'].replace(\'中考网\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1472624664.2960),('zhongkaocom_inc','zhongkao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-05-03 18:29:37\n# Project: zhongkaocom\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n       \n    }\n    \n    page_config = {\n       \n         \'http://www.zhongkao.com/baokao/zkzx/\':[\'政策资讯\',\'中考资讯\'],\n         \'http://www.zhongkao.com/baokao/zkzc/\':[\'政策资讯\',\'中考政策\'],\n\n         \'http://www.zhongkao.com/beikao/jyjl/\':[\'复习备考\',\'高分经验\'],\n         \'http://www.zhongkao.com/beikao/zy/zyjy/\':[\'复习备考\',\'高分经验\'],\n\n         \'http://www.zhongkao.com/beikao/ys/kqys/\':[\'考前调整\',\'中考饮食\'],\n         \'http://www.zhongkao.com/beikao/jz/jzbd/\':[\'考前调整\',\'中考家长\'],\n         \'http://www.zhongkao.com/beikao/xlzd/zkxl/\':[\'考前调整\',\'心理辅导\'],\n\n         \'http://www.zhongkao.com/beikao/zkfx/ywfx/\':[\'科目指导\',\'中考语文\'],\n         \'http://www.zhongkao.com/beikao/zkfx/sxfx/\':[\'科目指导\',\'中考数学\'],\n         \'http://www.zhongkao.com/beikao/zkfx/yyfx/\':[\'科目指导\',\'中考英语\'],\n         \'http://www.zhongkao.com/beikao/zkfx/wlfx/\':[\'科目指导\',\'中考物理\'],\n         \'http://www.zhongkao.com/beikao/zkfx/hxfx/\':[\'科目指导\',\'中考化学\'],\n         \'http://www.zhongkao.com/beikao/zkfx/zzfx/\':[\'科目指导\',\'中考政治\'],\n\n         \'http://www.zhongkao.com/beikao/zkzw/zkmf/\':[\'真题作文\',\'满分作文\'],\n         \'http://www.zhongkao.com/beikao/zkzw/zwdp/\':[\'真题作文\',\'作文指导\'],\n         \'http://www.zhongkao.com/beikao/zkzw/zkyy/\':[\'真题作文\',\'英语作文\'],\n         \'http://www.zhongkao.com/beikao/zkzw/15zkzw/\':[\'真题作文\',\'作文题目\'],\n         \'http://www.zhongkao.com/beikao/zkzw/14zkzw/\':[\'真题作文\',\'作文题目\']\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n        for k,v in self.page_config.items():\n            self.crawl(k,save = {\'bread\':v},callback=self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n       for each in response.doc(\'.bk-item\').items():\n            _dict = {}\n            _dict[\'bread\'] = response.save.get(\'bread\',[])\n            url = each.find(\'h3\').find(\'a\').attr.href\n            _dict[\'title\'] = each.find(\'h3\').find(\'a\').html()\n            _dict[\'date\'] =  each.find(\'.c-b9\').text().split()[0]\n            self.crawl(url,save = _dict ,callback=self.detail_page)         \n       #for each in response.doc(\'.pages > a[href]\').items():\n            #_dict = {}\n            #_dict[\'bread\'] = response.save.get(\'bread\',[])\n            #self.crawl(each.attr.href,save = _dict,callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'.ft14 > p\').items():\n               content = pyquery.PyQuery(info)           \n               items = content.remove(\'a\').html()\n               if content.find(\'img\'):\n                    continue\n               if items and items.find(\'推荐\')>=0 :\n                    break\n               if items and items.strip():\n                    content_list.append(items.replace(\'中考网\',\'\'))        \n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list if v and v.strip()])\n        res_dict[\'source\'] = \'zhongkao.com\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'class\'] = 33\n        res_dict[\'title\'] = res_dict[\'title\'].replace(\'中考网\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1471343989.9680),('zhongkaosian_inc','zhongkao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-25 17:44:23\n# Project: zhongkao_sina\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    page_dict = {\n    	\'http://roll.edu.sina.com.cn/lm/zk/zkzx/index.shtml\':[\'政策资讯\',\'中考资讯\'],\n        \'http://roll.edu.sina.com.cn/more/zk/fxjq/index.shtml\':[\'复习备考\',\'复习技巧\'],\n        \'http://roll.edu.sina.com.cn/more/zk/gfjy/index.shtml\':[\'复习备考\',\'高分经验\'],\n        \'http://roll.edu.sina.com.cn/lm/zk/zkzy/index.shtml\':[\'复习备考\',\'高分经验\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czyw/index.shtml\':[\'科目指导\',\'中考语文\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czsx/index.shtml\':[\'科目指导\',\'中考数学\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czyy/index.shtml\':[\'科目指导\',\'中考英文\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkst/index.shtml\':[\'中考试题\',\'模拟试题\'],\n        \'http://roll.edu.sina.com.cn/lm/zk/fxjq/index.shtml\':[\'复习备考\',\'复习技巧\'],\n    }\n\n    @every(minutes=1 * 60)\n    def on_start(self):\n          for k,v in self.page_dict.iteritems():\n            self.crawl(k,save={\'bread\': v}, callback = self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\"li\").items():\n             _dict = {}\n             url = each.find(\'a\').attr.href \n             _dict[\'bread\'] = response.save.get(\'bread\',[])\n             _dict[\'title\'] = each.find(\'a\').html()\n             _dict[\'date\'] = each.find(\'span\').html().split()[0].replace(\'(\',\'\').replace(\'年\',\'－\').replace(\'月\',\'－\').replace(\'日\',\'\')\n             self.crawl(url, save=_dict, callback=self.detail_page)     \n        #for each in response.doc(\".pagebox_num > a[href] , .pagebox_next > a[href]\").items():  \n             #_dict = {}\n             #_dict[\'bread\'] = response.save.get(\'bread\',[])\n             #self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'#artibody > p\').items():\n            content = pyquery.PyQuery(info)\n            items = content.remove(\'a\').html()\n            if content.find(\'img\'):\n                continue\n            if items and items.find(\'推荐\')>=0 :\n                break\n            if  items and (items.find(\'来源：\')>=0 or items.find(\'更多信息请访问\')>=0):\n                break\n            content_list.append(items.replace(\'新浪\',\'\'))\n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list if v and v.strip()])\n        res_dict[\'source\'] = \'edu.sina.com.cn\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'class\'] = 33\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'title\']  = res_dict[\'title\'].replace(\'新浪\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1471343960.4557),('zhongkaoxdfcn','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-25 17:44:23\n# Project: zhongkaoxdfcn\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    page_dict = {\n    	\'http://zhongkao.xdf.cn/list_7995_1.html\':[\'政策资讯\',\'志愿填报\'],\n        \'http://zhongkao.xdf.cn/list_4512_1.html\':[\'政策资讯\',\'中考资讯\'],\n        \'http://zhongkao.xdf.cn/list_1015_1.html\':[\'科目指导\',\'中考历史\'],\n        \'http://zhongkao.xdf.cn/list_1014_1.html\':[\'科目指导\',\'中考地理\'],\n        \'http://zhongkao.xdf.cn/list_1011_1.html\':[\'科目指导\',\'中考生物\'],\n\n        \'http://zhongkao.xdf.cn/list_1016_1.html\':[\'中考试题\',\'模拟试题\'],\n        \'http://zhongkao.xdf.cn/list_6132_1.html\':[\'中考试题\',\'期中末试题\'],\n        \'http://zhongkao.xdf.cn/list_1017_1.html\':[\'中考试题\',\'历年真题\'],\n\n\n        \'http://zhongkao.xdf.cn/list_1507_1.html\':[\'考前调整\',\'中考家长\'],\n        \'http://zhongkao.xdf.cn/list_1509_1.html\':[\'考前调整\',\'心理辅导\'],\n        \'http://zhongkao.xdf.cn/list_1510_1.html\':[\'考前调整\',\'中考饮食\'],\n\n        \'http://zhongkao.xdf.cn/list_1515_1.html\':[\'复习备考\',\'高分经验\'],\n        \n        \'http://zhongkao.xdf.cn/list_4809_1.html\':[\'真题作文\',\'英语作文\'],\n        \'http://zhongkao.xdf.cn/list_1512_1.html\':[\'真题作文\',\'作文题目\'],\n\n        \'http://zhongkao.xdf.cn/list_1000_1.html\':[\'复习备考\',\'备考策略\']\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n          for k,v in self.page_dict.iteritems():\n            self.crawl(k,save={\'bread\': v}, callback = self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\".txt_lists01 > li\").items():\n             _dict = {}\n             url = each.find(\'a\').attr.href \n             _dict[\'bread\'] = response.save.get(\'bread\',[])\n             _dict[\'title\'] = each.find(\'a\').html()\n             _dict[\'date\'] = each.find(\'cite\').html()\n             self.crawl(url, save=_dict, callback=self.detail_page)     \n        for each in response.doc(\".ch_conpage a[href$=\'.html\']\").items():  \n             _dict = {}\n             _dict[\'bread\'] = response.save.get(\'bread\',[])\n             self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'.air_con p\').items():\n            content = pyquery.PyQuery(info)\n            items = content.remove(\'a\').html()\n            if items and (items.find(\'推荐\')>=0 or items.find(\'编辑\')>=0):\n                    break\n            content_list.append(items.replace(\'新东方\',\'\'))\n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'source\'] = \'zhongkao.xdf.cn\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'class\'] = 33\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'title\']  = res_dict[\'title\'].replace(\'新东方\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1472624667.2227),('zhongkaoxdfcn_inc','zhongkao','RUNNING','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-25 17:44:23\n# Project: zhongkaoxdfcn\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    page_dict = {\n    	\'http://zhongkao.xdf.cn/list_7995_1.html\':[\'政策资讯\',\'志愿填报\'],\n        \'http://zhongkao.xdf.cn/list_4512_1.html\':[\'政策资讯\',\'中考资讯\'],\n        \'http://zhongkao.xdf.cn/list_1015_1.html\':[\'科目指导\',\'中考历史\'],\n        \'http://zhongkao.xdf.cn/list_1014_1.html\':[\'科目指导\',\'中考地理\'],\n        \'http://zhongkao.xdf.cn/list_1011_1.html\':[\'科目指导\',\'中考生物\'],\n\n        \'http://zhongkao.xdf.cn/list_1016_1.html\':[\'中考试题\',\'模拟试题\'],\n        \'http://zhongkao.xdf.cn/list_6132_1.html\':[\'中考试题\',\'期中末试题\'],\n        \'http://zhongkao.xdf.cn/list_1017_1.html\':[\'中考试题\',\'历年真题\'],\n\n\n        \'http://zhongkao.xdf.cn/list_1507_1.html\':[\'考前调整\',\'中考家长\'],\n        \'http://zhongkao.xdf.cn/list_1509_1.html\':[\'考前调整\',\'心理辅导\'],\n        \'http://zhongkao.xdf.cn/list_1510_1.html\':[\'考前调整\',\'中考饮食\'],\n\n        \'http://zhongkao.xdf.cn/list_1515_1.html\':[\'复习备考\',\'高分经验\'],\n        \n        \'http://zhongkao.xdf.cn/list_4809_1.html\':[\'真题作文\',\'英语作文\'],\n        \'http://zhongkao.xdf.cn/list_1512_1.html\':[\'真题作文\',\'作文题目\'],\n\n        \'http://zhongkao.xdf.cn/list_1000_1.html\':[\'复习备考\',\'备考策略\']\n    }\n\n    @every(minutes=1*60)\n    def on_start(self):\n          for k,v in self.page_dict.iteritems():\n            self.crawl(k,save={\'bread\': v}, callback = self.index_page)\n\n    @config(age=1*1)\n    def index_page(self, response):\n        for each in response.doc(\".txt_lists01 > li\").items():\n             _dict = {}\n             url = each.find(\'a\').attr.href \n             _dict[\'bread\'] = response.save.get(\'bread\',[])\n             _dict[\'title\'] = each.find(\'a\').html()\n             _dict[\'date\'] = each.find(\'cite\').html()\n             self.crawl(url, save=_dict, callback=self.detail_page)     \n        #for each in response.doc(\".ch_conpage a[href$=\'.html\']\").items():  \n             #_dict = {}\n             #_dict[\'bread\'] = response.save.get(\'bread\',[])\n             #self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'.air_con p\').items():\n            content = pyquery.PyQuery(info)\n            items = content.remove(\'a\').html()\n            if content.find(\'img\'):\n                continue\n            if items and (items.find(\'推荐\')>=0 or items.find(\'编辑\')>=0):\n                    break\n            content_list.append(items.replace(\'新东方\',\'\'))\n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list if v and v.strip()])\n        res_dict[\'source\'] = \'zhongkao.xdf.cn\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'class\'] = 33\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'title\']  = res_dict[\'title\'].replace(\'新东方\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1471343991.9025),('zhongkao_sina','delete','STOP','#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n# Created on 2016-02-25 17:44:23\n# Project: zhongkao_sina\n\nfrom pyspider.libs.base_handler import *\nimport pyquery\nreload(sys)\nsys.setdefaultencoding(\'utf-8\') \n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n    \n    page_dict = {\n    	\'http://roll.edu.sina.com.cn/lm/zk/zkzx/index.shtml\':[\'政策资讯\',\'中考资讯\'],\n        \'http://roll.edu.sina.com.cn/more/zk/fxjq/index.shtml\':[\'复习备考\',\'复习技巧\'],\n        \'http://roll.edu.sina.com.cn/more/zk/gfjy/index.shtml\':[\'复习备考\',\'高分经验\'],\n        \'http://roll.edu.sina.com.cn/lm/zk/zkzy/index.shtml\':[\'复习备考\',\'高分经验\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czyw/index.shtml\':[\'科目指导\',\'中考语文\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czsx/index.shtml\':[\'科目指导\',\'中考数学\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkzd/czyy/index.shtml\':[\'科目指导\',\'中考英文\'],\n        \'http://roll.edu.sina.com.cn/more/zk/zkst/index.shtml\':[\'中考试题\',\'模拟试题\'],\n        \'http://roll.edu.sina.com.cn/lm/zk/fxjq/index.shtml\':[\'复习备考\',\'复习技巧\'],\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n          for k,v in self.page_dict.iteritems():\n            self.crawl(k,save={\'bread\': v}, callback = self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc(\"li\").items():\n             _dict = {}\n             url = each.find(\'a\').attr.href \n             _dict[\'bread\'] = response.save.get(\'bread\',[])\n             _dict[\'title\'] = each.find(\'a\').html()\n             _dict[\'date\'] = each.find(\'span\').html().split()[0].replace(\'(\',\'\').replace(\'年\',\'－\').replace(\'月\',\'－\').replace(\'日\',\'\')\n             self.crawl(url, save=_dict, callback=self.detail_page)     \n        for each in response.doc(\".pagebox_num > a[href] , .pagebox_next > a[href]\").items():  \n             _dict = {}\n             _dict[\'bread\'] = response.save.get(\'bread\',[])\n             self.crawl(each.attr.href ,save = _dict, callback=self.index_page)\n\n    @config(priority=2)\n    def detail_page(self, response):\n        res_dict = response.save     \n        content_list = []\n        for info in response.doc(\'#artibody > p\').items():\n            content = pyquery.PyQuery(info)\n            items = content.remove(\'a\').html() \n            if items and items.find(\'推荐\')>=0 :\n                break\n            if  items and (items.find(\'来源：\')>=0 or items.find(\'更多信息请访问\')>=0):\n                break\n            content_list.append(items.replace(\'新浪\',\'\'))\n        if not content_list:\n            return None\n        res_dict[\'url\'] = response.url\n        res_dict[\'content\'] = \'\'.join([\"<p>%s</p>\"%v for v in content_list])\n        res_dict[\'source\'] = \'edu.sina.com.cn\'\n        res_dict[\'subject\'] = u\'中考\'\n        res_dict[\'class\'] = 33\n        res_dict[\'data_weight\'] = 0\n        res_dict[\'title\']  = res_dict[\'title\'].replace(\'新浪\',\'\')\n        return res_dict\n',NULL,1.0000,3.0000,1472624659.9344);
/*!40000 ALTER TABLE `projectdb` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2016-08-31 14:28:45
